Secondary study DOI,Primary study Scopus EID,Primary study DOI,Title,Abstract,Keywords,Publication venue,Publication date,Publication type,Authors,Ground truth,Reason
10.1016/j.infsof.2022.107128,2-s2.0-30344485154,10.1109/TSE.2005.112,Empirical validation of object-oriented metrics on open source software for fault prediction,"Open source software systems are becoming increasingly important these days. Many companies are investing in open source projects and lots of them are also using such software in their own work. But, because open source software is often developed with a different management style than the industrial ones, the quality and reliability of the code needs to be studied. Hence, the characteristics of the source code of these projects need to be measured to obtain more information about it. This paper describes how we calculated the object-oriented metrics given by Chidamber and Kemerer to illustrate how fault-proneness detection of the source code of the open source Web and e-mail suite called Mozilla can be carried out. We checked the values obtained against the number of bugs found in its bug database - called Bugzilla - using regression and machine learning methods to validate the usefulness of these metrics for fault-proneness prediction. We also compared the metrics of several versions of Mozilla to see how the predicted fault-proneness of the software system changed during its development cycle. © 2005 IEEE.",Bugzilla | C++ | Columbus | Compiler wrapping | Fact extraction | Fault-proneness detection | Metrics validation | Mozilla | Open source software | Reverse engineering,IEEE Transactions on Software Engineering,2005-10-01,Article,"Gyimóthy, Tibor;Ferenc, Rudolf;Siket, István",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85073921700,10.1109/TSE.2007.70773,Classifying software changes: Clean or buggy?,"This paper introduces a new technique for finding latent software bugs called change classification. Change classification uses a machine learning classifier to determine whether a new software change is more similar to prior buggy changes, or clean changes. In this manner, change classification predicts the existence of bugs in software changes. The classifier is trained using features (in the machine learning sense) extracted from the revision history of a software project, as stored in its software configuration management repository. The trained classifier can classify changes as buggy or clean with 78 percent accuracy and 65 percent buggy change recall (on average). Change classification has several desirable qualities: (1) the prediction granularity is small (a change to a single file), (2) predictions do not require semantic information about the source code, (3) the technique works for a broad array of project types and programming languages, and (4) predictions can be made immediately upon completion of a change. Contributions of the paper include a description of the change classification approach, techniques for extracting features from source code and change histories, a characterization of the performance of change classification across 12 open source projects, and evaluation of the predictive power of different groups of features. © 2008 IEEE.",interactive machine learning | novelty cases prediction | teaching case building | uncertainty sampling,Computer Applications in Engineering Education,2019-11-01,Article,"Wu, Xiaoxue;Zheng, Wei;Mu, Dejun;Li, Ning",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85016594110,10.1016/j.jss.2007.07.040,Predicting defect-prone software modules using support vector machines,"Effective prediction of defect-prone software modules can enable software developers to focus quality assurance activities and allocate effort and resources more efficiently. Support vector machines (SVM) have been successfully applied for solving both classification and regression problems in many applications. This paper evaluates the capability of SVM in predicting defect-prone software modules and compares its prediction performance against eight statistical and machine learning models in the context of four NASA datasets. The results indicate that the prediction performance of SVM is generally better than, or at least, is competitive against the compared models. © 2007 Elsevier Inc. All rights reserved.",cochanged types | coupling rules | discriminative feature | relevance of code changes,Journal of Software: Evolution and Process,2017-07-01,Article,"Huang, Yuan;Chen, Xiangping;Liu, Zhiyong;Luo, Xiaonan;Zheng, Zibin",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85071235277,10.1145/2884781.2884804,Automatically learning semantic features for defect prediction,"Software defect prediction, which predicts defective code regions, can help developers find bugs and prioritize their testing efforts. To build accurate prediction models, previous studies focus on manually designing features that encode the characteristics of programs and exploring different machine learning algorithms. Existing traditional features often fail to capture the semantic differences of programs, and such a capability is needed for building accurate prediction models. To bridge the gap between programs' semantics and defect prediction features, this paper proposes to leverage a powerful representation-learning algorithm, deep learning, to learn semantic representation of programs automatically from source code. Specifically, we leverage Deep Belief Network (DBN) to automatically learn semantic features from token vectors extracted from programs' Abstract Syntax Trees (ASTs). Our evaluation on ten open source projects shows that our automatically learned semantic features significantly improve both within-project defect prediction (WPDP) and cross-project defect prediction (CPDP) compared to traditional features. Our semantic features improve WPDP on average by 14.7% in precision, 11.5% in recall, and 14.2% in F1. For CPDP, our semantic features based approach outperforms the state-of-the-art technique TCA+ with traditional features by 8.9% in F1. © 2016 ACM.",change prediction | change risk | influential change | software changes | software evolution,Journal of Software: Evolution and Process,2019-12-01,Article,"Li, Daoyuan;Li, Li;Kim, Dongsun;Bissyandé, Tegawendé F.;Lo, David;Le Traon, Yves",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85076089201,10.1016/j.jss.2009.06.055,A systematic and comprehensive investigation of methods to build and evaluate fault prediction models,"This paper describes a study performed in an industrial setting that attempts to build predictive models to identify parts of a Java system with a high fault probability. The system under consideration is constantly evolving as several releases a year are shipped to customers. Developers usually have limited resources for their testing and would like to devote extra resources to faulty system parts. The main research focus of this paper is to systematically assess three aspects on how to build and evaluate fault-proneness models in the context of this large Java legacy system development project: (1) compare many data mining and machine learning techniques to build fault-proneness models, (2) assess the impact of using different metric sets such as source code structural measures and change/fault history (process measures), and (3) compare several alternative ways of assessing the performance of the models, in terms of (i) confusion matrix criteria such as accuracy and precision/recall, (ii) ranking ability, using the receiver operating characteristic area (ROC), and (iii) our proposed cost-effectiveness measure (CE). The results of the study indicate that the choice of fault-proneness modeling technique has limited impact on the resulting classification accuracy or cost-effectiveness. There is however large differences between the individual metric sets in terms of cost-effectiveness, and although the process measures are among the most expensive ones to collect, including them as candidate measures significantly improves the prediction models compared with models that only include structural measures and/or their deltas between releases - both in terms of ROC area and in terms of CE. Further, we observe that what is considered the best model is highly dependent on the criteria that are used to evaluate and compare the models. And the regular confusion matrix criteria, although popular, are not clearly related to the problem at hand, namely the cost-effectiveness of using fault-proneness prediction models to focus verification efforts to deliver software with less faults at less cost. © 2009 Elsevier Inc. All rights reserved.",equivalent mutants | machine learning | mutation testing | program semantics | static analysis,Journal of Software: Evolution and Process,2020-05-01,Article,"Naeem, Muhammad Rashid;Lin, Tao;Naeem, Hamad;Liu, Hailu",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84855652194,10.1016/j.infsof.2011.09.007,Transfer learning for cross-company software defect prediction,"Context: Software defect prediction studies usually built models using within-company data, but very few focused on the prediction models trained with cross-company data. It is difficult to employ these models which are built on the within-company data in practice, because of the lack of these local data repositories. Recently, transfer learning has attracted more and more attention for building classifier in target domain using the data from related source domain. It is very useful in cases when distributions of training and test instances differ, but is it appropriate for cross-company software defect prediction? Objective: In this paper, we consider the cross-company defect prediction scenario where source and target data are drawn from different companies. In order to harness cross company data, we try to exploit the transfer learning method to build faster and highly effective prediction model. Method: Unlike the prior works selecting training data which are similar from the test data, we proposed a novel algorithm called Transfer Naive Bayes (TNB), by using the information of all the proper features in training data. Our solution estimates the distribution of the test data, and transfers cross-company data information into the weights of the training data. On these weighted data, the defect prediction model is built. Results: This article presents a theoretical analysis for the comparative methods, and shows the experiment results on the data sets from different organizations. It indicates that TNB is more accurate in terms of AUC (The area under the receiver operating characteristic curve), within less runtime than the state of the art methods. Conclusion: It is concluded that when there are too few local training data to train good classifiers, the useful knowledge from different-distribution training data on feature level may help. We are optimistic that our transfer learning method can guide optimal resource allocation strategies, which may reduce software testing cost and increase effectiveness of software testing process. © 2011 Elsevier B.V. All rights reserved.",Different distribution | Machine learning | Naive Bayes | Software defect prediction | Transfer learning,Information and Software Technology,2012-03-01,Article,"Ma, Ying;Luo, Guangchun;Zeng, Xue;Chen, Aiguo",Include,
10.1016/j.infsof.2022.107128,2-s2.0-33947174112,10.1109/TSE.2006.102,Empirical analysis of object-oriented design metrics for predicting high and low severity faults,"In the last decade, empirical studies on object-oriented design metrics have shown some of them to be useful for predicting the fault-proneness of classes in object-oriented software systems. This research did not, however, distinguish among faults according to the severity of impact. It would be valuable to know how object-oriented design metrics and class fault-proneness are related when fault severity is taken into account. In this paper, we use logistic regression and machine learning methods to empirically investigate the usefulness of object-oriented design metrics, specifically, a subset of the Chidamber and Kemerer suite, in predicting fault-proneness when taking fault severity into account. Our results, based on a public domain NASA data set, indicate that 1) most of these design metrics are statistically related to fault-proneness of classes across fault severity, and 2) the prediction capabilities of the investigated metrics greatly depend on the severity of faults. More specifically, these design metrics are able to predict low severity faults in fault-prone classes better than high severity faults in fault-prone classes. © 2006 IEEE.",Cross validation | Fault-proneness | Faults | Metrics | Object-oriented | Prediction,IEEE Transactions on Software Engineering,2006-10-01,Article,"Zhou, Yuming;Leung, Hareton",Include,
10.1016/j.infsof.2022.107128,2-s2.0-79957799751,10.1109/TSE.2010.90,A general software defect-proneness prediction framework,"BACKGROUND-Predicting defect-prone software components is an economically important activity and so has received a good deal of attention. However, making sense of the many, and sometimes seemingly inconsistent, results is difficult. OBJECTIVE-We propose and evaluate a general framework for software defect prediction that supports 1) unbiased and 2) comprehensive comparison between competing prediction systems. METHOD-The framework is comprised of 1) scheme evaluation and 2) defect prediction components. The scheme evaluation analyzes the prediction performance of competing learning schemes for given historical data sets. The defect predictor builds models according to the evaluated learning scheme and predicts software defects with new data according to the constructed model. In order to demonstrate the performance of the proposed framework, we use both simulation and publicly available software defect data sets. RESULTS-The results show that we should choose different learning schemes for different data sets (i.e., no scheme dominates), that small details in conducting how evaluations are conducted can completely reverse findings, and last, that our proposed framework is more effective and less prone to bias than previous approaches. CONCLUSIONS-Failure to properly or fully evaluate a learning scheme can be misleading; however, these problems may be overcome by our proposed framework. © 2011 IEEE Published by the IEEE Computer Society.",Machine learning | Scheme evaluation | Software defect prediction | Software defect-proneness prediction,IEEE Transactions on Software Engineering,2011-05-03,Article,"Song, Qinbao;Jia, Zihan;Shepperd, Martin;Ying, Shi;Liu, Jin",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84856676371,10.1007/s10515-011-0090-3,An investigation on the feasibility of cross-project defect prediction,"Software defect prediction helps to optimize testing resources allocation by identifying defect-prone modules prior to testing. Most existing models build their prediction capability based on a set of historical data, presumably from the same or similar project settings as those under prediction. However, such historical data is not always available in practice. One potential way of predicting defects in projects without historical data is to learn predictors from data of other projects. This paper investigates defect predictions in the cross-project context focusing on the selection of training data. We conduct three large-scale experiments on 34 data sets obtained from 10 open source projects. Major conclusions from our experiments include: (1) in the best cases, training data from other projects can provide better prediction results than training data from the same project; (2) the prediction results obtained using training data from other projects meet our criteria for acceptance on the average level, defects in 18 out of 34 cases were predicted at a Recall greater than 70% and a Precision greater than 50%; (3) results of cross-project defect predictions are related with the distributional characteristics of data sets which are valuable for training data selection. We further propose an approach to automatically select suitable training data for projects without historical data. Prediction results provided by the training data selected by using our approach are comparable with those provided by training data from the same project. © 2011 Springer Science+Business Media, LLC.",Cross-project | Data characteristics | Defect prediction | Machine learning | Training data,Automated Software Engineering,2012-06-01,Article,"He, Zhimin;Shu, Fengdi;Yang, Ye;Li, Mingshu;Wang, Qing",Include,
10.1016/j.infsof.2022.107128,2-s2.0-59149091560,10.1016/j.ins.2008.12.001,"Investigating the effect of dataset size, metrics sets, and feature selection techniques on software fault prediction problem","Software quality engineering comprises of several quality assurance activities such as testing, formal verification, inspection, fault tolerance, and software fault prediction. Until now, many researchers developed and validated several fault prediction models by using machine learning and statistical techniques. There have been used different kinds of software metrics and diverse feature reduction techniques in order to improve the models' performance. However, these studies did not investigate the effect of dataset size, metrics set, and feature selection techniques for software fault prediction. This study is focused on the high-performance fault predictors based on machine learning such as Random Forests and the algorithms based on a new computational intelligence approach called Artificial Immune Systems. We used public NASA datasets from the PROMISE repository to make our predictive models repeatable, refutable, and verifiable. The research questions were based on the effects of dataset size, metrics set, and feature selection techniques. In order to answer these questions, there were defined seven test groups. Additionally, nine classifiers were examined for each of the five public NASA datasets. According to this study, Random Forests provides the best prediction performance for large datasets and Naive Bayes is the best prediction algorithm for small datasets in terms of the Area Under Receiver Operating Characteristics Curve (AUC) evaluation parameter. The parallel implementation of Artificial Immune Recognition Systems (AIRS2Parallel) algorithm is the best Artificial Immune Systems paradigm-based algorithm when the method-level metrics are used. © 2008 Elsevier Inc. All rights reserved.",Artificial Immune Systems | J48 | Machine learning | Naive Bayes | Random Forests | Software fault prediction,Information Sciences,2009-03-29,Article,"Catal, Cagatay;Diri, Banu",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84903176990,10.1109/TSE.2014.2322358,Researcher bias: The use of machine learning in software defect prediction,"Background. The ability to predict defect-prone software components would be valuable. Consequently, there have been many empirical studies to evaluate the performance of different techniques endeavouring to accomplish this effectively. However no one technique dominates and so designing a reliable defect prediction model remains problematic. Objective. We seek to make sense of the many conflicting experimental results and understand which factors have the largest effect onpredictive performance. Method. We conduct a meta-analysis of all relevant, high quality primary studies of defect prediction to determine what factors influence predictive performance. This is based on 42 primary studies that satisfy our inclusion criteria that collectively report 600 sets of empirical prediction results. By reverse engineering a common response variable we build arandom effects ANOVA model to examine the relative contribution of four model building factors (classifier, data set, input metrics and researcher group) to model prediction performance. Results. Surprisingly we find that the choice of classifier has little impact upon performance (1.3 percent) and in contrast the major (31 percent) explanatory factor is the researcher group. It matters more who does the work than what is done. Conclusion. To overcome this high level of researcher bias, defect prediction researchers should (i) conduct blind analysis, (ii) improve reporting protocols and (iii) conduct more intergroup studies in order to alleviate expertise issues. Lastly, research is required to determine whether this bias is prevalent in other applications domains. © 2014 IEEE.",meta-analysis | researcher bias | Software defect prediction,IEEE Transactions on Software Engineering,2014-06-01,Article,"Shepperd, Martin;Bowes, David;Hall, Tracy",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84955649457,10.1109/QRS.2015.14,Deep Learning for Just-in-Time Defect Prediction,"Defect prediction is a very meaningful topic, particularly at change-level. Change-level defect prediction, which is also referred as just-in-time defect prediction, could not only ensure software quality in the development process, but also make the developers check and fix the defects in time. Nowadays, deep learning is a hot topic in the machine learning literature. Whether deep learning can be used to improve the performance of just-in-time defect prediction is still uninvestigated. In this paper, to bridge this research gap, we propose an approach Deeper which leverages deep learning techniques to predict defect-prone changes. We first build a set of expressive features from a set of initial change features by leveraging a deep belief network algorithm. Next, a machine learning classifier is built on the selected features. To evaluate the performance of our approach, we use datasets from six large open source projects, i.e., Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL, containing a total of 137,417 changes. We compare our approach with the approach proposed by Kamei et al. The experimental results show that on average across the 6 projects, Deeper could discover 32.22% more bugs than Kamei et al's approach (51.04% versus 18.82% on average). In addition, Deeper can achieve F1-scores of 0.22-0.63, which are statistically significantly higher than those of Kamei et al.'s approach on 4 out of the 6 projects. © 2015 IEEE.",search-based software testing | test adequacy | test generation,Software Testing Verification and Reliability,2015-12-01,Article,"Fraser, Gordon;Walkinshaw, Neil",Include,
10.1016/j.infsof.2022.107128,2-s2.0-0024123707,10.1109/32.9061,Learning from Examples: Generation and Evaluation of Decision Trees for Software Resource Analysis,"Solutions to the problem of learning from examples will have far-reaching benefits, and therefore, the problem is one of the most widely studied in the field of machine learning. The purpose of this study is to investigate a general solution method for the problem, the automatic generation of decision (or classification) trees. The approach is to provide insights through in-depth empirical characterization and evaluation of decision trees for one problem domain, software resource data analysis. The purpose of the decision trees is to identify classes of objects (software modules) that had high development effort or faults, where “high” was defined to be in the uppermost quartile relative to past data. Sixteen software systems ranging from 3000 to 112 000 source lines have been selected for analysis from a NASA production environment. The collection and analysis of 74 attributes (or metrics), for over 4700 objects, capture a multitude of information about the objects: development effort, faults, changes, design style, and implementation style. A total of 9600 decision trees are automatically generated and evaluated based on several parameters: 1) attribute availability; 2) evaluation function heuristic; 3) tree termination criteria; 4) number of projects in the training set; 5) ordinal grouping of attributes; and 6) dependent variable. Sensitive 24 x 5 x 15 full-factorial analysis of variance models are employed to assess the performance contributions of the factors and their interactions simultaneously. The analysis focuses on the characterization and evaluation of decision tree accuracy, complexity, and composition. The decision trees correctly identified 79.3 percent of the software modules that had high development effort or faults, on the average across all 9600 trees. The decision trees generated from the best parameter combinations correctly identified 88.4 percent of the modules on the average. Visualization of the results is emphasized, and sample decision trees are included. © 1988 IEEE",Analysis of variance | decision trees | empirical measurement and evaluation | learning from examples | machine learning | software effort and error analysis | software metrics,IEEE Transactions on Software Engineering,1988-01-01,Article,"Selby, Richard W.;Porter, Adam A.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84875712394,10.1109/TSE.2012.43,Reducing features to improve code change-based bug prediction,"Machine learning classifiers have recently emerged as a way to predict the introduction of bugs in changes made to source code files. The classifier is first trained on software history, and then used to predict if an impending change causes a bug. Drawbacks of existing classifier-based bug prediction techniques are insufficient performance for practical use and slow prediction times due to a large number of machine learned features. This paper investigates multiple feature selection techniques that are generally applicable to classification-based bug prediction methods. The techniques discard less important features until optimal classification performance is reached. The total number of features used for training is substantially reduced, often to less than 10 percent of the original. The performance of Naive Bayes and Support Vector Machine (SVM) classifiers when using this technique is characterized on 11 software projects. Naive Bayes using feature selection provides significant improvement in buggy F-measure (21 percent improvement) over prior change classification bug prediction results (by the second and fourth authors [28]). The SVM's improvement in buggy F-measure is 9 percent. Interestingly, an analysis of performance for varying numbers of features shows that strong performance is achieved at even 1 percent of the original number of features. © 1976-2012 IEEE.",bug prediction | feature selection | machine learning | Reliability,IEEE Transactions on Software Engineering,2013-01-01,Article,"Shivaji, Shivkumar;James Whitehead, E.;Akella, Ram;Kim, Sunghun",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85055679867,10.1016/j.jss.2007.05.035,Applying machine learning to software fault-proneness prediction,"The importance of software testing to quality assurance cannot be overemphasized. The estimation of a module's fault-proneness is important for minimizing cost and improving the effectiveness of the software testing process. Unfortunately, no general technique for estimating software fault-proneness is available. The observed correlation between some software metrics and fault-proneness has resulted in a variety of predictive models based on multiple metrics. Much work has concentrated on how to select the software metrics that are most likely to indicate fault-proneness. In this paper, we propose the use of machine learning for this purpose. Specifically, given historical data on software metric values and number of reported errors, an Artificial Neural Network (ANN) is trained. Then, in order to determine the importance of each software metric in predicting fault-proneness, a sensitivity analysis is performed on the trained ANN. The software metrics that are deemed to be the most critical are then used as the basis of an ANN-based predictive model of a continuous measure of fault-proneness. We also view fault-proneness prediction as a binary classification task (i.e., a module can either contain errors or be error-free) and use Support Vector Machines (SVM) as a state-of-the-art classification method. We perform a comparative experimental study of the effectiveness of ANNs and SVMs on a data set obtained from NASA's Metrics Data Program data repository. © 2007 Elsevier Inc. All rights reserved.",Maintainability | Performance | Quality | Reliability | Security,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,Conference Paper,"Villalobos, Ignacio;Ferrer, Javier;Alba, Enrique",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84994121213,10.1145/2568225.2568320,Dictionary learning based software defect prediction,"In order to improve the quality of a software system, software defect prediction aims to automatically identify defective software modules for efficient software test. To predict software defect, those classification methods with static code attributes have attracted a great deal of attention. In recent years, machine learning techniques have been applied to defect prediction. Due to the fact that there exists the similarity among different software modules, one software module can be approximately represented by a small proportion of other modules. And the representation coefficients over the pre-defined dictionary, which consists of historical software module data, are generally sparse. In this paper, we propose to use the dictionary learning technique to predict software defect. By using the characteristics of the metrics mined from the open source software, we learn multiple dictionaries (including defective module and defective-free module sub-dictionaries and the total dictionary) and sparse representation coefficients. Moreover, we take the misclassification cost issue into account because the misclassification of defective modules generally incurs much higher risk cost than that of defective-free ones. We thus propose a cost-sensitive discriminative dictionary learning (CDDL) approach for software defect classification and prediction. The widely used datasets from NASA projects are employed as test data to evaluate the performance of all compared methods. Experimental results show that CDDL outperforms several representative state-of-the-art defect prediction methods. © 2014 ACM.",cost-sensitive discriminative dictionary learning (CDDL) | dictionary learning | Software defect prediction | sparse representation,Proceedings - International Conference on Software Engineering,2014-05-31,Conference Paper,"Jing, Xiao Yuan;Ying, Shi;Zhang, Zhi Wu;Wu, Shan Shan;Liu, Jin",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84893606732,10.1109/ASE.2013.6693087,Personalized defect prediction,"Many defect prediction techniques have been proposed. While they often take the author of the code into consideration, none of these techniques build a separate prediction model for each developer. Different developers have different coding styles, commit frequencies, and experience levels, causing different defect patterns. When the defects of different developers are combined, such differences are obscured, hurting prediction performance. This paper proposes personalized defect prediction - building a separate prediction model for each developer to predict software defects. As a proof of concept, we apply our personalized defect prediction to classify defects at the file change level. We evaluate our personalized change classification technique on six large software projects written in C and Java - the Linux kernel, PostgreSQL, Xorg, Eclipse, Lucene and Jackrabbit. Our personalized approach can discover up to 155 more bugs than the traditional change classification (210 versus 55) if developers inspect the top 20% lines of code that are predicted buggy. In addition, our approach improves the F1-score by 0.01-0.06 compared to the traditional change classification. © 2013 IEEE.",Change classification | machine learning | personalized defect prediction | software reliability,"2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings",2013-12-01,Conference Paper,"Jiang, Tian;Tan, Lin;Kim, Sunghun",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85058619119,10.1109/QRS.2017.42,Software defect prediction via convolutional neural network,"To improve software reliability, software defect prediction is utilized to assist developers in finding potential bugs and allocating their testing efforts. Traditional defect prediction studies mainly focus on designing hand-crafted features, which are input into machine learning classifiers to identify defective code. However, these hand-crafted features often fail to capture the semantic and structural information of programs. Such information is important in modeling program functionality and can lead to more accurate defect prediction.In this paper, we propose a framework called Defect Prediction via Convolutional Neural Network (DP-CNN), which leverages deep learning for effective feature generation. Specifically, based on the programs' Abstract Syntax Trees (ASTs), we first extract token vectors, which are then encoded as numerical vectors via mapping and word embedding. We feed the numerical vectors into Convolutional Neural Network to automatically learn semantic and structural features of programs. After that, we combine the learned features with traditional hand-crafted features, for accurate software defect prediction. We evaluate our method on seven open source projects in terms of F-measure in defect prediction. The experimental results show that in average, DP-CNN improves the state-of-the-art method by 12%. © 2017 IEEE.",Anomaly detection | Machine learning | Microservices | Performance monitoring,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,Conference Paper,"Du, Qingfeng;Xie, Tiandi;He, Yu",Include,
10.1016/j.infsof.2022.107128,2-s2.0-72549105852,10.1007/s11219-009-9079-6,Empirical validation of object-oriented metrics for predicting fault proneness models,"Empirical validation of software metrics used to predict software quality attributes is important to ensure their practical relevance in software organizations. The aim of this work is to find the relation of object-oriented (OO) metrics with fault proneness at different severity levels of faults. For this purpose, different prediction models have been developed using regression and machine learning methods. We evaluate and compare the performance of these methods to find which method performs better at different severity levels of faults and empirically validate OO metrics given by Chidamber and Kemerer. The results of the empirical study are based on public domain NASA data set. The performance of the predicted models was evaluated using Receiver Operating Characteristic (ROC) analysis. The results show that the area under the curve (measured from the ROC analysis) of models predicted using high severity faults is low as compared with the area under the curve of the model predicted with respect to medium and low severity faults. However, the number of faults in the classes correctly classified by predicted models with respect to high severity faults is not low. This study also shows that the performance of machine learning methods is better than logistic regression method with respect to all the severities of faults. Based on the results, it is reasonable to claim that models targeted at different severity levels of faults could help for planning and executing testing by focusing resources on fault-prone parts of the design and code that are likely to cause serious failures. © Springer Science+Business Media, LLC 2009.",Empirical validation | Fault prediction | Metrics | Object-oriented | Receiver operating characteristics analysis | Software quality,Software Quality Journal,2010-03-01,Article,"Singh, Yogesh;Kaur, Arvinder;Malhotra, Ruchika",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84867365738,10.1109/TSE.2012.20,Toward comprehensible software fault prediction models using bayesian network classifiers,"Software testing is a crucial activity during software development and fault prediction models assist practitioners herein by providing an upfront identification of faulty software code by drawing upon the machine learning literature. While especially the Naive Bayes classifier is often applied in this regard, citing predictive performance and comprehensibility as its major strengths, a number of alternative Bayesian algorithms that boost the possibility of constructing simpler networks with fewer nodes and arcs remain unexplored. This study contributes to the literature by considering 15 different Bayesian Network (BN) classifiers and comparing them to other popular machine learning techniques. Furthermore, the applicability of the Markov blanket principle for feature selection, which is a natural extension to BN theory, is investigated. The results, both in terms of the AUC and the recently introduced H-measure, are rigorously tested using the statistical framework of Demšar. It is concluded that simple and comprehensible networks with less nodes can be constructed using BN classifiers other than the Naive Bayes classifier. Furthermore, it is found that the aspects of comprehensibility and predictive performance need to be balanced out, and also the development context is an item which should be taken into account during model selection. © 1976-2012 IEEE.",Bayesian networks | classification | comprehensibility | Software fault prediction,IEEE Transactions on Software Engineering,2013-02-08,Article,"Dejaeger, Karel;Verbraken, Thomas;Baesens, Bart",Include,
10.1016/j.infsof.2022.107128,2-s2.0-28244461468,10.1109/MS.2005.149,Building effective defect-prediction models in practice,Successfully predicting defect-prone software modules can help developers improve product quality by focusing quality assurance activities on those modules. Emerging repositories of publicly available software engineering data sets support research in this area by providing static measures and defect data that developers can use to build prediction models and test their effectiveness. Stratifying NASA data sets from the PROMISE repository according to module size showed improved prediction performance in the subsets that included larger modules. © 2005 IEEE.,,IEEE Software,2005-11-01,Article,"Koru, A. Güneş;Liu, Hongfang",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85064900689,10.1016/j.asoc.2015.04.045,Software defect prediction using cost-sensitive neural network,"The software development life cycle generally includes analysis, design, implementation, test and release phases. The testing phase should be operated effectively in order to release bug-free software to end users. In the last two decades, academicians have taken an increasing interest in the software defect prediction problem, several machine learning techniques have been applied for more robust prediction. A different classification approach for this problem is proposed in this paper. A combination of traditional Artificial Neural Network (ANN) and the novel Artificial Bee Colony (ABC) algorithm are used in this study. Training the neural network is performed by ABC algorithm in order to find optimal weights. The False Positive Rate (FPR) and False Negative Rate (FNR) multiplied by parametric cost coefficients are the optimization task of the ABC algorithm. Software defect data in nature have a class imbalance because of the skewed distribution of defective and non-defective modules, so that conventional error functions of the neural network produce unbalanced FPR and FNR results. The proposed approach was applied to five publicly available datasets from the NASA Metrics Data Program repository. Accuracy, probability of detection, probability of false alarm, balance, Area Under Curve (AUC), and Normalized Expected Cost of Misclassification (NECM) are the main performance indicators of our classification approach. In order to prevent random results, the dataset was shuffled and the algorithm was executed 10 times with the use of n-fold cross-validation in each iteration. Our experimental results showed that a cost-sensitive neural network can be created successfully by using the ABC optimization algorithm for the purpose of software defect prediction. © 2015 Elsevier B.V. All rights reserved.",Automatic code review | Machine learning | Multi-instance learning | Software mining,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Li, Heng Yi;Shi, Shu Ting;Thung, Ferdian;Huo, Xuan;Xu, Bowen;Li, Ming;Lo, David",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85067802043,10.1007/s10515-011-0092-1,Sample-based software defect prediction with active and semi-supervised learning,"Software defect prediction can help us better understand and control software quality. Current defect prediction techniques are mainly based on a sufficient amount of historical project data. However, historical data is often not available for new projects and for many organizations. In this case, effective defect prediction is difficult to achieve. To address this problem, we propose sample-based methods for software defect prediction. For a large software system, we can select and test a small percentage of modules, and then build a defect prediction model to predict defect-proneness of the rest of the modules. In this paper, we describe three methods for selecting a sample: random sampling with conventional machine learners, random sampling with a semi-supervised learner and active sampling with active semi-supervised learner. To facilitate the active sampling, we propose a novel active semi-supervised learning method ACoForest which is able to sample the modules that are most helpful for learning a good prediction model. Our experiments on PROMISE datasets show that the proposed methods are effective and have potential to be applied to industrial practice. © 2011 Springer Science+Business Media, LLC.",ANN | Data clustering | Faults detection | Pattern recognition,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Choraś, Michał;Pawlicki, Marek;Kozik, Rafał",Include,
10.1016/j.infsof.2022.107128,2-s2.0-4544254085,,Finding latent code errors via machine learning over program executions,"This paper proposes a technique for identifying program properties that indicate errors. The technique generates machine learning models of program properties known to result from errors, and applies these models to program properties of user-written code to classify and rank properties that may lead the user to errors. Given a set of properties produced by the program analysis, the technique selects a subset of properties that are most likely to reveal an error. An implementation, the Fault Invariant Classifier, demonstrates the efficacy of the technique. The implementation uses dynamic invariant detection to generate program properties. It uses support vector machine and decision tree learning tools to classify those properties. In our experimental evaluation, the technique increases the relevance (the concentration of fault-revealing properties) by a factor of 50 on average for the C programs, and 4.8 for the Java programs. Preliminary experience suggests that most of the fault-revealing properties do lead a programmer to an error.",,Proceedings - International Conference on Software Engineering,2004,,"Brun Y., Ernst M.D.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84898474952,10.1109/CSMR-WCRE.2014.6747166,Cross-project defect prediction models: L'Union fait la force,"Existing defect prediction models use product or process metrics and machine learning methods to identify defect-prone source code entities. Different classifiers (e.g., linear regression, logistic regression, or classification trees) have been investigated in the last decade. The results achieved so far are sometimes contrasting and do not show a clear winner. In this paper we present an empirical study aiming at statistically analyzing the equivalence of different defect predictors. We also propose a combined approach, coined as CODEP (COmbined DEfect Predictor), that employs the classification provided by different machine learning techniques to improve the detection of defect-prone entities. The study was conducted on 10 open source software systems and in the context of cross-project defect prediction, that represents one of the main challenges in the defect prediction field. The statistical analysis of the results indicates that the investigated classifiers are not equivalent and they can complement each other. This is also confirmed by the superior prediction accuracy achieved by CODEP when compared to stand-alone defect predictors. © 2014 IEEE.",,"2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering, CSMR-WCRE 2014 - Proceedings",2014-01-01,Conference Paper,"Panichella, Annibale;Oliveto, Rocco;De Lucia, Andrea",Include,
10.1016/j.infsof.2022.107128,2-s2.0-0027702759,10.1109/32.256851,Developing Interpretable Models with Optimized Set Reduction for Identifying High-Risk Software Components,"Applying equal testing and verification effort to all parts of a software system is not very efficient, especially when resources are limited and scheduling is tight. Therefore, one needs to be able to differentiate low/high fault frequency components so that testing/verification effort can be concentrated where needed. Such a strategy is expected to detect more faults and thus improve the resulting reliability of the overall system. This paper presents the Optimized Set Reduction approach for constructing such models, intended to fulfill specific software-engineering needs. Our approach to classification is to measure the software system and build multivariate stochastic models for predicting high-risk system components. We present experimental results obtained by classifying Ada components into two classes: is or is not likely to generate faults during system and acceptance test. Also, we evaluate the accuracy of the model and the insights it provides into the error-making process. © 1993 IEEE",Ada components | Classification tree | data analysis | fault-prone | logistic regression | machine learning | Optimized Set Reduction | stochastic modeling,IEEE Transactions on Software Engineering,1993-01-01,Article,"Briand, Lionel C.;Basili, Victor R.;Hetmanski, Christopher J.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85072849560,10.1142/S021819400900426X,BP neural network-based effective fault localization,"In program debugging, fault localization identifies the exact locations of program faults. Finding these faults using an ad-hoc approach or based only on programmers' intuitive guesswork can be very time consuming. A better way is to use a well-justified method, supported by case studies for its effectiveness, to automatically identify and prioritize suspicious code for an examination of possible fault locations. To do so, we propose the use of a back-propagation (BP) neural network, a machine learning model which has been successfully applied to software risk analysis, cost prediction, and reliability estimation, to help programmers effectively locate program faults. A BP neural network is suitable for learning the input-output relationship from a set of data, such as the inputs and the corresponding outputs of a program. We first train a BP neural network with the coverage data (statement coverage in our case) and the execution result (success or failure) collected from executing a program, and then we use the trained network to compute the suspiciousness of each executable statement, in terms of its likelihood of containing faults. Suspicious code is ranked in descending order based on its suspiciousness. Programmers will examine such code from the top of the rank to identify faults. Four case studies on different programs (the Siemens suite, the Unix suite, grep and gzip) are conducted. Our results suggest that a BP neural network-based fault localization method is effective in locating program faults. © 2009 World Scientific Publishing Company.",Machine learning | Predictive analytics | Predictive modelling | Search-based software engineering | Software analytics,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Sarro, Federica",Include,
10.1016/j.infsof.2022.107128,2-s2.0-78649782445,10.1109/TSE.2010.51,Evolutionary optimization of software quality modeling with multiple repositories,"A novel search-based approach to software quality modeling with multiple software project repositories is presented. Training a software quality model with only one software measurement and defect data set may not effectively encapsulate quality trends of the development organization. The inclusion of additional software projects during the training process can provide a cross-project perspective on software quality modeling and prediction. The genetic-programming-based approach includes three strategies for modeling with multiple software projects: Baseline Classifier, Validation Classifier, and Validation-and-Voting Classifier. The latter is shown to provide better generalization and more robust software quality models. This is based on a case study of software metrics and defect data from seven real-world systems. A second case study considers 17 different (nonevolutionary) machine learners for modeling with multiple software data sets. Both case studies use a similar majority-voting approach for predicting fault-proneness class of program modules. It is shown that the total cost of misclassification of the search-based software quality models is consistently lower than those of the non-search-based models. This study provides clear guidance to practitioners interested in exploiting their organization's software measurement data repositories for improved software quality modeling. © 2010 IEEE.",defects | Genetic programming | machine learning | optimization | software measurement | software quality,IEEE Transactions on Software Engineering,2010-12-10,Article,"Liu, Yi;Khoshgoftaar, Taghi M.;Seliya, Naeem",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84894432505,10.1016/j.ins.2013.12.031,Software defect prediction using relational association rule mining,"This paper focuses on the problem of defect prediction, a problem of major importance during software maintenance and evolution. It is essential for software developers to identify defective software modules in order to continuously improve the quality of a software system. As the conditions for a software module to have defects are hard to identify, machine learning based classification models are still developed to approach the problem of defect prediction. We propose a novel classification model based on relational association rules mining. Relational association rules are an extension of ordinal association rules, which are a particular type of association rules that describe numerical orderings between attributes that commonly occur over a dataset. Our classifier is based on the discovery of relational association rules for predicting whether a software module is or it is not defective. An experimental evaluation of the proposed model on the open source NASA datasets, as well as a comparison to similar existing approaches is provided. The obtained results show that our classifier overperforms, for most of the considered evaluation measures, the existing machine learning based techniques for defect prediction. This confirms the potential of our proposal. © 2014 Elsevier Inc. All rights reserved.",Association rule | Data mining | Defect prediction | Software engineering,Information Sciences,2014-01-20,Article,"Czibula, Gabriela;Marian, Zsuzsanna;Czibula, Istvan Gergely",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85075565228,10.1016/j.infsof.2015.01.014,Negative samples reduction in cross-company software defects prediction,"Context: Software defect prediction has been widely studied based on various machine-learning algorithms. Previous studies usually focus on within-company defects prediction (WCDP), but lack of training data in the early stages of software testing limits the efficiency of WCDP in practice. Thus, recent research has largely examined the cross-company defects prediction (CCDP) as an alternative solution. Objective: However, the gap of different distributions between cross-company (CC) data and withincompany (WC) data usually makes it difficult to build a high-quality CCDP model. In this paper, a novel algorithm named Double Transfer Boosting (DTB) is introduced to narrow this gap and improve the performance of CCDP by reducing negative samples in CC data. Method: The proposed DTB model integrates two levels of data transfer: first, the data gravitation method reshapes the whole distribution of CC data to fit WC data. Second, the transfer boosting method employs a small ratio of labeled WC data to eliminate negative instances in CC data. Results: The empirical evaluation was conducted based on 15 publicly available datasets. CCDP experiment results indicated that the proposed model achieved better overall performance than compared CCDP models. DTB was also compared to WCDP in two different situations. Statistical analysis suggested that DTB performed significantly better than WCDP models trained by limited samples and produced comparable results to WCDP with sufficient training data. Conclusions: DTB reforms the distribution of CC data from different levels to improve the performance of CCDP, and experimental results and analysis demonstrate that it could be an effective model for early software defects detection. © 2015 Elsevier B.V. All rights reserved.",Data structure invariants | Korat | Machine learning,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Usman, Muhammad;Wang, Wenxi;Wang, Kaiyuan;Yelen, Cagdas;Dini, Nima;Khurshid, Sarfraz",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85075603666,10.1109/ICTAI.2010.27,Attribute selection and imbalanced data: Problems in software defect prediction,"The data mining and machine learning community is often faced with two key problems: working with imbalanced data and selecting the best features for machine learning. This paper presents a process involving a feature selection technique for selecting the important attributes and a data sampling technique for addressing class imbalance. The application domain of this study is software engineering, more specifically, software quality prediction using classification models. When using feature selection and data sampling together, different scenarios should be considered. The four possible scenarios are: (1) feature selection based on original data, and modeling (defect prediction) based on original data; (2) feature selection based on original data, and modeling based on sampled data; (3) feature selection based on sampled data, and modeling based on original data; and (4) feature selection based on sampled data, and modeling based on sampled data. The research objective is to compare the software defect prediction performances of models based on the four scenarios. The case study consists of nine software measurement data sets obtained from the PROMISE software project repository. Empirical results suggest that feature selection based on sampled data performs significantly better than feature selection based on original data, and that defect prediction models perform similarly regardless of whether the training data was formed using sampled or original data. © 2010 IEEE.",Classification | Ensemble | Heterogeneous | Machine learning | Mobile app reviews | SMOTE | Software engineering | Stacking,Advances in Intelligent Systems and Computing,2020-01-01,Conference Paper,"Gomaa, Ahmed;El-Shorbagy, Sara;El-Gammal, Wael;Magdy, Mohamed;Abdelmoez, Walid",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85076635963,10.1109/NAFIPS.2007.383813,Applying novel resampling strategies to software defect prediction,"Due to the tremendous complexity and sophistication of software, improving software reliability is an enormously difficult task. We study the software defect prediction problem, which focuses on predicting which modules will experience a failure during operation. Numerous studies have applied machine learning to software defect prediction; however, skewness in defect-prediction datasets usually undermines the learning algorithms. The resulting classifiers will often never predict the faulty (minorityO class. This problem is well known in machine learning and is often referred to as learning from imbalanced datasets. We examine stratification, a widely used technique for learning imbalanced data that has received little attention in software defect prediction. Our experiments are focused on the SMOTE technique, which is a method of over-sampling minority-class examples. Our goal is to determine if SMOTE can improve recognition of defect-prone modules, and at what cost. Our experiments demonstrate that after SMOTE resampling, we have a more balanced classification. We found an improvement of at least 23% in the average geometric mean classification accuracy on four benchmark datasets. © 2007 IEEE.",Classifier | Computational experiment | Error | Reliability,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Shepperd, Martin;Guo, Yuchen;Li, Ning;Arzoky, Mahir;Capiluppi, Andrea;Counsell, Steve;Destefanis, Giuseppe;Swift, Stephen;Tucker, Allan;Yousefi, Leila",Include,
10.1016/j.infsof.2022.107128,2-s2.0-77949343227,10.1145/1137983.1138012,Predicting defect densities in source code files with decision tree learners,"With the advent of open source software repositories the data available for defect prediction in source files increased tremendously. Although traditional statistics turned out to derive reasonable results the sheer amount of data and the problem context of defect prediction demand sophisticated analysis such as provided by current data mining and machine learning techniques.In this work we focus on defect density prediction and present an approach that applies a decision tree learner on evolution data extracted from the Mozilla open source web browser project. The evolution data includes different source code, modification, and defect measures computed from seven recent Mozilla releases. Among the modification measures we also take into account the change coupling, a measure for the number of change-dependencies between source files. The main reason for choosing decision tree learners, instead of for example neural nets, was the goal of finding underlying rules which can be easily interpreted by humans. To find these rules, we set up a number of experiments to test common hypotheses regarding defects in software entities. Our experiments showed, that a simple tree learner can produce good results with various sets of input data. Copyright 2006 ACM.",data mining | decision tree learner | defect prediction,Proceedings - International Conference on Software Engineering,2006-12-01,Conference Paper,"Knab, Patrick;Pinzger, Martin;Bernstein, Abraham",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85083433363,10.1109/MSR.2012.6224300,"Think locally, act globally: Improving defect and effort prediction models","Much research energy in software engineering is focused on the creation of effort and defect prediction models. Such models are important means for practitioners to judge their current project situation, optimize the allocation of their resources, and make informed future decisions. However, software engineering data contains a large amount of variability. Recent research demonstrates that such variability leads to poor fits of machine learning models to the underlying data, and suggests splitting datasets into more fine-grained subsets with similar properties. In this paper, we present a comparison of three different approaches for creating statistical regression models to model and predict software defects and development effort. Global models are trained on the whole dataset. In contrast, local models are trained on subsets of the dataset. Last, we build a global model that takes into account local characteristics of the data. We evaluate the performance of these three approaches in a case study on two defect and two effort datasets. We find that for both types of data, local models show a significantly increased fit to the data compared to global models. The substantial improvements in both relative and absolute prediction errors demonstrate that this increased goodness of fit is valuable in practice. Finally, our experiments suggest that trends obtained from global models are too general for practical recommendations. At the same time, local models provide a multitude of trends which are only valid for specific subsets of the data. Instead, we advocate the use of trends obtained from global models that take into account local characteristics, as they combine the best of both worlds. © 2012 IEEE.",,Lecture Notes on Data Engineering and Communications Technologies,2020-01-01,Book Chapter,"Piotrowski, Paweł;Madeyski, Lech",Include,
10.1016/j.infsof.2022.107128,2-s2.0-57049102518,10.1145/1370788.1370793,Comparing design and code metrics for software quality prediction,"The prediction of fault-prone modules continues to attract interest due to the significant impact it has on software quality assurance. One of the most important goals of such techniques is to accurately predict the modules where faults are likely to hide as early as possible in the development lifecycle. Design, code, and most recently, requirements metrics have been successfully used for predicting fault-prone modules. The goal of this paper is to compare the performance of predictive models which use design-level metrics with those that use code-level metrics and those that use both. We analyze thirteen datasets from NASA Metrics Data Program which offer design as well as code metrics. Using a range of modeling techniques and statistical significance tests, we confirmed that models built from code metrics typically outperform design metrics based models. However, both types of models prove to be useful as they can be constructed in different project phases. Code-based models can be used to increase the performance of design-level models and, thus, increase the efficiency of assigning verification and validation activities late in the development lifecycle. We also conclude that models that utilize a combination of design and code level metrics outperform models which use either one or the other metric set. Copyright 2008 ACM.",Code metrics | Design metrics | Fault-proneness prediction | Machine learning,Proceedings - International Conference on Software Engineering,2008-12-08,Conference Paper,"Jiang, Yue;Cukic, Bojan;Menzies, Tim;Bartlow, Nick",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85076980668,10.1016/j.is.2015.02.006,"Software defect prediction using a cost sensitive decision forest and voting, and a potential solution to the class imbalance problem","Abstract Software development projects inevitably accumulate defects throughout the development process. Due to the high cost that defects can incur, careful consideration is crucial when predicting which sections of code are likely to contain defects. Classification algorithms used in machine learning can be used to create classifiers which can be used to predict defects. While traditional classification algorithms optimize for accuracy, cost-sensitive classification methods attempt to make predictions which incur the lowest classification cost. In this paper we propose a cost-sensitive classification technique called CSForest which is an ensemble of decision trees. We also propose a cost-sensitive voting technique called CSVoting in order to take advantage of the set of decision trees in minimizing the classification cost. We then investigate a potential solution to class imbalance within our decision forest algorithm. We empirically evaluate the proposed techniques comparing them with six (6) classifier algorithms on six (6) publicly available clean datasets that are commonly used in the research on software defect prediction. Our initial experimental results indicate a clear superiority of the proposed techniques over the existing ones. © 2015 Elsevier Ltd. All rights reserved.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Tabernik, Domen;Šela, Samo;Skvarč, Jure;Skočaj, Danijel",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85078479877,10.3745/JIPS.2012.8.2.241,Fault prediction using statistical and machine learning methods for improving software quality,"An understanding of quality attributes is relevant for the software organization to deliver high software reliability. An empirical assessment of metrics to predict the quality attributes is essential in order to gain insight about the quality of software in the early phases of software development and to ensure corrective actions. In this paper, we predict a model to estimate fault proneness using Object Oriented CK metrics and QMOOD metrics. We apply one statistical method and six machine learning methods to predict the models. The proposed models are validated using dataset collected from Open Source software. The results are analyzed using Area Under the Curve (AUC) obtained from Receiver Operating Characteristics (ROC) analysis. The results show that the model predicted using the random forest and bagging methods outperformed all the other models. Hence, based on these results it is reasonable to claim that quality models have a significant relevance with Object Oriented metrics and that machine learning methods have a comparable performance with statistical methods © 2012 KIPS.",Machine learning | Requirements engineering | Similarity detection,Lecture Notes in Business Information Processing,2020-01-01,Conference Paper,"Femmer, Henning;Müller, Axel;Eder, Sebastian",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85025129443,10.1109/ICPC.2017.24,Bug Localization with Combination of Deep Learning and Information Retrieval,"The automated task of locating the potential buggy files in a softwareproject given a bug report is called bug localization. Buglocalization helps developers focus on crucial files. However, theexisting automated bug localization approaches face a key challenge, called lexical mismatch. Specifically, the terms used in bug reportsto describe a bug are different from the terms and code tokens used insource files. To address that, we present a novel approach that usesdeep neural network (DNN) in combination with rVSM, an informationretrieval (IR) technique. rVSM collects the feature on the textualsimilarity between bug reports and source files. DNN is used to learnto relate the terms in bug reports to potentially different codetokens and terms in source files. Our empirical evaluation onreal-world bug reports in the open-source projects shows that DNN andIR complement well to each other to achieve higher bug localizationaccuracy than individual models. Importantly, our new model, DNNLOC, with a combination of the features built from DNN, rVSM, and project'sbug-fixing history, achieves higher accuracy than the state-of-the-artIR and machine learning techniques. In half of the cases, it iscorrect with just a single suggested file. In 66% of the time, acorrect buggy file is in the list of three suggested files. With 5suggested files, it is correct in almost 70% of the cases. © 2017 IEEE.",Bug Localization | Code Retrieval | Deep Learning | Information Retrieval,IEEE International Conference on Program Comprehension,2017-06-28,Conference Paper,"Lam, An Ngoc;Nguyen, Anh Tuan;Nguyen, Hoan Anh;Nguyen, Tien N.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84876757483,10.1109/TCAD.2012.2234827,"Board-level functional fault diagnosis using artificial neural networks, support-vector machines, and weighted-majority voting","Increasing integration densities and high operating speeds lead to subtle manifestation of defects at the board level. Functional fault diagnosis is, therefore, necessary for board-level product qualification. However, ambiguous diagnosis results lead to long debug times and even wrong repair actions, which significantly increase repair cost and adversely impact yield. Advanced machine-learning (ML) techniques offer an unprecedented opportunity to increase the accuracy of board-level functional diagnosis and reduce high-volume manufacturing cost through successful repair. We propose a smart diagnosis method based on two ML classification models, namely, artificial neural networks (ANNs) and support-vector machines (SVMs) that can learn from repair history and accurately localize the root cause of a failure. Fine-grained fault syndromes extracted from failure logs and corresponding repair actions are used to train the classification models. We also propose a decision machine based on weighted-majority voting, which combines the benefits of ANNs and SVMs. Three complex boards from the industry, currently in volume production, and additional synthetic data, are used to validate the proposed methods in terms of diagnostic accuracy, resolution, and quantifiable improvement over current diagnostic software. © 1982-2012 IEEE.",Board-level | diagnosis | functional failure | machine learning (ML) | neural networks | support-vector machines (SVMs),IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,2013-05-01,Article,"Ye, Fangming;Zhang, Zhaobo;Chakrabarty, Krishnendu;Gu, Xinli",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85013328677,10.1145/2499393.2499397,Training data selection for cross-project defect prediction,"Software defect prediction has been a popular research topic in recent years and is considered as a means for the optimization of quality assurance activities. Defect prediction can be done in a withinproject or a cross-project scenario. The within-project scenario produces results with a very high quality, but requires historic data of the project, which is often not available. For the cross-project prediction, the data availability is not an issue as data from other projects is readily available, e.g., in repositories like PROMISE. However, the quality of the defect prediction results is too low for practical use. Recent research showed that the selection of appropriate training data can improve the quality of cross-project defect predictions. In this paper, we propose distance-based strategies for the selection of training data based on distributional characteristics of the available data. We evaluate the proposed strategies in a large case study with 44 data sets obtained from 14 open source projects. Our results show that our training data selection strategy improves the achieved success rate of cross-project defect predictions significantly. However, the quality of the results still cannot compete with within-project defect prediction.",Cross-project prediction | Defect-prediction | Machine learning,ACM International Conference Proceeding Series,2013-10-09,Conference Paper,"Herbold, Steffen",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85081616752,10.1109/ASE.2009.76,Reducing features to improve bug prediction,"Recently, machine learning classifiers have emerged as a way to predict the existence of a bug in a change made to a source code file. The classifier is first trained on software history data, and then used to predict bugs. Two drawbacks of existing classifier-based bug prediction are potentially insufficient accuracy for practical use, and use of a large number of features. These large numbers of features adversely impact scalability and accuracy of the approach. This paper proposes a feature selection technique applicable to classification-based bug prediction. This technique is applied to predict bugs in software changes, and performance of Naïve Bayes and Support Vector Machine (SVM) classifiers is characterized. © 2009 IEEE.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020-01-01,Conference Paper,"Nakajima, Shin",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84926613328,10.1007/s10515-015-0179-1,Multiple kernel ensemble learning for software defect prediction,"Software defect prediction aims to predict the defect proneness of new software modules with the historical defect data so as to improve the quality of a software system. Software historical defect data has a complicated structure and a marked characteristic of class-imbalance; how to fully analyze and utilize the existing historical defect data and build more precise and effective classifiers has attracted considerable researchers’ interest from both academia and industry. Multiple kernel learning and ensemble learning are effective techniques in the field of machine learning. Multiple kernel learning can map the historical defect data to a higher-dimensional feature space and make them express better, and ensemble learning can use a series of weak classifiers to reduce the bias generated by the majority class and obtain better predictive performance. In this paper, we propose to use the multiple kernel learning to predict software defect. By using the characteristics of the metrics mined from the open source software, we get a multiple kernel classifier through ensemble learning method, which has the advantages of both multiple kernel learning and ensemble learning. We thus propose a multiple kernel ensemble learning (MKEL) approach for software defect classification and prediction. Considering the cost of risk in software defect prediction, we design a new sample weight vector updating strategy to reduce the cost of risk caused by misclassifying defective modules as non-defective ones. We employ the widely used NASA MDP datasets as test data to evaluate the performance of all compared methods; experimental results show that MKEL outperforms several representative state-of-the-art defect prediction methods. © 2015, Springer Science+Business Media New York.",Ensemble learning | Multiple kernel ensemble learning (MKEL) | Multiple kernel learning | Software defect prediction,Automated Software Engineering,2016-12-01,Article,"Wang, Tiejian;Zhang, Zhiwu;Jing, Xiaoyuan;Zhang, Liqiang",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85085213137,10.1109/COMPSAC.2015.58,An Empirical Study of Classifier Combination for Cross-Project Defect Prediction,"To help developers better allocate testing and debugging efforts, many software defect prediction techniques have been proposed in the literature. These techniques can be used to predict classes that are more likely to be buggy based on past history of buggy classes. These techniques work well as long as a sufficient amount of data is available to train a prediction model. However, there is rarely enough training data for new software projects. To deal with this problem, cross-project defect prediction, which transfers a prediction model trained using data from one project to another, has been proposed and is regarded as a new challenge for defect prediction. So far, only a few cross-project defect prediction techniques have been proposed. To advance the state-of-the-art, in this work, we investigate 7 composite algorithms, which integrate multiple machine learning classifiers, to improve cross-project defect prediction. To evaluate the performance of the composite algorithms, we perform experiments on 10 open source software systems from the PROMISE repository which contain a total of 5,305 instances labeled as defective or clean. We compare the composite algorithms with CODEP Logistic, which is the latest cross-project defect prediction algorithm proposed by Panichella et al., in terms of two standard evaluation metrics: cost effectiveness and F-measure. Our experiment results show that several algorithms outperform CODEP Logistic: Max performs the best in terms of F-measure and its average F-measure outperforms that of CODEP Logistic by 36.88%. Bagging J48 performs the best in terms of cost effectiveness and its average cost effectiveness outperforms that of CODEP Logistic by 15.34%. © 2015 IEEE.",Failure prediction | Fault tolerance | Google cluster trace | Mustang trace | Trinity trace,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020-01-01,Conference Paper,"Jassas, Mohammad S.;Mahmoud, Qusay H.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-79959234049,10.1145/1985441.1985456,Comparing fine-grained source code changes and code churn for bug prediction,"A significant amount of research effort has been dedicated to learning prediction models that allow project managers to efficiently allocate resources to those parts of a software system that most likely are bug-prone and therefore critical. Prominent measures for building bug prediction models are product measures, e.g., complexity or process measures, such as code churn. Code churn in terms of lines modified (LM) and past changes turned out to be significant indicators of bugs. However, these measures are rather imprecise and do not reflect all the detailed changes of particular source code entities during maintenance activities. In this paper, we explore the advantage of using fine-grained source code changes (SCC) for bug prediction. SCC captures the exact code changes and their semantics down to statement level. We present a series of experiments using different machine learning algorithms with a dataset from the Eclipse platform to empirically evaluate the performance of SCC and LM. The results show that SCC outperforms LM for learning bug prediction models. © 2011 ACM.",code churn | nonlinear regression | prediction models | software bugs | source code changes,Proceedings - International Conference on Software Engineering,2011-06-22,Conference Paper,"Giger, Emanuel;Pinzger, Martin;Gall, Harald C.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85088741109,10.1109/TSE.2018.2836442,A Comprehensive Investigation of the Role of Imbalanced Learning for Software Defect Prediction,"Context: Software defect prediction (SDP) is an important challenge in the field of software engineering, hence much research work has been conducted, most notably through the use of machine learning algorithms. However, class-imbalance typified by few defective components and many non-defective ones is a common occurrence causing difficulties for these methods. Imbalanced learning aims to deal with this problem and has recently been deployed by some researchers, unfortunately with inconsistent results. Objective: We conduct a comprehensive experiment to explore (a) the basic characteristics of this problem; (b) the effect of imbalanced learning and its interactions with (i) data imbalance, (ii) type of classifier, (iii) input metrics and (iv) imbalanced learning method. Method: We systematically evaluate 27 data sets, 7 classifiers, 7 types of input metrics and 17 imbalanced learning methods (including doing nothing) using an experimental design that enables exploration of interactions between these factors and individual imbalanced learning algorithms. This yields 27 × 7 × 7 × 17 = 22491 results. The Matthews correlation coefficient (MCC) is used as an unbiased performance measure (unlike the more widely used F1 and AUC measures). Results: (a) we found a large majority (87 percent) of 106 public domain data sets exhibit moderate or low level of imbalance (imbalance ratio <<10; median = 3.94); (b) anything other than low levels of imbalance clearly harm the performance of traditional learning for SDP; (c) imbalanced learning is more effective on the data sets with moderate or higher imbalance, however negative results are always possible; (d) type of classifier has most impact on the improvement in classification performance followed by the imbalanced learning method itself. Type of input metrics is not influential. (e) only \sim 52\%∼52% of the combinations of Imbalanced Learner and Classifier have a significant positive effect. Conclusion: This paper offers two practical guidelines. First, imbalanced learning should only be considered for moderate or highly imbalanced SDP data sets. Second, the appropriate combination of imbalanced method and classifier needs to be carefully chosen to ameliorate the imbalanced learning problem for SDP. In contrast, the indiscriminate application of imbalanced learning can be harmful. © 1976-2012 IEEE.",Bug reports | Machine learning | Software requirements | Text mining,Advances in Intelligent Systems and Computing,2021-01-01,Conference Paper,"Onan, Aytuğ;Atik, Erdem;Yalçın, Adnan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85011708666,10.1007/s11219-016-9353-3,Software defect prediction: do different classifiers find the same defects?,"During the last 10 years, hundreds of different defect prediction models have been published. The performance of the classifiers used in these models is reported to be similar with models rarely performing above the predictive performance ceiling of about 80% recall. We investigate the individual defects that four classifiers predict and analyse the level of prediction uncertainty produced by these classifiers. We perform a sensitivity analysis to compare the performance of Random Forest, Naïve Bayes, RPart and SVM classifiers when predicting defects in NASA, open source and commercial datasets. The defect predictions that each classifier makes is captured in a confusion matrix and the prediction uncertainty of each classifier is compared. Despite similar predictive performance values for these four classifiers, each detects different sets of defects. Some classifiers are more consistent in predicting defects than others. Our results confirm that a unique subset of defects can be detected by specific classifiers. However, while some classifiers are consistent in the predictions they make, other classifiers vary in their predictions. Given our results, we conclude that classifier ensembles with decision-making strategies not based on majority voting are likely to perform best in defect prediction. © 2017, The Author(s).",Machine learning | Prediction modelling | Software defect prediction,Software Quality Journal,2018-06-01,Article,"Bowes, David;Hall, Tracy;Petrić, Jean",Include,
10.1016/j.infsof.2022.107128,2-s2.0-77956590859,10.1109/DSN.2010.5544275,Adaptive on-line software aging prediction based on machine learning,"The growing complexity of software systems is resulting in an increasing number of software faults. According to the literature, software faults are becoming one of the main sources of unplanned system outages, and have an important impact on company benefits and image. For this reason, a lot of techniques (such as clustering, fail-over techniques, or server redundancy) have been proposed to avoid software failures, and yet they still happen. Many software failures are those due to the software aging phenomena. In this work, we present a detailed evaluation of our chosen machine learning prediction algorithm (M5P) in front of dynamic and non-deterministic software aging. We have tested our prediction model on a three-tier web J2EE application achieving acceptable prediction accuracy against complex scenarios with small training data sets. Furthermore, we have found an interesting approach to help to determine the root cause failure: The model generated by machine learning algorithms. © 2010 IEEE.",,Proceedings of the International Conference on Dependable Systems and Networks,2010-09-20,Conference Paper,"Alonso, Javier;Torres, Jordi;Berral, Josep Ll;Gavaldà, Ricard",Include,
10.1016/j.infsof.2022.107128,2-s2.0-67650330214,10.1504/IJCAT.2009.026595,Comparative analysis of regression and machine learning methods for predicting fault proneness models,"Demand for quality software has undergone rapid growth during the last few years. This is leading to increase in development of machine learning techniques for exploring datasets which can be used in constructing models for predicting quality attributes such as Decision Tree (DT), Support Vector Machine (SVM) and Artificial Neural Network (ANN). This paper examines and compares Logistic Regression (LR), ANN (model predicted in an analogous study using the same dataset), SVM and DT methods. These two methods are explored empirically to find the effect of object-oriented metrics given by Chidamber and Kemerer on the fault proneness of object-oriented system classes. Data collected from Java applications is used in the study. The performance of the methods was compared by Receiver Operating Characteristic (ROC) analysis. DT modelling showed 84.7% of correct classifications of faulty classes and is a better model than the model predicted using LR, SVM and ANN method. The area under the ROC curve of LR, ANN, SVM and DT model is 0.826, 0.85, 0.85 and 0.87, respectively. The paper shows that machine learning methods are useful in constructing software quality models. Copyright © 2009 Inderscience Enterprises Ltd.",Decision tree | Logistic regression | Metrics | Receiver operating characteristics curve | Software quality | Support vector machine,International Journal of Computer Applications in Technology,2009-01-01,Article,"Singh, Yogesh;Kaur, Arvinder;Malhotra, Ruchika",Include,
10.1016/j.infsof.2022.107128,2-s2.0-44349135014,10.1142/S0218213008003947,Empirical assessment of machine learning based software defect prediction techniques,"Automated reliability assessment is essential for systems that entail dynamic adaptation based on runtime mission-specific requirements. One approach along this direction is to monitor and assess the system using machine learning-based software defect prediction techniques. Due to the dynamic nature of software data collected, Instance-based learning algorithms are proposed for the above purposes. To evaluate the accuracy of these methods, the paper presents an empirical analysis of four different real-time software defect data sets using different predictor models. The results show that a combination of 1R and Instance-based learning along with Consistency-based subset evaluation technique provides a relatively better consistency in achieving accurate predictions as compared with other models. No direct relationship is observed between the skewness present in the data sets and the prediction accuracy of these models. Principal Component Analysis (PCA) does not show a consistent advantage in improving the accuracy of the predictions. While random reduction of attributes gave poor accuracy results, simple Feature Subset Selection methods performed better than PCA for most prediction models. Based on these results, the paper presents a high-level design of an Intelligent Software Defect Analysis tool (ISDAT) for dynamic monitoring and defect assessment of software modules. © 2008 World Scientific Publishing Company.",Data analysis | Software defect prediction,International Journal on Artificial Intelligence Tools,2008-04-01,Article,"Challagulla, Venkata Udaya B.;Bastani, Farokh B.;Yen, I. Ling;Paul, Raymond A.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-78049530108,10.1016/j.eswa.2010.08.022,Practical development of an Eclipse-based software fault prediction tool using Naive Bayes algorithm,"Despite the amount of effort software engineers have been putting into developing fault prediction models, software fault prediction still poses great challenges. This research using machine learning and statistical techniques has been ongoing for 15 years, and yet we still have not had a breakthrough. Unfortunately, none of these prediction models have achieved widespread applicability in the software industry due to a lack of software tools to automate this prediction process. Historical project data, including software faults and a robust software fault prediction tool, can enable quality managers to focus on fault-prone modules. Thus, they can improve the testing process. We developed an Eclipse-based software fault prediction tool for Java programs to simplify the fault prediction process. We also integrated a machine learning algorithm called Naive Bayes into the plug-in because of its proven high-performance for this problem. This article presents a practical view to software fault prediction problem, and it shows how we managed to combine software metrics with software fault data to apply Naive Bayes technique inside an open source platform. © 2010 Elsevier Ltd. All rights reserved.",Eclipse technology | Machine learning | Naive Bayes | Software fault prediction,Expert Systems with Applications,2011-03-01,Article,"Catal, Cagatay;Sevim, Ugur;Diri, Banu",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85092608318,10.1016/j.asoc.2014.03.032,Comparative analysis of statistical and machine learning methods for predicting faulty modules,"The demand for development of good quality software has seen rapid growth in the last few years. This is leading to increase in the use of the machine learning methods for analyzing and assessing public domain data sets. These methods can be used in developing models for estimating software quality attributes such as fault proneness, maintenance effort, testing effort. Software fault prediction in the early phases of software development can help and guide software practitioners to focus the available testing resources on the weaker areas during the software development. This paper analyses and compares the statistical and six machine learning methods for fault prediction. These methods (Decision Tree, Artificial Neural Network, Cascade Correlation Network, Support Vector Machine, Group Method of Data Handling Method, and Gene Expression Programming) are empirically validated to find the relationship between the static code metrics and the fault proneness of a module. In order to assess and compare the models predicted using the regression and the machine learning methods we used two publicly available data sets AR1 and AR6. We compared the predictive capability of the models using the Area Under the Curve (measured from the Receiver Operating Characteristic (ROC) analysis). The study confirms the predictive capability of the machine learning methods for software fault prediction. The results show that the Area Under the Curve of model predicted using the Decision Tree method is 0.8 and 0.9 (for AR1 and AR6 data sets, respectively) and is a better model than the model predicted using the logistic regression and other machine learning methods. © 2014 Published by Elsevier B.V.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020-01-01,Conference Paper,"Kumari, Madhu;Singh, Ujjawal Kumar;Sharma, Meera",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85042197128,10.1016/j.ins.2018.02.027,A novel approach for software defect prediction through hybridizing gradual relational association rules with artificial neural networks,"The growing complexity of software projects requires increasing consideration of their analysis and testing. Identifying defective software entities is essential for software quality assurance and it also improves activities related to software testing. In this study, we developed a novel supervised classification method called HyGRAR for software defect prediction. HyGRAR is a non-linear hybrid model that combines gradual relational association rule mining and artificial neural networks to discriminate between defective and non-defective software entities. Experiments performed based on 10 open-source data sets demonstrated the excellent performance of the HYGRAR classifier. HyGRAR performed better than most of the previously proposed approaches for software defect prediction in performance evaluations using the same data sets. © 2018 Elsevier Inc.",Artificial neural network | Gradual relational association rule | Machine learning | Software defect prediction,Information Sciences,2018-05-01,Article,"Miholca, Diana Lucia;Czibula, Gabriela;Czibula, Istvan Gergely",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85097427859,10.1007/978-3-642-21843-9_20,A genetic algorithm to configure support vector machines for predicting fault-prone components,"In some studies, Support Vector Machines (SVMs) have been turned out to be promising for predicting fault-prone software components. Nevertheless, the performance of the method depends on the setting of some parameters. To address this issue, we propose the use of a Genetic Algorithm (GA) to search for a suitable configuration of SVMs parameters that allows us to obtain optimal prediction performance. The approach has been assessed carrying out an empirical analysis based on jEdit data from the PROMISE repository. We analyzed both the inter- and the intra-release performance of the proposed method. As benchmarks we exploited SVMs with Grid-search and several other machine learning techniques. The results show that the proposed approach let us to obtain an improvement of the performance with an increasing of the Recall measure without worsening the Precision one. This behavior was especially remarkable for the inter-release use with respect to the other prediction techniques. © 2011 Springer-Verlag.",AI Engineering | Artificial Intelligence | Machine learning | Quality management,Communications in Computer and Information Science,2020-01-01,Conference Paper,"Santhanam, P.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84883655922,10.1016/j.peva.2012.09.004,Predicting aging-related bugs using software complexity metrics,"Long-running software systems tend to show degraded performance and an increased failure occurrence rate. This problem, known as Software Aging, which is typically related to the runtime accumulation of error conditions, is caused by the activation of the so-called Aging-Related Bugs (ARBs). This paper aims to predict the location of Aging-Related Bugs in complex software systems, so as to aid their identification during testing. First, we carried out a bug data analysis on three large software projects in order to collect data about ARBs. Then, a set of software complexity metrics were selected and extracted from the three projects. Finally, by using such metrics as predictor variables and machine learning algorithms, we built fault prediction models that can be used to predict which source code files are more prone to Aging-Related Bugs. © 2012 Elsevier B.V. All rights reserved.",Aging-related bugs | Fault prediction | Software aging | Software complexity metrics,Performance Evaluation,2013-01-01,Article,"Cotroneo, Domenico;Natella, Roberto;Pietrantuono, Roberto",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85101832822,10.1145/1390817.1390822,Can data transformation help in the detection of fault-prone modules?,"Data preprocessing (transformation) plays an important role in data mining and machine learning. In this study, we investigate the effect of four different preprocessing methods to fault-proneness prediction using nine datasets from NASA Metrics Data Programs (MDP) and ten classification algorithms. Our experiments indicate that log transformation rarely improves classification performance, but discretization affects the performance of many different algorithms. The impact of different transformations differs. Random forest algorithm, for example, performs better with original and log transformed data set. Boosting and NaiveBayes perform significantly better with discretized data. We conclude that no general benefit can be expected from data transformations. Instead, selected transformation techniques are recommended to boost the performance of specific classification algorithms. Copyright 2008 ACM.",Bugs report | Github | Machine learning | Open source software | Recommendation,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021-01-01,Conference Paper,"Baloch, Muhammad Zubair;Hussain, Shahid;Afzal, Humaira;Mufti, Muhammad Rafiq;Ahmad, Bashir",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85104706606,10.1109/ISSRE.2014.35,Defect prediction between software versions with active learning and dimensionality reduction,"Accurate detection of defects prior to product release helps software engineers focus verification activities on defect prone modules, thus improving the effectiveness of software development. A common scenario is to use the defects from prior releases to build the prediction model for the upcoming release, typically through a supervised learning method. As software development is a dynamic process, fault characteristics in subsequent releases may vary. Therefore, supplementing the defect information from prior releases with limited information about the defects from the current release detected early seems to offer intuitive and practical benefits. We propose active learning as a way to automate the development of models which improve the performance of defect prediction between successive releases. Our results show that the integration of active learning with uncertainty sampling consistently outperforms the corresponding supervised learning approach. We further improve the prediction performance with feature compression techniques, where feature selection or dimensionality reduction is applied to defect data prior to active learning. We observe that dimensionality reduction techniques, particularly multidimensional scaling with random forest similarity, work better than feature selection due to their ability to identify and combine essential information in data set features. We present the improvements offered by this methodology through the prediction of defective modules in the three successive versions of Eclipse. © 2014 IEEE.",Literature review | Machine learning | Software engineering,Communications in Computer and Information Science,2021-01-01,Conference Paper,"Akinsanya, Barakat J.;Araújo, Luiz J.P.;Charikova, Mariia;Gimaeva, Susanna;Grichshenko, Alexandr;Khan, Adil;Mazzara, Manuel;Ozioma Okonicha, N.;Shilintsev, Daniil",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85106422731,10.1016/j.infsof.2010.06.006,Practical considerations in deploying statistical methods for defect prediction: A case study within the Turkish telecommunications industry,"Context: Building defect prediction models in large organizations has many challenges due to limited resources and tight schedules in the software development lifecycle. It is not easy to collect data, utilize any type of algorithm and build a permanent model at once. We have conducted a study in a large telecommunications company in Turkey to employ a software measurement program and to predict pre-release defects. Based on our prior publication, we have shared our experience in terms of the project steps (i.e. challenges and opportunities). We have further introduced new techniques that improve our earlier results. Objective: In our previous work, we have built similar predictors using data representative for US software development. Our task here was to check if those predictors were specific solely to US organizations or to a broader class of software. Method: We have presented our approach and results in the form of an experience report. Specifically, we have made use of different techniques for improving the information content of the software data and the performance of a Naïve Bayes classifier in the prediction model that is locally tuned for the company. We have increased the information content of the software data by using module dependency data and improved the performance by adjusting the hyper-parameter (decision threshold) of the Naïve Bayes classifier. We have reported and discussed our results in terms of defect detection rates and false alarms. We also carried out a cost-benefit analysis to show that our approach can be efficiently put into practice. Results: Our general result is that general defect predictors, which exist across a wide range of software (in both US and Turkish organizations), are present. Our specific results indicate that concerning the organization subject to this study, the use of version history information along with code metrics decreased false alarms by 22%, the use of dependencies between modules further reduced false alarms by 8%, and the decision threshold optimization for the Naïve Bayes classifier using code metrics and version history information further improved false alarms by 30% in comparison to a prediction using only code metrics and a default decision threshold. Conclusion: Implementing statistical techniques and machine learning on a real life scenario is a difficult yet possible task. Using simple statistical and algorithmic techniques produces an average detection rate of 88%. Although using dependency data improves our results, it is difficult to collect and analyze such data in general. Therefore, we would recommend optimizing the hyper-parameter of the proposed technique, Naïve Bayes, to calibrate the defect prediction model rather than employing more complex classifiers. We also recommend that researchers who explore statistical and algorithmic methods for defect prediction should spend less time on their algorithms and more time on studying the pragmatic considerations of large organizations. © 2010 Elsevier B.V. All rights reserved.",,Lecture Notes in Networks and Systems,2021-01-01,Conference Paper,"Atif, Farah;Rodriguez, Manuel;Araújo, Luiz J.P.;Amartiwi, Utih;Akinsanya, Barakat J.;Mazzara, Manuel",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85111355456,10.1109/SP.2019.00052,NEUZZ: Efficient fuzzing with neural program smoothing,"Fuzzing has become the de facto standard technique for finding software vulnerabilities. However, even state-of-the-art fuzzers are not very efficient at finding hard-to-trigger software bugs. Most popular fuzzers use evolutionary guidance to generate inputs that can trigger different bugs. Such evolutionary algorithms, while fast and simple to implement, often get stuck in fruitless sequences of random mutations. Gradient-guided optimization presents a promising alternative to evolutionary guidance. Gradient-guided techniques have been shown to significantly outperform evolutionary algorithms at solving high-dimensional structured optimization problems in domains like machine learning by efficiently utilizing gradients or higher-order derivatives of the underlying function. However, gradient-guided approaches are not directly applicable to fuzzing as real-world program behaviors contain many discontinuities, plateaus, and ridges where the gradient-based methods often get stuck. We observe that this problem can be addressed by creating a smooth surrogate function approximating the target program's discrete branching behavior. In this paper, we propose a novel program smoothing technique using surrogate neural network models that can incrementally learn smooth approximations of a complex, real-world program's branching behaviors. We further demonstrate that such neural network models can be used together with gradient-guided input generation schemes to significantly increase the efficiency of the fuzzing process. Our extensive evaluations demonstrate that NEUZZ significantly outperforms 10 state-of-the-art graybox fuzzers on 10 popular real-world programs both at finding new bugs and achieving higher edge coverage. NEUZZ found 31 previously unknown bugs (including two CVEs) that other fuzzers failed to find in 10 real-world programs and achieved 3X more edge coverage than all of the tested graybox fuzzers over 24 hour runs. Furthermore, NEUZZ also outperformed existing fuzzers on both LAVA-M and DARPA CGC bug datasets. © 2019 IEEE.",Ontology | Requirement traceability matrix | SRS (Software Requirement Specification) document | Sunflower optimization | Support vector classifier,Lecture Notes in Networks and Systems,2021-01-01,Conference Paper,"Adithya, V.;Deepak, Gerard",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85059147076,10.1016/j.eswa.2018.12.033,Iterated feature selection algorithms with layered recurrent neural network for software fault prediction,"Software fault prediction (SFP) is typically used to predict faults in software components. Machine learning techniques (e.g., classification) are widely used to tackle this problem. With the availability of the huge amount of data that can be obtained from mining software historical repositories, it becomes possible to have some features (metrics) that are not correlated with the faults, which consequently mislead the learning algorithm and thus decrease its performance. One possible solution to eliminate those metrics is Feature Selection (FS). In this paper, a novel FS approach is proposed to enhance the performance of a layered recurrent neural network (L-RNN), which is used as a classification technique for the SFP problem. Three different wrapper FS algorithms (i.e, Binary Genetic Algorithm (BGA), Binary Particle Swarm Optimization (BPSO), and Binary Ant Colony Optimization (BACO)) were employed iteratively. To assess the performance of the proposed approach, 19 real-world software projects from PROMISE repository are investigated and the experimental results are discussed. Receiver operating characteristic - area under the curve (ROC-AUC) is used as a performance measure. The results are compared with other state-of-art approaches including Naïve Bayes (NB), Artificial Neural Network (ANN), logistic regression (LR), the k-nearest neighbors (k-NN) and C4.5 decision trees, in terms of area under the curve (AUC). Our results have demonstrated that the proposed approach can outperform other existing methods. © 2018 Elsevier Ltd",Feature selection | Layered recurrent neural network | Software fault prediction,Expert Systems with Applications,2019-05-15,Article,"Turabieh, Hamza;Mafarja, Majdi;Li, Xiaodong",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85029576000,10.1109/TETCI.2017.2699224,Dynamic Selection of Classifiers in Bug Prediction: An Adaptive Method,"In the last decades, the research community has devoted a lot of effort in the definition of approaches able to predict the defect proneness of source code files. Such approaches exploit several predictors (e.g., product or process metrics) and use machine learning classifiers to predict classes into buggy or not buggy, or provide the likelihood that a class will exhibit a fault in the near future. The empirical evaluation of all these approaches indicated that there is no machine learning classifier providing the best accuracy in any context, highlighting interesting complementarity among them. For these reasons ensemble methods have been proposed to estimate the bug-proneness of a class by combining the predictions of different classifiers. Following this line of research, in this paper we propose an adaptive method, named ASCI (Adaptive Selection of Classifiers in bug prediction), able to dynamically select among a set of machine learning classifiers the one which better predicts the bug-proneness of a class based on its characteristics. An empirical study conducted on 30 software systems indicates that ASCI exhibits higher performances than five different classifiers used independently and combined with the majority voting ensemble method. © 2017 IEEE.",Bug Prediction | classifier selection | ensemble techniques,IEEE Transactions on Emerging Topics in Computational Intelligence,2017-06-01,Article,"Di Nucci, Dario;Palomba, Fabio;Oliveto, Rocco;De Lucia, Andrea",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84880406382,10.1007/s13042-012-0095-7,Investigation of relationship between object-oriented metrics and change proneness,"Software is the heartbeat of modern day technology. In order to keep up with the pace of modern day expansion, change in any software is inevitable. Defects and enhancements are the two main reasons for a software change. The aim of this paper is to study the relationship between object oriented metrics and change proneness. Software prediction models based on these results can help us identify change prone classes of a software which would lead to more rigorous testing and better results. In the previous research, the use of machine learning methods for predicting faulty classes was found. However till date no study determines the effectiveness of machine learning methods for predicting change prone classes. Statistical and machine learning methods are two different techniques for software quality prediction. We evaluate and compare the performance of these machine learning methods with statistical method (logistic regression). The results are based on three chosen open source software, written in java language. The performance of the predicted models was evaluated using receiver operating characteristic analysis. The study shows that machine learning methods are comparable to regression techniques. Testing based on change proneness of a software leads to better quality by targeting the most change prone classes. Thus, the developed models can be used to reduce the probability of defect occurrence and we commit ourselves to better maintenance. © 2012 Springer-Verlag.",Change proneness | Empirical validation | Object-oriented metric | Receiver operating characteristics analysis | Software quality,International Journal of Machine Learning and Cybernetics,2013-08-01,Article,"Malhotra, Ruchika;Khanna, Megha",Include,
10.1016/j.infsof.2022.107128,2-s2.0-79953007594,10.1007/s11390-011-9439-0,Software defect detection with Rocus,"Software defect detection aims to automatically identify defective software modules for efficient software test in order to improve the quality of a software system. Although many machine learning methods have been successfully applied to the task, most of them fail to consider two practical yet important issues in software defect detection. First, it is rather difficult to collect a large amount of labeled training data for learning a well-performing model; second, in a software system there are usually much fewer defective modules than defect-free modules, so learning would have to be conducted over an imbalanced data set. In this paper, we address these two practical issues simultaneously by proposing a novel semi-supervised learning approach named Rocus. This method exploits the abundant unlabeled examples to improve the detection accuracy, as well as employs under-sampling to tackle the class-imbalance problem in the learning process. Experimental results of real-world software defect detection tasks show that Rocus is effective for software defect detection. Its performance is better than a semi-supervised learning method that ignores the class-imbalance nature of the task and a class-imbalance learning method that does not make effective use of unlabeled data. © 2011 Springer 2011 Springer Science+Business Media, LLC & Science Press, China.",class-imbalance | data mining | machine learning | semi-supervised learning | software defect detection,Journal of Computer Science and Technology,2011-03-01,Article,"Jiang, Yuan;Li, Ming;Zhou, Zhi Hua",Include,
10.1016/j.infsof.2022.107128,2-s2.0-69649092809,10.1109/TSMCA.2009.2020804,Evolutionary sampling and software quality modeling of high-assurance systems,"Software quality modeling for high-assurance systems, such as safety-critical systems, is adversely affected by the skewed distribution of fault-prone program modules. This sparsity of defect occurrence within the software system impedes training and performance of software quality estimation models. Data sampling approaches presented in data mining and machine learning literature can be used to address the imbalance problem. We present a novel genetic algorithm-based data sampling method, named Evolutionary Sampling, as a solution to improving software quality modeling for high-assurance systems. The proposed solution is compared with multiple existing data sampling techniques, including random undersampling, one-sided selection, Wilson's editing, random oversampling, cluster-based oversampling, Synthetic Minority Oversampling Technique (SMOTE), and Borderline-SMOTE. This paper involves case studies of two real-world software systems and builds C4.5- and RIPPER-based software quality models both before and after applying a given data sampling technique. It is empirically shown that Evolutionary Sampling improves performance of software quality models for high-assurance systems and is significantly better than most existing data sampling techniques. © 2009 IEEE.",Data sampling | Evolutionary computing | High-assurance system | Imbalanced data | Software metrics,"IEEE Transactions on Systems, Man, and Cybernetics Part A:Systems and Humans",2009-06-04,Article,"Drown, Dennis J.;Khoshgoftaar, Taghi M.;Seliya, Naeem",Include,
10.1016/j.infsof.2022.107128,2-s2.0-57449100163,10.1145/1390630.1390654,The probabilistic program dependence graph and its application to fault diagnosis,"This paper presents an innovative model of a program's internal behavior over a set of test inputs, called the probabilistic program dependence graph (PPDG), that facilitates probabilistic analysis and reasoning about uncertain program behavior, particularly that associated with faults. The PPDG is an augmentation of the structural dependences represented by a program dependence graph with estimates of statistical dependences between node states, which are computed from the test set. The PPDG is based on the established framework of probabilistic graphical models, which are widely used in applications such as medical diagnosis. This paper presents algorithms for constructing PPDGs and applying the PPDG to fault diagnosis. This paper also presents preliminary evidence indicating that PPDGs can facilitate fault localization and fault comprehension.",Fault diagnosis | Machine learning | Probabilistic graphical models | Program analysis,ISSTA'08: Proceedings of the 2008 International Symposium on Software Testing and Analysis 2008,2008-12-16,Conference Paper,"Baah, George K.;Podgurski, Andy;Harrold, Mary Jean",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85115877287,10.1109/ICACTE.2008.204,Application of random forest in predicting fault-prone classes,"There are available metrics for predicting fault prone classes, which may help software organizations for planning and performing testing activities. This may be possible due to proper allocation of resources on fault prone parts of the design and code of the software. Hence, importance and usefulness of such metrics is understandable, but empirical validation of these metrics is always a great challenge. Random Forest (RF) algorithm has been successfully applied for solving regression and classification problems in many applications. This paper evaluates the capability of RF algorithm in predicting fault prone software classes using open source software. The results indicate that the prediction performance of Random Forest is good. However, similar types of studies are required to be carried out in order to establish the acceptability of the RF model. © 2008 IEEE.",Data generation | Fault detection | Machine learning,Lecture Notes in Networks and Systems,2022-01-01,Conference Paper,"Vélez Sánchez, Hernando;Hurtado Cortés, Luini Leonardo",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84945184388,10.1109/icsm.2003.1235410,Mining the Maintenance History of a Legacy Software System,"A considerable amount of system maintenance experience can be found in bug tracking and source code configuration management systems. Data mining and machine learning techniques allow one to extract models from past experience that can be used in future predictions. By mining the software change record, one can therefore generate models that can be used in future maintenance activities. In this paper we present an example of such a model that represents a relation between pairs of files and show how it can be extracted from the software update records of a real world legacy system. We show how different sources of data can be used to extract sets of features useful in describing this model, as well as how results are affected by these different feature sets and their combinations. Our best results were obtained from text-based features, i.e. those extracted from words in the problem reports as opposed to syntactic structures in the source code.",,"IEEE International Conference on Software Maintenance, ICSM",2003-01-01,Conference Paper,"Sayyad Shirabad, Jelber;Lethbridge, Timothy C.;Matwin, Stan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85071904016,10.1145/3338906.3338961,Latent error prediction and fault localization for microservice applications by learning from system trace logs,"In the production environment, a large part of microservice failures are related to the complex and dynamic interactions and runtime environments, such as those related to multiple instances, environmental configurations, and asynchronous interactions of microservices. Due to the complexity and dynamism of these failures, it is often hard to reproduce and diagnose them in testing environments. It is desirable yet still challenging that these failures can be detected and the faults can be located at runtime of the production environment to allow developers to resolve them efficiently. To address this challenge, in this paper, we propose MEPFL, an approach of latent error prediction and fault localization for microservice applications by learning from system trace logs. Based on a set of features defined on the system trace logs, MEPFL trains prediction models at both the trace level and the microservice level using the system trace logs collected from automatic executions of the target application and its faulty versions produced by fault injection. The prediction models thus can be used in the production environment to predict latent errors, faulty microservices, and fault types for trace instances captured at runtime. We implement MEPFL based on the infrastructure systems of container orchestrator and service mesh, and conduct a series of experimental studies with two opensource microservice applications (one of them being the largest open-source microservice application to our best knowledge). The results indicate that MEPFL can achieve high accuracy in intraapplication prediction of latent errors, faulty microservices, and fault types, and outperforms a state-of-the-art approach of failure diagnosis for distributed systems. The results also show that MEPFL can effectively predict latent errors caused by real-world fault cases. © 2019 ACM.",Debugging | Error prediction | Fault localization | Machine learning | Microservices | Tracing,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Zhou, Xiang;Peng, Xin;Xie, Tao;Sun, Jun;Ji, Chao;Liu, Dewei;Xiang, Qilin;He, Chuan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85043332099,10.1016/j.compeleceng.2018.02.043,Empirical analysis of change metrics for software fault prediction,"A quality assurance activity, known as software fault prediction, can reduce development costs and improve software quality. The objective of this study is to investigate change metrics in conjunction with code metrics to improve the performance of fault prediction models. Experimental studies are performed on different versions of Eclipse projects and change metrics are extracted from the GIT repositories. In addition to the existing change metrics, several new change metrics are defined and collected from the Eclipse project repository. Machine learning algorithms are applied in conjunction with the change and source code metrics to build fault prediction models. The classification model with new change metrics performs better than the models using existing change metrics. In this work, the experimental results demonstrate that change metrics have a positive impact on the performance of fault prediction models, and high-performance models can be built with several change metrics. © 2018 Elsevier Ltd",Change log | Defect prediction | Eclipse | Metrics | Software fault prediction | Software quality,Computers and Electrical Engineering,2018-04-01,Article,"Choudhary, Garvit Rajesh;Kumar, Sandeep;Kumar, Kuldeep;Mishra, Alok;Catal, Cagatay",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84991409678,10.1007/s11219-016-9342-6,Tackling class overlap and imbalance problems in software defect prediction,"Software defect prediction (SDP) is a promising solution to save time and cost in the software testing phase for improving software quality. Numerous machine learning approaches have proven effective in SDP. However, the unbalanced class distribution in SDP datasets could be a problem for some conventional learning methods. In addition, class overlap increases the difficulty for the predictors to learn the defective class accurately. In this study, we propose a new SDP model which combines class overlap reduction and ensemble imbalance learning to improve defect prediction. First, the neighbor cleaning method is applied to remove the overlapping non-defective samples. The whole dataset is then randomly under-sampled several times to generate balanced subsets so that multiple classifiers can be trained on these data. Finally, these individual classifiers are assembled with the AdaBoost mechanism to build the final prediction model. In the experiments, we investigated nine highly unbalanced datasets selected from a public software repository and confirmed that the high rate of overlap between classes existed in SDP data. We assessed the performance of our proposed model by comparing it with other state-of-the-art methods including conventional SDP models, imbalance learning and data cleaning methods. Test results and statistical analysis show that the proposed model provides more reasonable defect prediction results and performs best in terms of G-mean and AUC among all tested models. © 2016, Springer Science+Business Media New York.",Class imbalance | Class overlap | Machine learning | Software defect prediction,Software Quality Journal,2018-03-01,Article,"Chen, Lin;Fang, Bin;Shang, Zhaowei;Tang, Yuanyan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84928923706,10.1002/stvr.1570,Defect prediction as a multiobjective optimization problem,"In this paper, we formalize the defect-prediction problem as a multiobjective optimization problem. Specifically, we propose an approach, coined as multiobjective defect predictor (MODEP), based on multiobjective forms of machine learning techniques - logistic regression and decision trees specifically - trained using a genetic algorithm. The multiobjective approach allows software engineers to choose predictors achieving a specific compromise between the number of likely defect-prone classes or the number of defects that the analysis would likely discover (effectiveness), and lines of code to be analysed/tested (which can be considered as a proxy of the cost of code inspection). Results of an empirical evaluation on 10 datasets from the PROMISE repository indicate the quantitative superiority of MODEP with respect to single-objective predictors, and with respect to trivial baseline ranking classes by size in ascending or descending order. Also, MODEP outperforms an alternative approach for cross-project prediction, based on local prediction upon clusters of similar classes. Copyright © 2015 John Wiley & Sons, Ltd.",cost-effectiveness | cross-project defect prediction | defect prediction | multiobjective optimization,Software Testing Verification and Reliability,2015-06-01,Article,"Canfora, Gerardo;De Lucia, Andrea;Di Penta, Massimiliano;Oliveto, Rocco;Panichella, Annibale;Panichella, Sebastiano",Include,
10.1016/j.infsof.2022.107128,2-s2.0-80051550183,10.1016/j.ins.2011.06.017,Class noise detection based on software metrics and ROC curves,"Noise detection for software measurement datasets is a topic of growing interest. The presence of class and attribute noise in software measurement datasets degrades the performance of machine learning-based classifiers, and the identification of these noisy modules improves the overall performance. In this study, we propose a noise detection algorithm based on software metrics threshold values. The threshold values are obtained from the Receiver Operating Characteristic (ROC) analysis. This paper focuses on case studies of five public NASA datasets and details the construction of Naive Bayes-based software fault prediction models both before and after applying the proposed noise detection algorithm. Experimental results show that this noise detection approach is very effective for detecting the class noise and that the performance of fault predictors using a Naive Bayes algorithm with a logNum filter improves if the class labels of identified noisy modules are corrected. © 2011 Elsevier Inc. All rights reserved.",Metric threshold values | Noise detection | Receiver Operating Characteristic (ROC) curve | Software fault prediction | Software metrics | Software quality,Information Sciences,2011-11-01,Article,"Catal, Cagatay;Alan, Oral;Balkan, Kerime",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85030785829,10.1145/3106237.3117771,Automated identification of security issues from commit messages and bug reports,"The number of vulnerabilities in open source libraries is increasing rapidly. However, the majority of them do not go through public disclosure. These unidentified vulnerabilities put developers' products at risk of being hacked since they are increasingly relying on open source libraries to assemble and build software quickly. To find unidentified vulnerabilities in open source libraries and secure modern software development, we describe an efficient automatic vulnerability identification system geared towards tracking large-scale projects in real time using natural language processing and machine learning techniques. Built upon the latent information underlying commit messages and bug reports in open source projects using GitHub, JIRA, and Bugzilla, our K-fold stacking classiffer achieves promising results on vulnerability identification. Compared to the state of the art SVM-based classiffer in prior work on vulnerability identification in commit messages, we improve precision by 54.55% while maintaining the same recall rate. For bug reports, we achieve a much higher precision of 0.70 and recall rate of 0.71 compared to existing work. Moreover, observations fromrunning the trained model at SourceClear in production for over 3 months has shown 0.83 precision, 0.74 recall rate, and detected 349 hidden vulnerabilities, proving the efffectiveness and generality of the proposed approach. © 2017 Association for Computing Machinery.",Bug report | Commit | Machine learning | Vulnerability identification,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2017-08-21,Conference Paper,"Zhou, Yaqin;Sharma, Asankhaya",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85123288786,10.1007/s10586-018-1696-z,Deep neural network based hybrid approach for software defect prediction using software metrics,"In the field of early prediction of software defects, various techniques have been developed such as data mining techniques, machine learning techniques. Still early prediction of defects is a challenging task which needs to be addressed and can be improved by getting higher classification rate of defect prediction. With the aim of addressing this issue, we introduce a hybrid approach by combining genetic algorithm (GA) for feature optimization with deep neural network (DNN) for classification. An improved version of GA is incorporated which includes a new technique for chromosome designing and fitness function computation. DNN technique is also improvised using adaptive auto-encoder which provides better representation of selected software features. The improved efficiency of the proposed hybrid approach due to deployment of optimization technique is demonstrated through case studies. An experimental study is carried out for software defect prediction by considering PROMISE dataset using MATLAB tool. In this study, we have used the proposed novel method for classification and defect prediction. Comparative study shows that the proposed approach of prediction of software defects performs better when compared with other techniques where 97.82% accuracy is obtained for KC1 dataset, 97.59% accuracy is obtained for CM1 dataset, 97.96% accuracy is obtained for PC3 dataset and 98.00% accuracy is obtained for PC4 dataset. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",DevSecOps | Machine learning | Security | Software development,Communications in Computer and Information Science,2021-01-01,Conference Paper,"Yadav, Bhawna;Choudhary, Gaurav;Shandilya, Shishir Kumar;Dragoni, Nicola",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85051566880,10.1145/3213846.3213848,Compiler fuzzing through deep learning,"Random program generation-fuzzing-is an effective technique for discovering bugs in compilers but successful fuzzers require extensive development effort for every language supported by the compiler, and often leave parts of the language space untested. We introduce DeepSmith, a novel machine learning approach to accelerating compiler validation through the inference of generative models for compiler inputs. Our approach infers a learned model of the structure of real world code based on a large corpus of open source code. Then, it uses the model to automatically generate tens of thousands of realistic programs. Finally, we apply established differential testing methodologies on them to expose bugs in compilers. We apply our approach to the OpenCL programming language, automatically exposing bugs with little effort on our side. In 1,000 hours of automated testing of commercial and open source compilers, we discover bugs in all of them, submitting 67 bug reports. Our test cases are on average two orders of magnitude smaller than the state-of-the-art, require 3.03× less time to generate and evaluate, and expose bugs which the state-of-the-art cannot. Our random program generator, comprising only 500 lines of code, took 12 hours to train for OpenCL versus the state-of-the-art taking 9 man months to port from a generator for C and 50,000 lines of code. With 18 lines of code we extended our program generator to a second language, uncovering crashes in Solidity compilers in 12 hours of automated testing. © 2018 Association for Computing Machinery.",Compiler Fuzzing | Deep Learning | Differential Testing,ISSTA 2018 - Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis,2018-07-12,Conference Paper,"Cummins, Chris;Petoumenos, Pavlos;Murray, Alastair;Leather, Hugh",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84891894965,10.1007/s10115-013-0721-z,Detecting software design defects using relational association rule mining,"In this paper, we are approaching, from a machine learning perspective, the problem of automatically detecting defective software entities (classes and methods) in existing software systems, a problem of major importance during software maintenance and evolution. In order to improve the internal quality of a software system, identifying faulty entities such as classes, modules, methods is essential for software developers. As defective software entities are hard to identify, machine learning-based classification models are still developed to approach the problem of detecting software design defects. We are proposing a novel method based on relational association rule mining for detecting faulty entities in existing software systems. Relational association rules are a particular type of association rules and describe numerical orderings between attributes that commonly occur over a dataset. Our method is based on the discovery of relational association rules for identifying design defects in software. Experiments on open source software are conducted in order to detect defective classes in object-oriented software systems, and a comparison of our approach with similar existing approaches is provided. The obtained results show that our method is effective for software design defect detection and confirms the potential of our proposal. © 2014, Springer-Verlag London.",Association rule mining | Data mining | Defect detection | Machine learning | Software design,Knowledge and Information Systems,2015-03-01,Article,"Czibula, Gabriela;Marian, Zsuzsanna;Czibula, Istvan Gergely",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85062806210,10.1016/j.neucom.2018.04.090,An empirical study to investigate oversampling methods for improving software defect prediction using imbalanced data,"Software defect prediction is important to identify defects in the early phases of software development life cycle. This early identification and thereby removal of software defects is crucial to yield a cost-effective and good quality software product. Though, previous studies have successfully used machine learning techniques for software defect prediction, these techniques yield biased results when applied on imbalanced data sets. An imbalanced data set has non-uniform class distribution with very few instances of a specific class as compared to that of the other class. Use of imbalanced datasets leads to off-target predictions of the minority class, which is generally considered to be more important than the majority class. Thus, handling imbalanced data effectively is crucial for successful development of a competent defect prediction model. This study evaluates the effectiveness of machine learning classifiers for software defect prediction on twelve imbalanced NASA datasets by application of sampling methods and cost sensitive classifiers. We investigate five existing oversampling methods, which replicate the instances of minority class and also propose a new method SPIDER3 by suggesting modifications in SPIDER2 oversampling method. Furthermore, the work evaluates the performance of MetaCost learners for cost sensitive learning on imbalanced datasets. The results show improvement in the prediction capability of machine learning classifiers with the use of oversampling methods. Furthermore, the proposed SPIDER3 method shows promising results. © 2019",Defect prediction | Imbalanced data | Machine learning techniques | MetaCost learners | Oversampling methods | Procedural metrics,Neurocomputing,2019-05-28,Article,"Malhotra, Ruchika;Kamal, Shine",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85049504779,10.14569/IJACSA.2018.090212,Software Bug Prediction using machine learning approach,"Software Bug Prediction (SBP) is an important issue in software development and maintenance processes, which concerns with the overall of software successes. This is because predicting the software faults in earlier phase improves the software quality, reliability, efficiency and reduces the software cost. However, developing robust bug prediction model is a challenging task and many techniques have been proposed in the literature. This paper presents a software bug prediction model based on machine learning (ML) algorithms. Three supervised ML algorithms have been used to predict future software faults based on historical data. These classifiers are Naïve Bayes (NB), Decision Tree (DT) and Artificial Neural Networks (ANNs). The evaluation process showed that ML algorithms can be used effectively with high accuracy rate. Furthermore, a comparison measure is applied to compare the proposed prediction model with other approaches. The collected results showed that the ML approach has a better performance. © 2015 The Science and Information (SAI) Organization Limited.",Artificial Neural Networks (ANNs) | Decision Tree (DT) | Faults prediction | Machine learning | Naïve Bayes (NB) | Prediction model | Software bug prediction,International Journal of Advanced Computer Science and Applications,2018-01-01,Article,"Hammouri, Awni;Hammad, Mustafa;Alnabhan, Mohammad;Alsarayrah, Fatima",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84940725639,10.1007/s10664-014-9349-1,Should I follow this fault localization tool’s output?: Automated prediction of fault localization effectiveness,"Debugging is a crucial yet expensive activity to improve the reliability of software systems. To reduce debugging cost, various fault localization tools have been proposed. A spectrum-based fault localization tool often outputs an ordered list of program elements sorted based on their likelihood to be the root cause of a set of failures (i.e., their suspiciousness scores). Despite the many studies on fault localization, unfortunately, however, for many bugs, the root causes are often low in the ordered list. This potentially causes developers to distrust fault localization tools. Recently, Parnin and Orso highlight in their user study that many debuggers do not find fault localization useful if they do not find the root cause early in the list. To alleviate the above issue, we build an oracle that could predict whether the output of a fault localization tool can be trusted or not. If the output is not likely to be trusted, developers do not need to spend time going through the list of most suspicious program elements one by one. Rather, other conventional means of debugging could be performed. To construct the oracle, we extract the values of a number of features that are potentially related to the effectiveness of fault localization. Building upon advances in machine learning, we process these feature values to learn a discriminative model that is able to predict the effectiveness of a fault localization tool output. In this work, we consider an output of a fault localization tool to be effective if the root cause appears in the top 10 most suspicious program elements. We have evaluated our proposed oracle on 200 faulty versions of Space, NanoXML, XML-Security, and the 7 programs in Siemens test suite. Our experiments demonstrate that we could predict the effectiveness of 9 fault localization tools with a precision, recall, and F-measure (harmonic mean of precision and recall) of up to 74.38 %, 90.00 % and 81.45 %, respectively. The numbers indicate that many ineffective fault localization instances are identified correctly, while only few effective ones are identified wrongly. © 2014, Springer Science+Business Media New York.",Classification | Effectiveness prediction | Fault localization,Empirical Software Engineering,2015-10-04,Article,"Le, Tien Duy B.;Lo, David;Thung, Ferdian",Include,
10.1016/j.infsof.2022.107128,2-s2.0-37349077705,10.1016/j.eswa.2007.02.012,Regression via Classification applied on software defect estimation,"In this paper we apply Regression via Classification (RvC) to the problem of estimating the number of software defects. This approach apart from a certain number of faults, it also outputs an associated interval of values, within which this estimate lies with a certain confidence. RvC also allows the production of comprehensible models of software defects exploiting symbolic learning algorithms. To evaluate this approach we perform an extensive comparative experimental study of the effectiveness of several machine learning algorithms in two software data sets. RvC manages to get better regression error than the standard regression approaches on both datasets. © 2007 Elsevier Ltd. All rights reserved.",ISBSG data set | Machine learning | Regression via Classification | Software fault estimation | Software metrics | Software quality,Expert Systems with Applications,2008-04-01,Article,"Bibi, S.;Tsoumakas, G.;Stamelos, I.;Vlahavas, I.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85048483561,10.1109/ICTAI.2017.00019,Convolutional neural networks over control flow graphs for software defect prediction,"Existing defects in software components is unavoidable and leads to not only a waste of time and money but also many serious consequences. To build predictive models, previous studies focus on manually extracting features or using tree representations of programs, and exploiting different machine learning algorithms. However, the performance of the models is not high since the existing features and tree structures often fail to capture the semantics of programs. To explore deeply programs' semantics, this paper proposes to leverage precise graphs representing program execution flows, and deep neural networks for automatically learning defect features. Firstly, control flow graphs are constructed from the assembly instructions obtained by compiling source code; we thereafter apply multi-view multi-layer directed graph-based convolutional neural networks (DGCNNs) to learn semantic features. The experiments on four real-world datasets show that our method significantly outperforms the baselines including several other deep learning approaches. © 2017 IEEE.",Control-Flow-Graphs | Convolutional-Neural-Networks | Software-Defect-Prediction,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",2017-07-02,Conference Paper,"Viet Phan, Anh;Le Nguyen, Minh;Thu Bui, Lam",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85018470097,10.1109/TSMC.2016.2521840,Fuzzy Rule-Based Approach for Software Fault Prediction,"Knowing faulty modules prior to testing makes testing more effective and helps to obtain reliable software. Here, we develop a framework for automatic extraction of human understandable fuzzy rules for software fault detection/classification. This is an integrated framework to simultaneously identify useful determinants (attributes) of faults and fuzzy rules using those attributes. At the beginning of the training, the system assumes every attribute (feature) as a useless feature and then uses a concept of feature attenuating gate to select useful features. The learning process opens the gates or closes them more tightly based on utility of the features. Our system can discard derogatory and indifferent attributes and select the useful ones. It can also exploit subtle nonlinear interaction between attributes. In order to demonstrate the effectiveness of the framework, we have used several publicly available software fault data sets and compared the performance of our method with that of some existing methods. The results using tenfold cross-validation setup show that our system can find useful fuzzy rules for fault prediction. © 2016 IEEE.",Feature modulating gates | fuzzy rule generation | machine learning | software fault prediction | software metric selection,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",2017-05-01,Article,"Singh, Pradeep;Pal, Nikhil R.;Verma, Shrish;Vyas, Om Prakash",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84997207340,10.1109/TSE.2016.2553030,Comments on Researcher Bias: The Use of Machine Learning in Software Defect Prediction,"Shepperd et al. find that the reported performance of a defect prediction model shares a strong relationship with the group of researchers who construct the models. In this paper, we perform an alternative investigation of Shepperd et al.'s data. We observe that (a) research group shares a strong association with other explanatory variables (i.e., the dataset and metric families that are used to build a model); (b) the strong association among these explanatory variables makes it difficult to discern the impact of the research group on model performance; and (c) after mitigating the impact of this strong association, we find that the research group has a smaller impact than the metric family. These observations lead us to conclude that the relationship between the research group and the performance of a defect prediction model are more likely due to the tendency of researchers to reuse experimental components (e.g., datasets and metrics). We recommend that researchers experiment with a broader selection of datasets and metrics to combat any potential bias in their results. © 2016 IEEE.",defect prediction | researcher bias | Software quality assurance,IEEE Transactions on Software Engineering,2016-11-01,Article,"Tantithamthavorn, Chakkrit;McIntosh, Shane;Hassan, Ahmed E.;Matsumoto, Kenichi",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84969802903,10.1016/j.eswa.2016.05.018,Deriving thresholds of software metrics to predict faults on open source software: Replicated case studies,"Object-oriented metrics aim to exhibit the quality of source code and give insight to it quantitatively. Each metric assesses the code from a different aspect. There is a relationship between the quality level and the risk level of source code. The objective of this paper is to empirically examine whether or not there are effective threshold values for source code metrics. It is targeted to derive generalized thresholds that can be used in different software systems. The relationship between metric thresholds and fault-proneness was investigated empirically in this study by using ten open-source software systems. Three types of fault-proneness were defined for the software modules: non-fault-prone, more-than-one-fault-prone, and more-than-three-fault-prone. Two independent case studies were carried out to derive two different threshold values. A single set was created by merging ten datasets and was used as training data by the model. The learner model was created using logistic regression and the Bender method. Results revealed that some metrics have threshold effects. Seven metrics gave satisfactory results in the first case study. In the second case study, eleven metrics gave satisfactory results. This study makes contributions primarily for use by software developers and testers. Software developers can see classes or modules that require revising; this, consequently, contributes to an increment in quality for these modules and a decrement in their risk level. Testers can identify modules that need more testing effort and can prioritize modules according to their risk levels. © 2016 Elsevier Ltd. All rights reserved.",Bender method | Logistic regression | Machine learning | Software fault prediction | Software quality metrics | Threshold,Expert Systems with Applications,2016-11-01,Article,"Arar, Ömer Faruk;Ayan, Kürşat",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84921354062,10.1016/j.asoc.2016.04.032,An empirical framework for defect prediction using machine learning techniques with Android software,"Context Software defect prediction is important for identification of defect-prone parts of a software. Defect prediction models can be developed using software metrics in combination with defect data for predicting defective classes. Various studies have been conducted to find the relationship between software metrics and defect proneness, but there are few studies that statistically determine the effectiveness of the results. Objective The main objectives of the study are (i) comparison of the machine-learning techniques using data sets obtained from popular open source software (ii) use of appropriate performance measures for measuring the performance of defect prediction models (iii) use of statistical tests for effective comparison of machine-learning techniques and (iv) validation of models over different releases of data sets. Method In this study we use object-oriented metrics for predicting defective classes using 18 machine-learning techniques. The proposed framework has been applied to seven application packages of well known, widely used Android operating system viz. Contact, MMS, Bluetooth, Email, Calendar, Gallery2 and Telephony. The results are validated using 10-fold and inter-release validation methods. The reliability and significance of the results are evaluated using statistical test and post-hoc analysis. Results The results show that the area under the curve measure for Naïve Bayes, LogitBoost and Multilayer Perceptron is above 0.7 in most of the cases. The results also depict that the difference between the ML techniques is statistically significant. However, it is also proved that the Support Vector Machines based techniques such as Support Vector Machines and voted perceptron do not possess the predictive capability for predicting defects. Conclusion The results confirm the predictive capability of various ML techniques for developing defect prediction models. The results also confirm the superiority of one ML technique over the other ML techniques. Thus, the software engineers can use the results obtained from this study in the early phases of the software development for identifying defect-prone classes of given software. © 2016 Elsevier B.V.",Cloud software industry | Customer defection | Decision tree | Machine learning | Random forest,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2014-01-01,Article,"Martono, Niken Prasasti;Kanamori, Katsutoshi;Ohwada, Hayato",Include,
10.1016/j.infsof.2022.107128,2-s2.0-79959909996,10.1145/1993498.1993543,Isolating and understanding concurrency errors using reconstructed execution fragments,"In this paper we propose Recon, a new general approach to concurrency debugging. Recon goes beyond just detecting bugs, it also presents to the programmer short fragments of buggy execution schedules that illustrate how and why bugs happened. These fragments, called reconstructions, are inferred from inter-thread communication surrounding the root cause of a bug and significantly simplify the process of understanding bugs. The key idea in Recon is to monitor executions and build graphs that encode inter-thread communication with enough context information to build reconstructions. Recon leverages reconstructions built from multiple application executions and uses machine learning to identify which ones illustrate the root cause of a bug. Recon's approach is general because it does not rely on heuristics specific to any type of bug, application, or programming model. Therefore, it is able to deal with single- and multiple-variable concurrency bugs regardless of their type (e.g., atomicity violation, ordering, etc). To make graph collection efficient, Recon employs selective monitoring and allows metadata information to be imprecise without compromising accuracy. With these optimizations, Recon's graph collection imposes overheads typically between 5x and 20x for both C/C++ and Java programs, with overheads as low as 13% in our experiments. We evaluate Recon with buggy applications, and show it produces reconstructions that include all code points involved in bugs' causes, and presents them in an accurate order. We include a case study of understanding and fixing a previously unresolved bug to showcase Recon's effectiveness. © 2011 ACM.",concurrency | multithreading | statistical debugging,Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI),2011-01-01,Conference Paper,"Lucia, Brandon;Wood, Benjamin P.;Ceze, Luis",Include,
10.1016/j.infsof.2022.107128,2-s2.0-35148866968,,Software defect prediction using artificial immune recognition system,"Predicting fault-prone modules for software development projects enables companies to reach high reliable systems and minimizes necessary budget, personnel and resource to be allocated to achieve this goal. Researchers have investigated various statistical techniques and machine learning algorithms until now but most of them applied their models to the different datasets which are not public or used different criteria to decide the best predictor model. Artificial Immune Recognition System is a supervised learning algorithm which has been proposed in 2001 for the classification problems and its performance for UCI datasets (University of California machine learning repository) is remarkable. In this paper, we propose a novel software defect prediction model by applying Artificial Immune Recognition System (AIRS) along with the Correlation-Based Feature Selection (CFS) technique. In order to evaluate the performance of the proposed model, we apply it to the five NASA public defect datasets and compute G-mean1, G-mean2 and F-measure values to discuss the effectiveness of the model. Experimental results show that AIRS has a great potential for software defect prediction and AIRS along with CFS technique provides relatively better prediction for large scale projects which consist of many modules.",Artificial immune recognition system (AIRS) and correlation-based feature selection; Immune systems; Quality prediction; Software defect prediction,"Proceedings of the IASTED International Conference on Software Engineering, SE 2007",2007,,"Catal C., Diri B.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85068541005,10.1016/j.infsof.2019.07.003,Improving defect prediction with deep forest,"Context: Software defect prediction is important to ensure the quality of software. Nowadays, many supervised learning techniques have been applied to identify defective instances (e.g., methods, classes, and modules). Objective: However, the performance of these supervised learning techniques are still far from satisfactory, and it will be important to design more advanced techniques to improve the performance of defect prediction models. Method: We propose a new deep forest model to build the defect prediction model (DPDF). This model can identify more important defect features by using a new cascade strategy, which transforms random forest classifiers into a layer-by-layer structure. This design takes full advantage of ensemble learning and deep learning. Results: We evaluate our approach on 25 open source projects from four public datasets (i.e., NASA, PROMISE, AEEEM and Relink). Experimental results show that our approach increases AUC value by 5% compared with the best traditional machine learning algorithms. Conclusion: The deep strategy in DPDF is effective for software defect prediction. © 2019 Elsevier B.V.",Cascade strategy | Deep forest | Empirical evaluation | Software defect prediction,Information and Software Technology,2019-10-01,Article,"Zhou, Tianchi;Sun, Xiaobing;Xia, Xin;Li, Bin;Chen, Xiang",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84863587280,10.1145/2245276.2231967,A further analysis on the use of genetic algorithm to configure support vector machines for inter-release fault prediction,"Some studies have reported promising results on the use of Support Vector Machines (SVMs) for predicting fault-prone software components. Nevertheless, the performance of the method heavily depends on the setting of some parameters. To address this issue, we investigated the use of a Genetic Algorithm (GA) to search for a suitable configuration of SVMs to be used for inter-release fault prediction. In particular, we report on an assessment of the method on five software systems. As benchmarks we exploited SVMs with random and Grid-search configuration strategies and several other machine learning techniques. The results show that the combined use of GA and SVMs is effective for inter-release fault prediction. © 2012 ACM.",fault prediction | genetic algorithm | support vector machines,Proceedings of the ACM Symposium on Applied Computing,2012-07-12,Conference Paper,"Sarro, F.;Di Martino, S.;Ferrucci, F.;Gravino, C.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85026529437,10.1109/EUROMICRO.2006.56,Software defect identification using machine learning techniques,"Software engineering is a tedious job that includes people, tight deadlines and limited budgets. Delivering what customer wants involves minimizing the defects in the programs. Hence, it is important to establish quality measures early on in the project life cycle. The main objective of this research is to analyze problems in software code and propose a model that will help catching those problems earlier in the project life cycle. Our proposed model uses machine learning methods. Principal Component Analysis is used for dimensionality reduction, and Decision Tree, Multi Layer Perceptron and Radial Basis Functions are used for defect prediction. The experiments in this research are carried out with different software metric datasets that are obtained from real-life projects of three big software companies in Turkey. We can say that, the improved method that we proposed brings out satisfactory results in terms of defect prediction. © 2006 IEEE.",Bug fix | Clustering techniques | Co-occurrence models | Duplicate data records | Mining software repositories | Software maintenance | Vector space model,Data Science and Big Data Computing: Frameworks and Methodologies,2016-01-01,Book Chapter,"Pasala, Anjaneyulu;Guha, Sarbendu;Agnihotram, Gopichand;Prateek B, Satya;Padmanabhuni, Srinivas",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85065180275,10.1155/2019/6230953,Software Defect Prediction via Attention-Based Recurrent Neural Network,"In order to improve software reliability, software defect prediction is applied to the process of software maintenance to identify potential bugs. Traditional methods of software defect prediction mainly focus on designing static code metrics, which are input into machine learning classifiers to predict defect probabilities of the code. However, the characteristics of these artificial metrics do not contain the syntactic structures and semantic information of programs. Such information is more significant than manual metrics and can provide a more accurate predictive model. In this paper, we propose a framework called defect prediction via attention-based recurrent neural network (DP-ARNN). More specifically, DP-ARNN first parses abstract syntax trees (ASTs) of programs and extracts them as vectors. Then it encodes vectors which are used as inputs of DP-ARNN by dictionary mapping and word embedding. After that, it can automatically learn syntactic and semantic features. Furthermore, it employs the attention mechanism to further generate significant features for accurate defect prediction. To validate our method, we choose seven open-source Java projects in Apache, using F1-measure and area under the curve (AUC) as evaluation criteria. The experimental results show that, in average, DP-ARNN improves the F1-measure by 14% and AUC by 7% compared with the state-of-the-art methods, respectively. © 2019 Guisheng Fan et al.",,Scientific Programming,2019-01-01,Article,"Fan, Guisheng;Diao, Xuyang;Yu, Huiqun;Yang, Kang;Chen, Liqiong",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84992451628,10.1016/j.procs.2015.02.161,Open issues in software defect prediction,"Software Defect Prediction (SDP) is one of the most assisting activities of the Testing Phase of SDLC. It identifies the modules that are defect prone and require extensive testing. This way, the testing resources can be used efficiently without violating the constraints. Though SDP is very helpful in testing, it's not always easy to predict the defective modules. There are various issues that hinder the smooth performance as well as use of the Defect Prediction models. In this report, we have distinguished some of the major issues of SDP and studied what has been done so far to address them. © 2015 The Authors.",,Studies in Computational Intelligence,2017-01-01,Book Chapter,"Grahn, Kaj;Westerlund, Magnus;Pulkkis, GÖran",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84989313936,10.1145/1985793.1985950,Topic-based defect prediction (NIER track),"Defects are unavoidable in software development and fixing them is costly and resource-intensive. To build defect prediction models, researchers have investigated a number of factors related to the defect-proneness of source code, such as code complexity, change complexity, or socio-technical factors. In this paper, we propose a new approach that emphasizes on technical concerns/functionality of a system. In our approach, a software system is viewed as a collection of software artifacts that describe different technical concerns/-aspects. Those concerns are assumed to have different levels of defect-proneness, thus, cause different levels of defectproneness to the relevant software artifacts. We use topic modeling to measure the concerns in source code, and use them as the input for machine learning-based defect prediction models. Preliminary result on Eclipse JDT shows that the topic-based metrics have high correlation to the number of bugs (defect-proneness), and our topic-based defect prediction has better predictive performance than existing state-of-the-art approaches. © 2011 ACM.",Approximate dynamic programming | Data center | Multi-objective optimization | SaaS cloud | Task scheduling,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2016-01-01,Conference Paper,"Zhang, Puheng;Lin, Chuang;Ma, Xiao;Ren, Fengyuan;Li, Wenzhuo",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85120130347,10.1145/3360588,Improving bug detection via context-based code representation learning and attention-based neural networks,"Bug detection has been shown to be an effective way to help developers in detecting bugs early, thus, saving much effort and time in software development process. Recently, deep learning-based bug detection approaches have gained successes over the traditional machine learning-based approaches, the rule-based program analysis approaches, and mining-based approaches. However, they are still limited in detecting bugs that involve multiple methods and suffer high rate of false positives. In this paper, we propose a combination approach with the use of contexts and attention neural network to overcome those limitations. We propose to use as the global context the Program Dependence Graph (PDG) and Data Flow Graph (DFG) to connect the method under investigation with the other relevant methods that might contribute to the buggy code. The global context is complemented by the local context extracted from the path on the AST built from the method's body. The use of PDG and DFG enables our model to reduce the false positive rate, while to complement for the potential reduction in recall, we make use of the attention neural network mechanism to put more weights on the buggy paths in the source code. That is, the paths that are similar to the buggy paths will be ranked higher, thus, improving the recall of our model. We have conducted several experiments to evaluate our approach on a very large dataset with +4.973M methods in 92 different project versions. The results show that our tool can have a relative improvement up to 160% on F-score when comparing with the state-of-the-art bug detection approaches. Our tool can detect 48 true bugs in the list of top 100 reported bugs, which is 24 more true bugs when comparing with the baseline approaches. We also reported that our representation is better suitable for bug detection and relatively improves over the other representations up to 206% in accuracy. © 2019 Association for Computing Machinery. All rights reserved.",Attention Neural Networks | Bug Detection | Code Representation Learning | Deep Learning | Network Embedding | Program Graphs,Proceedings of the ACM on Programming Languages,2019-10-01,Article,"Li, Yi;Wang, Shaohua;Nguyen, Tien N.;Van Nguyen, Son",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85020846211,10.1111/j.1468-0394.2009.00509.x,Unlabelled extra data do not always mean extra performance for semi-supervised fault prediction,"This research focused on investigating and benchmarking several high performance classifiers called J48, random forests, naive Bayes, KStar and artificial immune recognition systems for software fault prediction with limited fault data. We also studied a recent semi-supervised classification algorithm called YATSI (Yet Another Two Stage Idea) and each classifier has been used in the first stage of YATSI. YATSI is a meta algorithm which allows different classifiers to be applied in the first stage. Furthermore, we proposed a semi-supervised classification algorithm which applies the artificial immune systems paradigm. Experimental results showed that YATSI does not always improve the performance of naive Bayes when unlabelled data are used together with labelled data. According to experiments we performed, the naive Bayes algorithm is the best choice to build a semi-supervised fault prediction model for small data sets and YATSI may improve the performance of naive Bayes for large data sets. In addition, the YATSI algorithm improved the performance of all the classifiers except naive Bayes on all the data sets. © 2009 Blackwell Publishing Ltd.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017-01-01,Conference Paper,"Florea, Adrian Cătălin;Anvik, John;Andonie, Răzvan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-48249094941,10.1007/978-3-540-69566-0_21,A fault prediction model with limited fault data to improve test process,"Software fault prediction models are used to identify the fault-prone software modules and produce reliable software. Performance of a software fault prediction model is correlated with available software metrics and fault data. In some occasions, there may be few software modules having fault data and therefore, prediction models using only labeled data can not provide accurate results. Semi-supervised learning approaches which benefit from unlabeled and labeled data may be applied in this case. In this paper, we propose an artificial immune system based semi-supervised learning approach. Proposed approach uses a recent semi-supervised algorithm called YATSI (Yet Another Two Stage Idea) and in the first stage of YATSI, AIRS (Artificial Immune Recognition Systems) is applied. In addition, AIRS, RF (Random Forests) classifier, AIRS based YATSI, and RF based YATSI are benchmarked. Experimental results showed that while YATSI algorithm improved the performance of AIRS, it diminished the performance of RF for unbalanced datasets. Furthermore, performance of AIRS based YATSI is comparable with RF which is the best machine learning classifier according to some researches. © 2008 Springer-Verlag Berlin Heidelberg.",AIRS | Artificial immune systems | Semi-supervised learning | Software fault prediction | YATSI,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2008-08-04,Conference Paper,"Catal, Cagatay;Diri, Banu",Include,
10.1016/j.infsof.2022.107128,2-s2.0-38049126044,10.1007/978-3-540-74958-5_5,Statistical debugging using latent topic models,"Statistical debugging uses machine learning to model program failures and help identify root causes of bugs. We approach this task using a novel Delta-Latent-Dirichlet-Allocation model. We model execution traces attributed to failed runs of a program as being generated by two types of latent topics: normal usage topics and bug topics. Execution traces attributed to successful runs of the same program, however, are modeled by usage topics only. Joint modeling of both kinds of traces allows us to identify weak bug topics that would otherwise remain undetected. We perform model inference with collapsed Gibbs sampling. In quantitative evaluations on four real programs, our model produces bug topics highly correlated to the true bugs, as measured by the Rand index. Qualitative evaluation by domain experts suggests that our model outperforms existing statistical methods for bug cause identification, and may help support other software tasks not addressed by earlier models. © Springer-Verlag Berlin Heidelberg 2007.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2007-01-01,Conference Paper,"Andrzejewski, David;Mulhern, Anne;Liblit, Ben;Zhu, Xiaojin",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85029722379,10.1016/S0164-1212(96)00153-7,Evaluating predictive quality models derived from software measures: Lessons learned,"This paper describes an empirical comparison of several modeling techniques for predicting the quality of software components early in the software life cycle. Using software product measures, we built models that classify components as high-risk, i.e., likely to contain faults, or low-risk, i.e., likely to be free of faults. The modeling techniques evaluated in this study include principal component analysis, discriminant analysis, logistic regression, logical classification models, layered neural networks, and holographic networks. These techniques provide a good coverage of the main problem-solving paradigms: statistical analysis, machine learning, and neural networks. Using the results of independent testing, we determined the absolute worth of the predictive models and compare their performance in terms of misclassification errors, achieved quality, and verification cost. Data came from 27 software systems, developed and tested during three years of project-intensive academic courses. A surprising result is that no model was able to effectively discriminate between components with faults and components without faults. © 1997 Elsevier Science Inc.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017-01-01,Conference Paper,"Amir-Khalili, Alborz;Kianzad, Soheil;Abugharbieh, Rafeef;Beschastnikh, Ivan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85041823642,10.1109/MSR.2019.00016,DeepJIT: An end-to-end deep learning framework for just-in-time defect prediction,"Software quality assurance efforts often focus on identifying defective code. To find likely defective code early, change-level defect prediction - aka. Just-In-Time (JIT) defect prediction - has been proposed. JIT defect prediction models identify likely defective changes and they are trained using machine learning techniques with the assumption that historical changes are similar to future ones. Most existing JIT defect prediction approaches make use of manually engineered features. Unlike those approaches, in this paper, we propose an end-to-end deep learning framework, named DeepJIT, that automatically extracts features from commit messages and code changes and use them to identify defects. Experiments on two popular software projects (i.e., QT and OPENSTACK) on three evaluation settings (i.e., cross-validation, short-period, and long-period) show that the best variant of DeepJIT (DeepJIT-Combined), compared with the best performing state-of-the-art approach, achieves improvements of 10.36-11.02% for the project QT and 9.51-13.69% for the project OPENSTACK in terms of the Area Under the Curve (AUC). © 2019 IEEE.",,Advances in Intelligent Systems and Computing,2018-01-01,Conference Paper,"Ciancarini, Paolo;Poggi, Francesco;Rossi, Davide;Sillitti, Alberto",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85046017853,10.1007/s11227-018-2326-5,A dimensionality reduction-based efficient software fault prediction using Fisher linear discriminant analysis (FLDA),"Software quality is an important factor in the success of software companies. Traditional software quality assurance techniques face some serious limitations especially in terms of time and budget. This leads to increase in the use of machine learning classification techniques to predict software faults. Software fault prediction can help developers to uncover software problems in early stages of software life cycle. The extent to which these techniques can be generalized to different sizes of software, class imbalance problem, and identification of discriminative software metrics are the most critical challenges. In this paper, we have analyzed the performance of nine widely used machine learning classifiers—Bayes Net, NB, artificial neural network, support vector machines, K nearest neighbors, AdaBoost, Bagging, Zero R, and Random Forest for software fault prediction. Two standard sampling techniques—SMOTE and Resample with substitution are used to handle the class imbalance problem. We further used FLDA-based feature selection approach in combination with SMOTE and Resample to select most discriminative metrics. Then the top four classifiers based on performance are used for software fault prediction. The experimentation is carried out over 15 publically available datasets (small, medium and large) which are collected from PROMISE repository. The proposed Resample-FLDA method gives better performance as compared to existing methods in terms of precision, recall, f-measure and area under the curve. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",Fault-tolerance | Fisher linear discriminant | Reliability | Robustness | Software fault prediction,Journal of Supercomputing,2018-09-01,Article,"Kalsoom, Anum;Maqsood, Muazzam;Ghazanfar, Mustansar Ali;Aadil, Farhan;Rho, Seungmin",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85034231451,10.1109/ACCESS.2017.2771460,Evaluating data filter on cross-project defect prediction: Comparison and improvements,"Cross-project defect prediction (CPDP) is a field of study where a software project lacking enough local data can use data from other projects to build defect predictors. To support CPDP, the cross-project data must be carefully filtered before being applied locally. Researchers have devised and implemented a plethora of various data filters for the improvement of CPDP performance. However, it is still unclear what data filter strategy is most effective, both generally and specifically, in CPDP. The objective of this paper is to provide an extensive comparison of well-known data filters and a novel filter devised in this paper. We perform experiments on 44 releases of 14 open-source projects, and use Naive Bayes and a support vector machine as the underlying classifier. The results demonstrate that the data filter strategy improves the performance of cross-project defect prediction significantly, and the hierarchical select-based filter proposed performs significantly better. Moreover, when using appropriate data filter strategy, the defect predictor built from cross-project data can outperform the predictor learned by using within-project data. © 2017 IEEE.",Cross-project defect prediction | Data filter | Machine learning | Software quality assurance,IEEE Access,2017-11-08,Article,"Li, Yong;Huang, Zhiqiu;Wang, Yong;Fang, Bingwu",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84961775086,10.1007/s00607-016-0489-6,A decision tree logic based recommendation system to select software fault prediction techniques,"Identifying a reliable fault prediction technique is the key requirement for building effective fault prediction model. It has been found that the performance of fault prediction techniques is highly dependent on the characteristics of the fault dataset. To mitigate this issue, researchers have evaluated and compared a plethora of fault prediction techniques by varying the context in terms of domain information, characteristics of input data, complexity, etc. However, the lack of an accepted benchmark makes it difficult to select fault prediction technique for a particular context of prediction. In this paper, we present a recommendation system that facilitates the selection of appropriate technique(s) to build fault prediction model. First, we have reviewed the literature to elicit the various characteristics of the fault dataset and the appropriateness of the machine learning and statistical techniques for the identified characteristics. Subsequently, we have formalized our findings and built a recommendation system that helps in the selection of fault prediction techniques. We performed an initial appraisal of our presented system and found that proposed recommendation system provides useful hints in the selection of the fault prediction techniques. © 2016, Springer-Verlag Wien.",Decision tree | Recommendation system | Software fault prediction | Software fault prediction techniques,Computing,2017-03-01,Article,"Rathore, Santosh S.;Kumar, Sandeep",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85047659967,10.1016/j.infsof.2013.05.002,A study of subgroup discovery approaches for defect prediction,"Context Although many papers have been published on software defect prediction techniques, machine learning approaches have yet to be fully explored. Objective In this paper we suggest using a descriptive approach for defect prediction rather than the precise classification techniques that are usually adopted. This allows us to characterise defective modules with simple rules that can easily be applied by practitioners and deliver a practical (or engineering) approach rather than a highly accurate result. Method We describe two well-known subgroup discovery algorithms, the SD algorithm and the CN2-SD algorithm to obtain rules that identify defect prone modules. The empirical work is performed with publicly available datasets from the Promise repository and object-oriented metrics from an Eclipse repository related to defect prediction. Subgroup discovery algorithms mitigate against characteristics of datasets that hinder the applicability of classification algorithms and so remove the need for preprocessing techniques. Results The results show that the generated rules can be used to guide testing effort in order to improve the quality of software development projects. Such rules can indicate metrics, their threshold values and relationships between metrics of defective modules. Conclusions The induced rules are simple to use and easy to understand as they provide a description rather than a complete classification of the whole dataset. Thus this paper represents an engineering approach to defect prediction, i.e., an approach which is useful in practice, easily understandable and can be applied by practitioners. © 2013 Elsevier B.V. All rights reserved.",,"Studies in Systems, Decision and Control",2019-01-01,Book Chapter,"Elhoseny, Mohamed;Hassanien, Aboul Ella",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85049787816,10.1109/CSMR.2013.43,A comparative study of supervised learning algorithms for re-opened bug prediction,"Bug fixing is a time-consuming and costly job which is performed in the whole life cycle of software development and maintenance. For many systems, bugs are managed in bug management systems such as Bugzilla. Generally, the status of a typical bug report in Bugzilla changes from new to assigned, verified and closed. However, some bugs have to be reopened. Reopened bugs increase the software development and maintenance cost, increase the workload of bug fixers, and might even delay the future delivery of a software. Only a few studies investigate the phenomenon of reopened bug reports. In this paper, we evaluate the effectiveness of various supervised learning algorithms to predict if a bug report would be reopened. We choose 7 state-of-the-art classical supervised learning algorithm in machine learning literature, i.e., kNN, SVM, Simple Logistic, Bayesian Network, Decision Table, CART and LWL, and 3 ensemble learning algorithms, i.e., AdaBoost, Bagging and Random Forest, and evaluate their performance in predicting reopened bug reports. The experiment results show that among the 10 algorithms, Bagging and Decision Table (IDTM) achieve the best performance. They achieve accuracy scores of 92.91% and 92.80%, respectively, and reopened bug reports F-Measure scores of 0.735 and 0.732, respectively. These results improve the reopened bug reports F-Measure of the state-of-the-art approaches proposed by Shihab et al. by up to 23.53%. © 2013 IEEE.",Bug report | Machine learning | Natural language processing | Security bug identification,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,Conference Paper,"Zou, Deqing;Deng, Zhijun;Li, Zhen;Jin, Hai",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84926473479,10.1111/exsy.12078,Fault prediction considering threshold effects of object-oriented metrics,"Software product quality can be enhanced significantly if we have a good knowledge and understanding of the potential faults therein. This paper describes a study to build predictive models to identify parts of the software that have high probability of occurrence of fault. We have considered the effect of thresholds of object-oriented metrics on fault proneness and built predictive models based on the threshold values of the metrics used. Prediction of fault prone classes in earlier phases of software development life cycle will help software developers in allocating the resources efficiently. In this paper, we have used a statistical model derived from logistic regression to calculate the threshold values of object oriented, Chidamber and Kemerer metrics. Thresholds help developers to alarm the classes that fall outside a specified risk level. In this way, using the threshold values, we can divide the classes into two levels of risk - low risk and high risk. We have shown threshold effects at various risk levels and validated the use of these thresholds on a public domain, proprietary dataset, KC1 obtained from NASA and two open source, Promise datasets, IVY and JEdit using various machine learning methods and data mining classifiers. Interproject validation has also been carried out on three different open source datasets, Ant and Tomcat and Sakura. This will provide practitioners and researchers with well formed theories and generalised results. The results concluded that the proposed threshold methodology works well for the projects of similar nature or having similar characteristics. © 2014 Wiley Publishing Ltd.",empirical validation | logistic regression | machine learning | object-oriented metrics | receiver operating characteristics | software quality,Expert Systems,2015-04-01,Article,"Malhotra, Ruchika;Bansal, Ankita Jain",Include,
10.1016/j.infsof.2022.107128,2-s2.0-34547676503,10.1145/1188895.1188911,On-line anomaly detection of deployed software: A statistical machine learning approach,"This paper presents a new machine-learning technique that performs anomaly detection as software is executing in the field. The technique uses a fully observable Markov model where each state in the model emits a number of distinct observations according to a probability distribution, and estimates the model parameters using the Baum-Welch algorithm. The trained model is then deployed with the software to perform anomaly detection. By performing the anomaly detection as the software is executing, faults associated with anomalies can be located and fixed before they cause critical failures in the system, and developers time to debug deployed software can be reduced. This paper also presents a prototype implementation of our technique, along with a case study that shows, for the subjects we studied, the effectiveness of the technique. Copyright 2006 ACM.",Anomaly detection | Anomaly diagnosis | Fault localization | Machine learning | Markov models,"Proceedings of the Third International Workshop on Software Quality Assurance, SOQUA 2006",2006-12-01,Conference Paper,"Baah, George K.;Gray, Alexander;Harrold, Mary Jean",Include,
10.1016/j.infsof.2022.107128,2-s2.0-35448959626,10.1016/j.infsof.2018.10.001,Automatically identifying code features for software defect prediction: Using AST N-grams,"Context: Identifying defects in code early is important. A wide range of static code metrics have been evaluated as potential defect indicators. Most of these metrics offer only high level insights and focus on particular pre-selected features of the code. None of the currently used metrics clearly performs best in defect prediction. Objective: We use Abstract Syntax Tree (AST) n-grams to identify features of defective Java code that improve defect prediction performance. Method: Our approach is bottom-up and does not rely on pre-selecting any specific features of code. We use non-parametric testing to determine relationships between AST n-grams and faults in both open source and commercial systems. We build defect prediction models using three machine learning techniques. Results: We show that AST n-grams are very significantly related to faults in some systems, with very large effect sizes. The occurrence of some frequently occurring AST n-grams in a method can mean that the method is up to three times more likely to contain a fault. AST n-grams can have a large effect on the performance of defect prediction models. Conclusions: We suggest that AST n-grams offer developers a promising approach to identifying potentially defective code. © 2018 Elsevier B.V.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2007-01-01,Conference Paper,"Thrun, Sebastian",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85041529359,10.1007/s10586-018-1730-1,Software defect prediction techniques using metrics based on neural network classifier,"Software industries strive for software quality improvement by consistent bug prediction, bug removal and prediction of fault-prone module. This area has attracted researchers due to its significant involvement in software industries. Various techniques have been presented for software defect prediction. Recent researches have recommended data-mining using machine learning as an important paradigm for software bug prediction. state-of-art software defect prediction task suffer from various issues such as classification accuracy. However, software defect datasets are imbalanced in nature and known fault prone due to its huge dimension. To address this issue, here we present a combined approach for software defect prediction and prediction of software bugs. Proposed approach delivers a concept of feature reduction and artificial intelligence where feature reduction is carried out by well-known principle component analysis (PCA) scheme which is further improved by incorporating maximum-likelihood estimation for error reduction in PCA data reconstruction. Finally, neural network based classification technique is applied which shows prediction results. A framework is formulated and implemented on NASA software dataset where four datasets i.e., KC1, PC3, PC4 and JM1 are considered for performance analysis using MATLAB simulation tool. An extensive experimental study is performed where confusion, precision, recall, classification accuracy etc. parameters are computed and compared with existing software defect prediction techniques. Experimental study shows that proposed approach can provide better performance for software defect prediction. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",Defect prediction models | Machine learning techniques | Software defect prediction | Software metrics,Cluster Computing,2019-01-16,Article,"Jayanthi, R.;Florence, Lilly",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84891692177,10.1109/ICSM.2013.42,Will fault localization work for these failures? An automated approach to predict effectiveness of fault localization tools,"Debugging is a crucial yet expensive activity to improve the reliability of software systems. To reduce debugging cost, various fault localization tools have been proposed. A spectrum-based fault localization tool often outputs an ordered list of program elements sorted based on their likelihood to be the root cause of a set of failures (i.e., their suspiciousness scores). Despite the many studies on fault localization, unfortunately, however, for many bugs, the root causes are often low in the ordered list. This potentially causes developers to distrust fault localization tools. Recently, Parnin and Orso highlight in their user study that many debuggers do not find fault localization useful if they do not find the root cause early in the list. To alleviate the above issue, we build an oracle that could predict whether the output of a fault localization tool can be trusted or not. If the output is not likely to be trusted, developers do not need to spend time going through the list of most suspicious program elements one by one. Rather, other conventional means of debugging could be performed. To construct the oracle, we extract the values of a number of features that are potentially related to the effectiveness of fault localization. Building upon advances in machine learning, we process these feature values to learn a discriminative model that is able to predict the effectiveness of a fault localization tool output. In this preliminary work, we consider an output of a fault localization tool to be effective if the root cause appears in the top 10 most suspicious program elements. We have experimented our proposed oracle on 200 faulty programs from Space, NanoXML, XML-Security, and the 7 programs in Siemens test suite. Our experiments demonstrate that we could predict the effectiveness of fault localization tool with a precision, recall, and F-measure (harmonic mean of precision and recall) of 54.36%, 95.29%, and 69.23%. The numbers indicate that many ineffective fault localization instances are identified correctly, while only very few effective ones are identified wrongly. © 2013 IEEE.",Classification | Effectiveness Prediction | Fault Localization,"IEEE International Conference on Software Maintenance, ICSM",2013-12-01,Conference Paper,"Le, Tien Duy B.;Lo, David",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84886410516,10.14257/ijseia.2013.7.5.16,Combining particle swarm optimization based feature selection and bagging technique for software defect prediction,"The costs of finding and correcting software defects have been the most expensive activity in software development. The accurate prediction of defect-prone software modules can help the software testing effort, reduce costs, and improve the software testing process by focusing on fault-prone module. Recently, static code attributes are used as defect predictors in software defect prediction research, since they are useful, generalizable, easy to use, and widely used. However, two common aspects of data quality that can affect performance of software defect prediction are class imbalance and noisy attributes. In this research, we propose the combination of particle swarm optimization and bagging technique for improving the accuracy of the software defect prediction. Particle swarm optimization is applied to deal with the feature selection, and bagging technique is employed to deal with the class imbalance problem. The proposed method is evaluated using the data sets from NASA metric data repository. Results have indicated that the proposed method makes an impressive improvement in prediction performance for most classifiers ©2013 SERSC.",Bagging | Feature selection | Machine learning | Particle swarm optimization | Software defect prediction,International Journal of Software Engineering and its Applications,2013-10-30,Article,"Wahono, Romi Satria;Suryana, Nanna",Include,
10.1016/j.infsof.2022.107128,2-s2.0-80052719428,10.1109/INM.2011.5990536,Mining unstructured log files for recurrent fault diagnosis,"Enterprise software systems are large and complex with limited support for automated root-cause analysis. Avoiding system downtime and loss of revenue dictates a fast and efficient root-cause analysis process. Operator practice and academic research have shown that about 80% of failures in such systems have recurrent causes; therefore, significant efficiency gains can be achieved by automating their identification. In this paper, we present a novel approach to modelling features of log files. This model offers a compact representation of log data that can be efficiently extracted from large amounts of monitoring data. We also use decision-tree classifiers to learn and classify symptoms of recurrent faults. This representation enables automated fault matching and, in addition, enables human investigators to understand manifestations of failure easily. Our model does not require any access to application source code, a specification of log messages, or deep application knowledge. We evaluate our proposal using fault-injection experiments against other proposals in the field. First, we show that the features needed for symptom definition can be extracted more efficiently than does related work. Second, we show that these features enable an accurate classification of recurrent faults using only standard machine learning techniques. This enables us to identify accurately up to 78% of the faults in our evaluation data set. © 2011 IEEE.",,"Proceedings of the 12th IFIP/IEEE International Symposium on Integrated Network Management, IM 2011",2011-09-19,Conference Paper,"Reidemeister, Thomas;Jiang, Miao;Ward, Paul A.S.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85035008892,10.1016/j.infsof.2017.11.005,Software metrics thresholds calculation techniques to predict fault-proneness: An empirical comparison,"Context: Nowadays, fault-proneness prediction is an important field of software engineering. It can be used by developers and testers to prioritize tests. This would allow a better allocation of resources, reducing testing time and costs, and improving the effectiveness of software testing. Non-supervised fault-proneness prediction models, especially thresholds-based models, can easily be automated and give valuable insights to developers and testers on the classification performed. Objective: In this paper, we investigated three thresholds calculation techniques that can be used for fault-proneness prediction: ROC Curves, VARL (Value of an Acceptable Risk Level) and Alves rankings. We compared the performance of these techniques with the performance of four machine learning and two clustering based models. Method: Threshold values were calculated on a total of twelve different public datasets: eleven from the PROMISE Repository and another based on the Eclipse project. Thresholds-based models were then constructed using each thresholds calculation technique investigated. For comparison, results were also computed for supervised machine learning and clustering based models. Inter-dataset experimentation between different systems and versions of a same system was performed. Results: Results show that ROC Curves is the best performing method among the three thresholds calculation methods investigated, closely followed by Alves Rankings. VARL method didn't give valuable results for most of the datasets investigated and was easily outperformed by the two other methods. Results also show that thresholds-based models using ROC Curves outperformed machine learning and clustering based models. Conclusion: The best of the three thresholds calculation techniques for fault-proneness prediction is ROC Curves, but Alves Rankings is a good choice too. In fact, the advantage of Alves Rankings over ROC Curves technique is that it is completely unsupervised and can therefore give pertinent threshold values when fault data is not available. © 2017 Elsevier B.V.",Class-level metrics | Clustering | Code quality | Cross-validation | Fault-proneness prediction | Faults | Machine learning | Metrics thresholds | Object-oriented metrics | Object-oriented programming,Information and Software Technology,2018-04-01,Article,"Boucher, Alexandre;Badri, Mourad",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84979650015,10.1016/j.asoc.2016.06.023,Classification with reject option for software defect prediction,"Context Software defect prediction (SDP) is an important task in software engineering. Along with estimating the number of defects remaining in software systems and discovering defect associations, classifying the defect-proneness of software modules plays an important role in software defect prediction. Several machine-learning methods have been applied to handle the defect-proneness of software modules as a classification problem. This type of “yes” or “no” decision is an important drawback in the decision-making process and if not precise may lead to misclassifications. To the best of our knowledge, existing approaches rely on fully automated module classification and do not provide a way to incorporate extra knowledge during the classification process. This knowledge can be helpful in avoiding misclassifications in cases where system modules cannot be classified in a reliable way. Objective We seek to develop a SDP method that (i) incorporates a reject option in the classifier to improve the reliability in the decision-making process; and (ii) makes it possible postpone the final decision related to rejected modules for an expert analysis or even for another classifier using extra domain knowledge. Method We develop a SDP method called rejoELM and its variant, IrejoELM. Both methods were built upon the weighted extreme learning machine (ELM) with reject option that makes it possible postpone the final decision of non-classified modules, the rejected ones, to another moment. While rejoELM aims to maximize the accuracy for a rejection rate, IrejoELM maximizes the F-measure. Hence, IrejoELM becomes an alternative for classification with reject option for imbalanced datasets. Results rejoEM and IrejoELM are tested on five datasets of source code metrics extracted from real world open-source software projects. Results indicate that rejoELM has an accuracy for several rejection rates that is comparable to some state-of-the-art classifiers with reject option. Although IrejoELM shows lower accuracies for several rejection rates, it clearly outperforms all other methods when the F-measure is used as a performance metric. Conclusion It is concluded that rejoELM is a valid alternative for classification with reject option problems when classes are nearly equally represented. On the other hand, IrejoELM is shown to be the best alternative for classification with reject option on imbalanced datasets. Since SDP problems are usually characterized as imbalanced learning problems, the use of IrejoELM is recommended. © 2016 Elsevier B.V.",Classification with reject option | Extreme learning machines | Software defect prediction,Applied Soft Computing Journal,2016-12-01,Article,"Mesquita, Diego P.P.;Rocha, Lincoln S.;Gomes, João Paulo P.;Rocha Neto, Ajalmar R.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84879478680,10.1109/ICMV.2009.54,Early software fault prediction using real time defect data,"Quality of a software component can be measured in terms of fault proneness of data. Quality estimations are made using fault proneness data available from previously developed similar type of projects and the training data consisting of software measurements. To predict faulty modules in software data different techniques have been proposed which includes statistical method, machine learning methods, neural network techniques and clustering techniques. The aim of proposed approach is to investigate that whether metrics available in the early lifecycle (i.e. requirement metrics), metrics available in the late lifecycle (i.e. code metrics) and metrics available in the early lifecycle (i.e. requirement metrics) combined with metrics available in the late lifecycle (i.e. code metrics) can be used to identify fault prone modules by using clustering techniques. This approach has been tested with three real time defect datasets of NASA software projects, JM1, PC1 and CM1. Predicting faults early in the software life cycle can be used to improve software process control and achieve high software reliability. The results show that when all the prediction techniques are evaluated, the best prediction model is found to be the fusion of requirement and code metric model.",Bug report | Machine learning | Task assignment | UJP | UNICASE | Unified model,Communications in Computer and Information Science,2011-12-01,Conference Paper,"Helming, Jonas;Arndt, Holger;Hodaie, Zardosht;Koegel, Maximilian;Narayan, Nitesh",Include,
10.1016/j.infsof.2022.107128,2-s2.0-82255164591,10.3390/app9102138,An improved CNN model for within-project software defect prediction,"To improve software reliability, software defect prediction is used to find software bugs and prioritize testing efforts. Recently, some researchers introduced deep learning models, such as the deep belief network (DBN) and the state-of-the-art convolutional neural network (CNN), and used automatically generated features extracted from abstract syntax trees (ASTs) and deep learning models to improve defect prediction performance. However, the research on the CNN model failed to reveal clear conclusions due to its limited dataset size, insufficiently repeated experiments, and outdated baseline selection. To solve these problems, we built the PROMISE Source Code (PSC) dataset to enlarge the original dataset in the CNN research, which we named the Simplified PROMISE Source Code (SPSC) dataset. Then, we proposed an improved CNN model for within-project defect prediction (WPDP) and compared our results to existing CNN results and an empirical study. Our experiment was based on a 30-repetition holdout validation and a 10 * 10 cross-validation. Experimental results showed that our improved CNN model was comparable to the existing CNN model, and it outperformed the state-of-the-art machine learning models significantly for WPDP. Furthermore, we defined hyperparameter instability and examined the threat and opportunity it presents for deep learning models on defect prediction. © 2019 by the authors.",Basis Learning | Classification | Multi-Modality | Multi-view Learning | Optimization | Tensor factorization,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2011-10-11,Conference Paper,"Batmanghelich, Nematollah;Dong, Aoyan;Taskar, Ben;Davatzikos, Christos",Include,
10.1016/j.infsof.2022.107128,2-s2.0-80052400545,10.1016/j.eswa.2018.07.042,A novel software defect prediction based on atomic class-association rule mining,"To ensure the rational allocation of software testing resources and reduce costs, software defect prediction has drawn notable attention to many “white-box” and “black-box” classification algorithms. Although there have been lots of studies on using software product metrics to identify defect-prone modules, defect prediction algorithms are still worth exploring. For instance, it is not easy to directly implement the Apriori algorithm to classify defect-prone modules across a skewed dataset. Therefore, we propose a novel supervised approach for software defect prediction based on atomic class-association rule mining (ACAR). It holds the characteristics of only one feature of the antecedent and a unique class label of the consequent, which is a specific kind of association rules that explores the relationship between attributes and categories. It holds the characteristics of only one feature of the antecedent and a unique class label of the consequent, which is a specific kind of association rules that explores the relationship between attributes and categories. Such association patterns can provide meaningful knowledge that can be easily understood by software engineers. A new software defect prediction model infrastructure based on association rules is employed to improve the prediction of defect-prone modules, which is divided into data preprocessing, rule model building and performance evaluation. Moreover, ACAR can achieve a satisfactory classification performance compared with other seven benchmark learners (the extension of classification based on associations (CBA2), Support Vector Machine, Naive Bayesian, Decision Tree, OneR, K-nearest Neighbors and RIPPER) on NASA MDP and PROMISE datasets. In light of software defect associative prediction, a comparative experiment between ACAR and CBA2 is discussed in details. It is demonstrated that ACAR is better than CBA2 in terms of AUC, G-mean, Balance, and understandability. In addition, the average AUC of ACAR is increased by 2.9% compared with CBA2, which can reach 81.1%. © 2018 Elsevier Ltd",classification | segmentation | statistical data compression | x86 binary disassembly,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2011-09-09,Conference Paper,"Wartell, Richard;Zhou, Yan;Hamlen, Kevin W.;Kantarcioglu, Murat;Thuraisingham, Bhavani",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85030760696,10.1109/ACCESS.2017.2759180,Integrated Approach to Software Defect Prediction,"Software defect prediction provides actionable outputs to software teams while contributing to industrial success. Empirical studies have been conducted on software defect prediction for both cross-project and within-project defect prediction. However, existing studies have yet to demonstrate a method of predicting the number of defects in an upcoming product release. This paper presents such a method using predictor variables derived from the defect acceleration, namely, the defect density, defect velocity, and defect introduction time, and determines the correlation of each predictor variable with the number of defects. We report the application of an integrated machine learning approach based on regression models constructed from these predictor variables. An experiment was conducted on ten different data sets collected from the PROMISE repository, containing 22 838 instances. The regression model constructed as a function of the average defect velocity achieved an adjusted R-square of 98.6%, with a p-value of < 0.001. The average defect velocity is strongly positively correlated with the number of defects, with a correlation coefficient of 0.98. Thus, it is demonstrated that this technique can provide a blueprint for program testing to enhance the effectiveness of software development activities. © 2013 IEEE.",class imbalance | defect velocity | machine learning | number of defects | Software defect prediction,IEEE Access,2017-10-04,Article,"Felix, Ebubeogu Amarachukwu;Lee, Sai Peck",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84862197943,10.1109/ICCAE.2010.5451695,A model for early prediction of faults in software systems,"Quality of a software component can be measured in terms of fault proneness of data. Quality estimations are made using fault proneness data available from previously developed similar type of projects and the training data consisting of software measurements. To predict faulty modules in software data different techniques have been proposed which includes statistical method, machine learning methods, neural network techniques and clustering techniques. Predicting faults early in the software life cycle can be used to improve software process control and achieve high software reliability. The aim of proposed approach is to investigate that whether metrics available in the early lifecycle (i.e. requirement metrics), metrics available in the late lifecycle (i.e. code metrics) and metrics available in the early lifecycle (i.e. requirement metrics) combined with metrics available in the late lifecycle (i.e. code metrics) can be used to identify fault prone modules using decision tree based Model in combination of K-means clustering as preprocessing technique. This approach has been tested with CM1 real time defect datasets of NASA software projects. The high accuracy of testing results show that the proposed Model can be used for the prediction of the fault proneness of software modules early in the software life cycle. ©2010 IEEE.",Automated Software Engineering (ASE) | Empirical Software Engineering and Measurements (ESEM) | Mining Software Repositories (MSR),Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2012-06-18,Conference Paper,"Sureka, Ashish",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84884951159,10.1145/1540438.1540466,Misclassification cost-sensitive fault prediction models,"Traditionally, software fault prediction models are built by assuming a uniform misclassification cost. In other words, cost implications of misclassifying a faulty module as fault free are assumed to be the same as the cost implications of misclassifying a fault free module as faulty. In reality, these two types of misclassification costs are rarely equal. They are project-specific, reflecting the characteristics of the domain in which the program operates. In this paper, using project information from a public repository, we analyze the benefits of techniques which incorporate misclassification costs in the development of software fault prediction models. We find that cost-sensitive learning does not provide operational points which outperform cost-insensitive classifiers. However, an advantage of cost-sensitive modeling is the explicit choice of the operational threshold appropriate for the cost differential. © ACM 2009.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2013-10-08,Conference Paper,"Yao, Xin",Include,
10.1016/j.infsof.2022.107128,2-s2.0-57649221639,10.1109/ICTAI.2008.76,Predicting fault proneness of classes trough a multiobjective particle swarm optimization algorithm,"Software testing is a fundamental software engineering activity for quality assurance that is also traditionally very expensive. To reduce efforts of testing strategies, some design metrics have been used to predict the fault-proneness of a software class or module. Recent works have explored the use of Machine Learning (ML) techniques for fault prediction. However most used ML techniques can not deal with unbalanced data and their results usually have a difficult interpretation. Because of this, this paper introduces a Multi-Objective Particle Swarm Optimization (MOPSO) algorithm for fault prediction. It allows the creation of classifiers composed by rules with specific properties by exploring Pareto dominance concepts. These rules are more intuitive and easier to understand because they can be interpreted independently one of each other. Furthermore, an experiment using the approach is presented and the results are compared to the other techniques explored in the area. ©2008 IEEE.",,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",2008-12-22,Conference Paper,"De Carvalho, Andre B.;Pozo, Aurora;Vergilio, Silvia;Lenz, Alexandre",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85075254797,10.1016/j.eswa.2019.113085,BPDET: An effective software bug prediction model using deep representation and ensemble learning techniques,"In software fault prediction systems, there are many hindrances for detecting faulty modules, such as missing values or samples, data redundancy, irrelevance features, and correlation. Many researchers have built a software bug prediction (SBP) model, which classify faulty and non-faulty module which are associated with software metrics. Till now very few works has been done which addresses the class imbalance problem in SBP. The main objective of this paper is to reveal the favorable result by feature selection and machine learning methods to detect defective and non-defective software modules. We propose a rudimentary classification based framework Bug Prediction using Deep representation and Ensemble learning (BPDET) techniques for SBP. It combinedly applies by ensemble learning (EL) and deep representation(DR). The software metrics which are used for SBP are mostly conventional. Staked denoising auto-encoder (SDA) is used for the deep representation of software metrics, which is a robust feature learning method. Propose model is mainly divided into two stages: deep learning stage and two layers of EL stage (TEL). The extraction of the feature from SDA in the very first step of the model then applied TEL in the second stage. TEL is also dealing with the class imbalance problem. The experiment mainly performed NASA (12) datasets, to reveal the efficiency of DR, SDA, and TEL. The performance is analyzed in terms of Mathew co-relation coefficient (MCC), the area under the curve (AUC), precision-recall area (PRC), F-measure and Time. Out of 12 dataset MCC values over 11 datasets, ROC values over 6 datasets, PRC values overall 12 datasets and F-measure over 8 datasets surpass the existing state of the art bug prediction methods. We have tested BPDET using Wilcoxon rank sum test which rejects the null hypothesis at α = 0.025. We have also tested the stability of the model over 5, 8, 10, 12, and 15 fold cross-validation and got similar results. Finally, we conclude that BPDET is a stable and outperformed on most of the datasets compared with EL and another state of the art techniques. © 2019 Elsevier Ltd",Boosting | Classification technique | Deep representation | Heterogeneous Ensemble learning technique | Software bug prediction | Software metrics | Staked denoising auto-encoder,Expert Systems with Applications,2020-04-15,Article,"Pandey, Sushant Kumar;Mishra, Ravi Bhushan;Tripathi, Anil Kumar",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85050314865,10.1109/ICTCS.2017.39,Software defect prediction using feature selection and random forest algorithm,"Software testing is the most important task in software production and it takes a lot of time, cost and effort. Thus, we need to reduce these resources. Software Defect Prediction (SDP) mechanisms are used to enhance the work of SQA process through the prediction of defective modules, many approaches have been conducted by researchers in order to predict the fault-proneness modules. This paper proposed an approach for the SDP purpose, it employs two existed algorithms to have a high performance, that are the Bat-based search Algorithm (BA) for the feature selection process, and the Random Forest algorithm (RF) for the prediction purpose. This paper also has tested a number of feature selection algorithms and classifiers to see their effectiveness in this problem. © 2017 IEEE.",Bat search algorithm (BA) | Classification | Feature selection (FS) | Machine learning | Metaheuristic algorithms | Random forest (RF) | Software defect prediction (SDP),"Proceedings - 2017 International Conference on New Trends in Computing Sciences, ICTCS 2017",2017-07-01,Conference Paper,"Ibrahim, Dyana Rashid;Ghnemat, Rawan;Hudaib, Amjad",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85021707156,10.1109/CONFLUENCE.2017.7943255,Software defect prediction analysis using machine learning algorithms,"Software Quality is the most important aspect of a software. Software Defect Prediction can directly affect quality and has achieved significant popularity in last few years. Defective software modules have a massive impact over software's quality leading to cost overruns, delayed timelines and much higher maintenance costs. In this paper we have analyzed the most popular and widely used Machine Learning algorithms - ANN (Artificial Neural Network), PSO(P article Swarm Optimization), DT (Decision Trees), NB(Naive Bayes) and LC (Linear classifier). The five algorithms were analyzed using KEEL tool and validated using k-fold cross validation technique. Datasets used in this research were obtained from open source NASA Promise dataset repository. Seven datasets were selected for defect prediction analysis. Classification was performed on these 7 datasets and validated using 10 fold cross validation. The results demonstrated the dominance of Linear Classifier over other algorithms in terms of defect prediction accuracy. © 2017 IEEE.",Defect Prediction | KEEL | Machine Learning | NASA Promise dataset | Software Quality,"Proceedings of the 7th International Conference Confluence 2017 on Cloud Computing, Data Science and Engineering",2017-06-07,Conference Paper,"Deep Singh, Praman;Chug, Anuradha",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84978251061,10.1007/978-3-319-42089-9_44,A public bug database of GitHub projects and its application in bug prediction,"Detecting defects in software systems is an evergreen topic, since there is no real world software without bugs. Many different bug locating algorithms have been presented recently that can help to detect hidden and newly occurred bugs in software. Papers trying to predict the faulty source code elements or code segments in the system always use experience from the past. In most of the cases these studies construct a database for their own purposes and do not make the gathered data publicly available. Public datasets are rare; however, a well constructed dataset could serve as a benchmark test input. Furthermore, open-source software development is rapidly increasing that also gives an opportunity to work with public data. In this study we selected 15 Java projects from GitHub to construct a public bug database from. We matched the already known and fixed bugs with the corresponding source code elements (classes and files) and calculated a wide set of product metrics on these elements. After creating the desired bug database, we investigated whether the built database is usable for bug prediction. We used 13 machine learning algorithms to address this research question and finally we achieved F-measure values between 0.7 and 0.8. Beside the F-measure values we calculated the bug coverage ratio on every project for every machine learning algorithm. We obtained very high and promising bug coverage values (up to 100%). © Springer International Publishing Switzerland 2016.",Bug database | Bug prediction,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2016-01-01,Conference Paper,"Tóth, Zoltán;Gyimesi, Péter;Ferenc, Rudolf",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85027095691,10.3233/IFS-141220,An improved semi-supervised learning method for software defect prediction,"This paper presents an improved semi-supervised learning approach for defect prediction involving class imbalanced and limited labeled data problem. This approach employs random under-sampling technique to resample the original training set and updating training set in each round for co-train style algorithm. It makes the defect predictor more practical for real applications, by combating these problems. In comparison with conventional machine learning approaches, our method has significant superior performance. Experimental results also show that with the proposed learning approach, it is possible to design better method to tackle the class imbalanced problem in semi-supervised learning. © 2014 - IOS Press and the authors.",IB1 | IBK | Instance-based learning | Kstar | LBR | LWL,Advances in Intelligent Systems and Computing,2017-01-01,Conference Paper,"Venkatesh, Bharathan;Singh, Danasingh Asir Antony Gnana;Leavline, Epiphany Jebamalar",Include,
10.1016/j.infsof.2022.107128,2-s2.0-37149011273,10.1007/978-3-540-71289-3_3,EQ-mine: Predicting short-term defects for software evolution,"We use 63 features extracted from sources such as versioning and issue tracking systems to predict defects in short time frames of two months. Our multivariate approach covers aspects of software projects such as size, team structure, process orientation, complexity of existing solution, difficulty of problem, coupling aspects, time constrains, and testing data. We investigate the predictability of several severities of defects in software projects. Are defects with high severity difficult to predict? Are prediction models for defects that are discovered by internal staff similar to models for defects reported from the field? We present both an exact numerical prediction of future defect numbers based on regression models as well as a classification of software components as defect-prone based on the C4.5 decision tree. We create models to accurately predict short-term defects in a study of 5 applications composed of more than 8.000 classes and 700.000 lines of code. The model quality is assessed based on 10-fold cross validation. © Springer-Verlag Berlin Heidelberg 2007.",Classification | Defect density | Machine learning | Quality prediction | Regression | Software evolution,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2007-01-01,Conference Paper,"Ratzinger, Jacek;Pinzger, Martin;Gall, Harald",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85049243837,10.1109/IESYS.2017.8233558,Convolutional neural networks on assembly code for predicting software defects,"Software defect prediction is one of the most attractive research topics in the field of software engineering. The task is to predict whether or not a program contains semantic bugs. Previous studies apply conventional machine learning techniques on software metrics, or deep learning on source code's tree representations called abstract syntax trees. This paper formulates an approach for software defect prediction, in which source code firstly is compiled into assembly code and then a multi-view convolutional neural network is applied to automatically learn defect features from the assembly instruction sequences. The experimental results on four real-world datasets indicate that exploiting assembly code is beneficial to detecting semantic bugs. Our approach significantly outperforms baselines that are based on software metrics and abstract syntax trees. © 2017 IEEE.",,"Proceedings - 2017 21st Asia Pacific Symposium on Intelligent and Evolutionary Systems, IES 2017",2017-12-21,Conference Paper,"Phan, Anh Viet;Le Nguyen, Minh",Include,
10.1016/j.infsof.2022.107128,2-s2.0-2642557078,10.1142/S0218194099000310,An investigation on the use of machine learned models for estimating software correctability,"In this paper we present the results of an empirical study in which we have investigated Machine Learning (ML) algorithms with regard to their capabilities to accurately assess the correctability of faulty software components. Three different families algorithms have been analyzed: divide and conquer (top down induction decision tree), covering, and inductive logic programming (ILP). We have used (1) fault data collected on corrective maintenance activities for the Generalized Support Software reuse asset library located at the Flight Dynamics Division of NASA's GSFC and (2) product measures extracted directly from the faulty components of this library. In our data set, the software quality models generated by both C4.5-rules (a divide and conquer algorithm) and FOIL (an inductive logic programming one) presented the best results from the point of view of model accuracy.",Machine learning algorithms | Predictive software quality model building | Software correctability,International Journal of Software Engineering and Knowledge Engineering,1999-01-01,Article,"De Almeida, Mauricio A.;Lounis, Hakim;Melo, Walcelio L.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85076393558,10.1145/3340482.3342742,SZZ unleashed: An open implementation of the SZZ algorithm-featuring example usage in a study of just-in-time bug prediction for the jenkins project,"Machine learning applications in software engineering often rely on detailed information about bugs. While issue trackers often contain information about when bugs were fixed, details about when they were introduced to the system are often absent. As a remedy, researchers often rely on the SZZ algorithm as a heuristic approach to identify bug-introducing software changes. Unfortunately, as reported in a recent systematic literature review, few researchers have made their SZZ implementations publicly available. Consequently, there is a risk that research effort is wasted as new projects based on SZZ output need to initially reimplement the approach. Furthermore, there is a risk that newly developed (closed source) SZZ implementations have not been properly tested, thus conducting research based on their output might introduce threats to validity. We present SZZ Unleashed, an open implementation of the SZZ algorithm for git repositories. This paper describes our implementation along with a usage example for the Jenkins project, and conclude with an illustrative study on just-in-time bug prediction. We hope to continue evolving SZZ Unleashed on GitHub, and warmly invite the community to contribute. © 2019 Association for Computing Machinery.",defect prediction | issue tracking | mining software repositories | SZZ,"MaLTeSQuE 2019 - Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation, co-located with ESEC/FSE 2019",2019-08-27,Conference Paper,"Borg, Markus;Svensson, Oscar;Berg, Kristian;Hansson, Daniel",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85048134024,10.1109/ISKE.2017.8258790,Enhancing software defect prediction using supervised-learning based framework,"Software Defect Prediction (SDP) proposes to define the exposure of software to defect by building prediction models through using defect data and the software metrics with several learning algorithms which aid in identifying potentially faulty program modules, thus leading to optimal resource allocation and utilization. However, the quality of data and robustness of classifiers affect the accuracy of prediction for these models of classification compromised by data quality such as high dimensionality, class imbalance and the presence of noise in the software defect datasets. This paper presents a combined framework to enhance SDP models in which we use ranker Feature Selection (FS) techniques, Data Sampling (DS) and Iterative-Partition Filter (IPF) to defeat high dimensionality, class imbalance and noisy, respectively. The experimental results confirm that the proposed framework is effective for SDP. © 2017 IEEE.",data sampling | feature selection | iterative-partition filter | software defect prediction,"Proceedings of the 2017 12th International Conference on Intelligent Systems and Knowledge Engineering, ISKE 2017",2017-07-01,Conference Paper,"Bashir, Kamal;Li, Tianrui;Yohannese, Chubato Wondaferaw;Mahama, Yahaya",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85018792345,10.2991/ijcis.2017.10.1.43,A Combined-Learning based framework for improved software fault prediction,"Software Fault Prediction (SFP) is found to be vital to predict the fault-proneness of software modules, which allows software engineers to focus development activities on fault-prone modules, thereby prioritize and optimize tests, improve software quality and make better use of resources. In this regard, machine learning has been successfully applied to solve classification problems for SFP. Nevertheless, the presence of different software metrics, the redundant and irrelevant features and the imbalanced nature of software datasets have created more and more challenges for the classification problems. Therefore, the objective of this study is to independently examine software metrics with multiple Feature Selection (FS) combined with Data Balancing (DB) using Synthetic Minority Oversampling Techniques for improving classification performance. Accordingly, a new framework that efficiently handles those challenges in a combined form on both Object Oriented Metrics (OOM) and Static Code Metrics (SCM) datasets is proposed. The experimental results confirm that the prediction performance could be compromised without suitable Feature Selection Techniques (FST). To mitigate that, data must be balanced. Thus our combined technique assures the robust performance. Furthermore, a combination of Random Forts (RF) with Information Gain (IG) FS yields the highest Receiver Operating Characteristic (ROC) curve (0.993) value, which is found to be the best combination when SCM are used, whereas the combination of RF with Correlation-based Feature Selection (CFS) guarantees the highest ROC (0.909) value, which is found to be the best choice when OOM are used. Therefore, as shown in this study, software metrics used to predict the fault proneness of the software modules must be carefully examined and suitable FST for software metrics must be cautiously selected. Moreover, DB must be applied in order to obtain robust performance. In addition to that, dealing with the challenges mentioned above, the proposed framework ensures the remarkable classification performance and lays the pathway to quality assurance of software. © 2017, the Authors.",Data Balancing | Feature Selection | Machine Learning | Software Fault Prediction | Software Metrics,International Journal of Computational Intelligence Systems,2017-01-01,Article,"Yohannese, Chubato Wondaferaw;Li, Tianrui",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85049055831,10.1109/ICRITO.2014.7014673,Software defect prediction using neural networks,"Defect severity assessment is the most crucial step in large industries and organizations where the complexity of the software is increasing at an exponential rate. Assigning the correct severity level to the defects encountered in large and complex software, would help the software practitioners to allocate their resources and plan for subsequent defect fixing activities. In order to accomplish this, we have developed a model based on text mining techniques that will be used to assign the severity level to each defect report based on the classification of existing reports done using the machine learning method namely, Radial Basis Function of neural network. The proposed model is validated using an open source NASA dataset available in the PITS database. Receiver Operating Characteristics (ROC) analysis is done to interpret the results obtained from model prediction by using the value of Area Under the Curve (AUC), sensitivity and a suitable threshold criterion known as the cut-off point. It is evident from the results that the model has performed very well in predicting high severity defects than in predicting the defects of the other severity levels. This observation is irrespective of the number of words taken into consideration as independent variables. © 2014 IEEE.",ANFIS | Anomaly detection | Apache spark | Big data | Hadoop,Communications in Computer and Information Science,2018-01-01,Conference Paper,"Santosh, Thakur;Ramesh, Dharavath",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85049750006,10.1145/2568225.2568307,Detecting memory leaks through introspective dynamic behavior modelling using machine learning,"This paper expands staleness-based memory leak detection by presenting a machine learning-based framework. The proposed framework is based on an idea that object staleness can be better leveraged in regard to similarity of objects; i.e., an object is more likely to have leaked if it shows significantly high staleness not observed from other similar objects with the same allocation context. A central part of the proposed framework is the modeling of heap objects. To this end, the framework observes the staleness of objects during a representative run of an application. From the observed data, the framework generates training examples, which also contain instances of hypothetical leaks. Via machine learning, the proposed framework replaces the error-prone user-definable staleness predicates used in previous research with a model-based prediction. The framework was tested using both synthetic and real-world examples. Evaluation with synthetic leakage workloads of SPEC2006 benchmarks shows that the proposed method achieves the optimal accuracy permitted by staleness-based leak detection. Moreover, by incorporating allocation context into the model, the proposed method achieves higher accuracy than is possible with object staleness alone. Evaluation with real-world memory leaks demonstrates that the proposed method is effective for detecting previously reported bugs with high accuracy. © 2014 ACM.",,SpringerBriefs in Computer Science,2018-01-01,Book Chapter,"Kumar, Sandeep;Rathore, Santosh Singh",Include,
10.1016/j.infsof.2022.107128,2-s2.0-80054080911,10.1145/2020390.2020405,An iterative semi-supervised approach to software fault prediction,"Background: Many statistical and machine learning techniques have been implemented to build predictive fault models. Traditional methods are based on supervised learning. Software metrics for a module and corresponding fault information, available from previous projects, are used to train a fault prediction model. This approach calls for a large size of training data set and enables the development of effective fault prediction models. In practice, data collection costs, the lack of data from earlier projects or product versions may make large fault prediction training data set unattainable. Small size of the training set that may be available from the current project is known to deteriorate the performance of the fault predictive model. In semi-supervised learning approaches, software modules with known or unknown fault content can be used for training. Aims: To implement and evaluate a semi-supervised learning approach in software fault prediction. Methods: We investigate an iterative semi-supervised approach to software quality prediction in which a base supervised learner is used within a semi-supervised application. Results: We varied the size of labeled software modules from 2% to 50% of all the modules in the project. After tracking the performance of each iteration in the semi-supervised algorithm, we observe that semi-supervised learning improves fault prediction if the number of initially labeled software modules exceeds 5%. Conclusion: The semi-supervised approach outperforms the corresponding supervised learning approach when both use random forest as base classification algorithm. Copyright © 2011 ACM.",Fault prediction | Semi-supervised learning,ACM International Conference Proceeding Series,2011-10-19,Conference Paper,"Lu, Huihua;Cukic, Bojan;Culp, Mark",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85055711957,10.1109/APSEC.2010.54,Using faults-slip-through metric as a predictor of fault-proneness,"Background: The majority of software faults are present in small number of modules, therefore accurate prediction of fault-prone modules helps improve software quality by focusing testing efforts on a subset of modules. Aims: This paper evaluates the use of the faults-slip-through (FST) metric as a potential predictor of fault-prone modules. Rather than predicting the fault-prone modules for the complete test phase, the prediction is done at the specific test levels of integration and system test. Method: We applied eight classification techniques, to the task of identifying fault-prone modules, representing a variety of approaches, including a standard statistical technique for classification (logistic regression), tree-structured classifiers (C4.5 and random forests), a Bayesian technique (Naïve Bayes), machine-learning techniques (support vector machines and back-propagation artificial neural networks) and search-based techniques (genetic programming and artificial immune recognition systems) on FST data collected from two large industrial projects from the telecommunication domain. Results: Using area under the receiver operating characteristic (ROC) curve and the location of (PF, PD) pairs in the ROC space, the faults-slip-through metric showed impressive results with the majority of the techniques for predicting fault-prone modules at both integration and system test levels. There were, however, no statistically significant differences between the performance of different techniques based on AUC, even though certain techniques were more consistent in the classification performance at the two test levels. Conclusions: We can conclude that the faults-slip-through metric is a potentially strong predictor of fault-proneness at integration and system test levels. The faults-slip-through measurements interact in ways that is conveniently accounted for by majority of the data mining techniques. © 2010 IEEE.",Bug report | Classification prediction | Integration method,Communications in Computer and Information Science,2018-01-01,Conference Paper,"Gao, Guofeng;Li, Hui;Chen, Rong;Ge, Xin;Guo, Shikai",Include,
10.1016/j.infsof.2022.107128,2-s2.0-47949101889,10.1109/IRI.2007.4296695,A practical method for the software fault-prediction,"In the paper, a novel machine learning method, SimBoost, is proposed to handle the software fault-prediction problem when highly skewed datasets are used. Although the method, proved by empirical results, can make the datasets much more balanced, the accuracy of the prediction is still not satisfactory. Therefore, a fuzzy-based representation of the software module fault state has been presented instead of the original faulty/non-faulty one. Several experiments were conducted using datasets from NASA Metrics Data Program. The discussion of the results of experiments is provided. © 2007 IEEE.",,"2007 IEEE International Conference on Information Reuse and Integration, IEEE IRI-2007",2007-12-01,Conference Paper,"Li, Zhan;Reformat, Marek",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85077495784,10.1016/j.eswa.2019.113156,SLDeep: Statement-level software defect prediction using deep-learning model on static code features,"Software defect prediction (SDP) seeks to estimate fault-prone areas of the code to focus testing activities on more suspicious portions. Consequently, high-quality software is released with less time and effort. The current SDP techniques however work at coarse-grained units, such as a module or a class, putting some burden on the developers to locate the fault. To address this issue, we propose a new technique called as Statement-Level software defect prediction using Deep-learning model (SLDeep). The significance of SLDeep for intelligent and expert systems is that it demonstrates a novel use of deep-learning models to the solution of a practical problem faced by software developers. To reify our proposal, we defined a suite of 32 statement-level metrics, such as the number of binary and unary operators used in a statement. Then, we applied as learning model, long short-term memory (LSTM). We conducted experiments using 119,989 C/C++ programs within Code4Bench. The programs comprise 2,356,458 lines of code of which 292,064 lines are faulty. The benchmark comprises a diverse set of programs and versions, written by thousands of developers. Therefore, it tends to give a model that can be used for cross-project SDP. In the experiments, our trained model could successfully classify the unseen data (that is, fault-proneness of new statements) with average performance measures 0.979, 0.570, and 0.702 in terms of recall, precision, and accuracy, respectively. These experimental results suggest that SLDeep is effective for statement-level SDP. The impact of this work is twofold. Working at statement-level further alleviates developer's burden in pinpointing the fault locations. Second, cross-project feature of SLDeep helps defect prediction research become more industrially-viable. © 2019",Defect | Fault prediction model | Machine learning | Software fault proneness | Software metric,Expert Systems with Applications,2020-06-01,Article,"Majd, Amirabbas;Vahidi-Asl, Mojtaba;Khalilian, Alireza;Poorsarvi-Tehrani, Pooria;Haghighi, Hassan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85034652407,10.1007/s11277-017-5069-3,Defect Prediction in Android Binary Executables Using Deep Neural Network,"Software defect prediction locates defective code to help developers improve the security of software. However, existing studies on software defect prediction are mostly limited to the source code. Defect prediction for Android binary executables (called apks) has never been explored in previous studies. In this paper, we propose an explorative study of defect prediction in Android apks. We first propose smali2vec, a new approach to generate features that capture the characteristics of smali (decompiled files of apks) files in apks. Smali2vec extracts both token and semantic features of the defective files in apks and such comprehensive features are needed for building accurate prediction models. Then we leverage deep neural network (DNN), which is one of the most common architecture of deep learning networks, to train and build the defect prediction model in order to achieve accuracy. We apply our defect prediction model to more than 90,000 smali files from 50 Android apks and the results show that our model could achieve an AUC (the area under the receiver operating characteristic curve) of 85.98% and it is capable of predicting defects in apks. Furthermore, the DNN is proved to have a better performance than the traditional shallow machine learning algorithms (e.g., support vector machine and naive bayes) used in previous studies. The model has been used in our practical work and helped locate many defective files in apks. © 2017, Springer Science+Business Media, LLC, part of Springer Nature.",Android binary executables | Deep neural network | Machine learning | Mobile security | Software defect prediction,Wireless Personal Communications,2018-10-01,Article,"Dong, Feng;Wang, Junfeng;Li, Qi;Xu, Guoai;Zhang, Shaodong",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85018351732,10.1109/MALTESQUE.2017.7882013,Automatic feature selection by regularization to improve bug prediction accuracy,"Bug prediction has been a hot research topic for the past two decades, during which different machine learning models based on a variety of software metrics have been proposed. Feature selection is a technique that removes noisy and redundant features to improve the accuracy and generalizability of a prediction model. Although feature selection is important, it adds yet another step to the process of building a bug prediction model and increases its complexity. Recent advances in machine learning introduce embedded feature selection methods that allow a prediction model to carry out feature selection automatically as part of the training process. The effect of these methods on bug prediction is unknown. In this paper we study regularization as an embedded feature selection method in bug prediction models. Specifically, we study the impact of three regularization methods (Ridge, Lasso, and ElasticNet) on linear and Poisson Regression as bug predictors for five open source Java systems. Our results show that the three regularization methods reduce the prediction error of the regressors and improve their stability. © 2017 IEEE.",Bug Prediction | Feature Selection | Machine Learning,"MaLTeSQuE 2017 - IEEE International Workshop on Machine Learning Techniques for Software Quality Evaluation, co-located with SANER 2017",2017-03-17,Conference Paper,"Osman, Haidar;Ghafari, Mohammad;Nierstrasz, Oscar",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84945477440,10.1007/s11334-015-0258-2,A hybrid one-class rule learning approach based on swarm intelligence for software fault prediction,"Software testing is a fundamental activity in the software development process aimed to determine the quality of software. To reduce the effort and cost of this process, defect prediction methods can be used to determine fault-prone software modules through software metrics to focus testing activities on them. Because of model interpretation and easily used by programmers and testers some recent studies presented classification rules to make prediction models. This study presents a rule-based prediction approach based on kernel k-means clustering algorithm and Distance based Multi-objective Particle Swarm Optimization (DSMOPSO). Because of discrete search space, we modified this algorithm and named it DSMOPSO-D. We prevent best global rules to dominate local rules by dividing the search space with kernel k-means algorithm and by taking different approaches for imbalanced and balanced clusters, we solved imbalanced data set problem. The presented model performance was evaluated by four publicly available data sets from the PROMISE repository and compared with other machine learning and rule learning algorithms. The obtained results demonstrate that our model presents very good performance, especially in large data sets. © 2015, Springer-Verlag London.",Classification rules | DSMOPSO-D | Fault prediction | Imbalanced data sets | Kernel k-means | Multi-objective particle swarm optimization,Innovations in Systems and Software Engineering,2015-12-01,Article,"Abdi, Yousef;Parsa, Saeed;Seyfari, Yousef",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84947591877,10.1145/2810146.2810150,What is the impact of imbalance on software defect prediction performance?,"Software defect prediction performance varies over a large range. Menzies suggested there is a ceiling effect of 80% Recall [8]. Most of the data sets used are highly imbalanced. This paper asks, what is the empirical effect of using different datasets with varying levels of imbalance on predictive performance? We use data synthesised by a previous meta-analysis of 600 fault prediction models and their results. Four model evaluation measures (the Mathews Correlation Coeficient (MCC), F-Measure, Precision and Re- call ) are compared to the corresponding data imbalance ratio. When the data are imbalanced, the predictive performance of software defect prediction studies is low. As the data become more balanced, the predictive performance of prediction models increases, from an average MCC of 0.15, until the minority class makes up 20% of the instances in the dataset, where the MCC reaches an average value of about 0.34. As the proportion of the minority class increases above 20%, the predictive performance does not significantly increase. Using datasets with more than 20% of the instances being defective has not had a significant impact on the predictive performance when using MCC. We conclude that comparing the results of defect prediction studies should take into account the imbalance of the data. © 2015 ACM.",Data Imbalance | Defect Prediction | Machine Learning,ACM International Conference Proceeding Series,2015-10-21,Conference Paper,"Mahmood, Zaheed;Bowes, David;Lane, Peter C.R.;Hall, Tracy",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85084171526,10.14429/dsj.61.1088,Predicting software faults in large space systems using machine learning techniques,"Recently, the use of machine learning (ML) algorithms has proven to be of great practical value in solving a variety of engineering problems including the prediction of failure, fault, and defect-proneness as the space system software becomes complex. One of the most active areas of recent research in ML has been the use of ensemble classifiers. How ML techniques (or classifiers) could be used to predict software faults in space systems, including many aerospace systems is shown, and further use ensemble individual classifiers by having them vote for the most popular class to improve system software fault-proneness prediction. Benchmarking results on four NASA public datasets show the Naive Bayes classifier as more robust software fault prediction while most ensembles with a decision tree classifier as one of its components achieve higher accuracy rates. © 2011, DESIDOC.",Feature selection | Machine learning model | Random forest | Software bug detection,Communications in Computer and Information Science,2020-01-01,Conference Paper,"Srivastava, Nidhi;Lamba, Tripti;Agarwal, Manisha",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85079054649,10.1016/j.infsof.2020.106269,Incorporating fault-proneness estimations into coverage-based test case prioritization methods,"Context: During the development process of a software program, regression testing is used to ensure that the correct behavior of the software is retained after updates to the source code. This regression testing becomes costly over time as the number of test cases increases and it makes sense to prioritize test cases in order to execute fault-detecting test cases as soon as possible. There are many coverage-based test case prioritization (TCP) methods that only use the code coverage data to prioritize test cases. By incorporating the fault-proneness estimations of code units into the coverage-based TCP methods, we can improve such techniques. Objective: In this paper, we aim to propose an approach which improves coverage-based TCP methods by considering the fault-proneness distribution over code units. Further, we present the results of an empirical study that shows using our proposed approach significantly improves the additional strategy, which is a widely used coverage-based TCP method. Method: The approach presented in this study uses the bug history of the software in order to introduce a defect prediction method to learn a neural network model. This model is then used to estimate fault-proneness of each area of the source code and then the estimations are incorporated into coverage-based TCP methods. Our proposed approach is a general idea that can be applied to many coverage-based methods, such as the additional and total TCP methods. Results: The proposed methods are evaluated on datasets collected from the development history of five real-world projects including 357 versions in total. The experiments show that using an appropriate bug history can improve coverage-based TCP methods. Conclusion: The proposed approach can be applied to various coverage-based TCP methods and the experiments show that it can improve these methods by incorporating estimations of code units fault-proneness. © 2020 Elsevier B.V.",Bug history | Defect prediction | Machine learning | Regression testing | Test case prioritization,Information and Software Technology,2020-05-01,Article,"Mahdieh, Mostafa;Mirian-Hosseinabadi, Seyed Hassan;Etemadi, Khashayar;Nosrati, Ali;Jalali, Sajad",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85089231398,10.1016/j.jksuci.2019.03.006,Software fault prediction based on change metrics using hybrid algorithms: An empirical study,"Quality of the developed software depends on its bug free operation. Although bugs can be introduced in any phase of the software development life-cycle but their identification in earlier phase can lead to reduce the allocation cost of testing and maintenance resources. Software defect prediction studies advocates the use of defect prediction models for identification of bugs prior to the release of the software. Use of bug prediction models can help to reduce the cost and efforts required to develop software. Defect prediction models use the historical data obtained from software projects for training the models and test the model on future release of the software. In the present work, software change metrics have been used for defect prediction. Performances of good machine learning and hybrid algorithms are accessed in prediction of defect with the change metrics. Android project has been used for experimental purpose. Git repository has been used to extract the v4–v5, v2–v5 of android for defect prediction. Obtained results showed that GFS-logitboost-c has best defect prediction capability. © 2019 The Authors",Empirical software engineering | Machine learning | Mining software repositories | Modern code review | Sentiment Analysis,Communications in Computer and Information Science,2020-01-01,Conference Paper,"Hossain, Syeda Sumbul;Arafat, Yeasir;Hossain, Md Ekram;Arman, Md Shohel;Islam, Anik",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85042098320,10.1007/s11704-017-6015-y,Combined classifier for cross-project defect prediction: an extended empirical study,"To facilitate developers in effective allocation of their testing and debugging efforts, many software defect prediction techniques have been proposed in the literature. These techniques can be used to predict classes that are more likely to be buggy based on the past history of classes, methods, or certain other code elements. These techniques are effective provided that a sufficient amount of data is available to train a prediction model. However, sufficient training data are rarely available for new software projects. To resolve this problem, cross-project defect prediction, which transfers a prediction model trained using data from one project to another, was proposed and is regarded as a new challenge in the area of defect prediction. Thus far, only a few cross-project defect prediction techniques have been proposed. To advance the state of the art, in this study, we investigated seven composite algorithms that integrate multiple machine learning classifiers to improve cross-project defect prediction. To evaluate the performance of the composite algorithms, we performed experiments on 10 open-source software systems from the PROMISE repository, which contain a total of 5,305 instances labeled as defective or clean. We compared the composite algorithms with the combined defect predictor where logistic regression is used as the meta classification algorithm (CODEPLogistic), which is the most recent cross-project defect prediction algorithm in terms of two standard evaluation metrics: cost effectiveness and F-measure. Our experimental results show that several algorithms outperform CODEPLogistic: Maximum voting shows the best performance in terms of F-measure and its average F-measure is superior to that of CODEPLogistic by 36.88%. Bootstrap aggregation (BaggingJ48) shows the best performance in terms of cost effectiveness and its average cost effectiveness is superior to that of CODEPLogistic by 15.34%. © 2018, Higher Education Press and Springer-Verlag GmbH Germany, part of Springer Nature.",classifier combination | cross-project | defect prediction,Frontiers of Computer Science,2018-04-01,Article,"Zhang, Yun;Lo, David;Xia, Xin;Sun, Jianling",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84902316977,10.1142/S021819401450003X,Investigating associative classification for software fault prediction: An experimental perspective,"It is a recurrent finding that software development is often troubled by considerable delays as well as budget overruns and several solutions have been proposed in answer to this observation, software fault prediction being a prime example. Drawing upon machine learning techniques, software fault prediction tries to identify upfront software modules that are most likely to contain faults, thereby streamlining testing efforts and improving overall software quality. When deploying fault prediction models in a production environment, both prediction performance and model comprehensibility are typically taken into consideration, although the latter is commonly overlooked in the academic literature. Many classification methods have been suggested to conduct fault prediction; yet associative classification methods remain uninvestigated in this context. This paper proposes an associative classification (AC)-based fault prediction method, building upon the CBA2 algorithm. In an empirical comparison on 12 real-world datasets, the AC-based classifier is shown to achieve a predictive performance competitive to those of models induced by five other tree/rule-based classification techniques. In addition, our findings also highlight the comprehensibility of the AC-based models, while achieving similar prediction performance. Furthermore, the possibilities of cross project prediction are investigated, strengthening earlier findings on the feasibility of such approach when insufficient data on the target project is available. © 2014 World Scientific Publishing Company.",associative classification | comprehensibility | cross project validation | prediction performance | Software fault prediction,International Journal of Software Engineering and Knowledge Engineering,2014-01-01,Article,"Ma, Baojun;Zhang, Huaping;Chen, Guoqing;Zhao, Yanping;Baesens, Bart",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85107070457,10.1109/CONSEG.2012.6349484,Investigating object-oriented design metrics to predict fault-proneness of software modules,"This paper empirically investigates the relationship of class design level object-oriented metrics with fault proneness of object-oriented software system. The aim of this study is to evaluate the capability of the design attributes related to coupling, cohesion, complexity, inheritance and size with their corresponding metrics in predicting fault proneness both in independent and combine basis. In this paper, we conducted two set of systematic investigations using publicly available project datasets over its multiple subsequent releases to performed our investigation and four machine learning techniques to validated our results. The first set of investigation consisted of applying the univariate logistic regression (ULR), Spearman's correlation and AUC (Area under ROC curve) analysis on four PROMISE datasets. This investigation evaluated the capability of each metric to predict fault proneness, when used in isolation. The second set of experiments consisted of applying the four machine learning techniques on the next two subsequent versions of the same project datasets to validate the effectiveness of the metrics. Based on the results of individual performance of metrics, we used only those metrics that are found significant, to build multivariate prediction models. Next, we evaluated the significant metrics related to design attributes both in isolation and in combination to validated their capability of predicting fault proneness. Our results suggested that models built on coupling and complexity metrics are better and more accurate than those built on using the rest of metrics. © 2012 IEEE.",Decision tree | Ensemble learning | Intrusion detection system | K-nearest neighbor | Malware detection | Synthetic data,Lecture Notes on Data Engineering and Communications Technologies,2021-01-01,Book Chapter,"Kaushik, Raghav;Dave, Mayank",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85066739131,10.14569/ijacsa.2019.0100538,Performance analysis of machine learning techniques on software defect prediction using NASA datasets,"Defect prediction at early stages of software development life cycle is a crucial activity of quality assurance process and has been broadly studied in the last two decades. The early prediction of defective modules in developing software can help the development team to utilize the available resources efficiently and effectively to deliver high quality software product in limited time. Until now, many researchers have developed defect prediction models by using machine learning and statistical techniques. Machine learning approach is an effective way to identify the defective modules, which works by extracting the hidden patterns among software attributes. In this study, several machine learning classification techniques are used to predict the software defects in twelve widely used NASA datasets. The classification techniques include: Naïve Bayes (NB), Multi-Layer Perceptron (MLP). Radial Basis Function (RBF), Support Vector Machine (SVM), K Nearest Neighbor (KNN), kStar (K*), One Rule (OneR), PART, Decision Tree (DT), and Random Forest (RF). Performance of used classification techniques is evaluated by using various measures such as: Precision, Recall, F-Measure, Accuracy, MCC, and ROC Area. The detailed results in this research can be used as a baseline for other researches so that any claim regarding the improvement in prediction through any new technique, model or framework can be compared and verified. © 2018 The Science and Information (SAI) Organization Limited.",Class imbalance | Classification | Data mining | Machine learning | Software defect prediction | Software metrics,International Journal of Advanced Computer Science and Applications,2019-01-01,Article,"Iqbal, Ahmed;Aftab, Shabib;Ali, Umair;Nawaz, Zahid;Sana, Laraib;Ahmad, Munir;Husen, Arif",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85119001557,10.1016/j.procs.2018.05.071,Software Bug Prediction Prototype Using Bayesian Network Classifier: A Comprehensive Model,"Software bug prediction becomes the vital activity during software development and maintenance. Fault prediction model able to engaged to identify flawed software code by utilizing machine learning techniques. Naive Bayes classifier has often used times for this kind problems, because of its high predictive performance and comprehensiveness toward most of the predictive issues. Bayesian network(BN) able to construct the simple network of a complex problem using the fewer number of nodes and unexplored arcs. The dataset is an essential phase in bugs prediction, NASA/Eclipse free-ware are freely available for better results. ROC/AUC is a performance measure for classification of fault-prone or non-fault prone, H-measure is also useful while prediction technique, we will explore every parameter and valuable expects for experiment perspective. © 2018 The Authors. Published by Elsevier Ltd.",Classification | Global software development | Machine learning | Mining software repositories | Modern code review,Lecture Notes in Networks and Systems,2022-01-01,Conference Paper,"Arafat, Yeasir;Sumbul, Syeda;Shamma, Hossain",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85034652039,10.1016/j.asoc.2017.10.048,Comparing learning to rank techniques in hybrid bug localization,"Bug localization is a software development and maintenance activity that aims to find relevant source code entities to be modified so that a specific bug can be fixed on the basis of the given bug report. Information retrieval (IR) techniques have been widely used to locate bugs in recent decades. These techniques mainly use the IR similarity between the bug report and source code entities. In addition to IR similarity, features that are extracted from version history, source code structure, dynamic analysis, and other resources are found to be beneficial for bug localization. The approaches utilizing extra features in IR-based bug localization are called hybrid bug localization. We conduct a short survey of the hybrid bug localization methods that use additional features in addition to IR similarity. We also use Learning to Rank (LtR) techniques to combine the beneficial features to improve bug localization. Learning to Rank is the application of machine learning in the ranking models for information retrieval. We compared eight LtR techniques in bug localization, and the experimental results show that coordinate ascent algorithms without normalization is a suitable LtR technique in bug localization for selected attributes, and it outperforms two state-of-the-art localization approaches for two large projects, Eclipse and SWT. © 2017 Elsevier B.V.",Bug localization | Fault localization | Information retrieval | Learn to rank,Applied Soft Computing Journal,2018-01-01,Article,"Shi, Zhendong;Keung, Jacky;Bennin, Kwabena Ebo;Zhang, Xingjun",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85048146263,10.1109/ISKE.2017.8258836,Ensembles based combined learning for improved software fault prediction: A comparative study,"Software Fault Prediction (SFP) research has made enormous endeavor to accurately predict fault proneness of software modules to maximize precious software test resources, reduce maintenance cost, help to deliver software products on time and satisfy customer, which ultimately contribute to produce quality software products. In this regard, Machine Learning (ML) has been successfully applied to solve classification problems for SFP. Moreover, from ML, it has been observed that Ensemble Learning Algorithms (ELA) are known to improve the performance of single learning algorithms. However, neither of ELA alone handles the challenges created by redundant and irrelevant features and class imbalance problem in software defect datasets. Therefore, the objective of this paper is to independently examine and compare prominent ELA and improves their performance combined with Feature Selection (FS) and Data Balancing (DB) techniques to identify more efficient ELA that better predict the fault proneness of software modules. Accordingly, a new framework that efficiently handles those challenges in a combined form is proposed. The experimental results confirm that the proposed framework has exhibited the robustness of combined techniques. Particularly the framework has high performance when using combined bagging ELA with DB on selected features. Therefore, as shown in this study, ensemble techniques used for SFP must be carefully examined and combined with both FS and DB in order to obtain robust performance. © 2017 IEEE.",Data Balancing | Ensemble Learning Algorithms | Feature Selection | Software Fault Prediction,"Proceedings of the 2017 12th International Conference on Intelligent Systems and Knowledge Engineering, ISKE 2017",2017-07-01,Conference Paper,"Yohannese, Chubato Wondaferaw;Li, Tianrui;Simfukwe, Macmillan;Khurshid, Faisal",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85026725782,10.1109/ICSE-C.2017.72,Cross-project and within-project semi-supervised software defect prediction problems study using a unified solution,"When there exists not enough historical defect data for building accurate prediction model, semi-supervised defect prediction (SSDP) and cross-project defect prediction (CPDP) are two feasible solutions. Existing CPDP methods assume that the available source data is well labeled. However, due to expensive human efforts for labeling a large amount of defect data, usually, we can only make use of the suitable unlabeled source data to help build the prediction model. We call CPDP in this scenario as cross-project semi-supervised defect prediction (CSDP). As to within-project semi-supervised defect prediction (WSDP), although some WSDP methods have been developed in recent years, there still exists much room for improvement. In this paper, we aim to provide an effective solution for both CSDP and WSDP problems. We introduce the semi-supervised dictionary learning technique, an effective machine learning technique, into defect prediction and propose a semi-supervised structured dictionary learning (SSDL) approach for CSDP and WSDP. SSDL can make full use of the useful information in limited labeled defect data and a large amount of unlabeled data. Experiments on two public datasets indicate that SSDL can obtain better prediction performance than related SSDP methods in the CSDP scenario. © 2017 IEEE.",Cross-project semi-supervised defect prediction | Semi-supervised structured dictionary learning | Within-project semi-supervised defect prediction,"Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering Companion, ICSE-C 2017",2017-06-30,Conference Paper,"Wu, Fei;Jing, Xiao Yuan;Dong, Xiwei;Cao, Jicheng;Xu, Mingwei;Zhang, Hongyu;Ying, Shi;Xu, Baowen",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85103352776,10.1007/s10664-018-9638-1,Balancing the trade-off between accuracy and interpretability in software defect prediction,"Context: Classification techniques of supervised machine learning have been successfully applied to various domains of practice. When building a predictive model, there are two important criteria: predictive accuracy and interpretability, which generally have a trade-off relationship. In particular, interpretability should be accorded greater emphasis in the domains where the incorporation of expert knowledge into a predictive model is required. Objective: The aim of this research is to propose a new classification model, called superposed naive Bayes (SNB), which transforms a naive Bayes ensemble into a simple naive Bayes model by linear approximation. Method: In order to evaluate the predictive accuracy and interpretability of the proposed method, we conducted a comparative study using well-known classification techniques such as rule-based learners, decision trees, regression models, support vector machines, neural networks, Bayesian learners, and ensemble learners, over 13 real-world public datasets. Results: A trade-off analysis between the accuracy and interpretability of different classification techniques was performed with a scatter plot comparing relative ranks of accuracy with those of interpretability. The experiment results show that the proposed method (SNB) can produce a balanced output that satisfies both accuracy and interpretability criteria. Conclusions: SNB offers a comprehensible predictive model based on a simple and transparent model structure, which can provide an effective way for balancing the trade-off between accuracy and interpretability. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",Artificial intelligence | Convolutional neural networks | Defect inspection | Head-In-Pillow | Solder ball,Machine Vision and Applications,2021-05-01,Article,"Tsan, Ting Chen;Shih, Teng Fu;Fuh, Chiou Shann",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85020193912,10.1109/ACIT-CSII-BCD.2016.042,Using Software Metrics Thresholds to Predict Fault-Prone Classes in Object-Oriented Software,"Most code-based quality measurement approaches are based, at least partially, on values of multiple source code metrics. A class will often be classified as being of poor quality if the values of its metrics are above given thresholds, which are different from one metric to another. The metrics thresholds are calculated using various techniques. In this paper, we investigated two specific techniques: ROC curves and Alves rankings. These techniques are supposed to give metrics thresholds which are practical for code quality measurements or even for fault-proneness prediction. However, Alves Rankings technique has not been validated as being a good choice for fault-proneness prediction, and ROC curves only partially on few datasets. Fault-proneness prediction is an important field of software engineering, as it can be used by developers and testers as a test effort indication to prioritize tests. This will allow a better allocation of resources, reducing therefore testing time and costs, and an improvement of the effectiveness of testing by testing more intensively the components that are likely more fault-prone. In this paper, we wanted to compare empirically the selected threshold calculation methods used as part of fault-proneness prediction techniques. We also used a machine learning technique (Bayes Network) as a baseline for comparison. Thresholds have been calculated for different object-oriented metrics using four different datasets obtained from the PROMISE Repository and another one based on the Eclipse project. © 2016 IEEE.",Class-Level Metrics | Code Quality | Fault-Proneness Prediction | Faults | Metrics Thresholds | Object-Oriented Metrics | Object-Oriented Programming,"Proceedings - 4th International Conference on Applied Computing and Information Technology, 3rd International Conference on Computational Science/Intelligence and Applied Informatics, 1st International Conference on Big Data, Cloud Computing, Data Science and Engineering, ACIT-CSII-BCD 2016",2017-05-01,Conference Paper,"Boucher, Alexandre;Badri, Mourad",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85018365128,10.1109/MALTESQUE.2017.7882014,Hyperparameter optimization to improve bug prediction accuracy,"Bug prediction is a technique that strives to identify where defects will appear in a software system. Bug prediction employs machine learning to predict defects in software entities based on software metrics. These machine learning models usually have adjustable parameters, called hyperparameters, that need to be tuned for the prediction problem at hand. However, most studies in the literature keep the model hyperparameters set to the default values provided by the used machine learning frameworks. In this paper we investigate whether optimizing the hyperparameters of a machine learning model improves its prediction power. We study two machine learning algorithms: k-nearest neighbours (IBK) and support vector machines (SVM). We carry out experiments on five open source Java systems. Our results show that (i) models differ in their sensitivity to their hyperparameters, (ii) tuning hyperparameters gives at least as accurate models for SVM and significantly more accurate models for IBK, and (iii) most of the default values are changed during the tuning phase. Based on these findings we recommend tuning hyperparameters as a necessary step before using a machine learning model in bug prediction. © 2017 IEEE.",Bug Prediction | Hyperparameter Optimization | Machine Learning | Model Selection,"MaLTeSQuE 2017 - IEEE International Workshop on Machine Learning Techniques for Software Quality Evaluation, co-located with SANER 2017",2017-03-17,Conference Paper,"Osman, Haidar;Ghafari, Mohammad;Nierstrasz, Oscar",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85029853227,,A dynamical quality model to continuously monitor software maintenance,"Context: several companies, particularly Small and Medium Sized Enterprises (SMEs), often face software maintenance issues due to the lack of Software Quality Assurance (SQA). SQA is a complex task that requires a lot of effort and expertise, often not available in SMEs. Several SQA models, including maintenance prediction models, have been defined in research papers. However, these models are commonly defined as ""one-size-fits-All"" and are mainly targeted at the big industry, which can afford software quality experts who undertake the data interpretation tasks. Objective: in this work, we propose an approach to continuously monitor the software operated by end users, automatically collecting issues and recommending possible fixes to developers. The continuous exception monitoring system will also serve as knowledge base to suggest a set of quality practices to avoid (re)introducing bugs into the code. Method: first, we identify a set of SQA practices applicable to SMEs, based on the main constraints of these. Then, we identify a set of prediction techniques, including regressions and machine learning, keeping track of bugs and exceptions raised by the released software. Finally, we provide each company with a tailored SQA model, automatically obtained from companies' bug/issue history. Developers are then provided with the quality models through a set of plug-ins for integrated development environments. These suggest a set of SQA actions that should be undertaken, in order to maintain a certain quality level and allowing to remove the most severe issues with the lowest possible effort. Conclusion: The collected measures will be made available as public dataset, so that researchers can also benefit of the project's results. This work is developed in collaboration with local SMEs and existing Open Source projects and communities.",Dynamic Software Measurement; Software Maintenance; Software Quality,"Proceedings of the 11th European Conference on Information Systems Management, ECISM 2017",2017,,"Lenarduzzi V., Stan A.C., Taibi D., Tosi D., Venters G.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84985972769,10.1109/ICICCS.2016.7542340,Improving software quality using machine learning,"Software is an entity that keeps on progressing and endures continuous changes, in order to boost its functionality and maintain its effectiveness. During the development of software, even with advanced planning, well documentation and proper process control, are problems that are countered. These defects influence the quality of software in one way or the other which may result into failure. Therefore, in today's neck to neck competition, it is our requirement to control and minimize these defects in software engineering. Software prediction models are typically used to map the patterns of classes of software that are prone to change. This paper highlights the significant analysis in the area's subject to learn and stimulate the association between the metric specifying the object orientation & the concept of change proneness. This would often lead us to rigorous testing so as to find all kinds of possibilities in the data set. We have two views to be addressed: (1) Parameters quantification that affects the quality, functionality and productivity of the software. (2) Machine learning technologies are used for predicting software Here, the focus of the research paper is to equate and compare all of learning methods corresponding to performance parameter with its statistical method & methodology which would often results enhanced. Data points are the basis for prediction of models. © 2016 IEEE.",Change proneness | Empirical validation | Receiver operating characteristics (ROC) | Software Defects and Prediction | Software Metric | Software Quality,"2016 1st International Conference on Innovation and Challenges in Cyber Security, ICICCS 2016",2016-08-11,Conference Paper,"Chandra, Kanika;Kapoor, Gagan;Kohli, Rashi;Gupta, Archana",Include,
10.1016/j.infsof.2022.107128,2-s2.0-48649106337,10.1109/ISCIS.2007.4456886,Defect prediction for embedded software,"As ubiquitous computing becomes the reality of our lives, the demand for high quality embedded software in shortened intervals increases. In order to cope with this pressure, software developers seek new approaches to manage the development cycle: to finish on time, within budget and with no defects. Software defect prediction is one area that has to be focused to lower the cost of testing as well as to improve the quality of the end product. Defect prediction has been widely studied for software systems in general, however there are very few studies which specifically target embedded software. This paper examines defect prediction techniques from an embedded software point of view. We present the results of combining several machine learning techniques for defect prediction. We believe that the results of this study will guide us in finding better predictors and models for this purpose. ©2007 IEEE.",Defect prediction | Embedded software | Machine learning | Metrics | Software quality,"22nd International Symposium on Computer and Information Sciences, ISCIS 2007 - Proceedings",2007-12-01,Conference Paper,"Oral, Ataç Deniz;Bener, Ayşe Başar",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85103359065,10.1109/SANER.2018.8330265,Keep it simple: Is deep learning good for linguistic smell detection?,"Deep neural networks is a popular technique that has been applied successfully to domains such as image processing, sentiment analysis, speech recognition, and computational linguistic. Deep neural networks are machine learning algorithms that, in general, require a labeled set of positive and negative examples that are used to tune hyper-parameters and adjust model coefficients to learn a prediction function. Recently, deep neural networks have also been successfully applied to certain software engineering problem domains (e.g., bug prediction), however, results are shown to be outperformed by traditional machine learning approaches in other domains (e.g., recovering links between entries in a discussion forum). In this paper, we report our experience in building an automatic Linguistic Antipattern Detector (LAPD) using deep neural networks. We manually build and validate an oracle of around 1,700 instances and create binary classification models using traditional machine learning approaches and Convolutional Neural Networks. Our experience is that, considering the size of the oracle, the available hardware and software, as well as the theory to interpret results, deep neural networks are outperformed by traditional machine learning algorithms in terms of all evaluation metrics we used and resources (time and memory). Therefore, although deep learning is reported to produce results comparable and even superior to human experts for certain complex tasks, it does not seem to be a good fit for simple classification tasks like smell detection. Researchers and practitioners should be careful when selecting machine learning models for the problem at hand. © 2018 IEEE.",Advance machine learning | Deep neural network | Micro-tectonic structures | Seismic method,Soft Computing,2021-07-01,Article,"Ahmed, Khawar Ashfaq;Khan, Sarfraz;Nisar, Umair Bin;Mughal, Muhammad Rizwan;Sultan, Mahmood",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85001610267,10.1007/s00500-016-2456-8,Multi-objective cross-version defect prediction,"Defect prediction models help software project teams to spot defect-prone source files of software systems. Software project teams can prioritize and put up rigorous quality assurance (QA) activities on these predicted defect-prone files to minimize post-release defects so that quality software can be delivered. Cross-version defect prediction is building a prediction model from the previous version of a software project to predict defects in the current version. This is more practical than the other two ways of building models, i.e., cross-project prediction model and cross- validation prediction models, as previous version of same software project will have similar parameter distribution among files. In this paper, we formulate cross-version defect prediction problem as a multi-objective optimization problem with two objective functions: (a) maximizing recall by minimizing misclassification cost and (b) maximizing recall by minimizing cost of QA activities on defect prone files. The two multi-objective defect prediction models are compared with four traditional machine learning algorithms, namely logistic regression, naïve Bayes, decision tree and random forest. We have used 11 projects from the PROMISE repository consisting of a total of 41 different versions of these projects. Our findings show that multi-objective logistic regression is more cost-effective than single-objective algorithms. © 2016, Springer-Verlag Berlin Heidelberg.",Cost-effectiveness | Cross-version defect prediction | Misclassification cost | Multi-objective optimization | Search-based software engineering,Soft Computing,2018-03-01,Article,"Shukla, Swapnil;Radhakrishnan, T.;Muthukumaran, K.;Neti, Lalita Bhanu Murthy",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85010505608,10.1109/ICRITO.2016.7785003,Improved approach for software defect prediction using artificial neural networks,"Software defect prediction (SDP) is a most dynamic research area in software engineering. SDP is a process used to predict the deformities in the software. To identifying the defects before the arrival of item or aimed the software improvement, to make software dependable, defect prediction model is utilized. It is always desirable to predict the defects at early stages of life cycle. Hence to predict the defects before testing the SDP is done at end of each phase of SDLC. It helps to reduce the cost as well as time. To produce high quality software, the artificial neural network approach is applied to predict the defect. Nine metrics are applied to the multiple phases of SDLC and twenty genuine software projects are used. The software project data were collected from a team of organization and their responses were recorded in linguistic terms. For assessment of model the mean magnitude of relative error (MMRE) and balanced mean magnitude of relative error (BMMRE) measures are used. In this research work, the implementation of neural network based software defect prediction is compared with the results of fuzzy logic basic approach. In the proposed approach, it is found that the neural network based training model is providing better and effective results on multiple parameters. © 2016 IEEE.",Artificial Neural Network(ANN) | Defect | Fuzzy logic | Machine Learning technique(MLT) | Software Defect Prediction | Software Metrics,"2016 5th International Conference on Reliability, Infocom Technologies and Optimization, ICRITO 2016: Trends and Future Directions",2016-12-15,Conference Paper,"Sethi, Tanvi;Gagandeep, ",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84984844840,,A combined ant colony optimization and simulated annealing algorithm to assess stability and fault-proneness of classes based on internal software quality attributes,"Several machine learning algorithms have been used to assess external quality attributes of software systems. Given a set of metrics that describe internal software attributes (cohesion, complexity, size, etc.), the purpose is to construct a model that can be used to assess external quality attributes (stability, reliability, maintainability, etc.) based on the internal ones. Most of these algorithms result in assessment models that are hard to generalize. As a result, they show a degradation in their assessment performance when used to estimate quality of new software modules. This paper presents a hybrid heuristic to construct software quality estimation models that can be used to predict software quality attributes of new unseen systems prior to re-using them or purchasing them. The technique relies on two heuristics: simulated annealing and ant colony optimization. It learns from the data available in a particular domain guidelines and rules to achieve a particular external software quality. These guidelines are presented as rule-based logical models. We validate our technique on two software quality attributes namely stability and fault-proneness - a subattribute of maintainability. We compare our technique to two state-of-the-art algorithms: Neural Networks (NN) and C4.5 as well as to a previously published Ant Colony Optimization algorithm. Results show that our hybrid technique out-performs both C4.5 and ACO in most of the cases. Compared to NN, our algorithm preserves the white-box nature of the predictive models hence, giving not only the classification of a particular module but also guidelines for software engineers to follow in order to reach a particular external quality attribute. Our algorithm gives promising results and is generic enough to apply to any software quality attribute. © 2016 [International Journal of Artificial Intelligence].",Ant colony optimization; C4.5; Metric; Prediction; Rule sets; Search-based software engineering; Simulated annealing; Software quality,International Journal of Artificial Intelligence,2016,,"Azar D., Fayad K., Daoud C.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84925310310,10.1109/ICICES.2014.7033809,Analyzing the effect of bagged ensemble approach for software fault prediction in class level and package level metrics,"Faults in a module tend to cause failure of the software product. These defective modules in the software pose considerable risk by increasing the developing cost and decreasing the customer satisfaction. Hence in a software development life cycle it is very important to predict the faulty modules in the software product. Prediction of the defective modules should be done as early as possible so as to improve software developers' ability to identify the defect-prone modules and focus quality assurance activities such as testing and inspections on those defective modules. For quality assurance activity, it is important to concentrate on the software metrics. Software metrics play a vital role in measuring the quality of software. Many researchers focused on classification algorithm for predicting the software defect. On the other hand, classifiers ensemble can effectively improve classification performance when compared with a single classifier. This paper mainly addresses using ensemble approach of Support Vector Machine (SVM) for fault prediction. Ensemble classifier was examined for Eclipse Package level dataset and NASA KC1 dataset. We showed that proposed ensemble of Support Vector Machine is superior to individual approach for software fault prediction in terms of classification rate through Root Mean Square Error Rate (RMSE), AUC-ROC, ROC curves. © 2014 IEEE.",class level metrics | defect prediction | machine learning | method level metrics | software metrics,"2014 International Conference on Information Communication and Embedded Systems, ICICES 2014",2015-02-05,Conference Paper,"Shanthini, A.;Chandrasekaran, R. M.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85104647118,10.1166/asl.2014.5640,A comparison framework of classification models for software defect prediction,"Software defects are expensive in quality and cost. The accurate prediction of defect-prone software modules can help direct test effort, reduce costs, and improve the quality of software. Machine learning classification algorithms is a popular approach for predicting software defect. Various types of classification algorithms have been applied for software defect prediction. However, no clear consensus on which algorithm perform best when individual studies are looked at separately. In this research, a comparison framework is proposed, which aims to benchmark the performance of a wide range of classification models within the field of software defect prediction. For the purpose of this study, 10 classifiers are selected and applied to build classification models and test their performance in 9 NASA MDP datasets. Area under curve (AUC) is employed as an accuracy indicator in our framework to evaluate the performance of classifiers. Friedman and Nemenyi post hoc tests are used to test for significance of AUC differences between classifiers. The results show that the logistic regression perform best in most NASA MDP datasets. Naïve bayes, neural network, support vector machine and k∗ classifiers also perform well. Decision tree based classifiers tend to underperform, as well as linear discriminant analysis and k-nearest neighbor. © 2014 American Scientific Publishers All rights reserved.",Extreme learning machine | Finite state machine | Integrated condition monitoring | Steer-by-wire system,Neural Computing and Applications,2022-04-01,Article,"Lan, Dun;Yu, Ming;Huang, Yunzhi;Ping, Zhaowu;Zhang, Jie",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84903639308,10.1145/2591062.2591151,Software defect prediction based on collaborative representation classification,"In recent years, machine learning techniques have been successfully applied into software defect prediction. Although they can yield reasonably good prediction results, there still exists much room for improvement on the aspect of prediction accuracy. Sparse representation is one of the most advanced machine learning techniques. It performs well with respect to signal compression and classification, but suffers from its time-consuming sparse coding. Compared with sparse representation, collaborative representation classification (CRC) can yield significantly lower computational complexity and competitive classification performance in pattern recognition domains. To achieve better defect prediction results, we introduce the CRC technique in this paper and propose a CRC based software defect prediction (CSDP) approach. We first design a CRC based learner to build a prediction model, whose computational burden is low. Then, we design a CRC based predictor to classify whether the query software modules are defective or defective-free. Experimental results on the widely used NASA datasets demonstrate the effectiveness and efficiency of the proposed approach. Copyright © 2014 ACM.",Collaborative representation classification | Machine learning | Prediction model | Software defect prediction,"36th International Conference on Software Engineering, ICSE Companion 2014 - Proceedings",2014-01-01,Conference Paper,"Jing, Xiao Yuan;Zhang, Zhi Wu;Ying, Shi;Wang, Feng;Zhu, Yang Ping",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84899929695,10.1016/j.ipl.2014.03.012,Combining the requirement information for software defect estimation in design time,"This paper analyzes the ability of requirement metrics for software defect prediction. Statistical significance tests are used to compare six machine learning algorithms on the requirement metrics, design metrics, and combination of both metrics in our analysis. The experimental results show the effectiveness of the predictor built on the combination of the requirement and design metrics in the early phase of the software development process. © 2014 Elsevier B.V. All rights reserved.",Design metric | Machine learning | Requirement metric | Software defect prediction | Software engineering,Information Processing Letters,2014-01-01,Article,"Ma, Ying;Zhu, Shunzhi;Qin, Ke;Luo, Guangchun",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85086127192,10.1109/AICCSA.2009.5069339,Software reliability prediction using multi-objective genetic algorithm,"Software reliability models are very useful to estimate the probability of the software fail along the time. Several different models have been proposed to predict the software reliability growth (SRGM); however, none of them has proven to perform well considering different project characteristics. The ability to predict the number of faults in the software during development and testing processes. In this paper, we explore Genetic Algorithms (GA) as an alternative approach to derive these models. GA is a powerful machine learning technique and optimization techniques to estimate the parameters of well known reliably growth models. Moreover, machine learning algorithms, proposed the solution overcome the uncertainties in the modeling by combining multiple models using multiple objective function to achieve the best generalization performance where. The objectives are conflicting and no design exists which can be considered best with respect to all objectives. In this paper, experiments were conducted to confirm these hypotheses. Then evaluating the predictive capability of the ensemble of models optimized using multi-objective GA has been calculated. Finally, the results were compared with traditional models. © 2009 IEEE.",Data structure invariants | Korat | Learnability | Machine learning,International Journal on Software Tools for Technology Transfer,2020-10-01,Article,"Usman, Muhammad;Wang, Wenxi;Wang, Kaiyuan;Yelen, Cagdas;Dini, Nima;Khurshid, Sarfraz",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85052535240,10.1109/QRS-C.2018.00031,Towards a Bayesian Network Model for Predicting Flaky Automated Tests,"Artificial intelligence and machine learning are making it possible for computers to diagnose some medical diseases more accurately than doctors. Such systems analyze millions of patient records and make generalizations to diagnose new patients. A key challenge is determining whether a patient's symptoms are attributed to a known disease or other factors. Software testers face a similar problem when troubleshooting automation failures. They investigate questions like: Is a given failure due to a defect, environmental issue, or flaky test? Flaky tests exhibit both passing and failing results although neither the code nor test has changed. Maintaining flaky tests is costly, especially in large-scale software projects. In this paper, we present an approach that leverages Bayesian networks for classifying and predicting flaky tests. Our approach views the test flakiness problem as a disease by specifying its symptoms and possible causes. Preliminary results from a case study suggest the approach is feasible. © 2018 IEEE.",Automation | Flaky | Machine learning | Model | Testing,"Proceedings - 2018 IEEE 18th International Conference on Software Quality, Reliability, and Security Companion, QRS-C 2018",2018-08-09,Conference Paper,"King, Tariq M.;Santiago, Dionny;Phillips, Justin;Clarke, Peter J.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85113811937,10.1166/asl.2014.5641,Neural network parameter optimization based on genetic algorithm for software defect prediction,"Software fault prediction approaches are much more efficient and effective to detect software faults compared to software reviews. Machine learning classification algorithms have been applied for software defect prediction. Neural network has strong fault tolerance and strong ability of nonlinear dynamic processing of software defect data. However, practicability of neural network is affected due to the difficulty of selecting appropriate parameters of network architecture. Software fault prediction datasets are often highly imbalanced class distribution. Class imbalance will reduce classifier performance. A combination of genetic algorithm and bagging technique is proposed for improving the performance of the software defect prediction. Genetic algorithm is applied to deal with the parameter optimization of neural network. Bagging technique is employed to deal with the class imbalance problem. The proposed method is evaluated using the datasets from NASA metric data repository. Results have indicated that the proposed method makes an improvement in neural network prediction performance. © 2014 American Scientific Publishers All rights reserved.",Encoding | Feature location | Learning to Rank | Machine learning | Neural networks | Software models,Software and Systems Modeling,2022-02-01,Article,"Marcén, Ana C.;Pérez, Francisca;Pastor, Óscar;Cetina, Carlos",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84924406471,,Stability of Software defect prediction in relation to levels of data imbalance,"Software defect prediction is an important decision support activity in software quality assurance. Its goal is reducing verification costs by predicting the system modules that are more likely to contain defects, thus enabling more efficient allocation of resources in verification process. The problem is that there is no widely applicable well performing prediction method. The main reason is in the very nature of software datasets, their imbalance, complexity and properties dependent on the application domain. In this paper we suggest a research strategy for the study of the performance stability using different machine learning methods over different levels of imbalance for software defect prediction datasets. We also provide a preliminary case study on a dataset from the NASA MDP open repository using multivariate binary logistic regression and forward and backward feature selection. Results indicate that the performance becomes unstable around 80% of imbalance. Copyright © by the paper's authors. Copying permitted only for private and academic purposes.",Data imbalance; Feature selection; Software defect prediction; Stability,CEUR Workshop Proceedings,2013,,"Grbac T.G., Mauša G., Dalbelo-Bašić B.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84866420561,10.1109/JCSSE.2012.6261973,Empirical investigation of fault prediction capability of object oriented metrics of open source software,"Open source software systems are playing important roles in many scientific and business software applications. To ensure acceptable levels of software quality Open source software (OSS) development process uses advanced and effective techniques. Quality improvement involves the detection of potential relationship between defect and open source software metrics. Many companies are investing in open source projects for making effective software. But, because open source software is often developed with a different management style and groups of people than the industrial ones, the quality and reliability of the code needs to be investigated. Hence, more projects need to be measured to obtain information about the characteristics and nature of the source code. This paper presents an empirical study of the fault prediction capabilities of object-oriented metrics given by Chidamber and Kemerer. We have carried out an empirical study and tried to find whether these metrics are significantly associated with faults or not. For this we have extracted source code processed it for metrics and associated it with the bugs. Finally the fault prediction capabilities of object oriented metrics have been evaluated by using Naïve Bayes and J48 machine learning algorithms © 2012 IEEE.",defects | open source software | Software metrics,JCSSE 2012 - 9th International Joint Conference on Computer Science and Software Engineering,2012-09-24,Conference Paper,"Singh, Pradeep;Verma, Shrish",Include,
10.1016/j.infsof.2022.107128,2-s2.0-79956104047,10.1007/s10489-009-0193-8,Mining software defect data to support software testing management,"Achieving high quality software would be easier if effective software development practices were known and deployed in appropriate contexts. Because our theoretical knowledge of the underlying principles of software development is far from complete, empirical analysis of past experience in software projects is essential for acquiring useful software practices. As advances in software technology continue to facilitate automated tracking and data collection, more software data become available. Our research aims to develop methods to exploit such data for improving software development practices. This paper proposes an empirical approach, based on the analysis of defect data, that provides support for software testing management in two ways: (1) construction of a predictive model for defect repair times, and (2) a method for assessing testing quality across multiple releases. The approach employs data mining techniques including statistical methods and machine learning. To illustrate the proposed approach, we present a case study using the defect reports created during the development of three releases of a large medical software system, produced by a large wellestablished software company. We validate our proposed testing quality assessment using a statistical test at a significance level of 0.1. Despite the limitations of the available data, our predictive models give accuracies as high as 93%. © Springer Science+Business Media, LLC 2009.",Data mining | Defect report | Quality assurance | Software testing management,Applied Intelligence,2011-04-01,Article,"Hewett, Rattikorn",Include,
10.1016/j.infsof.2022.107128,2-s2.0-77957365028,10.1016/j.infsof.2010.07.003,A novel composite model approach to improve software quality prediction,"Context:: How can quality of software systems be predicted before deployment? In attempting to answer this question, prediction models are advocated in several studies. The performance of such models drops dramatically, with very low accuracy, when they are used in new software development environments or in new circumstances. Objective: The main objective of this work is to circumvent the model generalizability problem. We propose a new approach that substitutes traditional ways of building prediction models which use historical data and machine learning techniques. Method: In this paper, existing models are decision trees built to predict module fault-proneness within the NASA Critical Mission Software. A genetic algorithm is developed to combine and adapt expertise extracted from existing models in order to derive a ""composite"" model that performs accurately in a given context of software development. Experimental evaluation of the approach is carried out in three different software development circumstances. Results: The results show that derived prediction models work more accurately not only for a particular state of a software organization but also for evolving and modified ones. Conclusion: Our approach is considered suitable for software data nature and at the same time superior to model selection and data combination approaches. It is then concluded that learning from existing software models (i.e., software expertise) has two immediate advantages; circumventing model generalizability and alleviating the lack of data in software-engineering. © 2010 Elsevier B.V. All rights reserved.",Decision trees | Fault-proneness | Genetic algorithm | Software quality prediction,Information and Software Technology,2010-12-01,Article,"Bouktif, Salah;Ahmed, Faheem;Khalil, Issa;Antoniol, Giuliano",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84898870457,10.1016/j.cl.2016.10.001,Empirical analysis of search based algorithms to identify change prone classes of open source software,"There are numerous reasons leading to change in software such as changing requirements, changing technology, increasing customer demands, fixing of defects etc. Thus, identifying and analyzing the change-prone classes of the software during software evolution is gaining wide importance in the field of software engineering. This would help software developers to judiciously allocate the resources used for testing and maintenance. Software metrics can be used for constructing various classification models which can be used for timely identification of change prone classes. Search based algorithms which form a subset of machine learning algorithms can be utilized for constructing prediction models to identify change prone classes of software. Search based algorithms use a fitness function to find the best optimal solution among all the possible solutions. In this work, we analyze the effectiveness of hybridized search based algorithms for change prediction. In other words, the aim of this work is to find whether search based algorithms are capable for accurate model construction to predict change prone classes. We have also constructed models using machine learning techniques and compared the performance of these models with the models constructed using Search Based Algorithms. The validation is carried out on two open source Apache projects, Rave and Commons Math. The results prove the effectiveness of hybridized search based algorithms in predicting change prone classes of software. Thus, they can be utilized by the software developers to produce an efficient and better developed software. © 2016 Elsevier Ltd",Confusion matrix | Fault | Machine learning,Automated Software Engineering,2014-04-01,Article,"Bowes, David;Hall, Tracy;Gray, David",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84962124868,10.1109/QRS.2015.17,Cross-Project Aging Related Bug Prediction,"In a long running system, software tends to encounter performance degradation and increasing failure rate during execution, which is called software aging. The bugs contributing to the phenomenon of software aging are defined as Aging Related Bugs (ARBs). Lots of manpower and economic costs will be saved if ARBs can be found in the testing phase. However, due to the low presence probability and reproducing difficulty of ARBs, it is usually hard to predict ARBs within a project. In this paper, we study whether and how ARBs can be located through cross-project prediction. We propose a transfer learning based aging related bug prediction approach (TLAP), which takes advantage of transfer learning to reduce the distribution difference between training sets and testing sets while preserving their data variance. Furthermore, in order to mitigate the severe class imbalance, class imbalance learning is conducted on the transferred latent space. Finally, we employ machine learning methods to handle the bug prediction tasks. The effectiveness of our approach is validated and evaluated by experiments on two real software systems. It indicates that after the processing of TLAP, the performance of ARB bug prediction can be dramatically improved. © 2015 IEEE.",aging related bug | bug prediction | ross-project | software aging | transfer learning,"Proceedings - 2015 IEEE International Conference on Software Quality, Reliability and Security, QRS 2015",2015-09-21,Conference Paper,"Qin, Fangyun;Zheng, Zheng;Bai, Chenggang;Qiao, Yu;Zhang, Zhenyu;Chen, Cheng",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84982298222,10.1016/j.procs.2015.12.108,Logistic Regression Ensemble for Predicting Customer Defection with Very Large Sample Size,"Predicting customer defection is an important subject for companies producing cloud based software. The studied company sell three products (High, Medium and Low Price), in which the consumer has choice to defect or retain the product after certain period of time. The fact that the company collected very large dataset leads to inapplicability of standard statistical models due to the curse of dimensionality. Parametric statistical models will tend to produce very big standard error which may lead to inaccurate prediction results. This research examines a machine learning approach developed for high dimensional data namely logistic regression ensemble (LORENS). Using computational approaches, LORENS has prediction ability as good as standard logistic regression model i.e. between 66% to 77% prediction accuracy. In this case, LORENS is preferable as it is more reliable and free of assumptions. © 2015 The Authors.",Change proneness | Empirical validation | Hybridized techniques | Object-oriented metrics | Predictive modeling | Search-based techniques,Automated Software Engineering,2017-09-01,Article,"Malhotra, Ruchika;Khanna, Megha",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85070984462,10.1109/ICACCI.2014.6968438,Performance analysis of ensemble learning for predicting defects in open source software,"Machine learning techniques have been earnestly explored by many software engineering researchers. At present state of art, there is no conclusive evidence on the kind of machine learning techniques which are most accurate and efficient for software defect prediction but some recent studies suggest that combining multiple machine learners, that is, ensemble learning, may be a more accurate alternative. This study contributes to software defect prediction literature by systematically evaluating the predictive accuracy of three well known homogeneous ensemble methods - Bagging, Boosting, and Rotation Forest, utilizing fifteen important underlying base learners, by exploiting the data of nine open source object-oriented systems obtained from the PROMISE repository. Results indicate while Bagging and Boosting may result in AUC performance loss, AUC performance improvement results in twelve of the fifteen investigated base learners with Rotation Forest ensemble. © 2014 IEEE.",B-method | Formal verification | Machine learning | Model checking | Model repair,Automated Software Engineering,2019-09-15,Article,"Cai, Cheng Hao;Sun, Jing;Dobbie, Gillian",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84904900815,10.1007/978-3-319-09156-3_17,Multiattribute based machine learning models for severity prediction in cross project context,"The severity level of a reported bug is an important attribute. It describes the impact of a bug on functionality of the software. In the available literature, machine learning techniques based prediction models have been proposed to assess the severity level of a bug. These prediction models have been developed by using summary of a reported bug i.e. the description of a bug reported by a user. This work has been also extended in cross project context to help the projects whose historical data is not available. Till now, the literature reveals that bug triager assess the severity level based on only the summary report of a bug but we feel that the severity level of a bug may change its value during the course of fixing and moreover, the severity level is not only characterized by the summary of bug report but also by other attributes namely priority, number of comments, number of dependents, number of duplicates, complexity, summary weight and cc list. In this paper, we have developed prediction models for determining the severity level of a reported bug based on these attributes in cross project context. For empirical validation, we considered 15,859 bug reports of Firefox, Thunderbird, Seamonkey, Boot2Gecko, Add-on SDK, Bugzilla, Webtools and addons.mozilla.org products of Mozilla open source project to develop the classification models based on Support Vector Machine (SVM), Naïve Bayes (NB) and K-Nearest Neighbors (KNN). © 2014 Springer International Publishing.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2014-01-01,Conference Paper,"Sharma, Meera;Kumari, Madhu;Singh, R. K.;Singh, V. B.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-79957566605,10.1109/ICCRD.2011.5763845,Software faults prediction using multiple classifiers,"In recent years, the use of machine learning algorithms (classifiers) has proven to be of great value in solving a variety of problems in software engineering including software faults prediction. This paper extends the idea of predicting software faults by using an ensemble of classifiers which has been shown to improve classification performance in other research fields. Benchmarking results on two NASA public datasets show all the ensembles achieving higher accuracy rates compared with individual classifiers. In addition, boosting with AR and DT as components of an ensemble is more robust for predicting software faults. © 2011 IEEE.",classifers | ensemble | fault prediction | machine learning | software metrics,ICCRD2011 - 2011 3rd International Conference on Computer Research and Development,2011-05-31,Conference Paper,"Twala, Bhekisipho",Include,
10.1016/j.infsof.2022.107128,2-s2.0-77949310357,10.1109/ACT.2009.212,An investigation of the effect of discretization on defect prediction using static measures,"Software repositories with defect logs are main resource for defect prediction. In recent years, researchers have used the vast amount of data that is contained by software repositories to predict the location of defect in the code that caused problems. In this paper we evaluate the effectiveness of software fault prediction with Naive-Bayes classifiers and J48 classifier by integrating with supervised discretization algorithm developed by Fayyad and Irani. Public datasets from the promise repository have been explored for this purpose. The repository contains software metric data and error data at the function/method level. Our experiment shows that integration of discretization method improves the software fault prediction accuracy when integrated with Naive-Bayes and J48 classifiers. © 2009 IEEE.",Defect prediction | Machine learning | Software metrics,"ACT 2009 - International Conference on Advances in Computing, Control and Telecommunication Technologies",2009-12-01,Conference Paper,"Singh, Pradeep;Verma, Shirish",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85086086309,10.5815/ijmecs.2020.01.03,A classification framework for software defect prediction using multi-filter feature selection technique and MLP,"Production of high quality software at lower cost can be possible by detecting defect prone software modules before the testing process. With this approach, less time and resources are required to produce a high quality software as only those modules are thoroughly tested which are predicted as defective. This paper presents a classification framework which uses Multi-Filter feature selection technique and Multi-Layer Perceptron (MLP) to predict defect prone software modules. The proposed framework works in two dimensions: 1) with oversampling technique, 2) without oversampling technique. Oversampling is introduced in the framework to analyze the effect of class imbalance issue on the performance of classification techniques. The framework is implemented by using twelve cleaned NASA MDP datasets and performance is evaluated by using: F-measure, Accuracy, MCC and ROC. According to results the proposed framework with class balancing technique performed well in all of the used datasets. © 2020 MECS.",Artificial Neural Network | Feature Selection | Machine Learning Techniques | MLP | Multi-Filter Feature Selection | Software Defect Prediction,International Journal of Modern Education and Computer Science,2020-01-01,Article,"Iqbal, Ahmed;Aftab, Shabib",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85119969463,10.1109/IconDSC.2019.8816965,Software Bug Prediction Using Supervised Machine Learning Algorithms,"Machine Learning algorithms sprawl their application in various fields relentlessly. Software Engineering is not exempted from that. Software bug prediction at the initial stages of software development improves the important aspects such as software quality, reliability, and efficiency and minimizes the development cost. In majority of software projects which are becoming increasingly large and complex programs, bugs are serious challenge for system consistency and efficiency. In our approach, three supervised machine learning algorithms are considered to build the model and predict the occurrence of the software bugs based on historical data by deploying the classifiers Logistic regression, Naive Bayes, and Decision Tree. Historical data has been used to predict the future software faults by deploying the classifier algorithms and make the models a better choice for predictions using random forest ensemble classifiers and validating the models with K-Fold cross validation technique which results in the model effectively working for all the scenarios. © 2019 IEEE.",Machine learning | Mobile applications | Opinion mining | Requirements engineering,Automated Software Engineering,2022-05-01,Article,"Araujo, Adailton F.;Gôlo, Marcos P.S.;Marcacini, Ricardo M.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85067980952,10.30534/ijatcse/2019/33822019,Progress on machine learning techniques for software fault prediction,"Software fault prediction is a significant part of software engineering. Fault prediction means to identify fault prone modules at the early stage of software development. It helps to reduce overall testing time, effort, and cost. It significantly improves the goodwill and profit of the organization by providing customer satisfaction. This area attracted many researchers over the years to improve overall software quality. Machine learning techniques are the most widely used techniques now-a-days in this area. This paper presents a comprehensive review on various machine learning techniques that will help the practitioners who are interested in building fault prediction model. This paper also discusses the substantial research performed in software fault prediction using machine learning techniques. A future dimension is also proposed to narrow the research gap by utilizing the research findings of existing models. © 2019, World Academy of Research in Science and Engineering. All rights reserved.",Classification | Machine learning | Software faults prediction | Software metrics,International Journal of Advanced Trends in Computer Science and Engineering,2019-03-01,Article,"Goyal, Jyoti;Kishan, Bal",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85122086312,10.1142/S0218213018500240,"Omni-Ensemble Learning (OEL): Utilizing Over-Bagging, Static and Dynamic Ensemble Selection Approaches for Software Defect Prediction","Machine learning methods in software engineering are becoming increasingly important as they can improve quality and testing efficiency by constructing models to predict defects in software modules. The existing datasets for software defect prediction suffer from an imbalance of class distribution which makes the learning problem in such a task harder. In this paper, we propose a novel approach by integrating Over-Bagging, static and dynamic ensemble selection strategies. The proposed method utilizes most of ensemble learning approaches called Omni-Ensemble Learning (OEL). This approach exploits a new Over-Bagging method for class imbalance learning in which the effect of three different methods of assigning weight to training samples is investigated. The proposed method first specifies the best classifiers along with their combiner for all test samples through Genetic Algorithm as the static ensemble selection approach. Then, a subset of the selected classifiers is chosen for each test sample as the dynamic ensemble selection. Our experiments confirm that the proposed OEL can provide better overall performance (in terms of G-mean, balance, and AUC measures) comparing with other six related works and six multiple classifier systems over seven NASA datasets. We generally recommend OEL to improve the performance of software defect prediction and the similar problem based on these experimental results. © 2018 World Scientific Publishing Company.",Machine learning | Refactoring | Software engineering | Software quality,Automated Software Engineering,2022-05-01,Article,"AlOmar, Eman Abdullah;Liu, Jiaqian;Addo, Kenneth;Mkaouer, Mohamed Wiem;Newman, Christian;Ouni, Ali;Yu, Zhe",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84979519042,10.1007/978-981-10-5547-8_56,Software fault prediction using machine-learning techniques,"Machine-learning techniques are used to find the defect, fault, ambiguity, and bad smell to accomplish quality, maintainability, and reusability in software. Software fault prediction techniques are used to predict software faults by using statistical techniques. However, Machine-learning techniques are also valuable in detecting software fault. This paper presents an overview of software fault prediction using machine-learning techniques to predict the occurrence of faults. This paper also presents the conventional techniques. It aims at describing the problem of fault proneness. © Springer Nature Singapore Pte Ltd. 2018.",Least square support vector machine | Machine learning | Real-time system | Software safety | Worst case execution time,Cluster Computing,2016-09-01,Article,"Meng, Fanqi;Su, Xiaohong;Qu, Zhaoyang",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85014904256,10.1007/978-981-10-3156-4_6,Predicting defect of software system,"Any particular study on software quality with all desirable attributes of software products can be treated as complete and perfect provided it is defective. Defects continue to be an emerging problem that leads to failure and unexpected behaviour of the system. Prediction of defect in software system in the initial stage may be favourable to a great extend in the process of finding out defects and making the software system efficient, defect-free and improving its over-all quality. To analyze and compare the work done by the researchers on predicting defects of software system, it is necessary to have a look on their varied work. The most frequently used methodologies for predicting defects in the software system have been highlighted in this paper and it has been observed that use of public datasets were considerably more than use of private datasets. On the basis of over-all findings, the key analysis and challenging issues have been identified which will help and encourage further work in this field with application of newer and more effective methodologies. © Springer Nature Singapore Pte Ltd. 2017.",Data mining | Defect prediction | Machine learning | Software datasets | Software defects,Advances in Intelligent Systems and Computing,2017-01-01,Conference Paper,"Ghosh, Soumi;Rana, Ajay;Kansal, Vineet",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85009812066,10.1109/ISSREW.2016.43,On Automatic Detection of Performance Bugs,"Context: Software performance is a critical non-functional requirement, appearing in many fields such as mission critical applications, financial, and real time systems. In this work we focused on early detection of performance bugs, our software under study was a real time system used in the mobile advertisement / marketing domain. Goal: Find a simple and easy to implement solution, predicting performance bugs. Method: We built several models using four machine learning methods, commonly used for defect prediction: C4.5 Decision Trees, Naive Bayes, Bayesian Networks, and Logistic Regression. Results: Our empirical results show that a C4.5 model, using lines of code changed, file's age and size as explanatory variables, can be used to predict performance bugs (recall = 0.73, accuracy = 0.85, and precision = 0.96). We show that reducing the number of changes delivered on a commit, can decrease the chance of performance bug injection. Conclusions: We believe that our approach can help practitioners to eliminate performance bugs early in the development cycle. Our results are also of interest to theoreticians, establishing a link between functional bugs and (non-functional) performance bugs, and explicitly showing that attributes used for prediction of functional bugs can be used for prediction of performance bugs. © 2016 IEEE.",Mobile Advertisement | Performance Bugs | Performance Optimization | Quality Assurance | Software Faults,"Proceedings - 2016 IEEE 27th International Symposium on Software Reliability Engineering Workshops, ISSREW 2016",2016-12-16,Conference Paper,"Tsakiltsidis, Sokratis;Miranskyy, Andriy;Mazzawi, Elie",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85013306212,10.1109/ISSRE.2016.29,CoLUA: Automatically Predicting Configuration Bug Reports and Extracting Configuration Options,"Configuration bugs are among the dominant causes of software failures. Software organizations often use bug tracking systems to manage bug reports collected from developers and users. In order for software developers to understand and reproduce configuration bugs, it is vital for them to know whether a bug in the bug report is related to configuration issues, this is not often easily discerned due to a lack of easy to spot terminology in the bug reports. In addition, to locate and fix a configuration bug, a developer needs to know which configuration options are associated with the bug. To address these two problems, we introduce CoLUA, a two-step automated approach that combines natural language processing, information retrieval, and machine learning. In the first step, CoLUA selects features from the textual information in the bug reports, and uses various machine learning techniques to build classification models, developers can use these models to label a bug report as either a configuration bug report or a non-configuration bug report. In the second step, CoLUA identifies which configuration options are involved in the labeled configuration bug reports. We evaluate CoLUA on 900 bug reports from three large open source software systems. The results show that CoLUA predicts configuration bug reports with high accuracy and that it effectively identifies the root causes of configuration options. © 2016 IEEE.",bug reports | configuration | machine learning,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2016-12-05,Conference Paper,"Wen, Wei;Yu, Tingting;Hayes, Jane Huffman",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85117892702,10.1145/2908812.2908938,A search-based training algorithm for cost-aware defect prediction,"Research has yielded approaches to predict future defects in software artifacts based on historical information, thus assisting companies in effectively allocating limited development resources and developers in reviewing each others' code changes. Developers are unlikely to devote the same effort to inspect each software artifact predicted to contain defects, since the effort varies with the artifacts' size (cost) and the number of defects it exhibits (effectiveness). We propose to use Genetic Algorithms (GAs) for training prediction models to maximize their cost-effectiveness. We evaluate the approach on two well-known models, Regression Tree and Generalized Linear Model, and predict defects between multiple releases of six open source projects. Our results show that regression models trained by GAs significantly outperform their traditional counterparts, improving the cost-effectiveness by up to 240%. Often the top 10% of predicted lines of code contain up to twice as many defects. © 2016 ACM.",A star search algorithm | Intelligent transmission control layer (ITCL) | Machine learning | Routing | Software defined networking (SDN) | Traffic engineering,Cluster Computing,2022-04-01,Article,"Aldabbas, Hamza;Khatatneh, Khalaf",Include,
10.1016/j.infsof.2022.107128,2-s2.0-61849088918,10.1109/ATC.2014.7043405,Similarity-based and rank-based defect prediction,"In this paper, we explore two new approaches for software defect prediction. The similarity-based approach predicts the number of latent defects of a software module from those of modules most similar to it. The rank-based approach uses machine learning models specially trained to predict the ranks of software modules based on their actual number of latent defects. In both approaches, we use technical concerns/functionalities recovered by topic modeling techniques as features to represent software modules. Empirical evaluation with five real software systems shows that the proposed approaches outperform the traditional one and a recently introduced defect prediction method. © 2014 IEEE.",Data mining | Defect report analysis | Quality assurance | Software testing | Testing management,Empirical Software Engineering,2009-04-01,Article,"Hewett, Rattikorn;Kijsanayothin, Phongphun",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84940725650,10.14257/ijseia.2014.8.12.17,Software defect prediction using a high performance neural network,"Predicting the existing defects in software products is one of the considerable issues in software engineering that contributes a lot toward saving time in software production and maintenance process. In fact, finding the desirable models for predicting software defects has nowadays turned into one of the main goals of software engineers. Since intricacies and restrictions of software development are increasing and unwilling consequences such as failure and errors decrease software quality and customer satisfaction, producing error-free software is very difficult and challenging. One of the efficient models in this field is multilayer neural network with proper learning algorithm. Many of the learning algorithms suffer from extra overfitting in the learning datasets. In this article, setting multilayer neural network method was used in order to improve and increase generalization capability of learning algorithm in predicting software defects. In order to solve the existing problems, a new method is proposed by developing new learning methods based on support vector machine principles and using evolutionary algorithms. The proposed method prevents from overfitting issue and maximizes classification margin. Efficiency of the proposed algorithm has been validated against 11 machine learning models and statistical methods within 3 NASA datasets. Results reveal that the proposed algorithm provides higher accuracy and precision compared to the other models. © 2014 SERSC.",Bug report management | Multi-factor analysis | Priority prediction,Empirical Software Engineering,2015-10-04,Article,"Tian, Yuan;Lo, David;Xia, Xin;Sun, Chengnian",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84903604284,10.1145/2591062.2591099,Software bug localization with markov logic,"Software bug localization is the problem of determining buggy statements in a software system. It is a crucial and expensive step in the software debugging process. Interest in it has grown rapidly in recent years, and many approaches have been proposed. However, existing approaches tend to use isolated information to address the problem, and are often ad hoc. In particular, most existing approaches predict the likelihood of a statement being buggy se-quentially and separately. This paper proposes a well-founded, integrated solution to the software bug localization problem based on Markov logic. Markov logic combines first-order logic and probabilistic graphical models by attaching weights to first-order formulas, and views them as templates for features of Markov networks. We show how a number of salient program features can be seamlessly combined in Markov logic, and how the resulting joint inference can be solved. We implemented our approach in a debugging system, called MLNDEBUGGER, and evaluated it on 4 small programs. Our initial results demonstrated that our approach achieved higher accuracy than a previous approach. Copyright © 2014 ACM.",Automated debugging | Machine learning,"36th International Conference on Software Engineering, ICSE Companion 2014 - Proceedings",2014-01-01,Conference Paper,"Zhang, Sai;Zhang, Congle",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84928629842,10.1587/transinf.E95.D.267,Kernel based asymmetric learning for software defect prediction,"A kernel based asymmetric learning method is developed for software defect prediction. This method improves the performance of the predictor on class imbalanced data, since it is based on kernel principal component analysis. An experiment validates its effectiveness. Copyright © 2012 The Institute of Electronics, Information and Communication Engineers.",Empirical study | Machine learning techniques | Software release notes,Empirical Software Engineering,2016-06-01,Article,"Abebe, Surafel Lemma;Ali, Nasir;Hassan, Ahmed E.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84941356343,10.1109/ICCCT.2011.6075151,Impact of attribute selection on defect proneness prediction in OO software,"Defect proneness prediction of software modules always attracts the developers because it can reduce the testing efforts as well as software development time. In the current context, with the piling up of constraints like requirement ambiguity and complex development process, developing fault free reliable software is a daunting task. To deliver reliable software, software engineers are required to execute exhaustive test cases which become tedious and costly for software enterprises. To ameliorate the testing process one can use a defect prediction model so that testers can focus their efforts on defect prone modules. Building a defect prediction model becomes very complex task when the number of attributes is very large and the attributes are correlated. It is not easy even for a simple classifier to cope with this problem. Therefore, while developing a defect proneness prediction model, one should always be careful about feature selection. This research analyzes the impact of attribute selection on Naive Bayes (NB) based prediction model. Our results are based on Eclipse and KC1 bug database. On the basis of experimental results, we show that careful combination of attribute selection and machine learning apparently useful and, on the Eclipse data set, yield reasonable good performance with 88% probability of detection and 49% false alarm rate. © 2011 IEEE.",Bug assignment | Bug reports | Classification | Ensemble learning | Industrial scale; Large scale | Machine learning,Empirical Software Engineering,2016-08-01,Article,"Jonsson, Leif;Borg, Markus;Broman, David;Sandahl, Kristian;Eldh, Sigrid;Runeson, Per",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85082174766,10.1145/3379247.3379278,The impact of software fault prediction in real-world application: An automated approach for software engineering,"Software fault prediction and proneness has long been considered as a critical issue for the tech industry and software professionals. In the traditional techniques, it requires previous experience of faults or a faulty module while detecting the software faults inside an application. An automated software fault recovery models enable the software to significantly predict and recover software faults using machine learning techniques. Such ability of the feature makes the software to run more effectively and reduce the faults, time and cost. In this paper, we proposed a software defect predictive development models using machine learning techniques that can enable the software to continue its projected task. Moreover, we used different prominent evaluation benchmark to evaluate the model's performance such as ten-fold crossvalidation techniques, precision, recall, specificity, f 1 measure, and accuracy. This study reports a significant classification performance of 98-100% using SVM on three defect datasets in terms of f1 measure. However, software practitioners and researchers can attain independent understanding from this study while selecting automated task for their intended application. © 2020 ACM International Conference Proceeding Series. All rights reserved.",Defect prediction | Machine learning | Software engineering | Software fault,ACM International Conference Proceeding Series,2020-01-04,Conference Paper,"Ahmed, Md Razu;Zamal, Md Fahad Bin;Ali, Md Asraf;Shamrat, F. M.Javed Mehedi;Ahmed, Nasim",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85092646001,10.1007/978-3-030-58817-5_45,SMOTE-Based Homogeneous Ensemble Methods for Software Defect Prediction,"Class imbalance is a prevalent problem in machine learning which affects the prediction performance of classification algorithms. Software Defect Prediction (SDP) is no exception to this latent problem. Solutions such as data sampling and ensemble methods have been proposed to address the class imbalance problem in SDP. This study proposes a combination of Synthetic Minority Oversampling Technique (SMOTE) and homogeneous ensemble (Bagging and Boosting) methods for predicting software defects. The proposed approach was implemented using Decision Tree (DT) and Bayesian Network (BN) as base classifiers on defects datasets acquired from NASA software corpus. The experimental results showed that the proposed approach outperformed other experimental methods. High accuracy of 86.8% and area under operating receiver characteristics curve value of 0.93% achieved by the proposed technique affirmed its ability to differentiate between the defective and non-defective labels without bias. © 2020, Springer Nature Switzerland AG.",Class imbalance | Data sampling | Ensemble methods | Software Defect Prediction,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020-01-01,Conference Paper,"Balogun, Abdullateef O.;Lafenwa-Balogun, Fatimah B.;Mojeed, Hammed A.;Adeyemo, Victor E.;Akande, Oluwatobi N.;Akintola, Abimbola G.;Bajeh, Amos O.;Usman-Hamza, Fatimah E.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85077025531,10.2298/CSIS180312039B,Majority vote feature selection algorithm in software fault prediction,"Identification and location of defects in software projects is an important task to improve software quality and to reduce software test effort estimation cost. In software fault prediction domain, it is known that 20% of the modules will in general contain about 80% of the faults. In order to minimize cost and effort, it is considerably important to identify those most error prone modules precisely and correct them in time. Machine Learning (ML) algorithms are frequently used to locate error prone modules automatically. Furthermore, the performance of the algorithms is closely related to determine the most valuable software metrics. The aim of this research is to develop a Majority Vote based Feature Selection algorithm (MVFS) to identify the most valuable software metrics. The core idea of the method is to identify the most influential software metrics with the collaboration of various feature rankers. To test the efficiency of the proposed method, we used CM1, JM1, KC1, PC1, Eclipse Equinox, Eclipse JDT datasets and J48, NB, K-NN (IBk) ML algorithms. The experiments show that the proposed method is able to find out the most significant software metrics that enhances defect prediction performance. © 2019, ComSIS Consortium. All rights reserved.",Machine learning | Mutant prioritization | Mutant selection | Mutation testing,Empirical Software Engineering,2020-01-01,Article,"Titcheu Chekam, Thierry;Papadakis, Mike;Bissyandé, Tegawendé F.;Le Traon, Yves;Sen, Koushik",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85041176093,10.1109/TSE.2018.2791521,ConPredictor: Concurrency Defect Prediction in Real-World Applications,"Concurrent programs are difficult to test due to their inherent non-determinism. To address this problem, testing often requires the exploration of thread schedules of a program; this can be time-consuming when applied to real-world programs. Software defect prediction has been used to help developers find faults and prioritize their testing efforts. Prior studies have used machine learning to build such predicting models based on designed features that encode the characteristics of programs. However, research has focused on sequential programs; to date, no work has considered defect prediction for concurrent programs, with program characteristics distinguished from sequential programs. In this paper, we present ConPredictor, an approach to predict defects specific to concurrent programs by combining both static and dynamic program metrics. Specifically, we propose a set of novel static code metrics based on the unique properties of concurrent programs. We also leverage additional guidance from dynamic metrics constructed based on mutation analysis. Our evaluation on four large open source projects shows that ConPredictor improved both within-project defect prediction and cross-project defect prediction compared to traditional features. © 1976-2012 IEEE.",Concurrency | defect prediction | software metrics | software quality,IEEE Transactions on Software Engineering,2019-06-01,Article,"Yu, Tingting;Wen, Wei;Han, Xue;Hayes, Jane Huffman",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85053514501,10.1109/SNPD.2018.8441143,Comparative study on defect prediction algorithms of supervised learning software based on imbalanced classification data sets,"With the development of high complexity and high integration of software systems, the quality of software has gradually received widespread attention in scientific research and engineering. Software defect prediction technology plays an important role in improving software quality, reducing software development time, and reducing testing expenses. It has also become one of the hot issues in the field of software engineering research in recent years. As an important learning method in machine learning, supervised learning is widely used in the classification and regression prediction with annotation data because of its high accuracy, mature theory, and simple calculation. However, the imbalanced classification problem of data sets is common in practical applications and seriously affects the performance of learning algorithm. This paper analyzes the characteristics of software forecasting technology from the perspective of supervised learning, and performs balance like processing on imbalanced classification NASA data sets (JM1, KC3, MC1). NASA data sets after application processing are used to conduct experiments on LWL, C4.5, Random forest, Bagging, Bayesian Belief Network, Multilayer Feed forward Neural Network, SVM and NB-K algorithms, and experimental data is analyzed and evaluated. © 2018 IEEE.",algorithm | data set | imbalanced classification | software defect prediction | supervised learning,"Proceedings - 2018 IEEE/ACIS 19th International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD 2018",2018-08-20,Conference Paper,"Ge, Jianxin;Liu, Jiaomin;Liu, Wenyuan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85013500003,,Software defect prediction in large space systems through hybrid feature selection and classification,"Data mining and machine learning techniques have been used in several scientific applications including software fault predictions in large space systems. State-of the-art research revealed that existing space systems succumb to enigmatic software faults leading to critical loss of life and capital. This article presents a novel approach to solve this issue of overlooking software faults by utilizing both features selection and classification techniques to accurately predict software defects in aerospace systems. The main objective was to identify the preeminent feature selection and prediction technique that enhanced the software fault prediction accuracy with the optimal set of features. The investigations affirmed that a novel hybrid feature selection method revealed the most optimal set of predictive features although no particular predictive technique was suitable to predict faults in all space system datasets. Besides, the exploration of data mining techniques in fault prediction on the NASA Lunar space system software data clearly portrayed the improved fault prediction accuracy (~82% to ~98%) with the feature set selected by the proposed Hybrid Feature Selection method. Also, the random sub sampling method revealed an improved mean Matthew’s Correlation Coefficient (MCC) and accuracy ranging from ~0.7 to ~0.9 and ~86% to ~98% respectively. This we believe generates further scope for future investigations on the most contributing space system features for fault prediction thus enabling design of aerospace systems with minimal faults and enhanced performance. © 2017, Zarka Private University. All rights reserved.",Classification; Data mining; Hybrid feature selection; NASA datasets; Prediction; Software defects,International Arab Journal of Information Technology,2017,,"Jacob S., Raju G.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85122659920,10.1016/j.asoc.2016.08.006,On deterministic chaos in software reliability growth models,"Software reliability growth models attempt to forecast the future reliability of a software system, based on observations of the historical occurrences of failures. This allows management to estimate the failure rate of the system in field use, and to set release criteria based on these forecasts. However, the current software reliability growth models have never proven to be accurate enough for widespread industry use. One possible reason is that the model forms themselves may not accurately capture the underlying process of fault injection in software; it has been suggested that fault injection is better modeled as a chaotic process rather than a random one. This possibility, while intriguing, has not yet been evaluated in large-scale, modern software reliability growth datasets. We report on an analysis of four software reliability growth datasets, including ones drawn from the Android and Mozilla open-source software communities. These are the four largest software reliability growth datasets we are aware of in the public domain, ranging from 1200 to over 86,000 observations. We employ the methods of nonlinear time series analysis to test for chaotic behavior in these time series; we find that three of the four do show evidence of such behavior (specifically, a multifractal attractor). Finally, we compare a deterministic time series forecasting algorithm against a statistical one on both datasets, to evaluate whether exploiting the apparent chaotic behavior might lead to more accurate reliability forecasts. © 2016 Elsevier B.V.",Expert opinion | Labeling datasets | Software fault proneness | Software metrics,Multimedia Tools and Applications,2022-04-01,Article,"Rizwan, Muhammad;Nadeem, Aamer;Sarwar, Sohail;Iqbal, Muddesar;Safyan, Muhammad;Qayyum, Zia Ul",Include,
10.1016/j.infsof.2022.107128,2-s2.0-79952760269,10.11591/ijece.v6i4.9991,Software reliability prediction using fuzzy min-max algorithm and recurrent neural network approach,"Fuzzy Logic (FL) together with Recurrent Neural Network (RNN) is used to predict the software reliability. Fuzzy Min-Max algorithm is used to optimize the number of the kgaussian nodes in the hidden layer and delayed input neurons. The optimized recurrent neural network is used to dynamically reconfigure in real-time as actual software failure. In this work, an enhanced fuzzy min-max algorithm together with recurrent neural network based machine learning technique is explored and a comparative analysis is performed for the modeling of reliability prediction in software systems. The model has been applied on data sets collected across several standard software projects during system testing phase with fault removal. The performance of our proposed approach has been tested using distributed system application failure data set. Copyright © 2016 Institute of Advanced Engineering and Science. All rights reserved.",Ad hoc wireless network | Artificial immune system | Co-stimulation | Energy efficient design | Misbehavior detection | Sensor network,Natural Computing,2011-03-01,Conference Paper,"Drozda, Martin;Schaust, Sven;Schildt, Sebastian;Szczerbicka, Helena",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84975873980,10.1145/2851613.2851788,A meta-learning framework for algorithm recommendation in software fault prediction,"Software fault prediction is a significant part of software quality assurance and it is commonly used to detect faulty software modules based on software measurement data. Several machine learning based approaches have been proposed for generating predictive models from collected data, although none has become standard given the specificities of each software project. Hence, we believe that recommending the best algorithm for each project is much more important and useful than developing a single algorithm for being used in any project. For achieving that goal, we propose in this paper a novel framework for recommending machine learning algorithms that is capable of automatically identifying the most suitable algorithm according to the software project that is being considered. Our solution, namely SFP-MLF, makes use of the meta-learning paradigm in order to learn the best learner for a particular project. Results show that the SFP-MLF framework provides both the best single algorithm recommendation and also the best ranking recommendation for the software fault prediction problem. © 2016 ACM.",Algorithm recommendation | Machine learning | Meta-learning | Software fault prediction | Software quality,Proceedings of the ACM Symposium on Applied Computing,2016-04-04,Conference Paper,"Das Dôres, Silvia N.;Alves, Luciano;Ruiz, Duncan D.;Barros, Rodrigo C.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84994663032,10.4114/ia.v18i56.1159,An automated defect prediction framework using genetic algorithms: A validation of empirical studies,"Today, it is common for software projects to collect measurement data through development processes. With these data, defect prediction software can try to estimate the defect proneness of a software module, with the objective of assisting and guiding software practitioners. With timely and accurate defect predictions, practi- tioners can focus their limited testing resources on higher risk areas. This paper reports the results of three empirical studies that uses an automated genetic defect prediction framework. This framework generates and compares different learning schemes (preprocessing + attribute selection + learning algorithms) and selects the best one using a genetic algorithm, with the objective to estimate the defect proneness of a software module. The first empirical study is a performance comparison of our framework with the most important framework of the literature. The second empirical study is a performance and runtime comparison between our framework and an exhaustive framework. The third empirical study is a sensitivity analysis. The last empirical study, is our main contribution in this paper. Performance of the software development defect prediction models (using AUC, Area Under the Curve) was validated using NASA-MDP and PROMISE data sets. Seventeen data sets from NASA-MDP (13) and PROMISE (4) projects were analyzed running a N × M-fold cross-validation. A genetic algorithm was used to select the components of the learning schemes automatically, and to assess and report the results. Our results reported similar performance between frameworks. Our framework reported better runtime than exhaustive framework. Finally, we reported the best configuration according to sensitivity analysis. © IBERAMIA and the authors.",Fault prediction models | Genetic algorithms | Learning algorithms | Learning schemes | Machine learning | Software quality,Inteligencia Artificial,2016-01-01,Article,"Murillo-Morera, Juan;Castro-Herrera, Carlos;Arroyo, Javier;Fuentes-Fernández, Rubén",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84985986786,10.1016/j.procs.2016.07.271,A Novel Machine Learning Approach for Bug Prediction,"With the growing complexities of the software, the number of potential bugs is also increasing rapidly. These bugs hinder the rapid software development cycle. Bugs, if left unresolved, might cause problems in the long run. Also, without any prior knowledge about the location and the number of bugs, managers may not be able to allocate resources in an efficient way. In order to overcome this problem, researchers have devised numerous bug prediction approaches so far. The problem with the existing models is that the researchers have not been able to arrive at an optimized set of metrics. So, in this paper, we make an attempt to select the minimal number of best performing metrics, thereby keeping the model both simple and accurate at the same time. Most of the bug prediction models use regression for prediction and since regression is a technique to best approximate the training data set, the approximations don't always fit well with the test data set. Keeping this in mind, we propose an algorithm to predict the bug proneness index using marginal R square values. Though regressions are performed as intermediary steps in this algorithm, the underlying logic is different in nature when compared with the models using regressions alone. © 2016 The Authors. Published by Elsevier B.V.",Bug prediction metrics | F-measure | Marginal R square | Multiple regression,Procedia Computer Science,2016-01-01,Conference Paper,"Puranik, Shruthi;Deshpande, Pranav;Chandrasekaran, K.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84946238088,10.1109/ICACCI.2015.7275614,Mining the impact of object oriented metrics for change prediction using Machine Learning and Search-based techniques,"Change in a software is crucial to incorporate defect correction and continuous evolution of requirements and technology. Thus, development of quality models to predict the change proneness attribute of a software is important to effectively utilize and plan the finite resources during maintenance and testing phase of a software. In the current scenario, a variety of techniques like the statistical techniques, the Machine Learning (ML) techniques and the Search-based techniques (SBT) are available to develop models to predict software quality attributes. In this work, we assess the performance of ten machine learning and search-based techniques using data collected from three open source software. We first develop a change prediction model using one data set and then we perform inter-project validation using two other data sets in order to obtain unbiased and generalized results. The results of the study indicate comparable performance of SBT with other employed statistical and ML techniques. This study also supports inter project validation as we successfully applied the model created using the training data of one project on other similar projects and yield good results. © 2015 IEEE.",Change proneness | Empirical validation | Inter project validation | Machine learning techniques | Search-based techniques | Software Quality,"2015 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2015",2015-09-24,Conference Paper,"Malhotra, Ruchika;Khanna, Megha",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84908892374,10.5220/0005099303830392,A framework for adoption of machine learning in industry for software defect prediction,"Machine learning algorithms are increasingly being used in a variety of application domains including software engineering. While their practical value have been outlined, demonstrated and highlighted in number of existing studies, their adoption in industry is still not widespread. The evaluations of machine learning algorithms in literature seem to focus on few attributes and mainly on predictive accuracy. On the other hand the decision space for adoption or acceptance of machine learning algorithms in industry encompasses much more factors. Companies looking to adopt such techniques want to know where such algorithms are most useful, if the new methods are reliable and cost effective. Further questions such as how much would it cost to setup, run and maintain systems based on such techniques are currently not fully investigated in the industry or in academia leading to difficulties in assessing the business case for adoption of these techniques in industry. In this paper we argue for the need of framework for adoption of machine learning in industry. We develop a framework for factors and attributes that contribute towards the decision of adoption of machine learning techniques in industry for the purpose of software defect predictions. The framework is developed in close collaboration within industry and thus provides useful insight for industry itself, academia and suppliers of tools and services.",Adoption | Machine learning | SDP: Software Defect Prediction | Software defect prediction | Software quality acronyms used - ML: Machine Learning | TAM: Technology Acceptance Model | Technology acceptance,ICSOFT-EA 2014 - Proceedings of the 9th International Conference on Software Engineering and Applications,2014-01-01,Conference Paper,"Rana, Rakesh;Staron, Miroslaw;Hansson, Jörgen;Nilsson, Martin;Meding, Wilhelm",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85106471496,10.1145/3377811.3380403,An investigation of cross-project learning in online just-in-time sotfware defect prediction,"Just-In-Time Software Defect Prediction (JIT-SDP) is concerned with predicting whether software changes are defect-inducing or clean based on machine learning classifiers. Building such classifiers requires a sufficient amount of training data that is not available at the beginning of a software project. Cross-Project (CP) JIT-SDP can overcome this issue by using data from other projects to build the classifier, achieving similar (not better) predictive performance to classifiers trained on Within-Project (WP) data. However, such approaches have never been investigated in realistic online learning scenarios, where WP software changes arrive continuously over time and can be used to update the classifiers. It is unknown to what extent CP data can be helpful in such situation. In particular, it is unknown whether CP data are only useful during the very initial phase of the project when there is little WP data, or whether they could be helpful for extended periods of time. This work thus provides the first investigation of when and to what extent CP data are useful for JIT-SDP in a realistic online learning scenario. For that, we develop three different CP JIT-SDP approaches that can operate in online mode and be updated with both incoming CP and WP training examples over time.We also collect 2048 commits from three software repositories being developed by a software company over the course of 9 to 10 months, and use 19,8468 commits from 10 active open source GitHub projects being developed over the course of 6 to 14 years. The study shows that training classifiers with incoming CP+WP data can lead to improvements in G-mean of up to 53.90% compared to classifiers using only WP data at the initial stage of the projects. For the open source projects, which have been running for longer periods of time, using CP data to supplement WP data also helped the classifiers to reduce or prevent large drops in predictive performance that may occur over time, leading to up to around 40% better G-Mean during such periods. Such use of CP data was shown to be beneficial even after a large number of WP data were received, leading to overall G-means up to 18.5% better than those of WP classifiers. © 2020 Association for Computing Machinery.",Classification model | Machine learning | Security | Software repository | Text classification,Software Quality Journal,2021-06-01,Article,"Oyetoyan, Tosin Daniel;Morrison, Patrick",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85076559059,10.1016/j.jss.2019.110486,A machine-learning based ensemble method for anti-patterns detection,"Anti-patterns are poor solutions to recurring design problems. Several empirical studies have highlighted their negative impact on program comprehension, maintainability, as well as fault-proneness. A variety of detection approaches have been proposed to identify their occurrences in source code. However, these approaches can identify only a subset of the occurrences and report large numbers of false positives and misses. Furthermore, a low agreement is generally observed among different approaches. Recent studies have shown the potential of machine-learning models to improve this situation. However, such algorithms require large sets of manually-produced training-data, which often limits their application in practice. In this paper, we present SMAD (SMart Aggregation of Anti-patterns Detectors), a machine-learning based ensemble method to aggregate various anti-patterns detection approaches on the basis of their internal detection rules. Thus, our method uses several detection tools to produce an improved prediction from a reasonable number of training examples. We implemented SMAD for the detection of two well known anti-patterns: God Class and Feature Envy. With the results of our experiments conducted on eight java projects, we show that: (1) Our method clearly improves the so aggregated tools; (2) SMAD significantly outperforms other ensemble methods. © 2019",Anti-patterns | Ensemble methods | Machine learning | Software quality,Journal of Systems and Software,2020-03-01,Article,"Barbez, Antoine;Khomh, Foutse;Guéhéneuc, Yann Gaël",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85074644798,10.1145/3345628,Precise learn-to-rank fault localization using dynamic and static features of target programs,"Finding the root cause of a bug requires a significant effort from developers. Automated fault localization techniques seek to reduce this cost by computing the suspiciousness scores (i.e., the likelihood of program entities being faulty). Existing techniques have been developed by utilizing input features of specific types for the computation of suspiciousness scores, such as program spectrum or mutation analysis results. This article presents a novel learn-to-rank fault localization technique called PRecise machINe-learning-based fault loCalization tEchnique (PRINCE). PRINCE uses genetic programming (GP) to combine multiple sets of localization input features that have been studied separately until now. For dynamic features, PRINCE encompasses both Spectrum Based Fault Localization (SBFL) and Mutation Based Fault Localization (MBFL) techniques. It also uses static features, such as dependency information and structural complexity of program entities. All such information is used by GP to train a ranking model for fault localization. The empirical evaluation on 65 real-world faults from CoREBench, 84 artificial faults from SIR, and 310 real-world faults from Defects4J shows that PRINCE outperforms the state-of-the-art SBFL, MBFL, and learn-to-rank techniques significantly. PRINCE localizes a fault after reviewing 2.4% of the executed statements on average (4.2 and 3.0 times more precise than the best of the compared SBFL and MBFL techniques, respectively). Also, PRINCE ranks 52.9% of the target faults within the top ten suspicious statements. © 2019 ACM.",Fault localization | Machine learning | Mutation analysis | Source file characteristics,ACM Transactions on Software Engineering and Methodology,2019-10-01,Article,"Kim, Yunho;Mun, Seokhyeon;Yoo, Shin;Kim, Moonzoo",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85056157660,10.1109/SECON.2018.8478911,"Applying Machine Learning to Predict Software Fault Proneness Using Change Metrics, Static Code Metrics, and a Combination of Them","Predicting software fault proneness is very important as the process of fixing these faults after the release is very costly and time-consuming. In order to predict software fault proneness, many machine learning algorithms (e.g., Logistic regression, Naive Bayes, and J48) were used on several datasets, using different metrics as features. The question is what algorithm is the best under which circumstance and what metrics should be applied. Related works suggested that using change metrics leads to the highest accuracy in prediction. In addition, some algorithms perform better than others in certain circumstances. In this work, we use three machine learning algorithms (i.e., logistic regression, Naive Bayes, and J48) on three Eclipse releases (i.e., 2.0, 2.1, 3.0). The results showed that accuracy is slightly better and false positive rates are lower, when we use the reduced set of metrics compared to all change metrics set. However, the recall and the G score are better when we use the complete set of change metrics. Furthermore, J48 outperformed the other classifiers with respect to the G score for the reduced set of change metrics, as well as in almost all cases when the complete set of change metrics, static code metrics, and the combination of both were used. © 2018 IEEE.",Change metrics | Decision tree | Logistic regression | Machine learning | Naive Bayes | Prediction | Reliability | Software Engineering | Software faults proneness | Static code metrics,Conference Proceedings - IEEE SOUTHEASTCON,2018-10-01,Conference Paper,"Alshehri, Yasser Ali;Goseva-Popstojanova, Katerina;Dzielski, Dale G.;Devine, Thomas",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85051232387,10.1145/3194104.3194110,A replication study: Just-in-time defect prediction with ensemble learning,"Just-in-time defect prediction, which is also known as change-level defect prediction, can be used to efficiently allocate resources and manage project schedules in the software testing and debugging process. Just-in-time defect prediction can reduce the amount of code to review and simplify the assignment of developers to bug fixes. This paper reports a replicated experiment and an extension comparing the prediction of defect-prone changes using traditional machine learning techniques and ensemble learning. Using datasets from six open source projects, namely Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL we replicate the original approach to verify the results of the original experiment and use them as a basis for comparison for alternatives in the approach. Our results from the replicated experiment are consistent with the original. The original approach uses a combination of data preprocessing and a two-layer ensemble of decision trees. The first layer uses bagging to form multiple random forests. The second layer stacks the forests together with equal weights. Generalizing the approach to allow the use of any arbitrary set of classifiers in the ensemble, optimizing the weights of the classifiers, and allowing additional layers, we apply a new deep ensemble approach, called deep super learner, to test the depth of the original study. The deep super learner achieves statistically significantly better results than the original approach on five of the six projects in predicting defects as measured by F1 score. © 2018 ACM.",deep learning | defect prediction,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Young, Steven;Abdou, Tamer;Bener, Ayse",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85034082083,10.1007/978-981-10-4603-2_6,Software defect prediction: A comparison between artificial neural network and support vector machine,"Software industry has stipulated the need for good quality software projects to be delivered on time and within budget. Software defect prediction (SDP) has led to the application of machine learning algorithms for building defect classification models using software metrics and defect proneness as the independent and dependent variables, respectively. This work performs an empirical comparison of the two classification methods: support vector machine (SVM) and artificial neural network (ANN), both having the predictive capability to handle the complex nonlinear relationships between the software attributes and the software defect. Seven data sets from the PROMISE repository are used and the prediction models’ are assessed on the parameters of accuracy, recall, and specificity. The results show that SVM is better than ANN in terms of recall, while the later one performed well along the dimensions of accuracy and specificity. Therefore, it is concluded that it is necessary to determine the evaluation parameters according to the criticality of the project, and then decide upon the classification model to be applied. © Springer Nature Singapore Pte Ltd. 2018.",Artificial neural networks | Back propagation | Software quality | Supervised learning | Support vector machine,Advances in Intelligent Systems and Computing,2018-01-01,Conference Paper,"Arora, Ishani;Saha, Anju",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85046399655,10.1109/ICCKE.2017.8167880,Pre-training of an artificial neural network for software fault prediction,"Software fault prediction is one of the significant stages in the software testing process. At this stage, the probability of fault occurrence is predicted based on the documented information of the software systems that are already tested. Using this prior knowledge, developers and testing teams can better manage the testing process. There are many efforts in the field of machine learning to solve this classification problem. We propose to use a pre-training technique for a shallow, i.e. with fewer hidden layers, Artificial Neural Network (ANN). While this method is usually employed to prevent over-fitting in deep ANNs, our results indicate that even in a shallow network, it improves the accuracy by escaping from local minima. We compare the proposed method with four SVM-based classifiers and a regular ANN without pre-training on seven datasets from NASA codes in the PROMISE repository. Results confirm that the pre-training improves accuracy by achieving the best overall ranking of 1.43. Among seven datasets, our method has higher accuracy in four of them, while ANN and support vector machine are the best for two and one datasets, respectively. © 2017 IEEE.",Component | Pre-Training | Shallow Artificial Neural Network | Software Fault Prediction,"2017 7th International Conference on Computer and Knowledge Engineering, ICCKE 2017",2017-12-05,Conference Paper,"Owhadi-Kareshk, Moein;Sedaghat, Yasser;Akbarzadeh, Mohammad R.T.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85037378589,10.1049/iet-sen.2014.0067,Zero-inflated prediction model in software-fault data,"Software fault data with many zeroes in addition to large non-zero values are common in the software estimation area. A two-component prediction approach that provides a robust way to predict this type of data is introduced in this study. This approach allows to combine parametric and non-parametric models to improve the prediction accuracy. This way provides a more flexible structure to understand data. To show the usefulness of the proposed approach, experiments using eight projects from the NASA repository are considered. In addition, this method is compared with methods from the machine learning and statistical literature. The performance of the methods is measured by the prediction accuracy that is assessed based on the mean magnitude of relative errors. © 2016 The Institution of Engineering and Technology.",code review | pull-request | review comment,Journal of Computer Science and Technology,2017-11-01,Article,"Li, Zhi Xing;Yu, Yue;Yin, Gang;Wang, Tao;Wang, Huai Min",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85073253487,10.1007/978-3-319-11218-3_35,An empirical study of robustness and stability of machine learning classifiers in software defect prediction,"Software is one of the key drivers of twenty first century business and society. Delivering high quality software systems is a challenging task for software developers. Early software defect prediction, based on software code metrics, has been intensely researched by the software engineering research community. Recent knowledge advancements in machine learning have been intensely explored for development of highly accurate automatic software defect prediction models. This study contributes to the application of machine learning in software defect prediction by investigating the robustness and stability of 17 classifiers on 44 open source software defect prediction data sets obtained from PROMISE repository. The Area under curve (AUC) of Receiver Operating Characteristic Curve (ROC) for each of the 17 classifiers is obtained for 44 defect prediction data sets. Our experiments show that Random Forests, Logistic Regression and Kstar are robust as well as stable classifiers for software defect prediction applications. Further, we demonstrate that Naïve Bayes and Bayes Networks, which have been shown to be robust and comprehensible classifiers in previous on software defect prediction, have poor stability in open source software defect prediction. © Springer International Publishing Switzerland 2015.",bug triaging | software repository minings | tossing sequence,Journal of Computer Science and Technology,2019-09-01,Article,"Xi, Sheng Qu;Yao, Yuan;Xiao, Xu Sheng;Xu, Feng;Lv, Jian",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84907352398,10.1007/978-3-319-11854-3_23,The Adoption of Machine Learning Techniques for Software Defect Prediction: An Initial Industrial Validation,"Existing methods for predicting reliability of software are static and need manual maintenance to adjust to the evolving data sets in software organizations. Machine learning has a potential to address the problem of manual maintenance but can also require changes in how companies works with defect prediction. In this paper we address the problem of identifying what the benefits of machine learning are compared to existing methods and which barriers exist for adopting them in practice. Our methods consist of literature studies and a case study at two companies - Ericsson and Volvo Car Group. By studying literature we develop a framework for adopting machine learning and using case studies we evaluate this framework through a series of four interviews with experts working with predictions at both companies - line manager, quality manager and measurement team leader. The findings of our research show that the most important perceived benefits of adopting machine learning algorithms for defect prediction are accuracy of predictions and ability to generate new insights from data. The two most important perceived barriers in this context are inability to recognize new patterns and low generalizability of the machine learning algorithms. We conclude that in order to support companies in making an informed decision to adopt machine learning techniques for software defect predictions we need to go beyond accuracy and also evaluate factors such as costs, generalizability and competence. © Springer International Publishing Switzerland 2014.",adoption | Machine Learning | software defect prediction | software quality | technology acceptance,Communications in Computer and Information Science,2014-01-01,Conference Paper,"Rana, Rakesh;Staron, Miroslaw;Berger, Christian;Hansson, Jörgen;Nilsson, Martin;Meding, Wilhelm",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84881141305,10.1109/LADC.2013.25,The time dimension in predicting failures: A case study,"Online Failure Prediction is a cutting-edge technique for improving the dependability of software systems. It makes extensive use of machine learning techniques applied to variables monitored from the system at regular intervals of time (e.g. mutexes/s, paged bytes/s, etc.). The goal of this work is to assess the impact of considering the time dimension in failure prediction, through the use of sliding windows. The state-of-the-art SVM (Support Vector Machine) classifier is used to support the study, predicting failure events occurring in a Windows XP machine. An extensive comparative analysis is carried out, in particular using a software fault injection technique to speed up the failure data generation process. © 2013 IEEE.",dependability | fault injection | online failure prediction | sliding window,"Proceedings - 6th Latin-American Symposium on Dependable Computing, LADC 2013",2013-08-12,Conference Paper,"Irrera, Ivano;Pereira, Carlos;Vieira, Marco",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84877677176,10.1109/ISISE.2012.114,Software defect prediction scheme based on feature selection,"Predicting defect-prone software modules accurately and effectively are important ways to control the quality of a software system during software development. Feature selection can highly improve the accuracy and efficiency of the software defect prediction model. The main purpose of this paper is to discuss the best size of feature subset for building a prediction model and prove that feature selection method is useful for establishing software defect prediction model. Mutual information is an outstanding indicator of relevance between variables, and it has been used as a measurement in our feature selection algorithm. We also introduce a nonlinear factor to our evaluation function for feature selection to improve its performance. The results of our feature selection algorithm are validated by different machine learning methods. The experiment results show that all the classifiers achieve higher accuracy by using the feature subset provided by our algorithm. © 2012 IEEE.",feature selection | mutual information | software defect prediction,"Proceedings of the 2012 4th International Symposium on Information Science and Engineering, ISISE 2012",2012-12-01,Conference Paper,"Wang, Pei;Jin, Cong;Jin, Shu Wei",Include,
10.1016/j.infsof.2022.107128,2-s2.0-83755163956,10.1109/ICSMC.2011.6084055,Predicting software defects: A cost-sensitive approach,"Find software defects is a complex and slow task which consumes most of the development budgets. In order to try reducing the cost of test activities, many researches have used machine learning to predict whether a module is defect-prone or not. Defect detection is a cost-sensitive task whereby a misclassification is more costly than a correct classification. Yet, most of the researches do not consider classification costs in the prediction models. This paper introduces an empirical method based in a COCOMO (COnstructive COst MOdel) that aims to assess the cost of each classifier decision. This method creates a cost matrix that is used in conjunction with a threshold-moving approach in a ROC (Receiver Operating Characteristic) curve to select the best operating point regarding cost. Public data sets from NASA (National Aeronautics and Space Administration) IV&V (Independent Verification & Validation) Facility Metrics Data Program (MDP) are used to train the classifiers and to provide some development effort information. The experiments are carried out through a methodology that complies with validation and reproducibility requirements. The experimental results have shown that the proposed method is efficient and allows the interpretation of the classifier performance in terms of tangible cost values. © 2011 IEEE.",COCOMO | Defect prediction | machine learning | MDP | NASA | pattern recognition | ROC curve | software metrics | testing costs,"Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics",2011-12-23,Conference Paper,"Bezerra, Miguel E.R.;Oliveira, Adriano L.I.;Adeodatoz, Paulo J.L.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-70149105243,10.1007/978-3-642-02152-7_3,Key questions in building defect prediction models in practice,"The information about which modules of a future version of a software system are defect-prone is a valuable planning aid for quality managers and testers. Defect prediction promises to indicate these defect-prone modules. However, constructing effective defect prediction models in an industrial setting involves a number of key questions. In this paper we discuss ten key questions identified in context of establishing defect prediction in a large software development project. Seven consecutive versions of the software system have been used to construct and validate defect prediction models for system test planning. Furthermore, the paper presents initial empirical results from the studied project and, by this means, contributes answers to the identified questions. © 2009 Springer Berlin Heidelberg.",Defect prediction | Machine learning | Software test management,Lecture Notes in Business Information Processing,2009-01-01,Conference Paper,"Ramler, Rudolf;Wolfmaier, Klaus;Stauder, Erwin;Kossak, Felix;Natschläger, Thomas",Include,
10.1016/j.infsof.2022.107128,2-s2.0-48049102737,10.1109/EUROMICRO.2007.13,A two-step model for defect density estimation,"Identifying and locating defects in software projects is a difficult task. Further, estimating the density of defects is more difficult. Measuring software in a continuous and disciplined manner brings many advantages such as accurate estimation of project costs and schedules, and improving product and process qualities. Detailed analysis of software metric data gives signijicant clues about the locations and magnitude of possible defects in a program. The aim of this research is to establish an improved method for predicting software quality via identifying the defect density of fault prone modules using machine-learning techniques. We constructed a two-step model that predicts defect density by taking module metric data into consideration. Our proposed model utilizes classification and regression type learning methods consecutively. The results of the experiments on public data sets show that the two-step model enhances the overall performance measures as compared to applying only regression methods. © 2007 IEEE.",,"EUROMICRO 2007 - Proceedings of the 33rd EUROMICRO Conference on Software Engineering and Advanced Applications, SEAA 2007",2007-12-01,Conference Paper,"Kutlubay, Onur;Turhan, Burak;Bener, Ayşe B.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85099732102,10.1109/ACCESS.2021.3052149,Boosted Whale Optimization Algorithm with Natural Selection Operators for Software Fault Prediction,"Software fault prediction (SFP) is a challenging process that any successful software should go through it to make sure that all software components are free of faults. In general, soft computing and machine learning methods are useful in tackling this problem. The size of fault data is usually huge since it is obtained from mining software historical repositories. This data consists of a large number of features (metrics). Determining the most valuable features (i.e., Feature Selection (FS) is an excellent solution to reduce data dimensionality. In this paper, we proposed an enhanced version of the Whale Optimization Algorithm (WOA) by combining it with a single point crossover method. The proposed enhancement helps the WOA to escape from local optima by enhancing the exploration process. Five different selection methods are employed: Tournament, Roulette wheel, Linear rank, Stochastic universal sampling, and random-based. To evaluate the performance of the proposed enhancement, 17 available SFP datasets are adopted from the PROMISE repository. The deep analysis shows that the proposed approach outperformed the original WOA and the other six state-of-the-art methods, as well as enhanced the overall performance of the machine learning classifier. © 2013 IEEE.",adaptive synthetic sampling | binary whale optimization algorithm | classification | feature selection | Software fault prediction,IEEE Access,2021-01-01,Article,"Hassouneh, Yousef;Turabieh, Hamza;Thaher, Thaer;Tumar, Iyad;Chantar, Hamouda;Too, Jingwei",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85087734994,10.1049/iet-sen.2019.0278,Within-project and cross-project just-in-time defect prediction based on denoising autoencoder and convolutional neural network,"Just-in-time defect prediction is an important and useful branch in software defect prediction. At present, deep learning is a research hotspot in the field of artificial intelligence, which can combine basic defect features into deep semantic features and make up for the shortcomings of machine learning algorithms. However, the mainstream deep learning techniques have not been applied yet in just-in-time defect prediction. Therefore, the authors propose a novel just-in-time defect prediction model named DAECNN-JDP based on denoising autoencoder and convolutional neural network in this study, which has three main advantages: (i) Different weights for the position vector of each dimension feature are set, which can be automatically trained by adaptive trainable vector. (ii) Through the training of denoising autoencoder, the input features that are not contaminated by noise can be obtained, thus learning more robust feature representation. (iii) The authors leverage a powerful representation-learning technique, convolution neural network, to construct the basic change features into the abstract deep semantic features. To evaluate the performance of the DAECNN-JDP model, they conduct extensive within-project and cross-project defect prediction experiments on six large open source projects. The experimental results demonstrate that the superiority of DAECNN-JDP on five evaluation metrics. © The Institution of Engineering and Technology 2020",,IET Software,2020-06-01,Article,"Zhu, Kun;Zhang, Nana;Ying, Shi;Zhu, Dandan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85063258162,10.1142/S0218001419590377,An Investigation of Imbalanced Ensemble Learning Methods for Cross-Project Defect Prediction,"Machine-learning-based software defect prediction (SDP) methods are receiving great attention from the researchers of intelligent software engineering. Most existing SDP methods are performed under a within-project setting. However, there usually is little to no within-project training data to learn an available supervised prediction model for a new SDP task. Therefore, cross-project defect prediction (CPDP), which uses labeled data of source projects to learn a defect predictor for a target project, was proposed as a practical SDP solution. In real CPDP tasks, the class imbalance problem is ubiquitous and has a great impact on performance of the CPDP models. Unlike previous studies that focus on subsampling and individual methods, this study investigated 15 imbalanced learning methods for CPDP tasks, especially for assessing the effectiveness of imbalanced ensemble learning (IEL) methods. We evaluated the 15 methods by extensive experiments on 31 open-source projects derived from five datasets. Through analyzing a total of 37504 results, we found that in most cases, the IEL method that combined under-sampling and bagging approaches will be more effective than the other investigated methods. © 2019 World Scientific Publishing Company.",cross-project defect prediction | ensemble learning | imbalanced learning | Intelligent software engineering,International Journal of Pattern Recognition and Artificial Intelligence,2019-11-01,Article,"Qiu, Shaojian;Lu, Lu;Jiang, Siyu;Guo, Yang",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85073209569,10.1109/IRI.2019.00030,Software quality prediction: An investigation based on machine learning,"Irrespective of the type of software system that is being developed, producing and delivering high-quality software within the specified time and budget is crucial for many software businesses. The software process model has a major impact on the quality of the overall system - the longer a defect remains in the system undetected, the harder it becomes to fix. However, predicting the quality of the software in the early phases would immensely assist developers in software maintenance and quality assurance activities, and to allocate effort and resources more efficiently. This paper presents an evaluation of eight machine learning techniques in the context of reliability and maintainability. Reliability is investigated as the number of defects in a system and the maintainability is analyzed as the number of changes made in the system. Software metrics are direct reflections of various characteristics of software and are used in our study as the major attributes for training the models for both defect and maintainability prediction. Among the eight different techniques we experimented with, Random Forest provided the best results with an AUC of over 0.8 during both defect and maintenance prediction. © 2019 IEEE.",Defect prediction | Machine learning | Maintainability prediction | Software maintainability | Software quality | Software reliability,"Proceedings - 2019 IEEE 20th International Conference on Information Reuse and Integration for Data Science, IRI 2019",2019-07-01,Conference Paper,"Reddivari, Sandeep;Raman, Jayalakshmi",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85091887728,10.1007/s41870-018-0211-3,A comparative analysis of soft computing techniques in software fault prediction model development,"In the process of software development, software fault prediction is a useful practice to ensure reliable and high quality software products. It plays a vital role in the process of software quality assurance. A high quality software product contains minimum number of faults and failures. Software fault prediction examines the vulnerability of software product towards faults. In this paper, a comparative analysis of various soft computing approaches in terms of the process of software fault prediction is considered. In addition, an analysis of various pros and cons of soft computing techniques in terms of software fault prediction process is also mentioned. The conclusive results show that the soft computing approach has the propensity to identify faults in the process of software development. © 2018, Bharati Vidyapeeth's Institute of Computer Applications and Management.",Evolutionary computing | Fuzzy logic | Machine learning | Neural network | Soft computing | Software fault prediction | Swarm intelligence,International Journal of Information Technology (Singapore),2019-03-12,Article,"Sharma, Deepak;Chandra, Pravin",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84919754115,10.1007/s00607-016-0538-1,A set of measures designed to identify overlapped instances in software defect prediction,"The performance of the learning models will intensely rely on the characteristics of the training data. The previous outcomes recommend that the overlapping between classes and the presence of noise have the most grounded impact on the performance of learning algorithm, and software defect datasets are no exceptions. The class overlap problem is concerned with the performance of machine learning classifiers critical problem is class overlap in which data samples appear as valid examples of more than one class which may be responsible for the presence of noise in datasets. We aim to investigate how the presence of overlapped instances in a dataset influences the classifier’s performance, and how to deal with class overlapping problem. To have a close estimate of class overlapping, we have proposed four different measures namely, nearest enemy ratio, subconcept ratio, likelihood ratio and soft margin ratio. We performed our investigations using 327 binary defect classification datasets obtained from 54 software projects, where we first identified overlapped datasets using three data complexity measures proposed in the literature. We also include treatment effort into the prediction process. Subsequently, we used our proposed measures to find overlapped instances in the identified overlapped datasets. Our results indicated that by training a classifier on a training data free from overlapped instances led to an improved classifier performance on the test data containing overlapped instances. The classifiers perform significantly better when the evaluation measure takes the effort into account. © 2017, Springer-Verlag Wien.",Machine learning | Software fault proneness | Systematic literature review,Applied Soft Computing Journal,2015-01-01,Article,"Malhotra, Ruchika",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84971654681,10.1016/j.asoc.2016.05.038,Defect prediction for Cascading Style Sheets,"Testing is a crucial activity in software development. However exhaustive testing of a given software is impossible in practice because projects have serious time and budget limitations. Therefore, software testing teams need guidance about which modules they should focus on. Defect prediction techniques are useful for this situation because they let testers to identify and focus on defect prone parts of software. These techniques are essential for software teams, because they help teams to efficiently allocate their precious resources in testing phase. Software defect prediction has been an active research area in recent years. Researchers in this field have been using different types of metrics in their prediction models. However, value of extracting static code metrics for style sheet languages has been ignored until now. User experience is a very important part of web applications and its mostly provided using Cascading Style Sheets (CSS). In this research, our aim is to improve defect prediction performance for web applications by utilizing metrics generated from CSS code. We generated datasets from four open source web applications to conduct our experiments. Defect prediction is then performed using three different well-known machine learning algorithms. The results revealed that static code metrics based defect prediction techniques can be performed effectively to improve quality of CSS code in web applications. Therefore we recommend utilizing domain-specific characteristics of applications in defect prediction as they result in significantly high prediction performance with low costs. © 2016 Elsevier B.V.",Defect prediction | Software Metrics | Software quality | Web sites,Applied Soft Computing Journal,2016-12-01,Article,"Serdar Biçer, M.;Diri, Banu",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85001930600,10.1145/3001867.3001874,Towards predicting feature defects in software product lines,"Defect-prediction techniques can enhance the quality assurance activities for software systems. For instance, they can be used to predict bugs in source files or functions. In the context of a software product line, such techniques could ideally be used for predicting defects in features or combinations of features, which would allow developers to focus quality assurance on the error-prone ones. In this preliminary case study, we investigate how defect prediction models can be used to identify defective features using machine-learning techniques. We adapt process metrics and evaluate and compare three classifiers using an open-source product line. Our results show that the technique can be effective. Our best scenario achieves an accuracy of 73% for accurately predicting features as defective or clean using a Naive Bayes classifier. Based on the results we discuss directions for future work.",Defect prediction | Features | Software product lines,"FOSD 2016 - Proceedings of the 7th International Workshop on Feature-Oriented Software Development, co-located with SPLASH 2016",2016-10-30,Conference Paper,"Queiroz, Rodrigo;Berger, Thorsten;Czarnecki, Krzysztof",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84958766163,10.1145/2811681.2811699,Performance evaluation of ensemble methods for software fault prediction: An experiment,"In object-oriented software development, a plethora of studies have been carried out to present the application of machine learning algorithms for fault prediction. Furthermore, it has been empirically validated that an ensemble method can improve classification performance as compared to a single classifier. But, due to the inherent differences among machine learning and data mining approaches, the classification performance of ensemble methods will be varied. In this study, we investigated and evaluated the performance of different ensemble methods with itself and base-level classifiers, in predicting the faults proneness classes. Subsequently, we used three ensemble methods AdaboostM1, Vote and StackingC with five base-level classifiers namely Naivebayes, Logistic, J48, VotedPerceptron and SMO in Weka tool. In order to evaluate the performance of ensemble methods, we retrieved twelve datasets of open source projects from PROMISE repository. In this experiment, we used k-fold (k=10) cross-validation and ROC analysis for validation. Besides, we used recall, precision, accuracy, F-value measures to evaluate the performance of ensemble methods and base-level Classifiers. Finally, we observed significant performance improvement of applying ensemble methods as compared to its base-level classifier, and among ensemble methods we observed StackingC outperformed other selected ensemble methods for software fault prediction. © 2015 ACM.",Chidamber and Kemerer (CK) Metrics | Classifiers | Ensemble Methods | Measures | Performance | Weka,ACM International Conference Proceeding Series,2015-09-28,Conference Paper,"Hussain, Shahid;Keung, Jacky;Khan, Arif Ali;Bennin, Kwabena Ebo",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84904904008,10.1007/978-3-319-09156-3_25,Recognizing antipatterns and analyzing their effects on software maintainability,"Similarly to design patterns and their inherent extra information about the structure and design of a system, antipatterns - or bad code smells - can also greatly influence the quality of software. Although the belief that they negatively impact maintainability is widely accepted, there are still relatively few objective results that would support this theory. In this paper we show our approach of detecting antipatterns in source code by structural analysis and use the results to reveal connections among antipatterns, number of bugs, and maintainability. We studied 228 open-source Java based systems and extracted bug-related information for 34 of them from the PROMISE database. For estimating the maintainability, we used the ColumbusQM probabilistic quality model. We found that there is a statistically significant, 0.55 Spearman correlation between the number of bugs and the number of antipatterns. Moreover, there is an even stronger, -0.62 reverse Spearman correlation between the number of antipatterns and code maintainability. We also found that even these few implemented antipatterns could nearly match the machine learning based bug-predicting power of 50 class level source code metrics. Although the presented analysis is not conclusive by far, these first results suggest that antipatterns really do decrease code quality and can highlight spots that require closer attention. © 2014 Springer International Publishing.",Antipatterns | Empirical validation | ISO/IEC 25010 | OO design | Software maintainability | SQuaRE,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2014-01-01,Conference Paper,"Bán, Dénes;Ferenc, Rudolf",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84864075513,10.1109/RACSS.2012.6212686,Software defect prediction using Two level data pre-processing,"Defect prediction can be useful to streamline testing efforts and reduce the development cost of software. Predicting defects is usually done by using certain data mining and machine learning techniques. A prediction model is said to be effective if it is able to classify defective and non defective modules accurately. In this paper we investigate the result of data pre-processing on the performance of four different K-NN classifiers and compare the results with random forest classifier. The method used for pre-processing includes attribute selection and instance filtering.We observed that Two-level data pre-processing enhances defect prediction results. We also report how these two filters influence the performance independently. The observed performance improvement can be attributed to the removal of irrelevant attributes by dimension (attribute) reduction and of class imbalance problem by Resampling, together leading to the improved performance capabilities of the classifiers. © 2012 IEEE.",,"Proceedings of the 2012 International Conference on Recent Advances in Computing and Software Systems, RACSS 2012",2012-07-25,Conference Paper,"Verma, Rashmi;Gupta, Atul",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85034604087,10.1016/j.jss.2020.110691,An automatically created novel bug dataset and its validation in bug prediction,"Bugs are inescapable during software development due to frequent code changes, tight deadlines, etc.; therefore, it is important to have tools to find these errors. One way of performing bug identification is to analyze the characteristics of buggy source code elements from the past and predict the present ones based on the same characteristics, using e.g. machine learning models. To support model building tasks, code elements and their characteristics are collected in so-called bug datasets which serve as the input for learning. We present the BugHunter Dataset: a novel kind of automatically constructed and freely available bug dataset containing code elements (files, classes, methods) with a wide set of code metrics and bug information. Other available bug datasets follow the traditional approach of gathering the characteristics of all source code elements (buggy and non-buggy) at only one or more pre-selected release versions of the code. Our approach, on the other hand, captures the buggy and the fixed states of the same source code elements from the narrowest timeframe we can identify for a bug's presence, regardless of release versions. To show the usefulness of the new dataset, we built and evaluated bug prediction models and achieved F-measure values over 0.74. © 2020 The Authors",Bayesian network classifiers | Learning from crowds | Missing ground truth | Orthogonal defect classification,Applied Soft Computing Journal,2018-01-01,Article,"Hernández-González, Jerónimo;Rodriguez, Daniel;Inza, Iñaki;Harrison, Rachel;Lozano, Jose A.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85094325423,10.1145/3377811.3380389,Software visualization and deep transfer learning for effective software defect prediction,"Software defect prediction aims to automatically locate defective code modules to better focus testing resources and human effort. Typically, software defect prediction pipelines are comprised of two parts: the first extracts program features, like abstract syntax trees, by using external tools, and the second applies machine learningbased classification models to those features in order to predict defective modules. Since such approaches depend on specific feature extraction tools, machine learning classifiers have to be customtailored to effectively build most accurate models. To bridge the gap between deep learning and defect prediction, we propose an end-to-end framework which can directly get prediction results for programs without utilizing feature-extraction tools. To that end, we first visualize programs as images, apply the self-attention mechanism to extract image features, use transfer learning to reduce the difference in sample distributions between projects, and finally feed the image files into a pre-trained, deep learning model for defect prediction. Experiments with 10 open source projects from the PROMISE dataset show that our method can improve cross-project and within-project defect prediction. Our code and data pointers are available at https://zenodo.org/record/3373409#.XV0Oy5Mza35. © 2020 Association for Computing Machinery.",Cross-project defect prediction | Deep transfer learning | Self-attention | Software visualization | Within-project defect prediction,Proceedings - International Conference on Software Engineering,2020-06-27,Conference Paper,"Chen, Jinyin;Hu, Keke;Yu, Yue;Chen, Zhuangzhi;Xuan, Qi;Liu, Yi;Filkov, Vladimir",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85091829805,10.1007/s41870-018-0244-7,Cross project defect prediction using hybrid search based algorithms,"Prediction of faulty modules in the software development cycle earlier helps to reduce the cost of software development. Test engineers give more attention to the faulty modules to remove any latent defect in the software. Most of the studies available in literature have used historical data related to the same projects for identification of faulty modules; however availability of historical data for new software projects is not possible. In case of new software projects, data for defect prediction is taken from similar types of projects developed earlier and this technique of defect prediction is called cross project defect prediction. In this study applicability of hybrid search based algorithms for cross project defect prediction is investigated. Performance of hybrid search based algorithms is compared for with-in and cross project defect prediction. Hybrid search based algorithms combine the advantages of search based algorithms with machine learning techniques. Results showed that hybrid search based algorithms are more suitable in case of cross project defect prediction in comparison to with-in project defect prediction. © 2018, Bharati Vidyapeeth's Institute of Computer Applications and Management.",Defects | Hybrid search based algorithms | Quality | Software metrics | Testing,International Journal of Information Technology (Singapore),2020-06-01,Article,"Rhmann, Wasiur",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85095565376,10.1109/ICOEI48184.2020.9142909,Software Defect Prediction Using Machine Learning Techniques,"Software defect prediction provides development groups with observable outcomes while contributing to industrial results and development faults predicting defective code areas can help developers identify bugs and organize their test activities. The percentage of classification providing the proper prediction is essential for early identification. Moreover, software-defected data sets are supported and at least partially recognized due to their enormous dimension. This Problem is handled by hybridized approach that includes the PCA, randomforest, naïve bayes and the SVM Software Framework, which as five datasets as PC3, MW1, KC1, PC4, and CM1, are listed in software analysis using the weka simulation tool. A systematic research analysis is conducted in which parameters of confusion, precision, recall, recognition accuracy, etc Are measured as well as compared with the prevailing schemes. The analytical analysis indicates that the proposed approach will provide more useful solutions for device defects prediction. © 2020 IEEE.",Active distribution networks | Adaptive protection | Fault detector | Machine learning | Micro-grid,Applied Soft Computing,2021-01-01,Article,"Marín-Quintero, J.;Orozco-Henao, C.;Percybrooks, W. S.;Vélez, Juan C.;Montoya, Oscar Danilo;Gil-González, W.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85048823521,10.3390/sym12030407,Class imbalance reduction (CIR): A novel approach to software defect prediction in the presence of class imbalance,"Software defect prediction (SDP) is the technique used to predict the occurrences of defects in the early stages of software development process. Early prediction of defects will reduce the overall cost of software and also increase its reliability. Most of the defect prediction methods proposed in the literature suffer from the class imbalance problem. In this paper, a novel class imbalance reduction (CIR) algorithm is proposed to create a symmetry between the defect and non-defect records in the imbalance datasets by considering distribution properties of the datasets and is compared with SMOTE (synthetic minority oversampling technique), a built-in package of many machine learning tools that is considered a benchmark in handling class imbalance problems, and with K-Means SMOTE. We conducted the experiment on forty open source software defect datasets from PRedict or Models in Software Engineering (PROMISE) repository using eight different classifiers and evaluated with six performance measures. The results show that the proposed CIR method shows improved performance over SMOTE and K-Means SMOTE. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.",Heart sound classification | Hidden Markov Model | Mel-Frequency Cepstral Coefficient | Signal processing | Smartphone application | Split detection,Biomedical Signal Processing and Control,2018-08-01,Article,"Thiyagaraja, Shanti R.;Dantu, Ram;Shrestha, Pradhumna L.;Chitnis, Anurag;Thompson, Mark A.;Anumandla, Pruthvi T.;Sarma, Tom;Dantu, Siva",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85079317663,10.1504/IJCSE.2020.106871,A benchmarking framework using nonlinear manifold detection techniques for software defect prediction,"Prediction of software defects in time improves quality and helps in locating the defect-prone areas accurately. Although earlier considerable methods were applied, actually none of those measures was found to be fool-proof and accurate. Hence, a newer framework includes nonlinear manifold detection model, and its algorithm originated for defect prediction using different techniques of nonlinear manifold detection (nonlinear MDs) along with 14 different machine learning techniques (MLTs) on eight defective software datasets. A critical analysis cum exhaustive comparative estimation revealed that nonlinear manifold detection model has a more accurate and effective impact on defect prediction as compared to feature selection techniques. The outcome of the experiment was statistically tested by Friedman and post hoc analysis using Nemenyi test, which validates that hidden Markov model (HMM) along with nonlinear manifold detection model outperforms and is significantly different from MLTs. Copyright © 2020 Inderscience Enterprises Ltd.",Apache spark | Brain computer interface | EEG data analysis | Non-parametric | Weighted feature extraction,Biomedical Signal Processing and Control,2020-04-01,Article,"Thakur, Santosh;Dharavath, Ramesh;Edla, Damodar Reddy",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85079752229,10.1109/ACCESS.2020.2968362,"Hyper-parameter optimization of classifiers, using an artificial immune network and its application to software bug prediction","Software testing is an important task in software development activities, and it requires most of the resources, namely, time, cost and effort. To minimize this fatigue, software bug prediction (SBP) models are applied to improve the software quality assurance (SQA) processes by predicting buggy components. The bug prediction models use machine learning classifiers so that bugs can be predicted in software components in some software metrics. These classifiers are characterized by some configurable parameters, called hyper-parameters that need to be optimized to ensure better performance. Many methods have been proposed by researchers to predict the defective components but these classifiers sometimes not perform well when default settings are used for machine learning classifiers. In this paper, software bug prediction model is proposed which uses machine learning classifiers in conjunction with the Artificial Immune Network (AIN) to improve bug prediction accuracy through its hyper-parameter optimization. For this purpose, seven machine learning classifiers, such as support vector machine Radial base function (SVM-RBF), K-nearest neighbor (KNN) (Minkowski metric), KNN (Euclidean metric), Naive Bayes (NB), Decision Tree (DT), Linear discriminate analysis (LDA), Random forest (RF) and adaptive boosting (AdaBoost), were used. The experiment was carried out on bug prediction dataset. The results showed that hyper-parameter optimization of machine learning classifiers, using AIN and its applications for software bug prediction, performed better than when classifiers with their default hyper-parameters were used. © 2013 IEEE.",Artificial immune network (AIN) | artificial immune system (AIS) | hyper-parameter optimization | optimized artificial immune network (opt-aiNet) | software bug prediction (SBP),IEEE Access,2020-01-01,Article,"Khan, Faiza;Kanwal, Summrina;Alamri, Sultan;Mumtaz, Bushra",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85087483181,10.1109/ASE.2019.00071,Empirical evaluation of the impact of class overlap on software defect prediction,"Software defect prediction (SDP) utilizes the learning models to detect the defective modules in project, and their performance depends on the quality of training data. The previous researches mainly focus on the quality problems of class imbalance and feature redundancy. However, training data often contains some instances that belong to different class but have similar values on features, and this leads to class overlap to affect the quality of training data. Our goal is to investigate the impact of class overlap on software defect prediction. At the same time, we propose an improved K-Means clustering cleaning approach (IKMCCA) to solve both the class overlap and class imbalance problems. Specifically, we check whether K-Means clustering cleaning approach (KMCCA) or neighborhood cleaning learning (NCL) or IKMCCA is feasible to improve defect detection performance for two cases (i) within-project defect prediction (WPDP) (ii) cross-project defect prediction (CPDP). To have an objective estimate of class overlap, we carry out our investigations on 28 open source projects, and compare the performance of state-of-the-art learning models for the above-mentioned cases by using IKMCCA or KMCCA or NCL VS. without cleaning data. The experimental results make clear that learning models obtain significantly better performance in terms of balance, Recall and AUC for both WPDP and CPDP when the overlapping instances are removed. Moreover, it is better to consider both class overlap and class imbalance. © 2019 IEEE.",Active contour | Atrial septal defect | Deep learning | Kass snake algorithm | MRI Diagnostics | U-Net,Computer Methods and Programs in Biomedicine,2020-11-01,Article,"Zhao, Ming;Wei, Yang;Lu, Yu;Wong, Kelvin K.L.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84928826771,10.1145/3352411.3352412,Software Defect Prediction Based on Ensemble Learning,"Software defect prediction is one of the important ways to guarantee the quality of software systems. Combining various algorithms in machine learning to predict software defects has become a hot topic in the current study. The paper uses the datasets of MDP as the experimental research objects and takes ensemble learning as research focus to construct software defect prediction model. With experimenting five different types of ensemble algorithms and analyzing the features and procedures, this paper discusses the best ensemble algorithm which is Random Forest through experimental comparison. Then we utilize the SMOTE over-sampling and Resample methods to improve the quality of datasets to build a complete new software defect prediction model. Therefore, the results show that the model can improve defect classification performance effectively. © 2019 Association for Computing Machinery.",DSL anomalies | Network management | One-class classifiers | Supervised learning | Support vector machines,Computer Communications,2015-05-15,Article,"Marnerides, A. K.;Malinowski, S.;Morla, R.;Kim, H. S.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85075379402,10.1109/ICCONS.2018.8663026,A Novel Model Based on Nonlinear Manifold Detection for Software Defect Prediction,"Development of software that may be encouraging for the developers and yield more customer satisfaction in lesser time and cost requires early prediction of defects lying already in the software system. Development of a defect-free and reliable software system involves conducting series of test cases which is actually a time consuming and cost oriented exercise. It requires framing a defect prediction model applying effective technique with suitable defect prediction performance measures that may be empirically validated for ensuring relevance to software organizations. Although series of defect prediction models have been developed using various classifiers and different techniques on defect datasets but those models were not at all fault-free and fully effective to achieve the goal. As such, it has become pertinent to set up an empirical framework and develop a newer Nonlinear Manifold Detection (NMD) Model along with various machine learning approaches for prediction of defects in software in most accurate manner. The new NMD Model ventured in identifying the attributes which are best and in that process all the unwanted, redundant and undesired attributes were eliminated. In this model, critical analysis and comparison with other Feature selection approaches have been made and the results have showed that NMD Model is more accurate and effective in predicting software defects. The prediction performance of various machine learning approaches have actually been compared by using measures like Accuracy, MAE, RMSE, AUC and they have also been tested statistically by use of Friedman and Nemenyi test. The experiment finally proved that NMD Model is more effective, significant and better result-oriented in terms of accuracy than other defect prediction approaches. © 2018 IEEE.",Classification | Cluster shapes | Collision cascades | Machine learning | Molecular dynamics | Pattern matching | Radiation damage,Computational Materials Science,2020-02-01,Article,"Bhardwaj, Utkarsh;Sand, Andrea E.;Warrier, Manoj",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85072269511,10.14569/ijacsa.2019.0100836,Machine learning approaches for predicting the severity level of software bug reports in closed source projects,"In Software Development Life Cycle, fixing defect bugs is one of the essential activities of the software maintenance phase. Bug severity indicates how major or minor the bug impacts on the execution of the system and how rapidly the developer should fix it. Triaging a vast amount of new bugs submitted to the software bug repositories is a cumbersome and time-consuming process. Manual triage might lead to a mistake in assigning the appropriate severity level for each bug. As a consequence, a delay for fixing severe software bugs will take place. However, the whole process of assigning the severity level for bug reports should be automated. In this paper, we aim to build prediction models that will be utilized to determine the class of the severity (severe or non-severe) of the reported bug. To validate our approach, we have constructed a dataset from historical bug reports stored in JIRA bug tracking system. These bug reports are related to different closed-source projects developed by INTIX Company located in Amman, Jordan. We compare eight popular machine learning algorithms, namely Naive Bayes, Naive Bayes Multinomial, Support Vector Machine, Decision Tree (J48), Random Forest, Logistic Model Trees, Decision Rules (JRip) and K-Nearest Neighbor in terms of accuracy, F-measure and Area Under the Curve (AUC). According to the experimental results, a Decision Tree algorithm called Logistic Model Trees achieved better performance compared to other machine learning algorithms in terms of Accuracy, AUC and F-measure with values of 86.31, 0.90 and 0.91, respectively. © 2018 The Science and Information (SAI) Organization Limited.",Bug severity | Bug tracking system | Closed-source projects | Data mining | Machine learning | Severity prediction | Software engineering | Software maintenance,International Journal of Advanced Computer Science and Applications,2019-01-01,Article,"Baarah, Aladdin;Aloqaily, Ahmad;Salah, Zaher;Zamzeer, Mannam;Sallam, Mohammad",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85052314795,10.1109/QRS.2018.00023,Is predicting software security bugs using deep learning better than the traditional machine learning algorithms?,"Software insecurity is being identified as one of the leading causes of security breaches. In this paper, we revisited one of the strategies in solving software insecurity, which is the use of software quality metrics. We utilized a multilayer deep feedforward network in examining whether there is a combination of metrics that can predict the appearance of security-related bugs. We also applied the traditional machine learning algorithms such as decision tree, random forest, naïve bayes, and support vector machines and compared the results with that of the Deep Learning technique. The results have successfully demonstrated that it was possible to develop an effective predictive model to forecast software insecurity based on the software metrics and using Deep Learning. All the models generated have shown an accuracy of more than sixty percent with Deep Learning leading the list. This finding proved that utilizing Deep Learning methods and a combination of software metrics can be tapped to create a better forecasting model thereby aiding software developers in predicting security bugs. © 2018 IEEE.",Bug propensity correlational analysis | Deep learning | Feedforward artificial network | Predictive models | Software insecurity | Software metrics,"Proceedings - 2018 IEEE 18th International Conference on Software Quality, Reliability, and Security, QRS 2018",2018-08-02,Conference Paper,"Clemente, Caesar Jude;Jaafar, Fehmi;Malik, Yasir",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85051142497,10.1007/978-3-319-96562-8_1,"Machine learning for software analysis: Models, methods, and applications","Machine Learning (ML) is the discipline that studies methods for automatically inferring models from data. Machine learning has been successfully applied in many areas of software engineering including: behaviour extraction, testing and bug fixing. Many more applications are yet to be defined. Therefore, a better fundamental understanding of ML methods, their assumptions and guarantees can help to identify and adopt appropriate ML technology for new applications. In this chapter, we present an introductory survey of ML applications in software engineering, classified in terms of the models they produce and the learning methods they use. We argue that the optimal choice of an ML method for a particular application should be guided by the type of models one seeks to infer. We describe some important principles of ML, give an overview of some key methods, and present examples of areas of software engineering benefiting from ML. We also discuss the open challenges for reaching the full potential of ML for software engineering and how ML can benefit from software engineering methods. © Springer International Publishing AG, part of Springer Nature 2018.",Machine learning | Software engineering,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,Conference Paper,"Bennaceur, Amel;Meinke, Karl",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85058367657,10.5277/e-Inf170105,NRFixer: Sentiment based model for predicting the fixability of non-reproducible bugs,"Software maintenance is an essential step in software development life cycle. Nowadays, software companies spend approximately 45% of total cost in maintenance activities. Large software projects maintain bug repositories to collect, organize and resolve bug reports. Sometimes it is difficult to reproduce the reported bug with the information present in a bug report and thus this bug is marked with resolution non-reproducible (NR). When NR bugs are reconsidered, a few of them might get fixed (NR-to-fix) leaving the others with the same resolution (NR). To analyse the behaviour of developers towards NR-to-fix and NR bugs, the sentiment analysis of NR bug report textual contents has been conducted. The sentiment analysis of bug reports shows that NR bugs' sentiments incline towards more negativity than reproducible bugs. Also, there is a noticeable opinion drift found in the sentiments of NR-to-fix bug reports. Observations driven from this analysis were an inspiration to develop a model that can judge the fixability of NR bugs. Thus a framework, NRFixer, which predicts the probability of NR bug fixation, is proposed. NRFixer was evaluated with two dimensions. The first dimension considers meta-fields of bug reports (model-1) and the other dimension additionally incorporates the sentiments (model-2) of developers for prediction. Both models were compared using various machine learning classifiers (Zero-R, naive Bayes, J48, random tree and random forest). The bug reports of Firefox and Eclipse projects were used to test NRFixer. In Firefox and Eclipse projects, J48 and Naive Bayes classifiers achieve the best prediction accuracy, respectively. It was observed that the inclusion of sentiments in the prediction model shows a rise in the prediction accuracy ranging from 2 to 5% for various classifiers.",Crystal structures | Evolutionary algorithm | Genetic algorithm | Materials discovery | Structure prediction | Superhard materials | X-ray diffraction pattern,Computer Physics Communications,2019-04-01,Article,"Avery, Patrick;Toher, Cormac;Curtarolo, Stefano;Zurek, Eva",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85040456672,10.1109/ESEM.2015.7321218,Will This Bug-Fixing Change Break Regression Testing?,"Context: Software source code is frequently changed for fixing revealed bugs. These bug-fixing changes might introduce unintended system behaviors, which are inconsistent with scenarios of existing regression test cases, and consequently break regression testing. For validating the quality of changes, regression testing is a required process before submitting changes during the development of software projects. Our pilot study shows that 48.7% bug-fixing changes might break regression testing at first run, which means developers have to run regression testing at least a couple of times for 48.7% changes. Such process can be tedious and time consuming. Thus, before running regression test suite, finding these changes and corresponding regression test cases could be helpful for developers to quickly fix these changes and improve the efficiency of regression testing. Goal: This paper proposes bug- fixing change impact prediction (BFCP), for predicting whether a bug-fixing change will break regression testing or not before running regression test cases, by mining software change histories. Method: Our approach employs the machine learning algorithms and static call graph analysis technique. Given a bug-fixing change, BFCP first predicts whether it will break existing regression test cases; second, if the change is predicted to break regression test cases, BFCP can further identify the might-be-broken test cases. Results: Results of experiments on 552 real bug-fixing changes from four large open source projects show that BFCP could achieve prediction precision up to 83.3% recall up to 92.3% and F-score up to 81.4%. For identifying the might-be-broken test cases, BFCP could achieve 100% recall. © 2015 IEEE.",Artificial Intelligence | Automated diagnosis | Automated troubleshooting | Software engineering | Software fault prediction,Engineering Applications of Artificial Intelligence,2018-03-01,Article,"Elmishali, Amir;Stern, Roni;Kalech, Meir",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84951567957,10.1145/2590748.2590755,A comparative study of feature-ranking and feature-subset selection techniques for improved fault prediction,"The quality of a fault prediction model depends on the software metrics that are used to build the prediction model. Feature selection represents a process of selecting a subset of relevant features that may lead to build improved prediction models. Feature selection techniques can be broadly categorized into two subcategories: feature-ranking and feature-subset selection. In this paper, we present a comparative investigation of seven feature-ranking techniques and eight feature-subset selection techniques for improved fault prediction. The performance of these feature selection techniques is evaluated using two popular machine-learning classi ers: Naive Bayes and Random Forest, over fourteen software project's fault-datasets obtained from the PROMISE data repository. The performances were measured using F-measure and AUC values. Our results demonstrated that feature-ranking techniques produced better results compared to feature-subset selection techniques. Among, the featureranking techniques used in the study, InfoGain and PCA techniques provided the best performance over all the datasets, while for feature-subset selection techniques Classi erSubsetEval and Logistic Regression produced better results against their peers. Copyright 2014 ACM.",Cliffs | Erosion | Landsliding | Machine learning | Maxent | Regression trees,Environmental Modelling and Software,2016-02-01,Article,"Dickson, Mark E.;Perry, George L.W.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84863490485,10.1587/transinf.E95.D.2006,Asymmetric learning based on kernel partial least squares for software defect prediction,"An asymmetric classifier based on kernel partial least squares is proposed for software defect prediction. This method improves the prediction performance on imbalanced data sets. The experimental results validate its effectiveness. © 2012 The Institute of Electronics, Information and Communication Engineers.",Class imbalance | Defect prediction | Kernel partial least squares | Machine learning,IEICE Transactions on Information and Systems,2012-01-01,Conference Paper,"Luo, Guangchun;Ma, Ying;Qin, Ke",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84861919426,10.1587/transinf.E95.D.1680,Active learning for software defect prediction,"An active learning method, called Two-stage Active learning algorithm (TAL), is developed for software defect prediction. Combining the clustering and support vector machine techniques, this method improves the performance of the predictor with less labeling effort. Experiments validate its effectiveness. © 2012 The Institute of Electronics, Information and Communication Engineers.",Active learning | Defect prediction | Machine learning | Support vector machine,IEICE Transactions on Information and Systems,2012-01-01,Article,"Luo, Guangchun;Ma, Ying;Qin, Ke",Include,
10.1016/j.infsof.2022.107128,2-s2.0-78650716308,10.1109/ISSRE.1992.285903,Providing an empirical basis for optimizing the verification and testing phases of software development,"Applying equal testing and verification effort to all parts of a software system is not very efficient, especially when resources are limited and scheduling is tight. Therefore, one needs to be able to differentiate low/high fault density components so that testing/verification effort can be concentrated where needed. Such a strategy is expected to detect more faults and thus improve the resulting reliability of the overall system. This paper presents an alternative approach for constructing such models that is intended to fulfill specific software engineering needs, (i.e. dealing with partial/incomplete information and creating models that are easy to interpret). Our approach to classification is to (1) measure the software system to be considered and (2) build multivariate stochastic models for prediction. We present experimental results obtained by classifying FORTRAN components developed at the NASA Goddard Space Flight Center into two fault density classes: low and high. Also, we evaluate the accuracy of the model and the insights it provides into the software process. © 1992 IEEE.",Empirical software engineering | Outlier detection | Software fault prediction | Software metrics thresholds,Expert Systems with Applications,2011-04-01,Article,"Alan, Oral;Catal, Cagatay",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85099608821,10.1016/j.infsof.2021.106530,Improving high-impact bug report prediction with combination of interactive machine learning and active learning,"Context: Bug reports record issues found during software development and maintenance. A high-impact bug report (HBR) describes an issue that can cause severe damage once occurred after deployment. Identifying HBRs from the bug repository as early as possible is crucial for guaranteeing software quality. Objective: In recent years, many machine learning-based approaches have been proposed for HBR prediction, and most of them are based on supervised machine learning. However, the assumption of supervised machine learning is that it needs a large number of labeled data, which is often difficult to gather in practice. Method: In this paper, we propose hbrPredictor, which combines interactive machine learning and active learning to HBR prediction. On the one hand, it can dramatically reduce the number of bug reports required for prediction model training; on the other hand, it improves the diversity and generalization ability of training samples via uncertainty sampling. Result: We take security bug report (SBR) prediction as an example of HBR prediction and perform a large-scale experimental evaluation on datasets from different open-source projects. The results show: (1) hbrPredictor substantially outperforms the two baselines and obtains the maximum values of F1-score (0.7939) and AUC (0.8789); (2) with the dynamic stop criteria, hbrPredictor could reach its best performance with only 45% and 13% of the total bug reports for small-sized datasets and large-sized datasets, respectively. Conclusion: By reducing the number of required training samples, hbrPredictor could substantially save the data labeling effort without decreasing the effectiveness of the model. © 2021 Elsevier B.V.",Active learning | High-impact bug report | Interactive machine learning | Security bug report prediction | Uncertainty-sampling,Information and Software Technology,2021-05-01,Article,"Wu, Xiaoxue;Zheng, Wei;Chen, Xiang;Zhao, Yu;Yu, Tingting;Mu, Dejun",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84977070618,10.1007/s10515-020-00277-4,Understanding machine learning software defect predictions,"Software defects are well-known in software development and might cause several problems for users and developers aside. As a result, researches employed distinct techniques to mitigate the impacts of these defects in the source code. One of the most notable techniques focuses on defect prediction using machine learning methods, which could support developers in handling these defects before they are introduced in the production environment. These studies provide alternative approaches to predict the likelihood of defects. However, most of these works concentrate on predicting defects from a vast set of software features. Another key issue with the current literature is the lack of a satisfactory explanation of the reasons that drive the software to a defective state. Specifically, we use a tree boosting algorithm (XGBoost) that receives as input a training set comprising records of easy-to-compute characteristics of each module and outputs whether the corresponding module is defect-prone. To exploit the link between predictive power and model explainability, we propose a simple model sampling approach that finds accurate models with the minimum set of features. Our principal idea is that features not contributing to increasing the predictive power should not be included in the model. Interestingly, the reduced set of features helps to increase model explainability, which is important to provide information to developers on features related to each module of the code which is more defect-prone. We evaluate our models on diverse projects within Jureczko datasets, and we show that (i) features that contribute most for finding best models may vary depending on the project and (ii) it is possible to find effective models that use few features leading to better understandability. We believe our results are useful to developers as we provide the specific software features that influence the defectiveness of selected projects. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Ensemble learning | Multiobjective optimization | Sentiment analysis | Weighted majority voting,Expert Systems with Applications,2016-11-15,Article,"Onan, Aytuğ;Korukoğlu, Serdar;Bulut, Hasan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85097272089,10.1145/3416508.3417114,Software defect prediction using tree-based ensembles,"Software defect prediction is an active research area in software engineering. Accurate prediction of software defects assists software engineers in guiding software quality assurance activities. In machine learning, ensemble learning has been proven to improve the prediction performance over individual machine learning models. Recently, many Tree-based ensembles have been proposed in the literature, and their prediction capabilities were not investigated in defect prediction. In this paper, we will empirically investigate the prediction performance of seven Tree-based ensembles in defect prediction. Two ensembles are classified as bagging ensembles: Random Forest and Extra Trees, while the other five ensembles are boosting ensembles: Ada boost, Gradient Boosting, Hist Gradient Boosting, XGBoost and CatBoost. The study utilized 11 publicly available MDP NASA software defect datasets. Empirical results indicate the superiority of Tree-based bagging ensembles: Random Forest and Extra Trees ensembles over other Tree-based boosting ensembles. However, none of the investigated Tree-based ensembles was significantly lower than individual decision trees in prediction performance. Finally, Adaboost ensemble was the worst performing ensemble among all Tree-based ensembles. © 2020 ACM.",Bagging | Boosting | Classification | Ensemble Learning | Machine Learning | Prediction | Software Defect,"PROMISE 2020 - Proceedings of the 16th ACM International Conference on Predictive Models and Data Analytics in Software Engineering, Co-located with ESEC/FSE 2020",2020-11-08,Conference Paper,"Aljamaan, Hamoud;Alazba, Amal",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85083711585,10.4018/IJKSS.2020040102,Comparison of machine learning techniques for software quality prediction,"Software quality prediction is one the most challenging tasks in the development and maintenance of software. Machine learning (ML) is widely being incorporated for the prediction of the quality of a final product in the early development stages of the software development life cycle (SDLC). An ML prediction model uses software metrics and faulty data from previous projects to detect highrisk modules for future projects, so that the testing efforts can be targeted to those specific 'risky' modules. Hence, ML-based predictors contribute to the detection of development anomalies early and inexpensively and ensure the timely delivery of a successful, failure-free and supreme quality software product within budget. This article has a comparison of 30 software quality prediction models (5 technique ∗ 6 dataset) built on five ML techniques: artificial neural network (ANN); support vector machine (SVMs); Decision Tree (DTs); k-Nearest Neighbor (KNN); and Naïve Bayes Classifiers (NBC), using six datasets: CM1, KC1, KC2, PC1, JM1, and a combined one. These models exploit the predictive power of static code metrics, McCabe complexity metrics, for quality prediction. All thirty predictors are compared using a receiver operator curve (ROC), area under the curve (AUC), and accuracy as performance evaluation criteria. The results show that the ANN technique for software quality prediction is promising for accurate quality prediction irrespective of the dataset used. Copyright © 2020 IGI Global.",Area under the curve (AUC) | Artificial neural network (ANN) | Classification tree | Fault prediction | KNN | Machine learning (ML) | Naïve-Bayes | Receiver operator curve (ROC) | Software quality | Support vector machine (SVM),International Journal of Knowledge and Systems Science,2020-04-01,Article,"Goyal, Somya",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85078658276,10.1002/spe.2784,Software fault prediction using particle swarm algorithm with genetic algorithm and support vector machine classifier,"Software fault prediction is a process of developing modules that are used by developers in order to help them to detect faulty classes or faulty modules in early phases of the development life cycle and to determine the modules that need more refactoring in the maintenance phase. Software reliability means the probability of failure has occurred during a period of time, so when we describe a system as not reliable, it means that it contains many errors, and these errors can be accepted in some systems, but it may lead to crucial problems in critical systems like aircraft, space shuttle, and medical systems. Therefore, locating faulty software modules is an essential step because it helps defining the modules that need more refactoring or more testing. In this article, an approach is developed by integrating genetics algorithm (GA) with support vector machine (SVM) classifier and particle swarm algorithm for software fault prediction as a stand though for better software fault prediction technique. The developed approach is applied into 24 datasets (12-NASA MDP and 12-Java open-source projects), where NASA MDP is considered as a large-scale dataset and Java open-source projects are considered as a small-scale dataset. Results indicate that integrating GA with SVM and particle swarm algorithm improves the performance of the software fault prediction process when it is applied into large-scale and small-scale datasets and overcome the limitations in the previous studies. © 2020 John Wiley & Sons, Ltd.",fault prediction | genetic algorithm | machine learning | particle swarm algorithm,Software - Practice and Experience,2020-04-01,Article,"Alsghaier, Hiba;Akour, Mohammed",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85083739365,10.1109/ACCESS.2020.2985290,The Influence of Deep Learning Algorithms Factors in Software Fault Prediction,"The discovery of software faults at early stages plays an important role in improving software quality; reduce the costs, time, and effort that should be spent on software development. Machine learning (ML) have been widely used in the software faults prediction (SFP), ML algorithms provide varying results in terms of predicting software fault. Deep learning achieves remarkable performance in various areas such as computer vision, natural language processing, speech recognition, and other fields. In this study, two deep learning algorithms are studied, Multi-layer perceptron's (MLPs) and Convolutional Neural Network (CNN) to address the factors that might have an influence on the performance of both algorithms. The experiment results show how modifying parameters is directly affecting the resulting improvement, these parameters are manipulated until the optimal number for each of them is reached. Moreover, the experiments show that the effect of modifying parameters had an important role in prediction performance, which reached a high rate in comparison with the traditional ML algorithm. To validate our assumptions, the experiments are conducted on four common NASA datasets. The result shows how the addressed factors might increase or decrease the fault detection rate measurement. The improvement rate was as follows up to 43.5% for PC1, 8% for KC1, 18% for KC2 and 76.5% for CM1. © 2013 IEEE.",classification | Deep learning algorithms | hyper parameters | software fault prediction,IEEE Access,2020-01-01,Article,"Qasem, Osama Al;Akour, Mohammed;Alenezi, Mamdouh",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85078242160,10.34028/iajit/17/1/4,Machine learning based prediction of complex bugs in source code,"During software development and maintenance phases, the fixing of severe bugs are mostly very challenging and needs more efforts to fix them on a priority basis. Several research works have been performed using software metrics and predict fault-prone software module. In this paper, we propose an approach to categorize different types of bugs according to their severity and priority basis and then use them to label software metrics’ data. Finally, we used labeled data to train the supervised machine learning models for the prediction of fault prone software modules. Moreover, to build an effective prediction model, we used genetic algorithm to search those sets of metrics which are highly correlated with severe bugs. © 2020, Zarka Private University. All rights reserved.",Fault prediction model | Machine learning | Software bugs | Software metrics,International Arab Journal of Information Technology,2020-01-01,Article,"Uqaili, Ishrat Un Nisa;Ahsan, Syed Nadeem",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85111071972,10.1145/3345629.3345632,On usefulness of the deep-learning-based bug localization models to practitioners,"Background: Developers spend a significant amount of time and effort to localize bugs. In the literature, many researchers proposed state-of-the-art bug localization models to help developers localize bugs easily. The practitioners, on the other hand, expect a bug localization tool to meet certain criteria, such as trustworthiness, scalability, and eficiency. The current models are not capable of meeting these criteria, making it harder to adopt these models in practice. Recently, deep-learning-based bug localization models have been proposed in the literature. They show a better performance than the state-of-the-art models. Aim: In this research, we would like to investigate whether deep learning models meet the expectations of practitioners or not. Method: We constructed a Convolution Neural Network and a Simple Logistic model to examine their effectiveness in localizing bugs. We train these models on five open source projects written in Java and compare their performance with the performance of other state-of-the-art models trained on these datasets. Results: Our experiments show that although the deep learning models perform better than classic machine learning models, they meet the adoption criteria set by the practitioners only partially. Conclusions: This work provides evidence that the practitioners should be cautious while using the current state of the art models for production-level use-cases. It also highlights the need for standardization of performance benchmarks to ensure that bug localization models are assessed equitably and realistically. © 2019 Association for Computing Machinery.",Classification | Code clones | Features | Machine learning | Semantic clones | Similarity measures | Software engineering | Syntactic clones,Expert Systems with Applications,2021-11-30,Article,"Sheneamer, Abdullah M.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85073771820,10.1109/QRS.2019.00014,A Cluster-Based Hybrid Feature Selection Method for Defect Prediction,Machine learning is an effective method for software defect prediction. The performance of learning models can be affected by irrelative and redundant features. Feature selection techniques select a subset of most impactful relevant features that will result in higher accuracy and efficiency of models. This paper proposed a Cluster-based Hybrid Feature Selection method (CHIFS) for software defect prediction. A spectral cluster-based Feature Quality coefficient (FQ) was defined as a comprehensive measurement of feature relevance and redundancy. The final feature subset was iteratively selected from feature sequence ranked by FQ. The proposed CHIFS method was validated in the experiments using 3 classifiers with 15 open datasets from Promise Repository. Experimental results showed that the CHIFS method performed better than traditional methods in terms of accuracy and efficiency on a wide range of datasets. © 2019 IEEE.,defect prediction | feature selection | software network | spectral cluster,"Proceedings - 19th IEEE International Conference on Software Quality, Reliability and Security, QRS 2019",2019-07-01,Conference Paper,"Wang, Fei;Ai, Jun;Zou, Zhuoliang",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85120887507,10.1109/ECACE.2019.8679382,Revisiting the Class Imbalance Issue in Software Defect Prediction,"Software defect prediction is related to the testing area of software industry. Several methods have been developed for the prediction of bugs in software source codes. The objective of this study is to find the inconsistency of performance between imbalances and balance data set and to find the distinction of performance between single classifier and aggregate classifier (voting). In this investigation, eight publicly available data sets have collected, also seven algorithms and hard voting are used for finding precision, recall and F-1 score to predict software defect. In these collected data, two sets are almost balanced. For this investigation, these balanced data sets have converted into imbalanced sets as average non-defective and defective ratio of the other 6 data sets. The experiment result shows that performance of the two balanced data sets is lower than other six sets. After conversion of two data sets, the performance has increased as like as other six data sets. Another observation is the performance metric that shows the results of precision, recall and F1-score for voting are 0.92, 0.84 and 0.87 respectively, which are better than other single classifier. This study has been able to shows that- imbalance of non-defective and defective classes have a big impact on software defect prediction and the voting is the best performer among the classifiers. © 2019 IEEE.",Bearing fault | Broken rotor bar | Dilated convolutional neural network (DCNN) | Squirrel cage induction motor (SCIM),Expert Systems with Applications,2022-04-01,Article,"Kumar, Prashant;Hati, Ananda Shankar",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84917709364,10.1109/ACCESS.2019.2924040,Experimental validation of inheritance metrics' impact on software fault prediction,"Software faults can cause trivial annoyance to catastrophic failures. Recent work in software fault prediction (SFP) advocates the need for predicting faults before deployment to aid testing process. Object-oriented programming is complex while comparing it with procedural languages having multiple dimensions wherein inheritance is an important aspect. In this paper, we aim to investigate how much inheritance metrics assist in predicting software fault proneness. We first select the Chidamber and Kemerer (CK) metrics, most accepted metric suite for predicting software faults and inheritance metrics. We use 65 publicly available base datasets having CK metrics and some other inheritance metrics to evaluate the impact of inheritance on SFP. We split each dataset into further two datasets: inheritance with CK and CK without inheritance for comparison of results. An artificial neural network (ANN) is used for model building, and accuracy, recall, precision, F1 measures, and true negative rate (TNR) are used for measuring performance. Comparison is made and the results show an acceptable contribution of inheritance metrics in SFP. The testing community can safely use inheritance metrics in predicting software faults. Moreover, high inheritance is not desirable, as this can potentially lead to software faults. © 2013 IEEE.",Complex distributed systems | Complex event processing | Critical infrastructures | Failure prediction | Machine learning,Future Generation Computer Systems,2015-01-01,Article,"Baldoni, Roberto;Montanari, Luca;Rizzuto, Marco",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84899897203,10.1002/cae.21962,A computer-assisted instructional method based on machine learning in software testing class,"With the development of computer technology, the old and outdated teaching cases cannot meet requirements on teaching currently. Therefore, in order to solve this problem, we need to rebuild the teaching content and teaching cases. Moreover, in the traditional classroom teaching, each student gets the same practice content, which is not pertinent. In order to solve these two problems, we propose a new method called Software Testing Computer Assistant Education (STCAE), which is based on machine learning, information retrieval, and natural language processing technology. STCAE has three steps: First, STCAE uses NLP to extract the text features from the classified bug reports in the database and to classify all the samples. Second, STCAE scores these bug reports according to the corresponding weight model. Third, STCAE updates the ratings based on the feedbacks of teachers and students on the case. In constructing STCAE, we consider the interactive, creative, pertinent, and error-correcting capabilities thoroughly in teaching needs, overcoming the four shortcomings of traditional CAI. In addition, we build Software Testing Computer Assistant Education System (STCAES) under STCAE and introduce STCAES into daily teaching. All the achievements in the new course shows that STCAE has achieved great success in practical classroom teaching. © 2018 Wiley Periodicals, Inc.",Bayesian networks | Expert judgments | Multinomial parameter learning,International Journal of Approximate Reasoning,2014-01-01,Article,"Zhou, Yun;Fenton, Norman;Neil, Martin",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85051235354,10.1145/3195538.3195545,Ambiguous software requirement specification detection: An automated approach,"Software requirement specification (SRS) document is the most crucial document in software development process. All subsequent steps in software development are influenced by this document. However, issues in requirement, such as ambiguity or incomplete specification may lead to misinterpretation of requirements which consequently, influence the testing activities and higher the risk of time and cost overrun of the project. Finding defects in the initial development phase is crucial since the defect that found late is more expensive than if it was found early. This study describes an automated approach for detecting ambiguous software requirement specification. To this end, we propose the combination of text mining and machine learning. Since the dataset is derived from Malaysian industrial SRS documents, this study only focuses on the Malaysian context. We used text mining for feature extraction and for preparing the training set. Based on this training set, the method 'learns' to detect the ambiguous requirement specification. In this paper, we study a set of nine (9) classification algorithms from the machine learning community and evaluate which algorithm performs best to detect the ambiguous software requirement specification. Based on the experiment's result, we develop a working prototype which later is used for our initial validation of our approach. The initial validation shows that the result produced by the classification model is reasonably acceptable. Even though this study is an experimental benchmark, we optimist that this approach may contributes to enhance the quality of SRS. © 2018 ACM.",machine learning | requirement engineering | software engineering | text mining,Proceedings - International Conference on Software Engineering,2018-06-02,Conference Paper,"Osman, Mohd Hafeez;Zaharin, Mohd Firdaus",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85048406719,10.1109/ICST.2018.00039,An Empirical Study on Software Failure Classification with Multi-label and Problem-Transformation Techniques,"Classification techniques have been used in software-engineering research to perform tasks such as categorizing software executions. Traditionally, existing work has proposed single-label failure classification techniques, in which the training and subsequent executions are labeled with a singular fault attribution. Although such approaches have received substantial attention in research on automated software engineering, in reality, recent work shows that the assumption of such a single attribution is often unrealistic: In practice, the inherent characteristics of software behavior, such as multiple faults that contribute to failures and fault interactions, may negatively influence the effectiveness of these techniques. To relax this unrealistic assumption, in the machine learning field, researchers have proposed new approaches for multi-label classification. However, the effectiveness and efficiency of such approaches varies widely based upon application domains. In this paper, we empirically investigate the performance of these new approaches on the failure classification task under different application settings. We conducted experiments using eight classification techniques on five subject programs with more than 8,000 faulty versions to investigate how each such technique accounts for the intricacies of software behavior. Our experimental results show that multi-label techniques provide improved accuracy over single-label. We also evaluated the efficiency of the training and prediction phases of each technique, and offer guidance as to the applicability for each technique for different usage contexts. © 2018 IEEE.",Empirical Study | Failure Classification | multi-label classification,"Proceedings - 2018 IEEE 11th International Conference on Software Testing, Verification and Validation, ICST 2018",2018-05-25,Conference Paper,"Feng, Yang;Jones, James;Chen, Zhenyu;Fang, Chunrong",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85049617051,10.1109/WISA.2017.17,What are the factors impacting build breakage?,"Continuous Integration (CI) has become a good practice of software development in recent years. As an essential part of CI, build creates software from source code. Predicting build outcome help developers to review and fix bugs before building to save time. However, we are missing objective evidence of practical factors affecting build result. Travis CI provides a hosted, distributed continuous integration service used to build and test software projects hosted at GitHub. The TravisTorrent is a dataset which deeply analyzes source code, process and dependency status of projects hosting on Travis CI. We use this dataset to investigate which factors may impact a build result. We first preprocess TravisTorrent data to extract 27 features. We then analyze the correlation between these features and the result of a build. Finally, we build four prediction models to predict the result of a build and perform a horizontal analysis. We found that in our study, the number of commits in a build (git-num-all-built-commits) is the most import factor that has significant impact on the build result, and SVM performs best in the four of the prediction models we used. © 2017 IEEE.",Build breakage | Continous Integration | Machine Learning | Univariate logistic regression,"Proceedings - 2017 14th Web Information Systems and Applications Conference, WISA 2017",2017-07-02,Conference Paper,"Luo, Yang;Zhao, Yangyang;Ma, Wanwangying;Chen, Lin",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85072534589,10.22059/JITM.2019.274968.2335,Investigating the role of code smells in preventive maintenance,"The quest for improving the software quality has given rise to various studies which focus on the enhancement of the quality of software through various processes. Code smells, which are indicators of the software quality have not been put to an extensive study for as to determine their role in the prediction of defects in the software. This study aims to investigate the role of code smells in prediction of non-faulty classes. We examine the Eclipse software with four versions (3.2, 3.3, 3.6, and 3.7) for metrics and smells. Further, different code smells, derived subjectively through iPlasma, are taken into conjugation and three efficient, but subjective models are developed to detect code smells on each of Random Forest, J48 and SVM machine learning algorithms. This model is then used to detect the absence of defects in the four Eclipse versions. The effect of balanced and unbalanced datasets is also examined for these four versions. The results suggest that the code smells can be a valuable feature in discriminating absence of defects in a software. © University of Tehran, Faculty of Management",Code smells | Machine learning | Preventive maintenance | Random forest,Journal of Information Technology Management,2018-01-01,Article,"Reshi, Junaid Ali;Singh, Satwinder",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85053206921,10.1145/3127005.3127017,An extensive analysis of efficient bug prediction configurations,"Background: Bug prediction helps developers steer maintenance activities towards the buggy parts of a software. There are many design aspects to a bug predictor, each of which has several options, i.e., software metrics, machine learning model, and response variable. Aims: These design decisions should be judiciously made because an improper choice in any of them might lead to wrong, misleading, or even useless results. We argue that bug prediction con?gurations are intertwined and thus need to be evaluated in their entirety, in contrast to the common practice in the ?eld where each aspect is investigated in isolation. Method: We use a cost-aware evaluation scheme to evaluate 60 di?erent bug prediction con?guration combinations on ?ve open source Java projects. Results:We ?nd out that the best choices for building a cost-e?ective bug predictor are change metrics mixed with source code metrics as independent variables, Random Forest as the machine learning model, and the number of bugs as the response variable. Combining these con?guration options results in the most e?cient bug predictor across all subject systems. Conclusions: We demonstrate a strong evidence for the interplay among bug prediction con?gurations and provide concrete guidelines for researchers and practitioners on how to build and evaluate e?cient bug predictors. © 2017 Association for Computing Machinery. All rights reserved.",Bug prediction | Effort-aware evaluation,ACM International Conference Proceeding Series,2017-11-08,Conference Paper,"Osman, Haidar;Ghafari, Mohammad;Nierstrasz, Oscar;Lungu, Mircea",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85011277924,10.1109/ICBDACI.2017.8070806,Empirical comparison of machine learning algorithms for bug prediction in open source software,"Bug tracking and analysis truly remains one of the most active areas of software engineering research. Bug tracking results may be employed by the software practitioners of large software projects effectively. The cost of detecting and correcting the defect becomes exponentially higher as we go from requirement analysis to the maintenance phase, where defects might even lead to loss of lives. Software metrics in conjunction with defect data can serve as basis for developing predictive models. Open source projects which encompass contributions from millions of people provide capacious dataset for testing. There have been diverse machine learning techniques proposed in the literature for analyzing complex relationships and extracting useful information from problems using optimal resources and time. However, more extensive research comparing these techniques is needed to establish superiority of one technique over another. This study aims at comparison of 14 ML techniques for development of effective defect prediction models. The issues addressed are 1) Construction of automated tool in Java to collect OO, inheritance and other metrics and detect bugs in classes extracted from open source repository, 2) Use of relevant performance measures to evaluate performance of predictive models to detect bugs in classes, 3) Statistical tests to compare predictive capability of different machine learning techniques, 4) Validation of defect prediction models. The results of the study show that Single Layer Perceptron is the best technique amongst all the techniques used in this study for development of defect prediction models. The conclusions drawn from this study can be used for practical applications by software practitioners to determine best technique for defect prediction and consequently carry out effective allocation of resources. © 2017 IEEE.",Analysis | Case study | Prediction | Software faults and failures | Software fix implementation effort,Information and Software Technology,2017-07-01,Article,"Hamill, Maggie;Goseva-Popstojanova, Katerina",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85026542178,10.1587/transinf.2016EDL8238,Kernel CCA based transfer learning for software defect prediction,"An transfer learning method, called Kernel Canonical Correlation Analysis plus (KCCA+), is proposed for heterogeneous Crosscompany defect prediction. Combining the kernel method and transfer learning techniques, this method improves the performance of the predictor with more adaptive ability in nonlinearly separable scenarios. Experiments validate its effectiveness. Copyright © 2017 The Institute of Electronics, Information and Communication Engineers.",Defect prediction | Kernel canonical correlation analysis | Machine learning | Transfer learning,IEICE Transactions on Information and Systems,2017-08-01,Conference Paper,"Ma, Ying;Zhu, Shunzhi;Chen, Yumin;Li, Jingjing",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84991757124,10.1109/QRS-C.2016.68,An Under-sampling Method: Based on Principal Component Analysis and Comprehensive Evaluation Model,"Machine learning method can be used to forecast software fault, and identify modules which have the tendency to cause soft-error at the early life cycle, then software developer can modify these defect modules early. It has an important significance on the improvement of software reliability. However, fault samples of software data sets are smaller in number (one or two order of magnitudes) compared with fault-free samples, learning machine's predictive ability to fault samples has been restrained by this kind of unbalanced data sets. This paper put forward an under-sampling method based on principal component analysis (PCA) and comprehensive evaluation model to get rid of redundant majority class samples under the premise of conserving data of majority class characteristic as far as possible, and reaches to a balance between this two kinds of samples. © 2016 IEEE.",comprehensive evaluation | PCA | Under-Sampling,"Proceedings - 2016 IEEE International Conference on Software Quality, Reliability and Security-Companion, QRS-C 2016",2016-09-21,Conference Paper,"Fu, Yangzhen;Zhang, Hong;Bai, Yaxin;Sun, Weixuan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85063908968,10.1007/978-3-319-24770-0_49,Predicting defect prone modules in web applications,"Predicting defect proneness of software products has been an active research area in software engineering domain in recent years. Researchers have been using static code metrics, code churn metrics, developer networks, and module networks as inputs to their proposed models until now. However, domain specific characteristics of software has not been taken into account. In this research, we propose to include a new set of metrics to improve defect prediction performance for web applications by utilizing their characteristics. To validate our hypotheses we used datasets from 3 open source web applications to conduct our experiments. Defect prediction is then performed using different machine learning algorithms. The results of experiments revealed that overall performance of defect predictors are improved compared to only using existing static code metrics. Therefore we recommend practitioners to utilise domain-specific characteristics in defect prediction as they can be informative. © Springer International Publishing Switzerland 2015.",Bug assignment | Bug reports | Bug repository | Bug tossing | Bug triaging | Developer contribution assessment | Developer expertise | Open source software (OSS) | Software metrics | Software process,Information and Software Technology,2019-08-01,Article,"Yadav, Asmita;Singh, Sandeep Kumar;Suri, Jasjit S.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85065891466,10.1007/978-3-642-05415-0_18,Empirical evaluation of hunk metrics as bug predictors,"Reducing the number of bugs is a crucial issue during software development and maintenance. Software process and product metrics are good indicators of software complexity. These metrics have been used to build bug predictor models to help developers maintain the quality of software. In this paper we empirically evaluate the use of hunk metrics as predictor of bugs. We present a technique for bug prediction that works at smallest units of code change called hunks. We build bug prediction models using random forests, which is an efficient machine learning classifier. Hunk metrics are used to train the classifier and each hunk metric is evaluated for its bug prediction capabilities. Our classifier can classify individual hunks as buggy or bug-free with 86 % accuracy, 83 % buggy hunk precision and 77% buggy hunk recall. We find that history based and change level hunk metrics are better predictors of bugs than code level hunk metrics. © Springer-Verlag Berlin Heidelberg 2009.",Duplicate bug reports | Hidden Markov models | Machine learning | Mining software repositories | Stack traces,Information and Software Technology,2019-09-01,Article,"Ebrahimi, Neda;Trabelsi, Abdelaziz;Islam, Md Shariful;Hamou-Lhadj, Abdelwahab;Khanmohammadi, Kobra",Include,
10.1016/j.infsof.2022.107128,2-s2.0-73949120113,10.1109/ISCIS.2009.5291882,An outlier detection algorithm based on object-oriented metrics thresholds,"Detection of outliers in software measurement datasets is a critical issue that affects the performance of software fault prediction models built based on these datasets. Two necessary components of fault prediction models, software metrics and fault data, are collected from the software projects developed with object-oriented programming paradigm. We proposed an outlier detection algorithm based on these kinds of metrics thresholds. We used Random Forests machine learning classifier on two software measurement datasets collected from jEdit open-source text editor project and experiments revealed that our outlier detection approach improves the performance of fault predictors based on Random Forests classifier. © 2009 IEEE.",Metrics thresholds | Object-oriented metrics | Outlier detection | Software fault prediction,"2009 24th International Symposium on Computer and Information Sciences, ISCIS 2009",2009-12-01,Conference Paper,"Alan, Oral;Catal, Cagatay",Include,
10.1016/j.infsof.2022.107128,2-s2.0-51949084076,,Effective fault localization using BP neural networks,"Fault localization is the most expensive activity of program debugging. It identifies the exact locations of program faults. Finding these faults using an ad-hoc approach or based only on programmers' intuitive guesswork can be very time consuming. A better way is to use a well-justified technique, supported by case studies for its effectiveness, to automatically identify and prioritize suspicious code for an examination of possible fault locations. To do so, we propose the use of a back-propagation (BP) neural network, a machine learning model which has been successful applied to software risk analysis, cost prediction, and reliability estimation, to help programmers effectively locate program faults. A BP neural network is suitable for learning the input-output relationship from a set of data, such as the inputs and the corresponding outputs of a program. We first train a BP neural network with the coverage data (e.g., statement coverage) collected from executing a program, and then we use it to compute the risk of each statement, in terms of its likelihood of containing faults. Suspicious code is ranked in descending order based on its risk. Programmers will examine such code from the top of the rank to identify faults. A case study using the seven programs in the Siemens suite is conducted. Our results suggest that a BP neural network-based fault localization method is effective in locating program faults. Copyright © (2007) by Knowledge Systems Institute (KSI).",BP (back-propagation) neural network; Execution slice; Failed test; Fault localization; Program debugging; Risk of code; Successful test,"19th International Conference on Software Engineering and Knowledge Engineering, SEKE 2007",2007,,"Wong W.E., Zhao L., Qi Y., Cai K.-Y., Dong J.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85091483684,10.1007/978-3-030-59003-1_27,An improved software defect prediction algorithm using self-organizing maps combined with hierarchical clustering and data preprocessing,"An improved software defects prediction algorithm based on combination of Kohonen map and hierarchical clustering is presented in this paper. The need for software reliability assessment and analysis growths rapidly due to increasing dependence of our day-to-day life on software-controlled devices and systems. Software reliability prediction is the only tool available at early stage of software development lifecycle when the debugging cost risk of faulty operation is minimal. Artificial intelligence and machine learning in particular are promising techniques to solve this task. Various classification methods have been used previously to build software defect prediction models, ranging from simple, like logistic regression, to advanced methods, e.g. multivariate adaptive regression splicing. However, the available literature still does not allow to make unambiguous conclusion concerning the choice of the best classifier and trying different dimensions to overcome potential bias is suggested. The purpose of the paper is to analyze the software code metrics to find dependences be-tween software module’s defect-proneness and its metrics. JM1 public NASA dataset from PROMISE Software Engineering Repository was used in this study. To increase the classification accuracy, we combine self-organizing maps with hierarchical clustering and data preprocessing. © Springer Nature Switzerland AG 2020.",Hierarchical clustering | Prediction algorithm | Software defect analysis,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020-01-01,Conference Paper,"Shakhovska, Natalya;Yakovyna, Vitaliy;Kryvinska, Natalia",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85083464754,10.5815/ijmecs.2020.01.01,Hybrid ensemble learning technique for software defect prediction,"The reliability of software depends on its ability to function without error. Unfortunately, errors can be generated during any phase of software development. In the field of software engineering, the prediction of software defects during the initial stages of development has therefore become a top priority. Scientific data are used to predict the software's future release. Study shows that machine learning and hybrid algorithms are change benchmarks in the prediction of defects. During the past two decades, various approaches to software defect prediction that rely on software metrics have been proposed. This paper explores and compares well-known supervised machine learning and hybrid ensemble classifiers in eight PROMISE datasets. The experimental results showed that AdaBoost support vector machines and bagging support vector machines were the best performing classifiers in Accuracy, AUC, recall and F-measure. © 2020 MECS.",Content-based filtering | Machine learning | Security bug report | Word embedding,Information and Software Technology,2020-08-01,Article,"Jiang, Yuan;Lu, Pengcheng;Su, Xiaohong;Wang, Tiantian",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85086502960,10.1155/2020/8509821,Surprise Bug Report Prediction Utilizing Optimized Integration with Imbalanced Learning Strategy,"In software projects, a large number of bugs are usually reported to bug repositories. Due to the limited budge and work force, the developers often may not have enough time and ability to inspect all the reported bugs, and thus they often focus on inspecting and repairing the highly impacting bugs. Among the high-impact bugs, surprise bugs are reported to be a fatal threat to the software systems, though they only account for a small proportion. Therefore, the identification of surprise bugs becomes an important work in practices. In recent years, some methods have been proposed by the researchers to identify surprise bugs. Unfortunately, the performance of these methods in identifying surprise bugs is still not satisfied for the software projects. The main reason is that surprise bugs only occupy a small percentage of all the bugs, and it is difficult to identify these surprise bugs from the imbalanced distribution. In order to overcome the imbalanced category distribution of the bugs, a method based on machine learning to predict surprise bugs is presented in this paper. This method takes into account the textual features of the bug reports and employs an imbalanced learning strategy to balance the datasets of the bug reports. Then these datasets after balancing are used to train three selected classifiers which are built by three different classification algorithms and predict the datasets with unknown type. In particular, an ensemble method named optimization integration is proposed to generate a unique and best result, according to the results produced by the three classifiers. This ensemble method is able to adjust the ability of the classifier to detect different categories based on the characteristics of different projects and integrate the advantages of three classifiers. The experiments performed on the datasets from 4 software projects show that this method performs better than the previous methods in terms of detecting surprise bugs. © 2020 Hui Li et al.",Bug reports | Dimension reduction | Duplicate detection | Feature extraction | Feature selection | Information retrieval | Natural language processing | Textual similarity metric,Information and Software Technology,2020-10-01,Article,"Soleimani Neysiani, Behzad;Babamir, Seyed Morteza;Aritsugi, Masayoshi",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85090970251,10.1109/APSEC48747.2019.00041,Deep Semantic Feature Learning with Embedded Static Metrics for Software Defect Prediction,"Software defect prediction, which locates defective code snippets, can assist developers in finding potential bugs and assigning their testing efforts. Traditional defect prediction features are static code metrics, which only contain statistic information of programs and fail to capture semantics in programs, leading to the degradation of defect prediction performance. To take full advantage of the semantics and static metrics of programs, we propose a framework called Defect Prediction via Attention Mechanism (DP-AM) in this paper. Specifically, DPAM first extracts vectors which are then encoded as digital vectors by mapping and word embedding from abstract syntax trees (ASTs) of programs. Then it feeds these numerical vectors into Recurrent Neural Network to automatically learn semantic features of programs. After that, it applies self-attention mechanism to further build relationship among these features. Furthermore, it employs global attention mechanism to generate significant features among them. Finally, we combine these semantic features with traditional static metrics for accurate software defect prediction. We evaluate our method in terms of F1-measure on seven open-source Java projects in Apache. Our experimental results show that DP-AM improves F1-measure by 11% in average, compared with the state-of-the-art methods. © 2019 IEEE.",Change intent analysis | Machine learning | Review effort,Information and Software Technology,2021-02-01,Article,"Wang, Song;Bansal, Chetan;Nagappan, Nachiappan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85069811147,10.1111/coin.12229,A novel modified undersampling (MUS) technique for software defect prediction,"Background and aim: Many sophisticated data mining and machine learning algorithms have been used for software defect prediction (SDP) to enhance the quality of software. However, real-world SDP data sets suffer from class imbalance, which leads to a biased classifier and reduces the performance of existing classification algorithms resulting in an inaccurate classification and prediction. This work aims to improve the class imbalance nature of data sets to increase the accuracy of defect prediction and decrease the processing time. Methodology: The proposed model focuses on balancing the class of data sets to increase the accuracy of prediction and decrease processing time. It consists of a modified undersampling method and a correlation feature selection (CFS) method. Results: The results from ten open source project data sets showed that the proposed model improves the accuracy in terms of F1-score to 0.52 ∼ 0.96, and hence it is proximity reached best F1-score value in 0.96 near to 1 then it is given a perfect performance in the prediction process. Conclusion: The proposed model focuses on balancing the class of data sets to increase the accuracy of prediction and decrease processing time using the proposed model. © 2019 Wiley Periodicals, Inc.",correlation feature selection | machine learning | modified undersampling | software defect prediction,Computational Intelligence,2019-11-01,Article,"Lingden, P.;Alsadoon, Abeer;Prasad, P. W.C.;Alsadoon, Omar Hisham;Ali, Rasha S.;Nguyen, Vinh Tran Quoc",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85073673916,10.1108/DTA-03-2019-0040,An approach for fault prediction in SOA-based systems using machine learning techniques,"Purpose: Software fault prediction is an important concept that can be applied at an early stage of the software life cycle. Effective prediction of faults may improve the reliability and testability of software systems. As service-oriented architecture (SOA)-based systems become more and more complex, the interaction between participating services increases frequently. The component services may generate enormous reports and fault information. Although considerable research has stressed on developing fault-proneness prediction models in service-oriented systems (SOS) using machine learning (ML) techniques, there has been little work on assessing how effective the source code metrics are for fault prediction. The paper aims to discuss this issue. Design/methodology/approach: In this paper, the authors have proposed a fault prediction framework to investigate fault prediction in SOS using metrics of web services. The effectiveness of the model has been explored by applying six ML techniques, namely, Naïve Bayes, Artificial Networks (ANN), Adaptive Boosting (AdaBoost), decision tree, Random Forests and Support Vector Machine (SVM), along with five feature selection techniques to extract the essential metrics. The authors have explored accuracy, precision, recall, f-measure and receiver operating characteristic curves of the area under curve values as performance measures. Findings: The experimental results show that the proposed system can classify the fault-proneness of web services, whether the service is faulty or non-faulty, as a binary-valued output automatically and effectively. Research limitations/implications: One possible threat to internal validity in the study is the unknown effects of undiscovered faults. Specifically, the authors have injected possible faults into the classes using Java C3.0 tool and only fixed faults are injected into the classes. However, considering the Java C3.0 community of development, testing and use, the authors can generalize that the undiscovered faults should be few and have less impact on the results presented in this study, and that the results may be limited to the investigated complexity metrics and the used ML techniques. Originality/value: In the literature, only few studies have been observed to directly concentrate on metrics-based fault-proneness prediction of SOS using ML techniques. However, most of the contributions are regarding the fault prediction of the general systems rather than SOS. A majority of them have considered reliability, changeability, maintainability using a logging/history-based approach and mathematical modeling rather than fault prediction in SOS using metrics. Thus, the authors have extended the above contributions further by applying supervised ML techniques over web services metrics and measured their capability by employing fault injection methods. © 2019, Emerald Publishing Limited.",Fault | Fault injection | Fault prediction | Machine learning | Metrics | Service-oriented systems | Web services,Data Technologies and Applications,2019-10-22,Article,"Bhandari, Guru Prasad;Gupta, Ratneshwer;Upadhyay, Satyanshu Kumar",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85103314692,10.1109/ESEM.2019.8870173,Predicting Merge Conflicts in Collaborative Software Development,"Background. During collaborative software development, developers often use branches to add features or fix bugs. When merging changes from two branches, conflicts may occur if the changes are inconsistent. Developers need to resolve these conflicts before completing the merge, which is an error-prone and time-consuming process. Early detection of merge conflicts, which warns developers about resolving conflicts before they become large and complicated, is among the ways of dealing with this problem. Existing techniques do this by continuously pulling and merging all combinations of branches in the background to notify developers as soon as a conflict occurs, which is a computationally expensive process. One potential way for reducing this cost is to use a machine-learning based conflict predictor that filters out the merge scenarios that are not likely to have conflicts, i.e.safe merge scenarios.Aims. In this paper, we assess if conflict prediction is feasible.Method. We design a classifier for predicting merge conflicts, based on 9 light-weight Git feature sets. To evaluate our predictor, we perform a large-scale study on 267,657 merge scenarios from 744 GitHub repositories in seven programming languages.Results. Our results show that we achieve high f1-scores, varying from 0.95 to 0.97 for different programming languages, when predicting safe merge scenarios. The f1-score is between 0.57 and 0.68 for the conflicting merge scenarios.Conclusions. Predicting merge conflicts is feasible in practice, especially in the context of predicting safe merge scenarios as a pre-filtering step for speculative merging. © 2019 IEEE.",Clone consistency prediction | Clone consistent change | Code clones | Machine learning | Software maintenance,Information and Software Technology,2021-08-01,Article,"Zhang, Fanlong;Khoo, Siau cheng",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85067133278,10.1109/AITest.2019.00019,Fault detection effectiveness of metamorphic relations developed for testing supervised classifiers,"In machine learning, supervised classifiers are used to obtain predictions for unlabeled data by inferring prediction functions using labeled data. Supervised classifiers are widely applied in domains such as computational biology, computational physics and healthcare to make critical decisions. However, it is often hard to test supervised classifiers since the expected answers are unknown. This is commonly known as the oracle problem and metamorphic testing (MT) has been used to test such programs. In MT, metamorphic relations (MRs) are developed from intrinsic characteristics of the software under test (SUT). These MRs are used to generate test data and to verify the correctness of the test results without the presence of a test oracle. Effectiveness of MT heavily depends on the MRs used for testing. In this paper we have conducted an extensive empirical study to evaluate the fault detection effectiveness of MRs that have been used in multiple previous studies to test supervised classifiers. Our study uses a total of 709 reachable mutants generated by multiple mutation engines and uses data sets with varying characteristics to test the SUT. Our results reveal that only 14.8% of these mutants are detected using the MRs and that the fault detection effectiveness of these MRs do not scale with the increased number of mutants when compared to what was reported in previous studies. © 2019 IEEE.",Machine Learning | Metamorphic relations | Metamorphic testing | Mutation Analysis | Random testing | Supervised classifiers,"Proceedings - 2019 IEEE International Conference on Artificial Intelligence Testing, AITest 2019",2019-05-17,Conference Paper,"Saha, Prashanta;Kanewala, Upulee",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85108368818,10.1109/IoT-SIU.2019.8777494,Software Fault Proneness Prediction Using Genetic Based Machine Learning Techniques,"This work is an attempt to propose a software replica to predict fault proneness by means of genetic based method implementing machine learning. The underlying method is collection of data from open source software, where the data will be in form of object oriented metrics. The said data would be used to create model for forecasting the faults. These techniques are known as genetic based Classifier Systems or learning classifier systems. Later in this work, there is in detail description about data collection technique and stepwise algorithm to get the results. In the end it can be concluded that these techniques can be used to make prediction model on object oriented data of software and can be useful pertaining to fault proneness prediction in the near the beginning stages in the development sequence. of any software (SDLC) © 2019 IEEE.",Empirical study | Issue management | Issue tracking | Machine learning,Information and Software Technology,2021-11-01,Article,"Panichella, Sebastiano;Canfora, Gerardo;Di Sorbo, Andrea",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85117324409,10.1007/978-3-030-24305-0_35,Machine Learning Approach for Reliability Assessment of Open Source Software,"Some of the quality parameters for any successful open source software may be attributed to affordability, availability of source code, re-distributability, and modifiability etc. Quality of software can be further improvised subsequently by either users or associated developers by constantly monitoring some of the reliability aspects. Since multiple users are allowed to modify the code there is a potential threat for security, which might degrade the reliability of software. Bug tracking systems are often considered to monitor various software faults, detected mostly in open source software projects. Various authors have made research in this direction by applying different techniques in order to improve the reliability of open source software projects. In this work, an various machine learning models have been implemented to examine the reliability of the software. An extensive numerical illustration has also been presented for bug data recorded on bug tracking system. The effectiveness of machine learning models for estimating the level of faults associated with the systems has been verified by comparing it with similar approaches as available in the literature. © 2019, Springer Nature Switzerland AG.",Classification | Convolution neural network | Data-based software engineering | Deep learning | Issue reports | Machine learning | Recurrent neural network | Software features | User manual,Information and Software Technology,2022-02-01,Article,"Cho, Heetae;Lee, Seonah;Kang, Sungwon",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85117959779,10.1109/CCCS.2018.8586805,Machine learning based software fault prediction utilizing source code metrics,"In the conventional techniques, it requires prior knowledge of faults or a special structure, which may not be realistic in practice while detecting the software faults. To deal with this problem, in this work, the proposed approach aims to predict the faults of the software utilizing the source code metrics. In addition, the purpose of this paper is to measure the capability of the software fault predictability in terms of accuracy, f-measure, precision, recall, Area Under ROC (Receiver Operating Characteristic) Curve (AUC). The study investigates the effect of the feature selection techniques for software fault prediction. As an experimental analysis, our proposed approach is validated from four publicly available datasets. The result predicted from Random Forest technique outperforms the other machine learning techniques in most of the cases. The effect of the feature selection techniques has increased the performance in few cases, however, in the maximum cases it is negligible or even the worse. © 2018 IEEE.",Abandoned | Code review | Early prediction | Merged | Patch,Information and Software Technology,2022-02-01,Article,"Islam, Khairul;Ahmed, Toufique;Shahriyar, Rifat;Iqbal, Anindya;Uddin, Gias",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85058291691,10.1109/SCAM.2018.00016,Periodic developer metrics in software defect prediction,"Defect prediction studies have proposed several data-driven approaches, and recently, this field has put more emphasis on whether the people factor is associated software defects. Developer metrics can capture experience, code ownership, coding skills and techniques, and commit activities. These metrics have so far been measured at a specified snapshot of the codebase although developer's knowledge on a source module could change over time. In this paper, we propose to measure periodic developer experience with regard to contextual knowledge on files and directories. We extract periodic experience metrics capturing the previous activities of developers on source files and investigate the explanatory effect of these metrics on defects. We also use activity-based (churn) metrics to observe the performance of both metric types on defect prediction. We used two large-scale open source projects, Lucene and Jackrabbit, for model evaluation. We calculate periodic developer experience metrics and churn metrics at two granularity levels: File level and commit level. We build the models using five popular machine learning algorithms in defect prediction literature. The models with the two best performing algorithms are assessed in terms of Precision, Recall, False Positive Rate, and F-measure. The set of metrics that explains software defects the best is also identified using correlation-based feature selection method. Results show that periodic developer experience metrics extracted at file level are good merits for defect prediction, accompanied with churn. When there is not enough data to extract the contextual knowledge of developers on source files, churn metrics play an important role on defect prediction. © 2018 IEEE.",Churn metrics | Code ownership | Periodic developer experience | Software defect prediction,"Proceedings - 18th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2018",2018-11-09,Conference Paper,"Ozcan Kini, Seldag;Tosun, Ayse",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85069183538,10.1109/ICRITO.2018.8748788,A Hybrid Nonlinear Manifold Detection Approach for Software Defect Prediction,"Rapid development of software technology has influence on substantial industrial growth. Wide application of software in business related matters leads to development of reliable and defect free software system which is a challenging task. It requires development of effective techniques for prediction of software defects at early stage. For complexities in manual prediction of defects, automated techniques have come into effect. They are basically based on learning of pattern from earlier versions of software development and finding out the defects from the current version. Considerable impact of these techniques on industrial growth by predicting defects in software system attracted researchers in this field.In-spite of many studies performed by applying these techniques, desirable performance level and accurate defect prediction still remains a challenging task. For solving this problem, a hybrid technique based on Nonlinear Manifold Detection Techniques (Nonlinear MDTs) and machine learning for prediction of defects has been proposed in this paper. A new hybrid Nonlinear Manifold Detection (Nonlinear MD) Model has been applied for selecting and optimizing the features of software datasets that have been processed using Decision Tree (DT) and Random Forest (RF) classifications. Finally, a comparison and statistical evaluation of the experimental results obtained using new hybrid Nonlinear MD Model-DT have been made by Friedman test followed by Wilcoxon Sign rank test. The statistical outcome revealed that the proposed new hybrid Nonlinear MD Model-DT classification is better result oriented and more accurate in software defect prediction. © 2018 IEEE.",Decision Tree | Dimensionality Reduction | Feature Optimization | Friedman test | Nonlinear Manifold Detection | Software Defect Prediction | Wilcoxon sign rank test,"2018 7th International Conference on Reliability, Infocom Technologies and Optimization: Trends and Future Directions, ICRITO 2018",2018-08-01,Conference Paper,"Ghosh, Soumi;Rana, Ajay;Kansal, Vineet",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85051165828,10.1145/3194104.3194112,Evaluating the adaptive selection of classifiers for cross-project bug prediction,"Bug prediction models are used to locate source code elements more likely to be defective. One of the key factors influencing their performances is related to the selection of a machine learning method (a.k.a., classifier) to use when discriminating buggy and non-buggy classes. Given the high complementarity of stand-alone classifiers, a recent trend is the definition of ensemble techniques, which try to effectively combine the predictions of different stand-alone machine learners. In a recent work we proposed ASCI, a technique that dynamically selects the right classifier to use based on the characteristics of the class on which the prediction has to be done. We tested it in a within-project scenario, showing its higher accuracy with respect to the Validation and Voting strategy. In this paper, we continue on the line of research, by (i) evaluating ASCI in a global and local cross-project setting and (ii) comparing its performances with those achieved by a stand-alone and an ensemble baselines, namely Naive Bayes and Validation and Voting, respectively. A key finding of our study shows that ASCI is able to perform better than the other techniques in the context of cross-project bug prediction. Moreover, despite local learning is not able to improve the performances of the corresponding models in most cases, it is able to improve the robustness of the models relying on ASCI. © 2018 ACM.",bug prediction | cross-project | ensemble classifiers,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Di Nucci, Dario;Palomba, Fabio;De Lucia, Andrea",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85055506371,10.1145/3220267.3220286,Using SMOTE and heterogeneous stacking in ensemble learning for software defect prediction,"Nowadays, there are a lot of classifications models used for predictions in the software engineering field such as effort estimation and defect prediction. One of these models is the ensemble learning machine that improves model performance by combining multiple models in different ways to get a more powerful model. One of the problems facing the prediction model is the misclassification of the minority samples. This problem mainly appears in the case of defect prediction. Our aim is the classification of defects which are considered minority samples during the training phase. This can be improved by implementing the Synthetic Minority Over-Sampling Technique (SMOTE) before the implementation of the ensemble model which leads to over-sample the minority class instances. In this paper, our work propose applying a new ensemble model by combining the SMOTE technique with the heterogeneous stacking ensemble to get the most benefit and performance in training a dataset that focus on the minority subset as in the software prediction study. Our proposed model shows better performance that overcomes other techniques results applied on the minority samples of the defect prediction. © 2018 Association for Computing Machinery.",Classification | Defect Prediction | Ensemble | Heterogeneous | Machine Learning | SMOTE | Software Engineering | Stacking,ACM International Conference Proceeding Series,2018-05-02,Conference Paper,"El-Shorbagy, Sara Adel;El-Gammal, Wael Mohamed;Abdelmoez, Walid M.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85047421055,10.1504/IJCAT.2018.095772,A deep learning approach to software evolution,"Software evolution techniques should be made as important as software development techniques. One possible way to help with the situation is to learn from software development, along with learning from software evolution techniques. The breakout of Machine Learning and Deep Learning (ML&DL) is becoming popular in technology and should be studied for being made available for servicing software evolution. Open source projects provide an open defect repository to which users and developers can report bugs. It is a challenge to document bug reports to the appropriate developers. In this paper, we apply deep learning approaches and a topic model to learn the features of defect reports and then make recommendations. Compared to the traditional machine learning approaches, the proposed approach based on deep learning can perform better in accuracy and assign defect reports to developers more effectively and correctly along with the dataset increasing. © 2018 Inderscience Enterprises Ltd.",Class imbalance | Cost-sensitive | Decision forest | Knowledge discovery | Software defect prediction,Information Sciences,2018-08-01,Article,"Siers, Michael J.;Islam, Md Zahidul",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85053767233,10.5277/e-Inf180108,Applying machine learning to software fault prediction,"Introduction: Software engineering continuously su ers from inadequate software testing. The automated prediction of possibly faulty fragments of source code allows developers to focus development e orts on fault-prone fragments first. Fault prediction has been a topic of many studies concentrating on C/C++ and Java programs, with little focus on such programming languages as Python. Objectives: In this study the authors want to verify whether the type of approach used in former fault prediction studies can be applied to Python. More precisely, the primary objective is conducting preliminary research using simple methods that would support (or contradict) the expectation that predicting faults in Python programs is also feasible. The secondary objective is establishing grounds for more thorough future research and publications, provided promising results are obtained during the preliminary research. Methods: It has been demonstrated [1] that using machine learning techniques, it is possible to predict faults for C/C++ and Java projects with recall 0.71 and false positive rate 0.25. A similar approach was applied in order to find out if promising results can be obtained for Python projects. The working hypothesis is that choosing Python as a programming language does not significantly alter those results. A preliminary study is conducted and a basic machine learning technique is applied to a few sample Python projects. If these e orts succeed, it will indicate that the selected approach is worth pursuing as it is possible to obtain for Python results similar to the ones obtained for C/C++ and Java. However, if these e orts fail, it will indicate that the selected approach was not appropriate for the selected group of Python projects. Results: The research demonstrates experimental evidence that fault-prediction methods similar to those developed for C/C++ and Java programs can be successfully applied to Python programs, achieving recall up to 0.64 with false positive rate 0.23 (mean recall 0.53 with false positive rate 0.24). This indicates that more thorough research in this area is worth conducting. Conclusion: Having obtained promising results using this simple approach, the authors conclude that the research on predicting faults in Python programs using machine learning techniques is worth conducting, natural ways to enhance the future research being: using more sophisticated machine learning techniques, using additional Python-specific features and extended data sets. © 2018 Politechnika Wroclawska. All Rights Reserved.",Classifier | Fault prediction | Machine learning | Metric | Naïve Bayes | Python | Quality | Software intelligence,E-Informatica Software Engineering Journal,2018-01-01,Article,"Wójcicki, Bartłomiej;Dąbrowski, Robert",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85031942342,10.1109/COMPSAC.2017.53,An Empirical Analysis on Effective Fault Prediction Model Developed Using Ensemble Methods,"Software fault prediction models are employed to optimize testing resource allocation by identifying fault-prone classes before testing phases. We apply three different ensemble methods to develop a model for predicting fault proneness. We propose a framework to validate the source code metrics and select the right set of metrics with the objective to improve the performance of the fault prediction model. The fault prediction models are then validated using a cost evaluation framework. We conduct a series of experiments on 45 open source project dataset. Key conclusions from our experiments are: (1) Majority Voting Ensemble (MVE) methods outperformed other methods (2) selected set of source code metrics using the suggested source code metrics using validation framework as the input achieves better results compared to all other metrics (3) fault prediction method is effective for software projects with a percentage of faulty classes lower than the threshold value (low-54.82%, medium-41.04%, high-28.10%). © 2017 IEEE.",Ensemble Methods | Machine Learning | Predictive Modeling | Software Fault Prediction | Source Code Metrics,Proceedings - International Computer Software and Applications Conference,2017-09-07,Conference Paper,"Kumar, Lov;Rath, Santanu;Sureka, Ashish",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85076026855,10.1109/ICIINFS.2016.8262999,Ensemble methods for the prediction of number of faults: A study on eclipse project,"Software fault prediction using different machine learning and statistical techniques has been reported by various researchers. However, different techniques produced different results for software fault prediction and thus showed the performance bottleneck of single techniques. Moreover, most of the researchers focused on predicting software modules being faulty and non-faulty, i.e., binary class prediction of faults. On the other hand, in recent year, some researchers showed that ensemble methods produced the improved performance for software fault prediction compared to single fault prediction techniques. Motivated by this reason, we perform an empirical study of different homogeneous ensemble methods for the prediction of number of faults. The study includes bagging, boosting, random subspace, rotation forest, and stacking ensemble methods and uses three different techniques, linear regression, multilayer perceptron, and decision tree regression as the base learners for the ensemble. The experiments are performed for three different fault datasets corresponding to the Eclipse project. To our knowledge, very few works on the prediction of number of faults using Eclipse dataset have been reported. Results indicated that overall ensemble methods produced better performance than using a single fault prediction technique. Out of five ensemble methods, random subspace outperformed other ensemble methods. Rotation forest, bagging, and boosting performed moderately. Stacking performed relatively poor compared to other ensemble methods. © 2016 IEEE.",Cost matrix | Error bound model | Evolutionary algorithm | Extreme learning machine | Imbalanced learning,ISA Transactions,2020-05-01,Article,"Li, Hui;Yang, Xi;Li, Yang;Hao, Li Ying;Zhang, Tian Lun",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85123893673,10.1007/978-81-322-2755-7_10,Application of machine learning on process metrics for defect prediction in mobile application,"This paper studied process metrics in detail for predicting defects in an open source mobile applications in continuation with our previous study (Moser et al. Software Engineering, 2008). Advanced modeling techniques have been applied on a vast dataset of mobile applications for proving that process metrics are better predictor of defects than code metrics for mobile applications. Mean absolute error, Correlation Coefficient and root mean squared error are determined using different machine learning techniques. In each case it was concluded that process metrics as predictors are significantly better than code metrics as predictors for bug prediction. It is shown that process metrics based defect prediction models are better for mobile applications in all regression based techniques, machine learning techniques and neuro-fuzzy modelling. Therefore separate model has been created based on only process metrics with large dataset of mobile application. © Springer India 2016.",Autonomous aerial vehicles | Autonomous flights | Failure classification | Failure detection | Failure prediction | Long short-term memory,ISA Transactions,2022-10-01,Article,"Ahmad, Muhammad Waqas;Akram, Muhammad Usman;Ahmad, Rashid;Hameed, Khurram;Hassan, Ali",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84960864852,,Improving software quality based on relationship among the change proneness and object oriented metrics,"With the demand of increasing functionality and arrival of defects, software goes through a lot of changes therefore its quite challenging task to maintain the quality of the software. In this paper we developed models to predict the change proneness of the classes in the object oriented system by analyzing the relationship between the object oriented metrics and change proneness. The model proposed is also validated by object oriented open source software. We have analyzed our results by the Receiver Operator Characteristics Curve. The results thus obtained shows that there is a significance relationship between the object oriented metrics and change proneness. We have analyzed statistical as well as machine learning techniques and the results shows that machine learning techniques are the good predictors of the change proneness. Rigorous testing of these change prone classes may improve the quality of the software and it may also reduce our work at the maintenance phase. © 2015 IEEE.",Change prediction; Empirical validation; Machine learning; Object Oriented; Receiver operating characteristics; Statistical methods,"2015 International Conference on Computing for Sustainable Global Development, INDIACom 2015",2015,,"Tripathi A., Sharma K.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84856686810,10.4018/978-1-61520-809-8.ch006,Genetic programming for cross-release fault count predictions in large and complex software projects,"Software fault prediction can play an important role in ensuring software quality through efficient resource allocation. This could, in turn, reduce the potentially high consequential costs due to faults. Predicting faults might be even more important with the emergence of short-timed and multiple software releases aimed at quick delivery of functionality. Previous research in software fault prediction has indicated that there is a need i) to improve the validity of results by having comparisons among number of data sets from a variety of software, ii) to use appropriate model evaluation measures and iii) to use statistical testing procedures. Moreover, cross-release prediction of faults has not yet achieved sufficient attention in the literature. In an attempt to address these concerns, this paper compares the quantitative and qualitative attributes of 7 traditional and machine-learning techniques for modeling the cross-release prediction of fault count data. The comparison is done using extensive data sets gathered from a total of 7 multi-release open-source and industrial software projects. These software projects together have several years of development and are from diverse application areas, ranging from a web browser to a robotic controller software. Our quantitative analysis suggests that genetic programming (GP) tends to have better consistency in terms of goodness of fit and accuracy across majority of data sets. It also has comparatively less model bias. Qualitatively, ease of configuration and complexity are less strong points for GP even though it shows generality and gives transparent models. Artificial neural networks did not perform as well as expected while linear regression gave average predictions in terms of goodness of fit and accuracy. Support vector machine regression and traditional software reliability growth models performed below average on most of the quantitative evaluation criteria while remained on average for most of the qualitative measures. © 2010, IGI Global.",,Evolutionary Computation and Optimization Algorithms in Software Engineering: Applications and Techniques,2010-12-01,Book Chapter,"Afzal, Wasif;Torkar, Richard;Feldt, Robert;Gorschek, Tony",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85116803611,10.1109/ICEIE.2010.5559753,A density based clustering approach for early detection of fault prone modules,"Quality of a software component can be measured in terms of fault proneness of data. Quality estimations are made using fault proneness data available from previously developed similar type of projects and the training data consisting of software measurements. To predict faultproneness of modules different techniques have been proposed which includes statistical methods, machine learning techniques, neural network techniques and clustering techniques. The aim of proposed approach is to investigate that whether metrics available in the early lifecycle (i.e. requirement metrics), metrics available in the late lifecycle (i.e. code metrics) and metrics available in the early lifecycle (i.e. requirement metrics) combined with metrics available in the late lifecycle (i.e. code metrics) can be used to identify fault prone modules by using Density Based Clustering technique. This approach has been tested with real time defect datasets of NASA software projects named as PC1. Predicting faults early in the software life cycle can be used to achieve high software quality. The results show that the fusion of requirement and code metric is the best prediction model for detecting the faults as compared with mostly used code based model. © 2010 IEEE.",Fault management | Fault prediction | Machine learning | Sensor health packets | WBAN,Journal of King Saud University - Computer and Information Sciences,2022-10-01,Article,"Awad, Mamoun;Sallabi, Farag;Shuaib, Khaled;Naeem, Faisal",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85085297137,10.1007/s11390-020-0323-7,Predicting Code Smells and Analysis of Predictions: Using Machine Learning Techniques and Software Metrics,"Code smell detection is essential to improve software quality, enhancing software maintainability, and decrease the risk of faults and failures in the software system. In this paper, we proposed a code smell prediction approach based on machine learning techniques and software metrics. The local interpretable model-agnostic explanations (LIME) algorithm was further used to explain the machine learning model’s predictions and interpretability. The datasets obtained from Fontana et al. were reformed and used to build binary-label and multi-label datasets. The results of 10-fold cross-validation show that the performance of tree-based algorithms (mainly Random Forest) is higher compared with kernel-based and network-based algorithms. The genetic algorithm based feature selection methods enhance the accuracy of these machine learning algorithms by selecting the most relevant features in each dataset. Moreover, the parameter optimization techniques based on the grid search algorithm significantly enhance the accuracy of all these algorithms. Finally, machine learning techniques have high potential in predicting the code smells, which contribute to detect these smells and enhance the software’s quality. © 2020, Institute of Computing Technology, Chinese Academy of Sciences.",data management | data model | Digital Twin | machine learning | Metal Additive Manufacturing | product lifecycle management,Journal of Manufacturing Systems,2022-01-01,Article,"Liu, Chao;Le Roux, Léopold;Körner, Carolin;Tabaste, Olivier;Lacan, Franck;Bigot, Samuel",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84955275883,10.1016/j.asoc.2020.106686,Empirical comparison and evaluation of Artificial Immune Systems in inter-release software fault prediction,"Artificial immune systems are bio-inspired machine learning algorithms based on the mammalian immune paradigms. One of the possible uses of these methods is Software Fault Prediction, which consists of classifying the modules of an application as being fault-prone or not, thus allowing a developer to better target the modules during the test phase leading to a high-quality software with lower cost. Despite the high number of works in the field, only five studies included Artificial Immune Systems in their approaches and exclusively focused on the intra-project fault prediction scheme. In this study, our objective is to appraise 8 immunological systems on the rarely treated inter-project software defect prediction scenario over three different benchmarks, hence, we selected 41 datasets corresponding to 11 java projects from the PROMISE data repository. According to the Friedman and Nemenyi Post-hoc test results, none of the performance of the studied algorithms was better than Immunos-1 and Immunos-99 in terms of the Recall measure. Furthermore, the outcomes of the Wilcoxon test suggest that the researches addressing the intra-projects defect prediction problems should also evaluate their models on inter-release scenarios. © 2020 Elsevier B.V.",Ability | Benevolence | Integrity | Security | Sociopsychological trust model | Wireless Sensor Network,Journal of Network and Computer Applications,2016-02-01,Article,"Rathore, Heena;Badarla, Venkataramana;J, George K.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85083420285,10.1016/j.jss.2020.110585,Automated defect identification via path analysis-based features with transfer learning,"Recently, artificial intelligence techniques have been widely applied to address various specialized tasks in software engineering, such as code generation, defect identification, and bug repair. Despite the diffuse usage of static analysis tools in automatically detecting potential software defects, developers consider the large number of reported alarms and the expensive cost of manual inspection to be a key barrier to using them in practice. To automate the process of defect identification, researchers utilize machine learning algorithms with a set of hand-engineered features to build classification models for identifying alarms as actionable or unactionable. However, traditional features often fail to represent the deep syntactic structure of alarms. To bridge the gap between programs’ syntactic structure and defect identification features, this paper first extracts a set of novel fine-grained features at variable-level, called path-variable characteristic, by applying path analysis techniques in the feature extraction process. We then raise a two-stage transfer learning approach based on our proposed features, called feature ranking-matching based transfer learning, to increase the performance of cross-project defect identification. Our experimental results for eight open-source projects show that the proposed features at variable-level are promising and can yield significant improvement on both within-project and cross-project defect identification. © 2020 The Author(s)",Automated defect identification | Machine learning | Model evaluation | Path analysis | Transfer learning,Journal of Systems and Software,2020-08-01,Article,"Zhang, Yuwei;Jin, Dahai;Xing, Ying;Gong, Yunzhan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85091007061,10.32604/cmc.2020.011001,Software defect prediction based on stacked contractive autoencoder and multi-objective optimization,"Software defect prediction plays an important role in software quality assurance. However, the performance of the prediction model is susceptible to the irrelevant and redundant features. In addition, previous studies mostly regard software defect prediction as a single objective optimization problem, and multi-objective software defect prediction has not been thoroughly investigated. For the above two reasons, we propose the following solutions in this paper: (1) we leverage an advanced deep neural network-Stacked Contractive AutoEncoder (SCAE) to extract the robust deep semantic features from the original defect features, which has stronger discrimination capacity for different classes (defective or non-defective). (2) we propose a novel multi-objective defect prediction model named SMONGE that utilizes the Multi-Objective NSGAII algorithm to optimize the advanced neural network-Extreme learning machine (ELM) based on state-of-the-art Pareto optimal solutions according to the features extracted by SCAE. We mainly consider two objectives. One objective is to maximize the performance of ELM, which refers to the benefit of the SMONGE model. Another objective is to minimize the output weight norm of ELM, which is related to the cost of the SMONGE model. We compare the SCAE with six state-of-the-art feature extraction methods and compare the SMONGE model with multiple baseline models that contain four classic defect predictors and the MONGE model without SCAE across 20 open source software projects. The experimental results verify that the superiority of SCAE and SMONGE on seven evaluation metrics. © 2020 Tech Science Press. All rights reserved.",Deep neural network | Extreme learning machine | Multi-objective optimization | Software defect prediction | Stacked contractive autoencoder,"Computers, Materials and Continua",2020-07-23,Article,"Zhang, Nana;Zhu, Kun;Ying, Shi;Wang, Xu",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85091410492,,Software Defect Prediction using Convolutional Neural Network,"The crucial part in software development lifecycle is finding the software faults. Detecting the faults in an early stage of software lifecycle can prevent the susceptibility and cost overruns. Many machine learning algorithms have been adopted for predicting the error-prone of software system such as Support Vector Machine (SVM), Bayesian Belief Network, Naïve Bayes, and Genetic Programming. In this paper, the Convolution Neural Network (CNN) is used to detect the defective modules in software system. This work used the static code metrics for a collection of software modules in five selective NASA datasets. The experimental results show that CNN was promising for defect prediction with an average accuracy of 70.2%. © 2020 IEICE.",Convolution Neural Network; Deep learning Introduction; Machine learning; Software defect prediction; Software fault; Software reliability,"ITC-CSCC 2020 - 35th International Technical Conference on Circuits/Systems, Computers and Communications",2020,,"Wongpheng K., Visutsak P.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84863612007,10.1109/ICRITO48877.2020.9197999,A Study on Software Defect Prediction using Feature Extraction Techniques,"Identification and elimination of defects in software is time and resource-consuming activity. The maintenance of a defective software system is burdensome. Software defect prediction (SDP) at an early stage of the Software Development Life Cycle (SDLC) results in quality software and reduces its development cost. In this study, a comparison is performed on nine open-source softwaresystems written in Java from PROMISE Repository using four mostly used feature extraction techniques such as Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), Kernel-based Principal Component Analysis (K-PCA) and Autoencoders with Support Vector Machine (SVM) as base machine learning classifier. The model validation is performed using a ten-fold cross-validation method and the efficiency of the model is evaluated using accuracy and ROCAUC. The results of this study indicate that Autoencoders is an effective method to reduce the dimensions of a software defect dataset successfully. © 2020 IEEE.",Bug assignment | Bug tossing | Empirical studies | Machine learning,Journal of Systems and Software,2012-10-01,Article,"Bhattacharya, Pamela;Neamtiu, Iulian;Shelton, Christian R.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84967329239,10.1109/ICIEM48762.2020.9160152,Novel XGBoost Tuned Machine Learning Model for Software Bug Prediction,"As internet users grow, the quantity of data available on the web increases with it. Virtually everything that needs human effort or human presence can be replaced by the Software. While developing an application it follows the Software Development Lifecycle (SDLC). Within the early stages of development, it's a compulsory task to take care of system or bugs to avoid wasting time and effort during initial development phase to forestall any runtime crisis. In this paper, we used the machine learning models - Logistic regression, Decision Tree, Random Forest, Adaboost and XGBoost as state-of-art models for four datasets of NASA-KC2, PC3, JM1, CM1. Later on, new model was proposed based on tuning the existing XGBoost model by changing its parameter namely N-estimator, learning rate, max depth, and subsample. The results achieved were compared with state-of-art models and our model outperformed them for all datasets. The authors believe that this research will contribute in correctly detecting the bugs with machine learning approach. © 2020 IEEE.",Impact analysis | Machine learning | Model recommendation | Software process model | Software project management,Journal of Systems and Software,2016-08-01,Article,"Song, Qinbao;Zhu, Xiaoyan;Wang, Guangtao;Sun, Heli;Jiang, He;Xue, Chenhao;Xu, Baowen;Song, Wei",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85042747168,10.1109/ITNEC48623.2020.9084745,Software Defect Prediction via Transformer,"In order to enhance software reliability, software defect prediction is used to predict potential defects and to improve efficiency of software examination. Traditional defect prediction methods mainly focus on design static code metrics, and building machine learning classifiers to predict pieces of code that potentially defective. However, these manual extracted features do not contain syntactic and semantic information of programs. These information is much more important than those metrics and can improve the accuracy of defect prediction. In this paper, we propose a framework called software defect prediction via transformer (DP-Transformer) which capture syntactic and semantic features from programs and use them to improve defect prediction. Specifically, we first parse source code into ASTs and then select representative nodes from ASTs to form token vectors. Then we employ mapping and word embedding to convert token vectors into numerical vectors and send the numerical vectors to transformer. Transformer will automatically extract syntactic and semantic features and eventually feed these features into a Logistic Regression classifier. We evaluate our method on seven open-source Java projects with certain labels and take F-measure as evaluation criteria. The experimental results show that averagely, the proposed DP-Transformer improves the state-of-art method by 8%. © 2020 IEEE.",Anomaly detection | Diagnosis | Fault injection | Machine learning | SLA | System monitoring | Virtualization,Journal of Systems and Software,2018-05-01,Article,"Sauvanaud, Carla;Kaâniche, Mohamed;Kanoun, Karama;Lazri, Kahina;Da Silva Silvestre, Guthemberg",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85074980613,10.1109/UBMK.2019.8907149,Software Defect Estimation Using Machine Learning Algorithms,"Software Engineering is a comprehensive domain since it requires a tight communication between system stakeholders and delivering the system to be developed within a determinate time and a limited budget. Delivering the customer requirements include procuring high performance by minimizing the system. Thanks to effective prediction of system defects on the front line of the project life cycle, the project's resources and the effort or the software developers can be allocated more efficiently for system development and quality assurance activities. The main aim of this paper is to evaluate the capability of machine learning algorithms in software defect prediction and find the best category while comparing seven machine learning algorithms within the context of four NASA datasets obtained from public PROMISE repository [12]. All in all, the results of ensemble learners category consisting of Random Forests (RF) and Bagging in defect prediction is pretty much its counterparts. © 2019 IEEE.",Common vulnerabilities and exposures | Dataset construction | Security bug report prediction | Voting classification,Journal of Systems and Software,2020-02-01,Article,"Wu, Xiaoxue;Zheng, Wei;Chen, Xiang;Wang, Fang;Mu, Dejun",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85076597363,10.1007/s00500-019-04047-7,A novel machine learning approach for software reliability growth modelling with pareto distribution function,"Software reliability is the important quantifiable attribute in gaining reliability by assessing faults at the time of testing in the software products. Time-based software reliability models used to identify the defects in the product, and it is not suitable for dynamic situations. Instead of time, test effect is used in few explorations through effort function and it is not realistic for infinite testing time. Identifying number of defects is essential in software reliability models, and this research work presents a Pareto distribution (PD) to predict the fault distribution of software under homogenous and nonhomogeneous conditions along with artificial neural network (ANN). This methodology enables the parallel evolution of a product through NN models which exhibit estimated Pareto optimality with respect to multiple error measures. The proposed PD-ANN-based SRGM describes types of failure data and also improves the accuracy of parameter estimation more than existing growth models such as homogeneous poison process and two fuzzy time series-based software reliability models. Experimental evidence is presented for general application and the proposed framework by generating solutions for different product and developer indexes. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.",Cloud computing | Data analytics | Failure prediction | Machine learning | Multi-tier distributed systems | Self-healing systems,Journal of Systems and Software,2020-03-01,Article,"Mariani, Leonardo;Pezzè, Mauro;Riganelli, Oliviero;Xin, Rui",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85074957138,10.1109/IEMECONX.2019.8877082,An empirical framework for code smell prediction using extreme learning machine∗,"The software containing code smells indicates the violation of standard design and coding practices by developer during the development of the software system. Recent empirical studies observed that classes having code smells have higher probability of change proneness or fault proneness with respect to classes having no code smells [1]. The effort of removing bugs due to code smells increases exponentially if the smells are not identified during the earlier phases of software development. The code smell prediction using source code metrics can be used in starting phases of software development life cycle to reduce the maintenance and testing effort of software and also help in improving the quality of the software. The work in this paper empirically investigates and evaluates different classification techniques, feature selection techniques, and data sampling techniques to handle imbalance data in predicting 7 different types of code smell. The conclusion of this research is assessed over 629 application packages. The experimental finding confirms the estimating capability of different classifiers, feature selection, and data imbalance techniques for developing code smell prediction models. Our analysis also reveals that the models developed using one technique are superior than the models developed using other techniques. © 2019 IEEE.",Code Smell | Feature selection | Machine Learning | Software Engineering | Source Code Metrics,"IEMECON 2019 - 9th Annual Information Technology, Electromechanical Engineering and Microelectronics Conference",2019-03-01,Conference Paper,"Gupta, Himanshu;Kumar, Lov;Neti, Lalita Bhanu Murthy",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85081030171,10.4018/IJOSSP.2019100101,Software fault prediction using deep learning algorithms,"Software faults prediction (SFP) processes can be used for detecting faulty constructs at early stages of the development lifecycle, in addition to its being used in several phases of the development process. Machine learning (ML) is widely used in this area. One of the most promising subsets from ML is deep learning that achieves remarkable performance in various areas. Two deep learning algorithms are used in this paper, the Multi-layer perceptrons (MLPs) and Convolutional Neural Network (CNN). In order to evaluate the studied algorithms, four commonly used datasets from NASA are used i.e. (PC1, KC1, KC2 and CM1). The experiment results show how the CNN algorithm achieves prediction superiority of the MLP algorithm. The accuracy and detection rate measurements when using CNN has reached the standard ratio respectively as follows: PC1 97.7% - 73.9%, KC1 100% - 100%, KC2 99.3% - 99.2% and CM1 97.3% - 82.3%. This study provides promising results in using the deep learning for software fault prediction research. Copyright © 2019, IGI Global.",Data cleaning | Feature engineering testing | Implementation testing | Machine learning | Model testing,Journal of Systems and Software,2020-06-01,Article,"Braiek, Houssem Ben;Khomh, Foutse",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85071757084,10.12785/ijcds/080503,Predicting software faults based on k-nearest neighbors classification,"Software defect prediction is one of the most important task during the development of software systems in order to save developers’ time and effort. Discovering defects in an early stage of software development will allow programmers and developers to take action and resolve these faulty parts in software before its launch. In this paper, the K- Nearest Neighbor (KNN) machine learning algorithm is used to predict faulty software projects. Experimental studies are conducted on five public datasets with different similarly measures. Results showed that KNN can be used to predict software faults with accuracy rate that can achieve up to 87.2%. © 2019 University of Bahrain. All rights reserved.",Halstead | KNN | Mccabe | Software Defect Prediction | Software Engineering | Software Faults | Software Metrics,International Journal of Computing and Digital Systems,2019-01-01,Article,"Hammad, Mustafa;Alqaddoumi, Abdulla;Al-Obaidy, Hadeel;Almseidein, Khalil",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85058400878,10.1109/EDCC.2018.00014,Exploratory Study of Machine Learning Techniques for Supporting Failure Prediction,"The growing complexity of software makes it difficult or even impossible to detect all faults before deployment, and such residual faults eventually lead to failures at runtime. Online Failure Prediction (OFP) is a technique that attempts to avoid or mitigate such failures by predicting their occurrence based on the analysis of past data and the current state of a system. Given recent technological developments, Machine Learning (ML) algorithms have shown their ability to adapt and extract knowledge in a variety of complex problems, and thus have been used for OFP. Still, they are highly dependent on the problem at hand, and their performance can be influenced by different factors. The problem with most works using ML for OFP is that they focus only on a small set of prediction algorithms and techniques, although there is no comprehensive study to support their choice. In this paper, we present an exploratory analysis of various ML algorithms and techniques on a dataset containing failure data. The results show that, for the same data, different algorithms and techniques directly influence the prediction performance and thus should be carefully selected. © 2018 IEEE.",Classification | Dependability | Failure Prediction | Machine Learning,"Proceedings - 2018 14th European Dependable Computing Conference, EDCC 2018",2018-11-09,Conference Paper,"Campos, João R.;Vieira, Marco;Costa, Ernesto",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85052656894,10.1016/j.compeleceng.2018.08.017,An extensive analysis of search-based techniques for predicting defective classes,"In spite of constant planning, effective documentation and proper implementation of a software during its life cycle, many defects still occur. Various empirical studies have found that prediction models developed using software metrics can be used to predict these defects. Researchers have advocated the use of search-based techniques and their hybridized versions in literature for developing software quality prediction models. This study conducts an extensive comparison of 20 search-based techniques, 16 hybridized techniques and 17 machine-learning techniques amongst each other, to develop software defect prediction models using 17 data sets. The comparison framework used in the study is efficient as it (i) deals with the stochastic nature of the techniques (ii) provides a fair comparison amongst the techniques (iii) promotes repeatability of the study and (iv) statistically validates the results. The results of the study indicate promising ability of search-based techniques and their hybridized versions for predicting defective classes. © 2018 Elsevier Ltd",Defect prediction | Empirical validation | Hybridized techniques | Object-oriented metrics | Search-based techniques,Computers and Electrical Engineering,2018-10-01,Article,"Malhotra, Ruchika",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85040967773,10.1109/TCAD.2017.2706558,Online soft-error vulnerability estimation for memory arrays and logic cores,"Radiation-induced soft errors are a major reliability concern in circuits fabricated at advanced technology nodes. Online soft-error vulnerability estimation offers the flexibility of exploiting dynamic fault-tolerant mechanisms for cost-effective reliability enhancement. We propose a generic run-time method with low area and power overhead to predict the soft-error vulnerability of on-chip memory arrays as well as logic cores. The vulnerability prediction is based on signal probabilities (SPs) of a small set of flip-flops, chosen at design time, by studying the correlation between the soft-error vulnerability and the flip-flop SPs for representative workloads. We exploit machine learning to develop a predictive model that can be deployed in the system in software form. Simulation results on two processor designs show that the proposed technique can accurately estimate the soft-error vulnerability of on-chip logic core, such as sequential pipeline logic and functional units as well as memory arrays that constitute the instruction cache, the data cache, and the register file. © 2017 IEEE.",Architectural vulnerability factor | gradient boosting | machine learning | signal probabilities | soft error | static aging | support vector machines,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,2018-02-01,Article,"Vijayan, Arunkumar;Kiamehr, Saman;Ebrahimi, Mojtaba;Chakrabarty, Krishnendu;Tahoori, Mehdi B.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85062695056,10.2991/ijcis.11.1.92,A three-stage based ensemble learning for improved software fault prediction: An empirical comparative study,"Software Fault Prediction (SFP) research has made enormous endeavor to accurately predict fault proneness of software modules, thus maximize precious software test resources, reduce maintenance cost and contributes to produce quality software products. In this regard, Machine Learning (ML) has been successfully applied to solve classification problems for SFP. However, SFP has many challenges that are created due to redundant and irrelevant features, class imbalance problem and the presence of noise in software defect datasets. Yet, neither of ML techniques alone handles those challenges and those may deteriorate the performance depending on the predictor’s sensitiveness to data corruptions. In the literature, it is widely claimed that building ensemble classifiers from preprocessed datasets and combining their predictions is an interesting method of overcoming the individual problems produced by each classifier. This statement is usually not supported by thorough empirical studies considering problems in combined implementation with resolving different types of challenges in defect datasets and, therefore, it must be carefully studied. Thus, the objective of this paper is to conduct large scale comprehensive experiments to study the effect of resolving those challenges in SFP in three stages in order to improve the practice and performance of SFP. In addition to that, the paper presents a thorough and statistically sound comparison of these techniques in each stage. Accordingly, a new three-stage based ensemble learning framework that efficiently handles those challenges in a combined form is proposed. The experimental results confirm that the proposed framework has exhibited the robustness of combined techniques in each stage. Particularly high performance results have achieved using combined ELA on selected features of balanced data after removing noise instances. Therefore, as shown in this study, ensemble techniques used for SFP must be carefully examined and combined with techniques to resolve those challenges and obtain robust performance so as to accurately identify the fault prone software modules. © 2018, the Authors.",Data balancing | Ensemble learning algorithms | Feature selection | Noise filtering | Software fault prediction | Software testing,International Journal of Computational Intelligence Systems,2018-01-01,Article,"Yohannese, Chubato Wondaferaw;Li, Tianrui;Bashir, Kamal",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85116351574,10.18293/SEKE2018-181,Bayesian logistic regression for software defect prediction,"Timely identification of bugs plays an important role in delivering quality software. Defect prediction models help to detect or rank the defect prone files so that the project management team can allocate resources diligently or may seek help from external sources to enable rigorous quality assurance activities on defect prone files. Though defect prediction models have been built using several machine learning algorithms, Bayesian approach of these models is not explored. We propose Bayesian logistic regression with non-informative and informative priors to build defect prediction models. We seek to study if there are any advantages of using Bayesian logistic regression over logistic regression and the role of priors in the performance of Bayesian logistic regression. A comparative study of the performance of Bayesian logistic regression with other widely known classifies is also presented. © 2018 Universitat zu Koln. All rights reserved.",Case study | Fault tolerance | Machine learning | Software architecture | Software engineering,Journal of Systems and Software,2022-01-01,Article,"Myllyaho, Lalli;Raatikainen, Mikko;Männistö, Tomi;Nurminen, Jukka K.;Mikkonen, Tommi",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84902359961,10.3745/JIPS.04.0077,Analyzing machine learning techniques for fault prediction using web applications,"Web applications are indispensable in the software industry and continuously evolve either meeting a newer criteria and/or including new functionalities. However, despite assuring quality via testing, what hinders a straightforward development is the presence of defects. Several factors contribute to defects and are often minimized at high expense in terms of man-hours. Thus, detection of fault proneness in early phases of software development is important. Therefore, a fault prediction model for identifying fault-prone classes in a web application is highly desired. In this work, we compare 14 machine learning techniques to analyse the relationship between object oriented metrics and fault prediction in web applications. The study is carried out using various releases of Apache Click and Apache Rave datasets. En-route to the predictive analysis, the input basis set for each release is first optimized using filter based correlation feature selection (CFS) method. It is found that the LCOM3, WMC, NPM and DAM metrics are the most significant predictors. The statistical analysis of these metrics also finds good conformity with the CFS evaluation and affirms the role of these metrics in the defect prediction of web applications. The overall predictive ability of different fault prediction models is first ranked using Friedman technique and then statistically compared using Nemenyi post-hoc analysis. The results not only upholds the predictive capability of machine learning models for faulty classes using web applications, but also finds that ensemble algorithms are most appropriate for defect prediction in Apache datasets. Further, we also derive a consensus between the metrics selected by the CFS technique and the statistical analysis of the datasets. © KIPS.",Cost-sensitive learning | Data mining | Defect escalation | Machine learning | Software defect escalation prediction,Knowledge-Based Systems,2014-01-01,Article,"Sheng, Victor S.;Gu, Bin;Fang, Wei;Wu, Jian",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85018955129,10.4018/IJOSSP.2018010101,Combining data preprocessing methods with imputation techniques for software defect prediction,"Software Defect Prediction (SDP) models are used to predict, whether software is clean or buggy using the historical data collected from various software repositories. The data collected from such repositories may contain some missing values. In order to estimate missing values, imputation techniques are used, which utilizes the complete observed values in the dataset. The objective of this study is to identify the best-suited imputation technique for handling missing values in SDP dataset. In addition to identifying the imputation technique, the authors have investigated for the most appropriate combination of imputation technique and data preprocessing method for building SDP model. In this study, four combinations of imputation technique and data preprocessing methods are examined using the improved NASA datasets. These combinations are used along with five different machine-learning algorithms to develop models. The performance of these SDP models are then compared using traditional performance indicators. Experiment results show that among different imputation techniques, linear regression gives the most accurate imputed value. The combination of linear regression with correlation based feature selector outperforms all other combinations. To validate the significance of data preprocessing methods with imputation the findings are applied to open source projects. It was concluded that the result is in consistency with the above conclusion. Copyright © 2018, IGI Global.",Code smell severity | Code smells detection | Machine learning | Ordinal classification | Refactoring prioritization,Knowledge-Based Systems,2017-07-15,Article,"Arcelli Fontana, Francesca;Zanoni, Marco",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85046366514,10.1007/978-3-319-89656-4_35,Software defect prediction from code quality measurements via machine learning,"Improvement in software development practices to predict and reduce software defects can lead to major cost savings. The goal of this study is to demonstrate the value of static analysis metrics in predicting software defects at a much larger scale than previous efforts. The study analyses data collected from more than 500 software applications, across 3 multi-year software development programs, and uses over 150 software static analysis measurements. A number of machine learning techniques such as neural network and random forest are used to determine whether seemingly innocuous rule violations can be used as significant predictors of software defect rates. © Springer International Publishing AG, part of Springer Nature 2018.",Defect prediction | Machine learning | Software defects,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,Conference Paper,"MacDonald, Ross",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85124466041,10.1109/ICACCI.2017.8125957,Tool to handle imbalancing problem in software defect prediction using oversampling methods,"Data imbalancing is becoming a common problem to tackle in different fields like, defect prediction, change prediction, oil spills, medical diagnose etc. Various methods have been developed to handle imbalanced datasets in order to improve accuracy of the prediction models. Many studies have been carried out in the field of defect prediction for imbalanced datasets but most of them uses SMOTE oversampling method to handle the imbalanced data problem. There are many other oversampling methods which help to deal with imbalancing problem and are still unexplored particularly in the field of software defect prediction. This study develops a tool by implementing three of those unexplored oversampling methods namely ADASYN, SPIDER and Safe-Level-SMOTE. Furthermore, we analyze their performance in comparison to traditional method SMOTE. The performance of oversampling methods is evaluated by applying three machine learning techniques for defect prediction using object oriented metrics. The results are evaluated using two open source defect datasets. The result analysis showed that the prediction error decreased and performance of the machine learning techniques improved when balanced datasets were used with respect to three oversampling methods. Further, all of the three methods outperformed SMOTE while SPIDER oversampling method performed best in majority of the cases. © 2017 IEEE.",Attention | Bug triage | Graph neural network | Random walk | Representation learning,Knowledge-Based Systems,2022-04-06,Article,"Wu, Hongrun;Ma, Yutao;Xiang, Zhenglong;Yang, Chen;He, Keqing",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85119193292,10.4018/IJOSSP.2017100102,Software defect prediction using genetic programming and neural networks,"This article describes how classification methods on software defect prediction is widely researched due to the need to increase the software quality and decrease testing efforts. However, findings of past researches done on this issue has not shown any classifier which proves to be superior to the other. Additionally, there is a lack of research that studies the effects and accuracy of genetic programming on software defect prediction. To find solutions for this problem, a comparative software defect prediction experiment between genetic programming and neural networks are performed on four datasets from the NASA Metrics Data repository. Generally, an interesting degree of accuracy is detected, which shows how the metric-based classification is useful. Nevertheless, this article specifies that the application and usage of genetic programming is highly recommended due to the detailed analysis it provides, as well as an important feature in this classification method which allows the viewing of each attributes impact in the dataset. © 2017, IGI Global.",Feature fusion | Multi-scale | Residual encoding/decoding | Skin lesion segmentation | Soft-pool,Medical Image Analysis,2022-01-01,Article,"Dai, Duwei;Dong, Caixia;Xu, Songhua;Yan, Qingsen;Li, Zongfang;Zhang, Chunyan;Luo, Nana",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85092535990,10.1109/ICRITO.2016.7785021,Assessment of defect prediction models using machine learning techniques for object-oriented systems,"Software development is an essential field today. The advancement in software systems leads to risk of them being exposed to defects. It is important to predict the defects well in advance in order to help the researchers and developers to build cost effective and reliable software. Defect prediction models extract information about the software from its past releases and predict the occurrence of defects in future releases. A number of Machine Learning (ML) algorithms proposed and used in the literature to efficiently develop defect prediction models. What is required is the comparison of these ML techniques to quantify the advantage in performance of using a particular technique over another. This study scrutinizes and compares the performances of 17 ML techniques on the selected datasets to find the ML technique which gives the best performance for determining defect prone classes in an Object-Oriented(OO) software. Also, the superiority of the best ML technique is statistically evaluated. The result of this study demonstrates the predictive capability of ML techniques and advocates the use of Bagging as the best ML technique for defect prediction. © 2016 IEEE.",Anomaly intrusion detection | Machine learning model | Multi-layer classification | Network traffic | OpenFlow | Packet data flow | QoS | Software defined networking,Microprocessors and Microsystems,2020-11-01,Article,"Satheesh, N.;Rathnamma, M. V.;Rajeshkumar, G.;Sagar, P. Vidya;Dadheech, Pankaj;Dogiwal, S. R.;Velayutham, Priya;Sengan, Sudhakar",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85021186793,10.1109/ICMLC.2016.7873016,Software fault prediction based on one-class SVM,"Software fault prediction (SFP) is useful for helping the software engineer to locate potential faulty modules in software testing more easily, so that it can save a lot of time and budgets to improve the software quality. In this paper, aiming at solving the problem that the faulty samples are too rare to train a classifier, an one-class SFP model is proposed by using only non-faulty samples based on one-class SVM. The empirical validation is conducted on 6 extremely imbalanced datasets collected from real-world software containing only small amounts of faulty instances. The test results suggest that the proposed model can achieve a reasonable fault prediction performance when using only a small proportion of training samples, and performs much better than conventional and class imbalanced learning based SFP models in terms of G-mean measure. Thus the proposed model provided a considerable solution for SFP with a few faulty modules in early life of software testing. © 2016 IEEE.",Class imbalance learning | Machine learning | One-class SVM | Software fault prediction,Proceedings - International Conference on Machine Learning and Cybernetics,2016-07-02,Conference Paper,"Chen, Lin;Fang, Bin;Shang, Zhaowei",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85044327325,10.1109/ICRITO.2015.7359220,An investigation of the accuracy of code and process metrics for defect prediction of mobile applications,"Mobile applications have been around for a long time and proved to be a new excited market where everyone want to engage themselves. They have become more important than webpages nowadays. Companies are giving more preference to mobile apps as compared to websites because of their user friendliness , better visibility and ease of social networking. This paper compares static code metrics and process metrics for predicting defects in an open source mobile applications. Correlation coefficient, mean absolute error and root mean squared error with process metrics as predictors are significantly better than with code metrics as predictors. Also the combined model based on process and code metrics is better than the model based on code metrics. It is shown that process metrics based defect prediction models are better for mobile applications in all 7 machine learning techniques used for modelling. © 2015 IEEE.",Artificial damages | Convolutional neural network | End-to-end | Intelligent fault diagnosis | Real damages,Neurocomputing,2018-06-14,Article,"Chen, Yuanhang;Peng, Gaoliang;Xie, Chaohao;Zhang, Wei;Li, Chuanhao;Liu, Shaohui",Include,
10.1016/j.infsof.2022.107128,,10.1145/2875913.2875915,Enhancing defect prediction with static defect analysis,"In the software development process, how to develop better software at lower cost has been a major issue of concern. One way that helps is to find more defects as early as possible, on which defect prediction can provide effective guidance. The most popular defect prediction technique is to build defect prediction models based on machine learning. To improve the performance of defect prediction model, selecting appropriate features is critical. On the other hand, static analysis is usually used in defect detection. As static defect analyzers detects defects by matching some well-defined ""defect patterns"", its result is useful for locating defects. However, defect prediction and static defect analysis are supposed to be two parallel areas due to the differences in research motivation, solution and granularity. In this paper, we present a possible approach to improve the performance of defect prediction with the help of static analysis techniques. Specifically, we present to extract features based on defect patterns from static defect analyzers to improve the performance of defect prediction models. Based on this approach, we implemented a defect prediction tool and set up experiments to measure the effect of the features. © 2015 ACM.",Code feature; Defect; Defect pattern; Machine learning; Predictive model; Static defect analyzer,ACM International Conference Proceeding Series,2015,,"Tang H., Lan T., Hao D., Zhang L.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85055479097,10.14257/ijmue.2015.10.8.01,Residual defect prediction using multiple technologies,"Finding defects in a software system is not easy. Effective detection of software defects is an important activity of software development process. In this paper, we propose an approach to predict residual defects, which applies machine learning algorithms (classifiers) and defect distribution model. This approach includes two steps. Firstly, use machine learning Algorithms and Association Rules to get defect classification table, then confirm the defect distribution trend referring to several distribution models. Experiment results on a GUI project show that the approach can effectively improve the accuracy of defect prediction and be used for test planning and implementation. © 2015 SERSC.",abstract syntax trees (ASTs) | Control flow graphs (CFGs) | Convolutional neural networks (CNNs) | Labeled directed graphs,Neural Networks,2018-12-01,Article,"Phan, Anh Viet;Nguyen, Minh Le;Nguyen, Yen Lam Hoang;Bui, Lam Thu",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84921485487,10.1007/978-3-319-10389-1_13,Increasing the accuracy of software fault prediction using majority ranking fuzzy clustering,"Although many machine-learning and statistical techniques have been proposed widely for defining fault prone modules during software fault prediction, but this area have yet to be explored as still there is a room for stable and consistent model with high accuracy. In this paper, a new method is proposed to increase the accuracy of fault prediction based on fuzzy clustering and majority ranking. In the proposed method, the effect of irrelevant and inconsistent modules on fault prediction is decreased by designing a new framework, in which the entire project’s modules are clustered. The obtained results showed that fuzzy clustering could decrease the negative effect of irrelevant modules on accuracy of estimations. We used eight data sets from NASA and Turkish white-goods software to evaluate our results. Performance evaluation in terms of false positive rate, false negative rate, and overall error showed the superiority of our model compared to other predicting strategies. Our proposed majority ranking fuzzy clustering approach showed between 3% to 18% and 1% to 4% improvement in false negative rate and overall error respectively compared to other available proposed models (ACF and ACN) in at least half of the testing cases. The results show that our systems can be used to guide testing effort by prioritizing the module’s faults in order to improve the quality of software development and software testing in a limited time and budget. © Springer International Publishing Switzerland 2015.",False negative rate (FNR) | False positive rate (FPR) | Fuzzy clustering | Software fault prediction,Studies in Computational Intelligence,2015-01-01,Article,"Abaei, Golnoush;Selamat, Ali",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84901913588,10.1049/iet-sen.2013.0008,Empirical study of fault prediction for open-source systems using the chidamber and kemerer metrics,"Software testers are usually provoked with projects that have faults. Predicting a class's fault-proneness is vital for minimising cost and improving the effectiveness of the software testing. Previous research on software metrics has shown strong relationships between software metrics and faults in object-oriented systems using a binary variable. However, these models do not consider the history of faults in classes. In this work, a dependent variable is proposed that uses fault history to rate classes into four categories (none, low risk, medium risk and high risk) and to improve the predictive capability of fault models. The study is conducted on many releases of four open-source systems. The study tests the statistical differences in seven machine learning algorithms to find whether the proposed variable can be used to build better prediction models. The performance of the classifiers using the four categories is significantly better than the binary variable. In addition, the results show improvements on the reliability of the prediction models as the software matures. Therefore the fault history improves the prediction of fault-proneness of classes in open-source systems. © The Institution of Engineering and Technology 2014.",,IET Software,2014-01-01,Article,"Shatnawi, Raed",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84962772563,10.1109/ESEM.2013.45,Constructing defect predictors and communicating the outcomes to practitioners,"Background: An alternative to expert-based decisions is to take data-driven decisions and software analytics is the key enabler for this evidence-based management approach. Defect prediction is one popular application area of software analytics, however with serious challenges to deploy into practice. Goal: We aim at developing and deploying a defect prediction model for guiding practitioners to focus their activities on the most problematic parts of the software and improve the efficiency of the testing process. Method: We present a pilot study, where we developed a defect prediction model and different modes of information representation of the data and the model outcomes, namely: commit hotness ranking, error probability mapping to the source and visualization of interactions among teams through errors. We also share the challenges and lessons learned in the process. Result: In terms of standard performance measures, the constructed defect prediction model performs similar to those reported in earlier studies, e.g. 80% of errors can be detected by inspecting 30% of the source. However, the feedback from practitioners indicates that such performance figures are not useful to have an impact in their daily work. Pointing out most problematic source files, even isolating error-prone sections within files are regarded as stating the obvious by the practitioners, though the latter is found to be helpful for activities such as refactoring. On the other hand, visualizing the interactions among teams, based on the errors introduced and fixed, turns out to be the most helpful representation as it helps pinpointing communication related issues within and across teams. Conclusion: The constructed predictor can give accurate information about the most error prone parts. Creating practical representations from this data is possible, but takes effort. The error prediction research done in Elektrobit Wireless Ltd is concluded to be useful and we will further improve the presentations made from the error prediction data. © 2013 IEEE.",Bug | Bug Tracking System | Bug Triaging System | Feature selection methods | Machine Learning Algorithms,Procedia Computer Science,2015-01-01,Conference Paper,"Sharma, Gitika;Sharma, Sumit;Gujral, Shruti",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85111923667,10.1109/INFOCOM42981.2021.9488772,Fix with P6: Verifying programmable switches at runtime,"We design, develop, and evaluate P6, an automated approach to (a) detect, (b) localize, and (c) patch software bugs in P4 programs. Bugs are reported via a violation of pre-specified expected behavior that is captured by P6. P6 is based on machine learning-guided fuzzing that tests P4 switch non-intrusively, i.e., without modifying the P4 program for detecting runtime bugs. This enables an automated and real-time localization and patching of bugs. We used a P6 prototype to detect and patch existing bugs in various publicly available P4 application programs deployed on two different switch platforms: behavioral model (bmv2) and Tofino. Our evaluation shows that P6 significantly outperforms bug detection baselines while generating fewer packets and patches bugs in large P4 programs such as switch.p4 without triggering any regressions. © 2021 IEEE.",,Proceedings - IEEE INFOCOM,2021-05-10,Conference Paper,"Shukla, Apoorv;Hudemann, Kevin;Vagi, Zsolt;Hugerich, Lily;Smaragdakis, Georgios;Hecker, Artur;Schmid, Stefan;Feldmann, Anja",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85099569580,10.1109/TSE.2021.3051492,Within-Project Defect Prediction of Infrastructure-as-Code Using Product and Process Metrics,"Infrastructure-as-code (IaC) is the DevOps practice enabling management and provisioning of infrastructure through the definition of machine-readable files, hereinafter referred to as IaC scripts. Similarly to other source code artefacts, these files may contain defects that can preclude their correct functioning. In this paper, we aim at assessing the role of product and process metrics when predicting defective IaC scripts. We propose a fully integrated machine-learning framework for IaC Defect Prediction, that allows for repository crawling, metrics collection, model building, and evaluation. To evaluate it, we analyzed 104 projects and employed five machine-learning classifiers to compare their performance in flagging suspicious defective IaC scripts. The key results of the study report Random Forest as the best-performing model, with a median AUC-PR of 0.93 and MCC of 0.80. Furthermore, at least for the collected projects, product metrics identify defective IaC scripts more accurately than process metrics. Our findings put a baseline for investigating IaC Defect Prediction and the relationship between the product and process metrics, and IaC scripts&#x0027; quality. CCBY",Defect prediction | Empirical software engineering | Infrastructure-as-code,IEEE Transactions on Software Engineering,2022-06-01,Article,"Palma, Stefano Dalla;Di Nucci, Dario;Palomba, Fabio;Tamburri, Damian A.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85028649486,10.1007/s00500-020-05159-1,Software defect prediction model based on distance metric learning,"Software defect prediction (SDP) is a very important way for analyzing software quality and reducing development costs. The data during software lifecycle can be used to predict software defect. Currently, many SDP models have been proposed; however, their performance was not always ideal. In many existing prediction models based on machine learning, the distance metric between samples has significant impact on the performance of the SDP model. In addition, most samples are usually class imbalanced. To solve these issues, in this paper, a novel distance metric learning based on cost-sensitive learning (CSL) is proposed for reducing the impact of class imbalance of samples, which is then applied to the large margin distribution machine (LDM) to substitute the traditional kernel function. Further, the improvement and optimization of LDM based on CSL are also studied, and the improved LDM is used as the SDP model, called as CS-ILDM. Subsequently, the proposed CS-ILDM is applied to five publicly available data sets from the NASA Metrics Data Program repository and its performance is compared to other existing SDP models. The experimental results confirm that the proposed CS-ILDM not only has good prediction performance, but also can reduce the misprediction cost and avoid the impact of class imbalance of samples. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",Anomaly Detection | Cloud | Deep Packet Inspection | Network | Network Function Virtualization,Procedia Computer Science,2017-01-01,Conference Paper,"Wallschläger, Marcel;Gulenko, Anton;Schmidt, Florian;Kao, Odej;Liu, Feng",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85097807768,10.1145/3382025.3414960,Feature-oriented defect prediction,"Software errors are a major nuisance in software development and can lead not only to reputation damages, but also to considerable financial losses for companies. Therefore, numerous techniques for predicting software defects, largely based on machine learning methods, have been developed over the past decades. These techniques usually rely on code and process metrics in order to predict defects at the granularity of typical software assets, such as subsystems, components, and files. In this paper, we present the first systematic investigation of feature-oriented defect prediction: the prediction of defects at the granularity of features - -domain-oriented entities abstractly representing (and often cross-cutting) typical software assets. Feature-oriented prediction can be beneficial, since: (i) particular features might be more error-prone than others, (ii) characteristics of features known as defective might be useful to predict other error-prone features, (iii) feature-specific code might be especially prone to faults arising from feature interactions. We present a dataset derived from 12 software projects and introduce two metric sets for feature-oriented defect prediction. We evaluated seven machine learning classifiers with three different attribute sets each, using our two new metric sets as well as an existing metric set from the literature. We observe precision and recall values of around 85% and better robustness when more diverse metrics sets with richer feature information are used. © 2020 ACM.",classification | defect | feature | prediction,ACM International Conference Proceeding Series,2020-10-19,Conference Paper,"Strüder, Stefan;Mukelabai, Mukelabai;Strüber, Daniel;Berger, Thorsten",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85049103501,10.1145/3382494.3410692,An empirical study of software exceptions in the field using search logs,"Background: Software engineers spend a substantial amount of time using Web search to accomplish software engineering tasks. Such search tasks include finding code snippets, API documentation, seeking help with debugging, etc. While debugging a bug or crash, one of the common practices of software engineers is to search for information about the associated error or exception traces on the internet. Aims: In this paper, we analyze query logs from Bing to carry out a large scale study of software exceptions. To the best of our knowledge, this is the first large scale study to analyze how Web search is used to find information about exceptions. Method: We analyzed about 1 million exception related search queries from a random sample of 5 billion web search queries. To extract exceptions from unstructured query text, we built a novel machine learning model. With the model, we extracted exceptions from raw queries and performed popularity, effort, success, query characteristic and web domain analysis.We also performed programming language-specific analysis to give a better view of the exception search behavior. Results: Using the model with an F1-score of 0.82, our study identifies most frequent, most effort-intensive, or less successful exceptions and popularity of community Q&A sites. Conclusion: These techniques can help improve existing methods, documentation and tools for exception analysis and prediction. Further, similar techniques can be applied for APIs, frameworks, etc. © 2020 IEEE Computer Society. All rights reserved.",machine learning | Object Oriented Coupling | Object Oriented Testing | Software fault prediction | software faults prediction,Procedia Computer Science,2018-01-01,Conference Paper,"Singh, Ajmer;Bhatia, Rajesh;Sighrova, Anita",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85049101609,10.1016/j.jss.2020.110663,Descriptions of issues and comments for predicting issue success in software projects,"Software development tasks must be performed successfully to achieve software quality and customer satisfaction. Knowing whether software tasks are likely to fail is essential to ensure the success of software projects. Issue Tracking Systems store information of software tasks (issues) and comments, which can be useful to predict issue success; however; almost no research on this topic exists. This work studies the usefulness of textual descriptions of issues and comments for predicting whether issues will be resolved successfully or not. Issues and comments of 588 software projects were extracted from four popular Issue Tracking Systems. Seven machine learning classifiers were trained on 30k issues and more than 120k comments, and more than 6000 experiments were performed to predict the success of three types of issues: bugs, improvements and new features. The results provided evidence that descriptions of issues and comments are useful for predicting issue success with more than 85% of accuracy and precision, and that the predictions of issue success vary over time. Words related to software development were particularly relevant for predicting issue success. Other communication aspects and their relationship to the success of software projects must be researched in detail using data from software tools. © 2020 Elsevier Inc.",Bug Triaging System | development | Incorporate | K-nearest neighbor | Natural Language Processing | Software maintenance | textual fields based prediction,Procedia Computer Science,2018-01-01,Conference Paper,"Kukkar, Ashima;Mohana, Rajni",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85084921082,10.1145/3384517,A Defect Estimator for Source Code,"An important issue faced during software development is to identify defects and the properties of those defects, if found, in a given source file. Determining defectiveness of source code assumes significance due to its implications on software development and maintenance cost. We present a novel system to estimate the presence of defects in source code and detect attributes of the possible defects, such as the severity of defects. The salient elements of our system are: (i) a dataset of newly introduced source code metrics, called PROgramming CONstruct (PROCON) metrics, and (ii) a novel Machine-Learning (ML)-based system, called Defect Estimator for Source Code (DESCo), that makes use of PROCON dataset for predicting defectiveness in a given scenario. The dataset was created by processing 30,400+ source files written in four popular programming languages, viz., C, C++, Java, and Python. The results of our experiments show that DESCo system outperforms one of the state-of-the-art methods with an improvement of 44.9%. To verify the correctness of our system, we compared the performance of 12 different ML algorithms with 50+ different combinations of their key parameters. Our system achieves the best results with SVM technique with a mean accuracy measure of 80.8%. © 2020 ACM.",AI in software engineering | automated software engineering | Maintaining software | software defect prediction | software faults and failures | software metrics | source code mining,ACM Transactions on Software Engineering and Methodology,2020-03-04,Article,"Kapur, Ritu;Sodhi, Balwinder",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85094560272,10.5815/ijmecs.2020.05.03,Software defect prediction using variant based ensemble learning and feature selection techniques,"Testing is considered as one of the expensive activities in software development process. Fixing the defects during testing process can increase the cost as well as the completion time of the project. Cost of testing process can be reduced by identifying the defective modules during the development (before testing) stage. This process is known as “Software Defect Prediction”, which has been widely focused by many researchers in the last two decades. This research proposes a classification framework for the prediction of defective modules using variant based ensemble learning and feature selection techniques. Variant selection activity identifies the best optimized versions of classification techniques so that their ensemble can achieve high performance whereas feature selection is performed to get rid of such features which do not participate in classification and become the cause of lower performance. The proposed framework is implemented on four cleaned NASA datasets from MDP repository and evaluated by using three performance measures, including: F-measure, Accuracy, and MCC. According to results, the proposed framework outperformed 10 widely used supervised classification techniques, including: “Naïve Bayes (NB), Multi-Layer Perceptron (MLP), Radial Basis Function (RBF), Support Vector Machine (SVM), K Nearest Neighbor (KNN), kStar (K*), One Rule (OneR), PART, Decision Tree (DT), and Random Forest (RF)”. © 2020 MECS.",Classifier Variant | Ensemble Learning. Machine Learning Techniques | Feature Selection | Software Defect Prediction,International Journal of Modern Education and Computer Science,2020-01-01,Article,"Ali, Umair;Aftab, Shabib;Iqbal, Ahmed;Nawaz, Zahid;Bashir, Muhammad Salman;Saeed, Muhammad Anwaar",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85091800167,,Temporal convolutional networks for just-in-Time software defect prediction,"Defect prediction and estimation techniques play a significant role in software maintenance and evolution. Recently, several research studies proposed just-in-time techniques to predict defective changes. Such prediction models make the developers check and fix the defects just at the time they are introduced (commit level). Nevertheless, early prediction of defects is still a challenging task that needs to be addressed and can be improved by getting higher performances. To address this issue this paper proposes an approach exploiting a large set of features corresponding to source code metrics detected from commits history of software projects. In particular, the approach uses deep temporal convolutional networks to make the fault prediction. The evaluation is performed on a large data-set, concerning four well-known open-source projects and shows that, under certain considerations, the proposed approach has effective defect proneness prediction ability. Copyright © 2020 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.",Fault prediction; Machine learning; Software metrics,ICSOFT 2020 - Proceedings of the 15th International Conference on Software Technologies,2020,,"Ardimento P., Aversano L., Bernardi M.L., Cimitile M.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85084427783,10.1016/j.procs.2020.03.332,Assessing Cross-Project Technique for Software Maintainability Prediction,"Software Maintainability refers to the ease with which software maintenance activities like correction of faults, deletion of obsolete code, addition of new code etc. can be carried out to adapt to the modified environment. Predicting maintainability in early stages of development helps in reducing the cost of maintenance and ensures optimum utilization of resources. Sometimes, it becomes difficult to train prediction models using historical data of the same dataset for which the model is being developed because of the unavailability of sufficient amount of training data, in turn making a way for Cross-Project technique for Software Maintainability Prediction (CPSMP). In order to evaluate the proposed CPSMP technique, QUES dataset is used as training set and UIMS dataset is used as test set in this study with 19 different regression modelling methods. Performance of CPSMP model is evaluated using Root Mean Square Error (RMSE) as an accuracy measure. Results show that cross-project technique can successfully be applied for maintainability prediction. The average RMSE value calculated for all the modelling methods is found to be 82.310 without CPSMP whereas an average RMSE value of 71.532 is obtained with CPSMP resulting in an overall improvement in prediction performance by 13.09%. Also, 84.21% of the total techniques used in this study performed better with CPSMP. © 2020 The Authors. Published by Elsevier B.V.",Cross-Project | Machine Learning | Regression | Software Maintainability Prediction,Procedia Computer Science,2020-01-01,Conference Paper,"Gupta, Shikha;Chug, Anuradha",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85075719487,10.1007/978-981-13-9282-5_14,Evaluating the Impact of Sampling-Based Nonlinear Manifold Detection Model on Software Defect Prediction Problem,"Accurate prediction of defects is considered an essential factor, depending mainly on how efficiently testing of different prediction models has been done. Earlier, most of the models were restricted to the use of feature selection methods that had limited effects in solving this problem in initial stage of software development. To overcome it, the application of software defect prediction model using modern nonlinear manifold detection (nonlinear MD) combined with SMOTE using four machine learning classification approaches has been proposed in a way that the challenging task of defect prediction has been categorized as problem of high-dimensional datasets, problem of imbalanced class, and identification of most relevant and effective software attributes. Then, statistically evaluated and compared performance of prediction model with or without SMOTE-nonlinear MD approaches and results validated that proposed SMOTE-nonlinear MD approach prediction model predicts defect with better accuracy than others using RMSE, accuracy, and area under the curve. © 2020, Springer Nature Singapore Pte Ltd.",Defect prediction | Dimension reduction | Friedman test | High dimensional | Imbalanced class | Machine learning | Nonlinear manifold detection | Oversampling | SMOTE | Software datasets,"Smart Innovation, Systems and Technologies",2020-01-01,Conference Paper,"Ghosh, Soumi;Rana, Ajay;Kansal, Vineet",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85121770917,10.1007/s00500-019-03942-3,A variable-level automated defect identification model based on machine learning,"Static analysis tools, automatically detecting potential source code defects at an early phase during the software development process, are diffusely applied in safety-critical software fields. However, alarms reported by the tools need to be inspected manually by developers, which is inevitable and costly, whereas a large proportion of them are found to be false positives. Aiming at automatically classifying the reported alarms into true defects and false positives, we propose a defect identification model based on machine learning. We design a set of novel features at variable level, called variable characteristics, for building the classification model, which is more fine-grained than the existing traditional features. We select 13 base classifiers and two ensemble learning methods for model building based on our proposed approach, and the reported alarms classified as unactionable (false positives) are pruned for the purpose of mitigating the effort of manual inspection. In this paper, we firstly evaluate the approach on four open-source C projects, and the classification results show that the proposed model achieves high performance and reliability in practice. Then, we conduct a baseline experiment to evaluate the effectiveness of our proposed model in contrast to traditional features, indicating that features at variable level improve the performance significantly in defect identification. Additionally, we use machine learning techniques to rank the variable characteristics in order to identify the contribution of each feature to our proposed model. © 2019, The Author(s).",Deep Learning | Design Debugging | ML | Sparse Autoencoder,Procedia Computer Science,2021-01-01,Conference Paper,"Gaber, Lamya;Hussein, Aziza I.;Moness, Mohammed",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85100265697,10.1186/s40537-019-0250-z,Unsupervised software defect prediction using median absolute deviation threshold based spectral classifier on signed Laplacian matrix,"Area of interest: The trend of current software inevitably leads to the big data era. There are much of large software developed from hundreds to thousands of modules. In software development projects, finding the defect proneness manually on each module in large software dataset is probably inefficient in resources. In this task, the use of a software defect prediction model becomes a popular solution with much more cost-effective rather than manual reviews. This study presents a specific machine learning algorithm, which is the spectral classifier, to develop a software defect prediction model using unsupervised learning approach. Background and objective: The spectral classifier has been successfully used in software defect prediction because of its reliability to consider the similarities between software entities. However, there are conditional issues when it uses the zero value as partitioning threshold. The classifier will produce the predominantly cluster when the eigenvector values are mostly positives. Besides, it will also generate low clusters compactness when the eigenvector contains outliers. The objective of this study is mainly to propose an alternative partitioning threshold in dealing with the zero threshold issues. Generally, the proposed method is expected to improve the spectral classifier based software defect prediction performances. Methods: This study proposes the median absolute deviation threshold based spectral classifier to carry out the zero value threshold issues. The proposed method considers the eigenvector values dispersion measure as the new partitioning threshold, rather than using a central tendency measure (e.g., zero, mean, median). The baseline method of this study is the zero value threshold based spectral classifier. Both methods are performed on the signed Laplacian matrix to meet the non-negative Laplacian graph assumption. For classification, the heuristic row sum method is used to assign the entity class as the prediction label. Results and conclusion: In terms of clustering, the proposed method can produce better cluster memberships that affect the cluster compactness and the classifier performances improvement. The cluster compactness average of both the proposed and baseline methods are 1.4 DBI and 1.8 DBI, respectively. In classification performance, the proposed method performs better accuracy with lower error rates than the baseline method. The proposed method also has high precision but low in the recall, which means that the proposed method can detect the software defect more precisely, although in the small number in detection. The proposed method has the accuracy, precision, recall, and error rates with average values of 0.79, 0.84, 0.72, and 0.21, respectively. While the baseline method has the accuracy, precision, recall, and error rates with average values of 0.74, 0.74, 0.89, and 0.26, respectively. Based on those results, the proposed method able to provide a viable solution to address the zero threshold issues in the spectral classifier. Hence, this study concludes that the use of the median absolute deviation threshold can improve the spectral based unsupervised software defect prediction method. © 2019, The Author(s).",Issue reports management | Labeling unstructured data | Software maintenance and evolution,Science of Computer Programming,2021-05-01,Article,"Kallis, Rafael;Di Sorbo, Andrea;Canfora, Gerardo;Panichella, Sebastiano",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85077969906,10.1109/SNPD.2019.8935839,Prediction of Software Defects Using Automated Machine Learning,"The effectiveness of defect prediction depends on modeling techniques as well as their parameter optimization, data preprocessing and ensemble development. This paper focuses on auto-sklearn, which is a recently-developed software library for automated machine learning, that can automatically select appropriate prediction models, hyperparameters and data preprocessing techniques for a given data set and develop their ensemble with optimized weights. In this paper we empirically evaluate the effectiveness of auto-sklearn in predicting the number of defects in software modules. In the experiment, we used software metrics of 20 OSS projects for cross-release defect prediction and compared auto-sklearn with random forest, decision tree and linear discriminant analysis by using Norm(Popt) as a performance measure. As a result, auto-sklearn showed similar prediction performance as random forest, which is one of the best prediction models for defect prediction in past studies. This indicates that auto-sklearn can obtain good prediction performance for defect prediction without any knowledge of machine learning techniques and models. © 2019 IEEE.",auto-sklearn | cross-release prediction | defect prediction | meta-learning | software quality,"Proceedings - 20th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD 2019",2019-07-01,Conference Paper,"Tanaka, Kazuya;Monden, Akito;Yucel, Zeynep",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85072677327,10.1109/COMPSAC.2019.10229,Software fault proneness prediction with group lasso regression: On factors that affect classification performance,"Machine learning algorithms have been used extensively for software fault proneness prediction. This paper presents the first application of Group Lasso Regression (G-Lasso) for software fault proneness classification and compares its performance to six widely used machine learning algorithms. Furthermore, we explore the effects of two factors on the prediction performance: the effect of imbalance treatment using the Synthetic Minority Over-sampling Technique (SMOTE), and the effect of datasets used in building the prediction models. Our experimental results are based on 22 datasets extracted from open source projects. The main findings include: (1) G-Lasso is robust to imbalanced data and significantly outperforms the other machine learning algorithms with respect to the Recall and G-Score, i.e., the harmonic mean of Recall and (1- False Positive Rate). (2) Even though SMOTE improved the performance of all learners, it did not have statistically significant effect on G-Lasso’s Recall and G-Score. Random Forest was in the top performing group of learners for all performance metrics, while Naive Bayes performed the worst of all learners. (3) When using the same change metrics as features, the choice of the dataset had no effect on the performance of most learners, including G-Lasso. Naive Bayes was the most affected, especially when balanced datasets were used. © 2019 IEEE",Group Lasso regression | Imbalance treatment | SMOTE | Software fault proneness prediction,Proceedings - International Computer Software and Applications Conference,2019-07-01,Conference Paper,"Goseva-Popstojanova, Katerina;Ahmad, Mohammad Jamil;Alshehri, Yasser Ali",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85086427001,10.1109/ICCONS.2018.8663154,A Comparative Study of Unsupervised Learning Algorithms for Software Fault Prediction,Software fault prediction is an important task in software development process which enables software practitioners to easily detect and rectify the errors in modules or classes. Various fault prediction techniques have been studied in the past and unsupervised learning methods such as clustering techniques are drawing much attention in the recent years. K-means is a well known clustering algorithm which is applied on various exploratory analysis including software fault prediction. This paper provides a comparative study on software fault prediction using K-means clustering algorithm and its variants. We use five software fault prediction datasets taken from PROMISE repository to evaluate the prediction accuracy of the clustering algorithms. Experimental results indicate that proper initial seed selection enables K-means algorithm to effectively group the faulty modules. © 2018 IEEE.,Data visualization | Deep-learning | Machine learning | Model management | Software analysis,SoftwareX,2020-07-01,Article,"Ferenc, Rudolf;Viszkok, Tamás;Aladics, Tamás;Jász, Judit;Hegedűs, Péter",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85002466321,10.1109/TCAD.2018.2878193,Bug prediction of systemC models using machine learning,"In system-on-chip design, resources for verification is limited by time-to-market and cost. In order to allocate verification resources effectively, managers need to rely on their experience backed by design related metrics. However, often there are also other aspects of development process, such as bug history and developer information that can improve the effectiveness of verification. Software bug prediction is a machine learning (ML)-based technique which predicts whether a given software module is bug-prone by using product and process metrics of the module. Therefore, it can help direct verification effort, reduce costs, and improve the quality of software. Although there is a plethora of work in software bug prediction, no such work exists for SystemC. We propose an ML-based software bug prediction solution for verification of SystemC models used in virtual prototypes that takes into account system level design metrics and demonstrate its effectiveness on several open source system level designs. We find that 96% of modules could be correctly predicted as buggy or clean. © 1982-2012 IEEE.",Change prediction | Defect prediction | Effort estimation | Maintainability prediction | Search-based techniques | Software quality,Swarm and Evolutionary Computation,2017-02-01,Article,"Malhotra, Ruchika;Khanna, Megha;Raje, Rajeev R.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85110057833,10.1109/ICMLA.2018.00224,Upsilon-SVR Polynomial Kernel for Predicting the Defect Density in New Software Projects,"An important product measure to determine the effectiveness of software processes is the defect density (DD). In this study, we propose the application of support vector regression (SVR) to predict the DD of new software projects obtained from the International Software Benchmarking Standards Group (ISBSG) Release 2018 data set. Two types of SVR (i.e., ϵ-SVR and υ-SVR) were applied to train and test these projects. Each SVR used four types of kernels. The prediction accuracy of each SVR was compared to that of a statistical regression (i.e., a simple linear regression, SLR). Statistical significance test showed that υ-SVR with polynomial kernel was better than that of SLR when new software projects were developed on mainframes and coded in programming languages of third generation. © 2018 IEEE.",Beer bottle defect detection | Image processing | Training tool | Virtual simulation experiment,Virtual Reality and Intelligent Hardware,2020-08-01,Article,"Zhao, Yuxiang;An, Xiaowei;Sun, Nongliang",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85082325059,10.1007/978-3-030-03146-6_44,Improvement in Software Defect Prediction Outcome Using Principal Component Analysis and Ensemble Machine Learning Algorithms,"Improving customer experience is the focus of IT Industry. It is no longer about customer satisfaction, but it is about creating memorable experiences which will help build loyal customers. Hence it is extremely critical to release defect free software. While machine learning techniques were widely used for prediction modelling, creating a reliable predictor which can perform satisfactorily is always a challenge. In this paper, we have proposed a framework using PCA for feature selection and ensemble machine learning algorithms with stratified 10-fold cross validation for building the classification model. The proposed model is tested using 5 projects from NASA Metrics Data program and 4 ensemble machine learning algorithms. Our results show that the prediction accuracy is improved by 0.6% when the reduced dataset is used for classification than using the whole dataset. In comparison with previous research studies, our framework has shown an average of 4.2% increase in performance. © 2019, Springer Nature Switzerland AG.",Adaboost | Bagging | Classification | Classification via regression | Data mining | Dimensionality reduction | Ensemble machine learning algorithms | Fault prediction | Fault proneness | Feature selection | Machine learning techniques | Meta learning | NASA Metrics Data Program | Prediction modelling | Principal component analysis | Random forest | Reliable software | Software defect prediction | Software quality | Stratified 10 fold cross validation,Lecture Notes on Data Engineering and Communications Technologies,2019-01-01,Book Chapter,"Dhamayanthi, N.;Lavanya, B.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85078333388,10.6084/ijact.v8i12.1054,Mining historical software testing outcomes to predict future results,"Software bugs and program defects have significant negative effect on the cost and duration of software development process. Finding such bugs in early stages of the development process will cuts development time and maintenance costs. This investigation presents three different machine learning algorithms: K-Nearest Neighbors (KNN), Random Forest (RF), and Multilayer Perceptron (MLP) to build a new proposed software defect prediction model using different types of software performance metrics. This proposed model was tested on three public datasets obtained from NASA to assess its accuracy and revealed that the KNN was outperforms RF and MLP. © 2019, National Institute of Science Communication and Information Resources (NISCAIR).",Machine learning | Prediction model | Software defects | Software engineering | Software evolution,Compusoft,2019-01-01,Article,"Abdulshaheed, Mohamed;Hammad, Mustafa;Alqaddoumi, Abdulla;Obeidat, Qasem",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85073340711,10.1504/IJISTA.2019.102667,Statistical assessment of nonlinear manifold detection-based software defect prediction techniques,"Prediction of software defects has immense importance for obtaining desired outcome at minimised cost and so attracted researchers working on this topic applying various techniques, which were not found fully effective. Software datasets comprise of redundant features that hinder effective application of techniques resulting inappropriate defect prediction. Hence, it requires newer application of nonlinear manifold detection techniques (nonlinear MDTs) that has been examined for accurate prediction of defects at lesser time and cost using different classification techniques. In this work, we analysed and tested the effect of nonlinear MDTs to find out accurate and best classification technique for all datasets. Comparison has been made between the results of without or with nonlinear MDTs and paired two-tailed T-test has been performed for statistical testing and verifying the performance of classifiers using nonlinear MDTs on all datasets. Outcome revealed that among all nonlinear MDTs, FastMVU makes most accurate prediction of software defects. Copyright © 2019 Inderscience Enterprises Ltd.",Dimensionality reduction | Fast maximum variance unfolding | FastMVU | Machine learning | Manifold detection | Nonlinear | Promise repository | Software defect prediction,International Journal of Intelligent Systems Technologies and Applications,2019-01-01,Article,"Ghosh, Soumi;Rana, Ajay;Kansal, Vineet",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85071512925,10.5220/0006911401850193,Clustering-based under-sampling for software defect prediction,"Detection of software defective modules is important for reducing the time and resources consumed by software testing. Software defect data sets usually suffer from imbalance, where the number of defective modules is fewer than the number of defect-free modules. Imbalanced data sets make the machine learning algorithms to be biased toward the majority class. Clustering-based under-sampling shows its ability to find good representatives of the majority data in different applications. This paper presents an approach for software defect prediction based on clustering-based under-sampling and Artificial Neural Network (ANN). Firstly, clustering-based under-sampling is used for selecting a subset of the majority samples, which is then combined with the minority samples to produce a balanced data set. Secondly, an ANN model is built and trained using the resulted balanced data set. The used ANN is trained to classify the software modules into defective or defect-free. In addition, a sensitivity analysis is conducted to choose the number of majority samples that yields the best performance measures. Results show the high prediction capability for the detection of defective modules while maintaining the ability of detecting defect-free modules. Copyright © 2018 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved",Artificial Neural Network | Clustering | K-means | Software Defect Prediction | Under-sampling,ICSOFT 2018 - Proceedings of the 13th International Conference on Software Technologies,2019-01-01,Conference Paper,"Henein, Moheb M.R.;Shawky, Doaa M.;Abd-El-Hafiz, Salwa K.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85071479643,10.5220/0006926308790887,Software engineering approach to bug prediction models using machine learning as a service (MLaaS),"The presence of bugs in a software release has become inevitable. The loss incurred by a company due to the presence of bugs in a software release is phenomenal. Modern methods of testing and debugging have shifted focus from “detecting” to “predicting” bugs in the code. The existing models of bug prediction have not been optimized for commercial use. Moreover, the scalability of these models has not been discussed in depth yet. Taking into account the varying costs of fixing bugs, depending on which stage of the software development cycle the bug is detected in, this paper uses two approaches - one model which can be employed when the 'cost of changing code' curve is exponential and the other model can be used otherwise. The cases where each model is best suited are discussed. This paper proposes a model that can be deployed on a cloud platform for software development companies to use. The model in this paper aims to predict the presence or absence of a bug in the code, using machine learning classification models. Using Microsoft Azure's machine learning platform this model can be distributed as a web service worldwide, thus providing Bug Prediction as a Service (BPaaS). Copyright © 2018 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved",Bug Prediction | Bug Prediction as a Service | Machine Learning | Machine Learning as a Service | Microsoft Azure,ICSOFT 2018 - Proceedings of the 13th International Conference on Software Technologies,2019-01-01,Conference Paper,"Subbiah, Uma;Ramachandran, Muthu;Mahmood, Zaigham",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85068673557,10.1109/ACCESS.2019.2923821,Analyses of Classifier's Performance Measures Used in Software Fault Prediction Studies,"Assessing the quality of the software is both important and difficult. For this purpose, software fault prediction (SFP) models have been extensively used. However, selecting the right model and declaring the best out of multiple models are dependent on the performance measures. We analyze 14 frequently used, non-graphic classifier's performance measures used in SFP studies. These analyses would help machine learning practitioners and researchers in SFP to select the most appropriate performance measure for the models' evaluation. We analyze the performance measures for resilience against producing invalid values through our proposed plausibility criterion. After that, consistency and discriminancy analyses are performed to find the best out of the 14 performance measures. Finally, we draw the order of the selected performance measures from better to worse in both balance and imbalance datasets. Our analyses conclude that the F-measure and the G-mean1 are equally the best candidates to evaluate the SFP models with careful analysis of the result, as there is a risk of invalid values in certain scenarios. © 2019 IEEE.",Classification | evaluation parameters | machine learning | performance measures | software fault prediction,IEEE Access,2019-01-01,Article,"Rizwan, Muhammad;Nadeem, Aamer;Sindhu, Muddassar Azam",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85056719436,10.1145/3273934.3273944,An improvement to test case failure prediction in the context of test case prioritization,"Aim: In this study, we aim to re-evaluate research questions on the ability of a logistic regression model proposed in a previous work to predict and prioritize the failing test cases based on some test quality metrics. Background: The process of prioritizing test cases aims to come up with a ranked test suite where test cases meeting certain criteria are prioritized. One criterion may be the ability of test cases to find faults that can be predicted a priori. Ranking test cases and executing the top-ranked test cases is particularly beneficial when projects have tight schedules and budgets. Method: We performed the comparison by first rebuilding the predictive models using the features from the original study and then we extended the original work to improve the predictive models using new features by combining with the existing ones. Results: The results of our study, using a dataset of five open-source systems, confirm that the findings from the original study hold and that our predictive models with new features outperform the original models in predicting and prioritizing the failing test cases. Conclusions: We plan to apply this method to a large-scale dataset from a large commercial enterprise project, to better demonstrate the improvement that our modified features provide and to explore the model’s performance at scale. © 2018 Association for Computing Machinery.",Logistic regression model | Machine learning | Prediction | Test Case prioritization | Test quality metrics,ACM International Conference Proceeding Series,2018-10-10,Conference Paper,"Palma, Francis;Abdou, Tamer;Bener, Ayse;Maidens, John;Liu, Stella",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85031778357,10.1109/INDICON.2017.8487925,Aging Related Bug Prediction using Extreme Learning Machines,"Aging-Related Bugs (ARBs) occur in long running systems due to error conditions caused because of accumulation of problems such as memory leakage or unreleased files and locks. Aging-Related Bugs are hard to discover during software testing and also challenging to replicate. Automatic identification and prediction of aging related fault-prone files and classes in an object oriented system can help the software quality assurance team to optimize their testing efforts. In this paper, we present a study on the application of static source code metrics and machine learning techniques to predict aging related bugs. We conduct a series of experiments on publicly available dataset from two large open-source software systems: Linux and MySQL. Class imbalance and high dimensionality are the two main technical challenges in building effective predictors for aging related bugs. We investigate the application of five different feature selection techniques (OneR, Information Gain, Gain Ratio, RELEIF and Symmetric Uncertainty) for dimensionality reduction and SMOTE method to counter the effect of class imbalance in our proposed machine learning based solution approach. We apply Extreme Learning Machines (ELM) with three different kernels (linear, polynomial and RBF) and present experimental results which demonstarte the effectiveness of our approach. © 2017 IEEE.",enterprise architecture | fault injection | machine learning | Online failure prediction,Enterprise Information Systems,2018-05-28,Article,"Jordan, Paul L.;Peterson, Gilbert L.;Lin, Alan C.;Mendenhall, Michael J.;Sellers, Andrew J.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85070709311,10.1109/ICECDS.2017.8390007,An approach for software defect prediction by combined soft computing,"Nowadays, software project success is the key challenge. Prediction of software defects is main focus for the engineering community. A recent study in literature shows that data mining techniques are wildly used to predict software projects success. Many software development companies maintain their own software repositories, it is helpful for the prediction of software defects. There is dire need to reduce the gap between the software engineering and data mining community to increase the rate of software projects success. Although software defect prediction using classification/clustering algorithms has been encouraged by many researchers. However the lack of performance due to single classifier/clustering algorithms used for defect prediction. This paper provides software defect prediction using integrated approaches are advocated, instead of single classifier/clustering. The experimental results obtained shows better prediction performance could be achieved using the soft computing techniques (genetic algorithm, fuzzy c-means clustering and random forest classifier) © 2017 IEEE.",cancer | drug design | drug sensitivity | drug target prediction,Journal of Computational Biology,2019-08-01,Conference Paper,"Hussain, Shahid;Ferzund, Javed;Ul-Haq, Raza",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85013915220,10.1109/SYNASC.2016.046,A novel approach for software defect prediction using fuzzy decision trees,"Detecting defective entities from existing software systems is a problem of great importance for increasing both the software quality and the efficiency of software testing related activities. We introduce in this paper a novel approach for predicting software defects using fuzzy decision trees. Through the fuzzy approach we aim to better cope with noise and imprecise information. A fuzzy decision tree will be trained to identify if a software module is or not a defective one. Two open source software systems are used for experimentally evaluating our approach. The obtained results highlight that the fuzzy decision tree approach outperforms the non-fuzzy one on almost all case studies used for evaluation. Compared to the approaches used in the literature, the fuzzy decision tree classifier is shown to be more efficient than most of the other machine learning-based classifiers. © 2016 IEEE.",decision tree | fuzzy theory | machine learning | software defect prediction,"Proceedings - 18th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing, SYNASC 2016",2017-01-23,Conference Paper,"Marian, Zsuzsanna;Mircea, Ioan Gabriel;Czibula, Istvan Gergely;Czibula, Gabriela",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84995446661,10.1109/QRS.2016.54,Automatic Localization of Bugs to Faulty Components in Large Scale Software Systems Using Bayesian Classification,"We suggest a Bayesian approach to the problem of reducing bug turn-Around time in large software development organizations. Our approach is to use classification to predict where bugs are located in components. This classification is a form of automatic fault localization (AFL) at the component level. The approach only relies on historical bug reports and does not require detailed analysis of source code or detailed test runs. Our approach addresses two problems identified in user studies of AFL tools. The first problem concerns the trust in which the user can put in the results of the tool. The second problem concerns understanding how the results were computed. The proposed model quantifies the uncertainty in its predictions and all estimated model parameters. Additionally, the output of the model explains why a result was suggested. We evaluate the approach on more than 50000 bugs. © 2016 IEEE.",Fault Detection | Fault Location | Machine Learning | Software Debugging | Software Engineering | Software Maintenance,"Proceedings - 2016 IEEE International Conference on Software Quality, Reliability and Security, QRS 2016",2016-10-12,Conference Paper,"Jonsson, Leif;Broman, David;Magnusson, Mans;Sandahl, Kristian;Villani, Mattias;Eldh, Sigrid",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84991688431,10.1145/2961111.2962601,Predicting Defectiveness of Software Patches,"Context: Software code review, as an engineering best practice, refers to the inspection of the code change in order to find possible defects and ensure change quality. Code reviews, however, may not guarantee finding the defects. Thus, there is a risk for a defective code change in a given patch, to pass the review process and be submitted. Goal: In this research, we aim to apply different machine learning algorithms in order to predict the defectiveness of a patch after being reviewed, at the time of its submission. Method: We built three models using three different machine learning algorithms: Logistic Regression, Nave Bayes, and Bayesian Network model. To build the models, we consider different factors involved in review process in terms of Product, Process and People (3P). Results: Our empirical results show that, Bayesian Networks is able to better predict the defectiveness of the changed code with 76% accuracy. Conclusions: Predicting defectiveness of change code is beneficial in making patch release decisions. The Bayesian Network model outperforms the others since it capturs the relationship among the factors in the review process. © 2016 ACM.",Code review | Code Review Quality | Defect Prediction | Software Patch Defectiveness,International Symposium on Empirical Software Engineering and Measurement,2016-09-08,Conference Paper,"Soltanifar, Behjat;Erdem, Atakan;Bener, Ayse",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84988299770,10.1109/ICOSP.2014.7015250,SEU-tolerant Restricted Boltzmann Machine learning on DSP-based fault detection,"Restricted Boltzmann Machine (RBM) is the main building block of many deep learning models which are now becoming one of the most important kind of algorithms in machine leaning community. With the growing demand for information processing in aerospace computer, RBM can be used as a promising block supporting intelligent applications such as space self-control, intelligent recognition, and target classification. As for aerospace computer, a common challenge is the Single Event Upset (SEU) effect, which would change the state of some information bits stochastically and lead to error results. Due to the time-consuming and data-intensive computation of aerospace computer, digital signal processing (DSP) is the most suitable for aerospace computation. In this paper, we first implement representative RBM learning algorithm in a power efficient DSP platform. Then, we explore the possible SEU effects in our hardware architecture and RBM learning algorithm. Further, we integrate three software fault detection techniques (i.e. duplication, increasing data diversity, shortening life cycle of variable) into RBM learning. In the experiment, we utilize the simulated fault injection technique to evaluate fault detective RBM learning. The evaluation shows that our fault detection designs effectively detect SEU-induced error during RBM learning in DSP with low computational complexity. © 2014 IEEE.",DSP | RBM | SEU | Software fault detection,"International Conference on Signal Processing Proceedings, ICSP",2014-01-01,Conference Paper,"Jian, Songlei;Jiang, Jingfei;Lu, Kai;Zhang, Yanping",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85100270069,10.1007/978-3-319-07593-8_21,Software fault prediction based on improved fuzzy clustering,"Predicting parts of the software programs that are more defects prone could ease up the software testing process and helps effectively to reduce the cost and time of developments. Although many machine-learning and statistical techniques have been proposed widely for defining fault prone modules in software fault prediction, but this area have yet to be explored with high accuracy and less error. Unfortunately, several earlier methods including artificial neural networks and its variants that have been used, marred by limitations such as inability to adequately handle uncertainties in software measurement data which leads to low accuracy, instability and inconsistency in prediction. In this paper, first the effect of irrelevant and inconsistent modules on fault prediction is decreased by designing a new framework, in which the entire project's modules are clustered. The generated output is then passed to the next model in the hybrid setting, which is a probabilistic neural network (PNN) for training and prediction. We used four NASA data sets to evaluate our results. Performance evaluation in terms of false positive rate, false negative rate, and overall error are calculated and showed 30% to 60% improvement in false negative rate compared to other well-performed training methods such as naïve Bayes and random forest. © Springer International Publishing Switzerland 2014.",Bug reports | duplicated bug | duplicated records word embedding | machine-learning | natural language processing | prediction,"2020 International Conference on Innovation and Intelligence for Informatics, Computing and Technologies, 3ICT 2020",2020-12-20,Conference Paper,"Mahfoodh, Hussain;Hammad, Mustafa",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84888411522,10.1007/978-3-319-03095-1_36,An Approach to Predict Software Project Success Based on Random Forest Classifier,"The success or failure of a software project depends on the product's quality and reliability. The predictions of defects are important since it helps direct test effort, reduce costs and improve the quality of software. Software defects are expensive in terms of quality and cost. Data mining techniques and machine learning algorithms can be applied on these repositories to extract the useful information. This paper presents a software defect prediction model based on Random Forest (RF) ensemble classifier, which is more robust and beneficial for large-scale software system. The difference in the performance of the proposed methodology over other methods is statistically significant. Two fold information, one is RF is efficient irrespective of the domain of applications that is from the point of project, complexity of project, domain of project. Second is this inference enabled to predict the success level of projects. RF is travels light to project managers to predict the success of the projects based on the mining carried out using RF from empirical investigations. © Springer International Publishing Switzerland 2014.",Clustering | Data Mining | Metrics | Project Management | Random forest | Software Engineering | Software Quality,Advances in Intelligent Systems and Computing,2014-01-01,Conference Paper,"Suma, V.;Pushphavathi, T. P.;Ramaswamy, V.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84878002068,10.1109/ICCIC.2012.6510294,Predicting fault-prone software modules using feature selection and classification through data mining algorithms,"Software defect detection has been an important topic of research in the field of software engineering for more than a decade. This research work aims to evaluate the performance of supervised machine learning techniques on predicting defective software through data mining algorithms. This paper places emphasis on the performance of classification algorithms in categorizing seven datasets (CM1, JM1, MW1, KC3, PC1, PC2, PC3 and PC4) under two classes namely Defective and Normal. In this study, publicly available data sets from different organizations are used. This permitted us to explore the impact of data from different sources on different processes for finding appropriate classification models. We propose a computational framework using data mining techniques to detect the existence of defects in software components. The framework comprises of data pre-processing, data classification and classifier evaluation. In this paper; we report the performance of twenty classification algorithms on seven publicly available datasets from the NASA MDP Repository. Random Tree Classification algorithm produced 100 percent accuracy in classifying the datasets and hence the features selected by this technique were considered to be the most significant features. The results were validated with suitable test data. © 2012 IEEE.",Classification | Data Mining | Feature Selection | Machine learning,"2012 IEEE International Conference on Computational Intelligence and Computing Research, ICCIC 2012",2012-12-01,Conference Paper,"Geetha Ramani, R.;Vinodh Kumar, S.;Jacob, Shomona Gracia",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85105990594,10.1007/s11334-021-00399-2,Bootstrap aggregation ensemble learning-based reliable approach for software defect prediction by using characterized code feature,"To ensure software quality, software defect prediction plays a prominent role for the software developers and practitioners. Software defect prediction can assist us with distinguishing software defect modules and enhance the software quality. In present days, many supervised machine learning algorithms have proved their efficacy to identify defective modules. However, those are limited to prove their major significance due to the limitations such as the adaptation of parameters with the environment and complexity. So, it is important to develop a key methodology to improve the efficiency of the prediction module. In this paper, an ensemble learning technique called Bootstrap aggregating has been proposed for software defect prediction object-oriented modules. The proposed method's accuracy, recall, precision, F-measure, and AUC-ROC efficiency were compared to those of many qualified machine learning algorithms. Simulation results and performance comparison are evident that the proposed method outperformed well compared to other approaches. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Ensemble learning | Machine learning | Software defect prediction | Software reliability,Innovations in Systems and Software Engineering,2021-12-01,Article,"Suresh Kumar, P.;Behera, H. S.;Nayak, Janmenjoy;Naik, Bighnaraj",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85108275396,10.1016/j.jss.2021.111026,Software defect prediction based on enhanced metaheuristic feature selection optimization and a hybrid deep neural network,"Software defect prediction aims to identify the potential defects of new software modules in advance by constructing an effective prediction model. However, the model performance is susceptible to irrelevant and redundant features. In addition, previous studies mainly use traditional data mining or machine learning techniques for defect prediction, the prediction performance is not superior enough. For the first issue, motivated by the idea of search based software engineering, we leverage the recently proposed whale optimization algorithm (WOA) and another complementary simulated annealing (SA) to construct an enhanced metaheuristic search based feature selection algorithm named EMWS, which can effectively select fewer but closely related representative features. For the second issue, we employ a hybrid deep neural network — convolutional neural network (CNN) and kernel extreme learning machine (KELM) to construct a unified defect prediction predictor called WSHCKE, which can further integrate the selected features into the abstract deep semantic features by CNN and boost the prediction performance by taking full advantage of the strong classification capacity of KELM. We conduct extensive experiments for feature selection or extraction and defect prediction across 20 widely-studied software projects on four evaluation indicators. Experimental results demonstrate the superiority of EMWS and WSHCKE. © 2021",Convolutional neural network | Kernel extreme learning machine | Metaheuristic feature selection | Software defect prediction | Whale optimization algorithm,Journal of Systems and Software,2021-10-01,Article,"Zhu, Kun;Ying, Shi;Zhang, Nana;Zhu, Dandan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85049343831,10.1016/j.eswa.2021.114637,Cross-project software defect prediction based on domain adaptation learning and optimization,"Software defect prediction (SDP) is very helpful for optimizing the resource allocation of software testing and improving the quality of software products. The cross-project defect prediction (CPDP) model based on machine learning is first learned through the existing training data with sufficient number and defect labels on one project, and then used to predict the defect labels of another new project with insufficient number and fewer labeled data. However, its prediction performance has a large gap compared with the within-project defect prediction (WPDP) model. The main reason is that there are usually differences between the distributions of training data in different software projects, and it has a greater impact on the prediction performance of the CPDP model. To solve this problem, the kernel twin support vector machines (KTSVMs) is used to implement domain adaptation (DA) to match the distributions of training data for different projects. Moreover, KTSVMs with DA function (called DA-KTSVM) is further used as the CPDP model in this paper. Since the parameters of DA-KTSVM have an impact on its predictive performance, these parameters are optimized by an improved quantum particle swarm optimization algorithm (IQPSO), and the optimized DA-KTSVM is called as DA-KTSVMO. In order to confirm the effectiveness of DA-KTSVMO, some experiments are implemented on 17 open source software projects. Experimental results and analysis show that DA-KTSVMO can not only achieve better prediction performance than other CPDP models compared, but also achieve almost the same or better compared performance than WPDP models when the training sample data is sufficient. In addition, DA-KTSVMO can make better use of existing sufficient data knowledge and realize the reuse of defective data to improve the prediction performance of DA-KTSVMO. © 2021 Elsevier Ltd",Bug reports | classification | machine learning | priority prediction | software maintenance,IEEE Access,2018-06-29,Article,"Umer, Qasim;Liu, Hui;Sultan, Yasir",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85111461626,10.1109/ICSE-SEIP52600.2021.00020,D2A: A Dataset Built for AI-Based Vulnerability Detection Methods Using Differential Analysis,"Static analysis tools are widely used for vulnerability detection as they understand programs with complex behavior and millions of lines of code. Despite their popularity, static analysis tools are known to generate an excess of false positives. The recent ability of Machine Learning models to understand programming languages opens new possibilities when applied to static analysis. However, existing datasets to train models for vulnerability identification suffer from multiple limitations such as limited bug context, limited size, and synthetic and unrealistic source code. We propose D2A, a differential analysis based approach to label issues reported by static analysis tools. The D2A dataset is built by analyzing version pairs from multiple open source projects. From each project, we select bug fixing commits and we run static analysis on the versions before and after such commits. If some issues detected in a before-commit version disappear in the corresponding after-commit version, they are very likely to be real bugs that got fixed by the commit. We use D2A to generate a large labeled dataset to train models for vulnerability identification. We show that the dataset can be used to build a classifier to identify possible false alarms among the issues reported by static analysis, hence helping developers prioritize and investigate potential true positives first. © 2021 IEEE.",Auto-labeler | Dataset | Vulnerability detection,Proceedings - International Conference on Software Engineering,2021-05-01,Conference Paper,"Zheng, Yunhui;Pujar, Saurabh;Lewis, Burn;Buratti, Luca;Epstein, Edward;Yang, Bo;Laredo, Jim;Morari, Alessandro;Su, Zhong",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85097534520,10.1002/smr.2330,MPT-embedding: An unsupervised representation learning of code for software defect prediction,"Software project defect prediction can help developers allocate debugging resources. Existing software defect prediction models are usually based on machine learning methods, especially deep learning. Deep learning-based methods tend to build end-to-end models that directly use source code-based abstract syntax trees (ASTs) as input. They do not pay enough attention to the front-end data representation. In this paper, we propose a new framework to represent source code called multiperspective tree embedding (MPT-embedding), which is an unsupervised representation learning method. MPT-embedding parses the nodes of ASTs from multiple perspectives and encodes the structural information of a tree into a vector sequence. Experiments on both cross-project defect prediction (CPDP) and within-project defect prediction (WPDP) show that, on average, MPT-embedding provides improvements over the state-of-the-art method. © 2020 John Wiley & Sons, Ltd.",deep learning | defect prediction | representation learning | tree embedding,Journal of Software: Evolution and Process,2021-04-01,Article,"Shi, Ke;Lu, Yang;Liu, Guangliang;Wei, Zhenchun;Chang, Jingfei",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85074701182,10.1109/TSE.2018.2889771,Coverage Prediction for Accelerating Compiler Testing,"Compilers are one of the most fundamental software systems. Compiler testing is important for assuring the quality of compilers. Due to the crucial role of compilers, they have to be well tested. Therefore, automated compiler testing techniques (those based on randomly generated programs) tend to run a large number of test programs (which are test inputs of compilers). The cost for compilation and execution for these test programs is significant. These techniques can take a long period of testing time to detect a relatively small number of compiler bugs. That may cause many practical problems, e.g., bringing a lot of costs including time costs and financial costs, and delaying the development/release cycle. Recently, some approaches have been proposed to accelerate compiler testing by executing test programs that are more likely to trigger compiler bugs earlier according to some criteria. However, these approaches ignore an important aspect in compiler testing: different test programs may have similar test capabilities (i.e., testing similar functionalities of a compiler, even detecting the same compiler bug), which may largely discount their acceleration effectiveness if the test programs with similar test capabilities are executed all the time. Test coverage is a proper approximation to help distinguish them, but collecting coverage dynamically is infeasible in compiler testing since most test programs are generated on the fly by automatic test-generation tools like Csmith. In this paper, we propose the first method to predict test coverage statically for compilers, and then propose to prioritize test programs by clustering them according to the predicted coverage information. The novel approach to accelerating compiler testing through coverage prediction is called COP (short for COverage Prediction). Our evaluation on GCC and LLVM demonstrates that COP significantly accelerates compiler testing, achieving an average of 51.01 percent speedup in test execution time on an existing dataset including three old release versions of the compilers and achieving an average of 68.74 percent speedup on a new dataset including 12 latest release versions. Moreover, COP outperforms the state-of-the-art acceleration approach significantly by improving 17.16\%∼82.51\%17.16%∼82.51% speedups in different settings on average. © 1976-2012 IEEE.",5G | D2D | network node planning,IEEE Access,2019-01-01,Article,"Han, Zidong;Liang, Junyu",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85106816941,10.1109/ACCESS.2021.3049823,A Novel Four-Way Approach Designed with Ensemble Feature Selection for Code Smell Detection,"Purpose: Code smells are residuals of technical debt induced by the developers. They hinder evolution, adaptability and maintenance of the software. Meanwhile, they are very beneficial in indicating the loopholes of problems and bugs in the software. Machine learning has been extensively used to predict Code Smells in research. The current study aims to optimise the prediction using Ensemble Learning and Feature Selection techniques on three open-source Java data sets. Design and Results: The work Compares four varied approaches to detect code smells using four performance measures Accuracy(P1), G-mean1 (P2), G-mean2 (P3), and F-measure (P4). The study found out that values of the performance measures did not degrade it instead of either remained same or increased with feature selection and Ensemble Learning. Random Forest turns out to be the best classifier while Correlation-based Feature selection(BFS) is best amongst Feature Selection techniques. Ensemble Learning aggregators, i.e. ET5C2 (BFS intersection Relief with classifier Random Forest), ET6C2 (BFS union Relief with classifier Random Forest), and ET5C1 (BFS intersection Relief with Bagging) and Majority Voting give best results from all the aggregation combinations studied. Conclusion: Though the results are good, but using Ensemble learning techniques needs a lot of validation for a variety of data sets before it can be standardised. The Ensemble Learning techniques also pose a challenge concerning diversity and reliability and hence needs exhaustive studies. © 2013 IEEE.",Aggregator | code smell | ensemble | feature selection | machine learning | open-source projects and performance measures,IEEE Access,2021-01-01,Article,"Kaur, Inderpreet;Kaur, Arvinder",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85101742244,10.1049/iet-sen.2020.0119,Software defect prediction using k-pca and various kernel-based extreme learning machine: An empirical study,"Predicting defects during software testing reduces an enormous amount of testing effort and help to deliver a high-quality software system. Owing to the skewed distribution of public datasets, software defect prediction (SDP) suffers from the class imbalance problem, which leads to unsatisfactory results. Overfitting is also one of the biggest challenges for SDP. In this study, the authors performed an empirical study of these two problems and investigated their probable solution. They have conducted 4840 experiments over five different classifiers using eight NASA projects and 14 PROMISE repository datasets. They suggested and investigated the varying kernel function of an extreme learning machine (ELM) along with kernel principal component analysis (K-PCA) and found better results compared with other classical SDP models. They used the synthetic minority oversampling technique as a sampling method to address class imbalance problems and k-fold cross-validation to avoid the overfitting problem. They found ELM-based SDP has a high receiver operating characteristic curve over 11 out of 22 datasets. The proposed model has higher precision and F-score values over ten and nine, respectively, compared with other state-of-the-art models. The Mathews correlation coefficient (MCC) of 17 datasets of the proposed model surpasses other classical models' MCC. © The Institution of Engineering and Technology 2020.",,IET Software,2020-12-30,Article,"Pandey, Sushant Kumar;Rathee, Deevashwer;Tripathi, Anil Kumar",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85097203127,10.1145/3368089.3409723,MTFuzz: Fuzzing with a multi-task neural network,"Fuzzing is a widely used technique for detecting software bugs and vulnerabilities. Most popular fuzzers generate new inputs using an evolutionary search to maximize code coverage. Essentially, these fuzzers start with a set of seed inputs, mutate them to generate new inputs, and identify the promising inputs using an evolutionary fitness function for further mutation.Despite their success, evolutionary fuzzers tend to get stuck in long sequences of unproductive mutations. In recent years, machine learning (ML) based mutation strategies have reported promising results. However, the existing ML-based fuzzers are limited by the lack of quality and diversity of the training data. As the input space of the target programs is high dimensional and sparse, it is prohibitively expensive to collect many diverse samples demonstrating successful and unsuccessful mutations to train the model.In this paper, we address these issues by using a Multi-Task Neural Network that can learn a compact embedding of the input space based on diverse training samples for multiple related tasks (i.e.,predicting for different types of coverage). The compact embedding can guide the mutation process by focusing most of the mutations on the parts of the embedding where the gradient is high. MTFuzz uncovers 11 previously unseen bugs and achieves an average of 2× more edge coverage compared with 5 state-of-the-art fuzzer on 10 real-world programs © 2020 ACM.",Fuzzing | Machine learning | Multi-task learning,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2020-11-08,Conference Paper,"She, Dongdong;Krishna, Rahul;Yan, Lu;Jana, Suman;Ray, Baishakhi",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85097808866,10.23919/EECSI50503.2020.9251874,Software defect prediction using neural network based smote,"Software defect prediction is a practical approach to improve the quality and efficiency of time and costs for software testing by focusing on defect modules. The dataset of software defect prediction naturally has a class imbalance problem with very few defective modules compared to non-defective modules. This situation has a negative impact on the Neural Network, which can lead to overfitting and poor accuracy. Synthetic Minority Over-sampling Technique (SMOTE) is one of the popular techniques that can solve the problem of class imbalance. However, Neural Network and SMOTE both have hyperparameters which must be determined by the user before the modelling process. In this study, we applied the Neural Networks Based SMOTE, a combination of Neural Network and SMOTE with each hyperparameter of SMOTE and Neural Network that are optimized using random search to solve the class imbalance problem in the six NASA datasets. The results use a 5*5 cross-validation show that increases Bal by 25.48% and Recall by 45.99% compared to the original Neural Network. We also compare the performance of Neural Network-based SMOTE with ”Traditional” Machine Learning-based SMOTE. The Neural Network-based SMOTE takes first place in the average rank. © 2020 Institute of Advanced Engineering and Science (IAES). All Rights Reserved.",Class Imbalance | Neural Network | Software Defect Prediction | Synthetic Minority Over-sampling Technique,"International Conference on Electrical Engineering, Computer Science and Informatics (EECSI)",2020-10-01,Conference Paper,"Bahaweres, Rizal Broer;Agustian, Fajar;Hermadi, Irman;Suroso, Arif Imam;Arkeman, Yandra",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85089501673,10.34028/iajit/17/5/5,A novel feature selection method based on maximum likelihood logistic regression for imbalanced learning in software defect prediction,"The most frequently used machine learning feature ranking approaches failed to present optimal feature subset for accurate prediction of defective software modules in out-of-sample data. Machine learning Feature Selection (FS) algorithms such as Chi-Square (CS), Information Gain (IG), Gain Ratio (GR), RelieF (RF) and Symmetric Uncertainty (SU) perform relatively poor at prediction, even after balancing class distribution in the training data. In this study, we propose a novel FS method based on the Maximum Likelihood Logistic Regression (MLLR). We apply this method on six software defect datasets in their sampled and unsampled forms to select useful features for classification in the context of Software Defect Prediction (SDP). The Support Vector Machine (SVM) and Random Forest (RaF) classifiers are applied on the FS subsets that are based on sampled and unsampled datasets. The performance of the models captured using Area Ander Receiver Operating Characteristics Curve (AUC) metrics are compared for all FS methods considered. The Analysis Of Variance (ANOVA) F-test results validate the superiority of the proposed method over all the FS techniques, both in sampled and unsampled data. The results confirm that the MLLR can be useful in selecting optimal feature subset for more accurate prediction of defective modules in software development process. © 2020, Zarka Private University. All rights reserved.",big data Spark | cloud computing software framework | empirical mode decomposition | intelligent fault recognition | mesos cluster manager | mobile robotic roller bearing | parallel deep belief network | Parallel machine learning algorithm | parallel support vector machine,IEEE Access,2020-01-01,Article,"Xian, Guangming",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85102749792,10.1109/ICCCNT49239.2020.9225352,A Comparative Analysis for Machine Learning based Software Defect Prediction Systems,"In the Software Engineering concept, the prediction of the software defects plays a vital role in increasing the quality of the software systems, which is one of the most critical and expensive phases of the software development lifecycle. While the use of software systems is increasing in our daily lives, their dependencies and complexities are also increasing, and this results in a suitable environment for defects. Due to the existence of software defects, the software produces incorrect results and behaviors. What is more critical than defects, is finding them before they occur. Therefore detection (and also prediction) of the software defects enables the managers of the software to make an efficient allocation of the resources for the maintenance and testing phases. In the literature, there are different proposals for the prediction of software defects. In this paper, we made a comparative analysis about the machine learning-based software defect prediction systems by comparing 10 learning algorithms like Decision Tree, Naive Bayes, K-Nearest Neighbor, Support Vector Machine, Random Forest, Extra Trees, Adaboost, Gradient Boosting, Bagging, and Multi-Layer Perceptron, on the public datasets CM1, KC1, KC2, JM1, and PC1 from the PROMISE warehouse. The experimental results showed that proposed models result in proper accuracy levels for software defect prediction to increase the quality of the software. © 2020 IEEE.",Classification algorithms | Fault diagnosis | Fault-tolerant controller | Heavy road vehicle | Machine learning | Random forests | Sliding mode controller | Supervised learning,IEEE Access,2020-01-01,Article,"Raveendran, Radhika;Devika, K. B.;Subramanian, Shankar C.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85096232168,10.1007/s10515-020-00270-x,Algorithm selection for software validation based on graph kernels,"Algorithm selection is the task of choosing an algorithm from a given set of candidate algorithms when faced with a particular problem instance. Algorithm selection via machine learning (ML) has recently been successfully applied for various problem classes, including computationally hard problems such as SAT. In this paper, we study algorithm selection for software validation, i.e., the task of choosing a software validation tool for a given validation instance. A validation instance consists of a program plus properties to be checked on it. The application of machine learning techniques to this task first of all requires an appropriate representation of software. To this end,we propose a dedicated kernel function, which compares two programs in terms of their similarity, thus making the algorithm selection task amenable to kernel-based machine learning methods. Our kernel operates on a graph representation of source code mixing elements of control-flow and program-dependence graphs with abstract syntax trees.Thus, given two such representations as input, the kernel function yields a real-valued score that can be interpreted as a degree of similarity. We experimentally evaluate our kernel in two learning scenarios, namely a classification and a ranking problem: (1) selecting between a verification and a testing tool for bug finding (i.e., property violation), and (2) ranking several verification tools,from presumably best to worst, for property proving. The evaluation, which is based on data sets from the annual software verification competition SV-COMP, demonstrates our kernel to generalize well and to achieve rather high prediction accuracy, both for the classification and the ranking task. © 2020, The Author(s).",bug tracking system | convolutional neural network | deep learning | Duplicate bug report detection | natural language processing | Siamese networks | software development | software engineering | software maintenance,IEEE Access,2020-01-01,Article,"Kukkar, Ashima;Mohana, Rajni;Kumar, Yugal;Nayyar, Anand;Bilal, Muhammad;Kwak, Kyung Sup",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85097199941,10.1109/ICCSEA49143.2020.9132886,Prediction of Accuracy on Open Source Java Projects Using Class Level Refactoring,"The refactoring approach is used to restructure the software system without altering its external functionalities. Our research objective is to analyze the accuracy of the software metrics using machine learning classifiers. After predicting the accuracy we can refactor the inaccurate source metrics by using different refactoring tools and machine learning algorithms. We have considered 30 code metrics from 3 open source java projects by using different framework like antlr4,junit and oryx at the class level and predict its accuracy using different machine learning classifiers. We have used Gaussian, Bernoulli and Multinomial classifiers to predict the accuracy of the software metrics. Statistical significant test reveals the mean accuracy for the above sources are 33.33%,39%,48.33% respectively. As per the prediction analysis we can correct the fault and improve the performance of our source metrics by conducting unit level testing which leads to software refactoring. © 2020 IEEE.",bug fixing | bug report | Bug triage | CNN | deep learning | ELMo | GloVe | recommending bug fixer | word embedding | word representation | Word2Vec,IEEE Access,2020-01-01,Article,"Zaidi, Syed Farhan Alam;Awan, Faraz Malik;Lee, Minsoo;Woo, Honguk;Lee, Chan Gun",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85074566426,10.1007/s11219-019-09460-7,An empirical study of factors affecting cross-project aging-related bug prediction with TLAP,"Software aging is a phenomenon in which long-running software systems show an increasing failure rate and/or progressive performance degradation. Due to their nature, Aging-Related Bugs (ARBs) are hard to discover during software testing and are also challenging to reproduce. Therefore, automatically predicting ARBs before software release can help developers reduce ARB impact or avoid ARBs. Many bug prediction approaches have been proposed, and most of them show effectiveness in within-project prediction settings. However, due to the low presence and reproducing difficulty of ARBs, it is usually hard to collect sufficient training data to build an accurate prediction model. A recent work proposed a method named Transfer Learning based Aging-related bug Prediction (TLAP) for performing cross-project ARB prediction. Although this method considerably improves cross-project ARB prediction performance, it has been observed that its prediction result is affected by several key factors, such as the normalization methods, kernel functions, and machine learning classifiers. Therefore, this paper presents the first empirical study to examine the impact of these factors on the effectiveness of cross-project ARB prediction in terms of single-factor pattern, bigram pattern, and triplet pattern and validates the results with the Scott-Knott test technique. We find that kernel functions and classifiers are key factors affecting the effectiveness of cross-project ARB prediction, while normalization methods do not show statistical influence. In addition, the order of values in three single-factor patterns is maintained in three bigram patterns and one triplet pattern to a large extent. Similarly, the order of values in the three bigram patterns is also maintained in the triplet pattern. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Aging-related bugs | Cross-project | Empirical study | Software aging,Software Quality Journal,2020-03-01,Article,"Qin, Fangyun;Wan, Xiaohui;Yin, Beibei",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85073827187,10.1007/s11219-019-09467-0,Cross-project bug type prediction based on transfer learning,"The prediction of bug types provides useful insights into the software maintenance process. It can improve the efficiency of software testing and help developers adopt corresponding strategies to fix bugs before releasing software projects. Typically, the prediction tasks are performed through machine learning classifiers, which rely heavily on labeled data. However, for a software project that has insufficient labeled data, it is difficult to train the classification model for predicting bug types. Although labeled data of other projects can be used as training data, the results of the cross-project prediction are often poor. To solve this problem, this paper proposes a cross-project bug type prediction framework based on transfer learning. Transfer learning breaks the assumption of traditional machine learning methods that the training set and the test set should follow the same distribution. Our experiments show that the results of cross-project bug type prediction have significant improvement by adopting transfer learning. In addition, we have studied the factors that influence the prediction results, including different pairs of source and target projects, and the number of bug reports in the source project. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Bug prediction | Bug report | Cross-project | Transfer learning,Software Quality Journal,2020-03-01,Article,"Du, Xiaoting;Zhou, Zenghui;Yin, Beibei;Xiao, Guanping",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85097819898,10.1155/2020/6688075,Investigating tree family machine learning techniques for a predictive system to unveil software defects,"Software defects prediction at the initial period of the software development life cycle remains a critical and important assignment. Defect prediction and correctness leads to the assurance of the quality of software systems and has remained integral to study in the previous years. The equick forecast of imperfect or defective modules in software development can serve the development squad to use the existing assets competently and effectively to provide remarkable software products in a given short timeline. Hitherto, several researchers have industrialized defect prediction models by utilizing statistical and machine learning techniques that are operative and effective approaches to pinpoint the defective modules. Tree family machine learning techniques are well-thought-out to be one of the finest and ordinarily used supervised learning methods. In this study, different tree family machine learning techniques are employed for software defect prediction using ten benchmark datasets. These techniques include Credal Decision Tree (CDT), Cost-Sensitive Decision Forest (CS-Forest), Decision Stump (DS), Forest by Penalizing Attributes (Forest-PA), Hoeffding Tree (HT), Decision Tree (J48), Logistic Model Tree (LMT), Random Forest (RF), Random Tree (RT), and REP-Tree (REP-T). Performance of each technique is evaluated using different measures, i.e., mean absolute error (MAE), relative absolute error (RAE), root mean squared error (RMSE), root relative squared error (RRSE), specificity, precision, recall, F-measure (FM), G-measure (GM), Matthew's correlation coefficient (MCC), and accuracy. The overall outcomes of this paper suggested RF technique by producing best results in terms of reducing error rates as well as increasing accuracy on five datasets, i.e., AR3, PC1, PC2, PC3, and PC4. The average accuracy achieved by RF is 90.2238%. The comprehensive outcomes of this study can be used as a reference point for other researchers. Any assertion concerning the enhancement in prediction through any new model, technique, or framework can be benchmarked and verified. Copyright © 2020 Rashid Naseem et al.",,Complexity,2020-01-01,Article,"Naseem, Rashid;Khan, Bilal;Ahmad, Arshad;Almogren, Ahmad;Jabeen, Saima;Hayat, Bashir;Shah, Muhammad Arif",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85079689835,10.1007/978-981-15-0199-9_64,Class Imbalance in Software Fault Prediction Data Set,"Classification has been the prominent technique in machine learning domain, due to its ability of forecasting and predicts capabilities it is widely used in various domains such as health care, networking, social network, and software engineering with enhancement of different algorithm. The performance of the classifier majorly depends on the quality and amount of data present in the training sample. In real-world scenario, the majority of training samples suffered from class imbalance problem, that is, most of the data samples belong to one particular category, i.e., majority class while very few represent the minority class. In this case, classification techniques tend to be overwhelmed by the majority class and ignore the minority class. To solve class imbalance problem people relay on the different kind of sampling techniques either by generating synthetic data or by concentrating on minority class samples, but those approaches have introduced adverse effect in the learnability. In this paper, we attempt to study different techniques proposed to solve the class imbalance problem. © Springer Nature Singapore Pte Ltd 2020.",Class imbalance | Classification | Machine learning | Majority | Minority | Sampling | Training,Advances in Intelligent Systems and Computing,2020-01-01,Conference Paper,"Arun, C.;Lakshmi, C.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85074686452,10.1007/s11227-019-03051-w,Software defect prediction using over-sampling and feature extraction based on Mahalanobis distance,"As the size of software projects becomes larger, software defect prediction (SDP) will play a key role in allocating testing resources reasonably, reducing testing costs, and speeding up the development process. Most SDP methods have used machine learning techniques based on common software metrics such as Halstead and McCabe’s cyclomatic. Datasets produced by these metrics usually do not follow Gaussian distribution, and also, they have overlaps in defect and non-defect classes. In addition, in many of software defect datasets, the number of defective modules (minority class) is considerably less than non-defective modules (majority class). In this situation, the performance of machine learning methods is reduced dramatically. Therefore, we first need to create a balance between minority and majority classes and then transfer the samples into a new space in which pair samples with same class (must-link set) are near to each other as close as possible and pair samples with different classes (cannot-link) stay as far as possible. To achieve the mentioned objectives, in this paper, Mahalanobis distance in two manners will be used. First, the minority class is oversampled based on the Mahalanobis distance such that generated synthetic data are more diverse from other minority data, and minority class distribution is not changed significantly. Second, a feature extraction method based on Mahalanobis distance metric learning is used which try to minimize distances of sample pairs in must-links and maximize the distance of sample pairs in cannot-links. To demonstrate the effectiveness of the proposed method, we performed some experiments on 12 publicly available datasets which are collected NASA repositories and compared its result by some powerful previous methods. The performance is evaluated in F-measure, G-Mean, and Matthews correlation coefficient. Generally, the proposed method has better performance as compared to the mentioned methods. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Feature extraction | Mahalanobis distance | Over-sampling | Software defect prediction | Software metrics,Journal of Supercomputing,2020-01-01,Article,"NezhadShokouhi, Mohammad Mahdi;Majidi, Mohammad Ali;Rasoolzadegan, Abbas",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85092396833,10.1109/ICMLA.2019.00292,A cost-sensitive approach to enhance the use of ML classifiers in software testing efforts,"The use of Machine Learning (ML) classifiers to predict defective software modules are useful to help on planning software testing activities. Most of those studies use the accuracy as the main metric to evaluate the quality of the ML classifier. However, when unbalanced datasets are used to train and test the classifier, the ML model becomes biased. Biased ML models hide their real accuracy. In this context, this study proposes an approach to enhance the use of ML classifiers for predicting defective software modules even with unbalanced datasets. The results indicate: (1) a significant reduction on the number of false negatives; (2) a considerable gain on the efficacy of the software testing; (3) an increase of the number of modules correctly indicated as defective; however, there were also (4) an increase of the scope of the test suggested by the model; (5) a reduction of the software testing efficiency; (6) an increase of the number of the false positives; and (7) reduction of the overall accuracy. Therefore, the proposed approach imposes a trade-off to be considered when planning the software testing activities. Finally, this study also proposes an approach to help managers to deal with those trade-offs considering the resource constraints. © 2019 IEEE.",Attribute Rule | Data Mining | Datasets Model | Machine Learning | ONER | Software Deformity Prophecy,"2019 International Conference on Advances in the Emerging Computing Technologies, AECT 2019",2020-02-01,Conference Paper,"Shaikh, Salahuddin;Changan, Liu;Malik, Maaz Rasheed",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85088402861,10.1109/IACC48062.2019.8971588,Software bug count prediction via AdaBoost.R-ET,"Finding the bug count in software modules in order to utilized limited testing resources that could help the testers is a challenging task. Mostly the regression methods are used for finding the bug count. AdaBoost is the one of machine learning technique which has been used for both classification and regression problems. However, it cannot perform better if the weak learner cannot accomplish at least 50% accuracy when running on the skewed dataset. Extra Tree regression is computationally effective and delivers excellent prediction performance. In this paper, we present a new approach, AdaBoost.R-ET algorithm to predict the bug count in a software module. We used the extra tree regression as a weak learner of AdaBoost regression to provide a significant improvement over the model. An experimental study is performed on five projects from the PROMISE repository consisting of 15 different versions. To estimate the performance of the AdaBoost.R-ET algorithm, we compare this algorithm with varying models of AdaBoost. The outcomes show that the proposed model is better than the different variants of AdaBoost models. © 2019 IEEE.",Classification | Fault diagnostic | Power system,IEEE AFRICON Conference,2019-09-01,Conference Paper,"Moloi, K.;Jordaan, J. A.;Abe, B. T.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85084755652,10.1109/AITC.2019.8921374,Software Defect Prediction using Hybrid Approach,"Defective software modules have significant impact over software quality leading to system crashes and software running error. Thus, Software Defect Prediction (SDP) mechanisms become essential part to enhance quality assurance activities, to allocate effort and resources more efficiently. Various machine learning approaches have been proposed to remove fault and unnecessary data. However, the imbalance distribution of software defects still remains as challenging task and leads to loss accuracy for most SDP methods. To overcome it, this paper proposed a hybrid method, which combine Support Vector Machine (SVM)-Radial Basis Function (RBF) as base learner for Adaptive Boost, with the use of Minimum-Redundancy-Maximum-Relevance (MRMR) feature selection. Then, the comparative analysis applied based on 5 datasets from NASA Metrics Data Program. The experimental results showed that hybrid approach with MRMR give better accuracy compared to SVM single learner, which is effective to deal with the imbalance datasets because the proposed method have good generalization and better performance measures. © 2019 IEEE.",clone consistency prediction | Code clones | cross-project | machine learning,"Proceedings - 2019 International Conference on Artificial Intelligence and Advanced Manufacturing, AIAM 2019",2019-10-01,Conference Paper,"Zhang, Fanlong",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85077081528,10.1109/KSE.2019.8919292,"Combining feature selection, feature learning and ensemble learning for software fault prediction","This paper studies a combination of feature selection and ensemble learning to address the feature redundancy and class imbalance problems in software fault prediction. Also, a deep learning model is used to generate deep representation from defect data to improve the performance of fault prediction models. The proposed method, GFsSDAEsTSE, is evaluated on 12 NASA datasets, and the results show that GFsSDAEsTSE outperforms state-of-the-art methods in both small and large datasets. © 2019 IEEE.",Autoencoder | Ensemble learning | Feature generation | Feature selection | Software fault prediction,"Proceedings of 2019 11th International Conference on Knowledge and Systems Engineering, KSE 2019",2019-10-01,Conference Paper,"Tran, Hung Duy;Hanh, Le Thi My;Binh, Nguyen Thanh",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85055479920,10.1007/s10664-018-9656-z,High-level software requirements and iteration changes: a predictive model,"Knowing whether a software feature will be completed in its planned iteration can help with release planning decisions. However, existing research has focused on predictions of only low-level software tasks, like bug fixes. In this paper, we describe a mixed-method empirical study on three large IBM projects. We investigated the types of iteration changes that occur. We show that up to 54% of high-level requirements do not make their planned iteration. Requirements are most often pushed out to the next iteration, but high-level requirements are also commonly moved to the next minor or major release or returned to the product or release backlog. We developed and evaluated a model that uses machine learning to predict if a high-level requirement will be completed within its planned iteration. The model includes 29 features that were engineered based on prior work, interviews with IBM developers, and domain knowledge. Predictions were made at four different stages of the requirement lifetime. Our model is able to achieve up to 100% precision. We ranked the importance of our model features and found that some features are highly dependent on project and prediction stage. However, some features (e.g., the time remaining in the iteration and creator of the requirement) emerge as important across all projects and stages. We conclude with a discussion on future research directions. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",Completion prediction | Machine learning | Mining software repositories | Release planning | Software requirements,Empirical Software Engineering,2019-06-15,Article,"Blincoe, Kelly;Dehghan, Ali;Salaou, Abdoul Djawadou;Neal, Adam;Linaker, Johan;Damian, Daniela",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85069634081,,Machine learning methods for software defect prediction a revisit.,"Software defect prediction (SDP) is a challenging factor in the area of Computer Science. Software engineering is the fertile ground to each and every computer science project, which results the Computers the feature to develop the planning on an accurate job by means data. ML Machine based learning was enhanced by those implemented research on Pattern Identification with Computational intelligence based on Artificial Intelligence (AI)”. These (ML) Machine based knowledge tactics are boosted in resolving those faults which are occurring from validation in addition with Domain based systems. Those programming-based difficulties which are designated as the procedure-oriented knowledge with in those situations and alterations. A predictive model which is measured into two ways. First one is Defective Module and second one is Non-defective Module. The two predictive modules are formed by using (ML) Machine Learning techniques. Machine learning methods be cooperative in software defect prediction. For the existing data sets are collected from NASA and Eclipse from promise repository which is a motivated version of UCI repository which is developed in 2005. We have a lot of learning ways to notice defects in software. Here we are revisiting the ML methods for SDP (software defect prediction). © 2019, Blue Eyes Intelligence Engineering and Sciences Publication. All rights reserved.",ML Techniques; Performance measures; Predictive analytics; Software defect prediction,International Journal of Innovative Technology and Exploring Engineering,2019,,"Venkata Raghava Rao Y., Burri R.D., Prasad V.B.V.N.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85070686200,10.1109/IoT-SIU.2019.8777507,Substantiation of Software Defect Prediction using Statistical Learning: An Empirical Study,"Defect Prediction plays a very important role in the development of the software. The defect prediction leads to a more focused testing thereby reducing the testing cost and efforts. In this paper an empirical study is done to investigate the predictive performance of the statistical model for defect prediction. Three open source software projects are taken for the experiment. The Select K Best algorithm is used for feature selection. Random Forest, Gradient Boosting, K Nearest Neighbors, Decision Tree and Naïve Bayes are used as the statistical models. The data models are analyzed using various performance measures. The findings of this study infer that Select-K-Best with chi-square score function with the K Nearest Neighbors gives the optimal performance for software defect prediction. © 2019 IEEE.",classification | defect prediction | machine learning | Select K-Best,"Proceedings - 2019 4th International Conference on Internet of Things: Smart Innovation and Usages, IoT-SIU 2019",2019-04-01,Conference Paper,"Agarwal, Shiwang;Gupta, Sajal;Aggarwal, Rishabh;Maheshwari, Shashank;Goel, Lipika;Gupta, Sonam",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84960942189,10.1007/978-981-13-6459-4_24,Software defect prediction using principal component analysis and naïve bayes algorithm,"How can I deliver defect-free software? Can I achieve more with less resources? How can I reduce time, effort, and cost involved in developing software? Software defect prediction is an important area of research which can significantly help the software development teams grappling with these questions in an effective way. A small increase in prediction accuracy will go a long way in helping software development teams improve their efficiency. In this paper, we have proposed a framework which uses PCA for dimensionality reduction and Naïve Bayes classification algorithm for building the prediction model. We have used seven projects from NASA Metrics Data Program for conducting experiments. We have seen an average increase of 10.3% in prediction accuracy when the learning algorithm is applied with the key features extracted from the datasets. © Springer Nature Singapore Pte Ltd 2019.",code testability | correlation analysis | human analyst | machine learning | requirement testability | statistical analysis | subjective assessment | supervised classification learning,"2nd International Workshop on Artificial Intelligence for Requirements Engineering, AIRE 2015 - Proceedings",2015-11-25,Conference Paper,"Hayes, Jane Huffman;Li, Wenbin;Yu, Tingting;Han, Xue;Hays, Mark;Woodson, Clinton",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85076184495,10.1007/978-3-030-33709-4_5,Effect of Feature Selection in Software Fault Detection,"The quality of software is enormously affected by the faults associated with it. Detection of faults at a proper stage in software development is a challenging task and plays a vital role in the quality of the software. Machine learning is, now a days, a commonly used technique for fault detection and prediction. However, the effectiveness of the fault detection mechanism is impacted by the number of attributes in the publicly available datasets. Feature selection is the process of selecting a subset of all the features that are most influential to the classification and it is a challenging task. This paper thoroughly investigates the effect of various feature selection techniques on software fault classification by using NASA’s some benchmark publicly available datasets. Various metrics are used to analyze the performance of the feature selection techniques. The experiment discovers that the most important and relevant features can be selected by the adopted feature selection techniques without sacrificing the performance of fault detection. © Springer Nature Switzerland AG 2019.",Fault detection | Feature classification | Feature selection,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Tasnim Cynthia, Shamse;Rasul, Md Golam;Ripon, Shamim",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85071367948,10.18293/SEKE2019-070,Cross-project defect prediction via transferable deep learning-generated and handcrafted features,"Although the machine learning-based software defect prediction (SDP) method has shown promising value in software engineering, yet challenges remain. To improve the performance of SDP, some researchers have used deep learning algorithms to extract the semantic and structural features of the program. However, in more practical cross-project defect prediction (CPDP) tasks, whether deep learning-generated features can be directly used should be explored due to the data distribution shift that usually exists in different projects. In this paper, we propose a Transferable Hybrid Features Learning with Convolutional Neural Network (CNN-THFL) framework to conduct CPDP. Specially, CNN-THFL mines deep learning-generated features from token vectors extracted from programs' abstract syntax trees via convolutional neural network. Furthermore, CNN-THFL learns the transferable joint features simultaneously considering deep learning-generated and handcrafted features by applying a transfer component analysis algorithm. Finally, the features generated by CNN-THFL are fed to the classifier to train a defect prediction model. Extensive experiments verify that CNN-THFL can outperform referential methods on 72 pairs of CPDP tasks formed by 9 open-source projects. © 2019 Knowledge Systems Institute Graduate School. All rights reserved.",Cross-project defect prediction | Semantic feature learning | Software defect prediction | Transfer learning,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",2019-01-01,Conference Paper,"Qiu, Shaojian;Lu, Lu;Cai, Ziyi;Jiang, Siyu",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85068343269,10.1007/978-3-030-23281-8_20,Bug Severity Prediction Using a Hierarchical One-vs.-Remainder Approach,"Assigning severity level to reported bugs is a critical part of software maintenance to ensure an efficient resolution process. In many bug trackers, e.g.Â Bugzilla, this is a time consuming process, because bug reporters must manually assign one of seven severity levels to each bug. In addition, some bug types may be reported more often than others, leading to a disproportionate distribution of severity labels. Machine learning techniques can be used to predict the label of a newly reported bug automatically. However, learning from imbalanced data in a multi-class task remains one of the major difficulties for machine learning classifiers. In this paper, we propose a hierarchical classification approach that exploits class imbalance in the training data, to reduce classification bias. Specifically, we designed a classification tree that consists of multiple binary classifiers organised hierarchically, such that instances from the most dominant class are trained against the remaining classes but are not used for training the next level of the classification tree. We used FastText classifier to test and compare between the hierarchical and standard classification approaches. Based on 93,051 bug reports from 38 Eclipse open-source products, the hierarchical approach was shown to perform relatively well with 65 % Micro F-Score and 45 % Macro F-Score. © 2019, Springer Nature Switzerland AG.",Bug severity | FastText | Imbalanced data | Machine learning | Multi-class classification | Text mining,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Nnamoko, Nonso;Cabrera-Diego, Luis Adrián;Campbell, Daniel;Korkontzelos, Yannis",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85123494064,10.5220/0007727702690276,A practical guide to support change-proneness prediction,"During the development and maintenance of a system of software, changes can occur due to new features, bug fix, code refactoring or technological advancements. In this context, software change prediction can be very useful in guiding the maintenance team to identify change-prone classes in early phases of software development to improve their quality and make them more flexible for future changes. A myriad of related works use machine learning techniques to lead with this problem based on different kinds of metrics. However, inadequate description of data source or modeling process makes research results reported in many works hard to interpret or reproduce. In this paper, we firstly propose a practical guideline to support change-proneness prediction for optimal use of predictive models. Then, we apply the proposed guideline over a case study using a large imbalanced data set extracted from a wide commercial software. Moreover, we analyze some papers which deal with change-proneness prediction and discuss them about missing points. Copyright © 2019 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.",FBG Sensor | GPON | Passive Optical Network | Support vector Machine,"Proceeding - 2021 26th IEEE Asia-Pacific Conference on Communications, APCC 2021",2021-01-01,Conference Paper,"Usman, Auwalu;Zulkifli, Nadiatulhuda;Salim, Mohd Rashidi;Khairi, Kharina",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85111159271,10.1007/978-981-13-3140-4_31,An ANN based approach for software fault prediction using object oriented metrics,"During recent years, the enormous increase in demand for software products has been experienced. High quality software is the major demand of users. Predicting the faults in early stages will improve the quality of software and apparently reduce the development efforts or cost. Fault prediction is majorly based on the selection of technique and the metrics to predict the fault. Thus metrics selection is a critical part of software fault prediction. Currently techniques been evaluated based on traditional set of metrics. There is a need to identify the different techniques and evaluate them on the bases of appropriate metrics. In this research, Artificial neural network is used. For classification task, ANN is one of the most effective technique. Artificial neural network based SFP model is designed for classification in this study. Prediction is performed on the basis of object-oriented metrics. 5 object oriented metrics from CK and Martin metric sets are selected as input parameters. The experiments are performed on 18 public datasets from PROMISE repository. Receiver operating characteristíc curve, accuracy, and Mean squared error are taken as performance parameters for the prediction task. Results of the proposed systems signify that ANN provides significant results in terms of accuracy and error rate. © Springer Nature Singapore Pte Ltd. 2019.",Automatic Program Repair | Code Embeddings | Doc2vec | Machine learning | Patch Correctness,"Proceedings - 2021 IEEE/ACM International Workshop on Automated Program Repair, APR 2021",2021-06-01,Conference Paper,"Csuvik, Viktor;Horváth, Dániel;Lajkó, Márk;Vidács, László",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85049772205,10.1145/3183399.3183402,Combining spreadsheet smells for improved fault prediction,"Spreadsheets are commonly used in organizations as a programming tool for business-related calculations and decision making. Since faults in spreadsheets can have severe business impacts, a number of approaches from general software engineering have been applied to spreadsheets in recent years, among them the concept of code smells. Smells can in particular be used for the task of fault prediction. An analysis of existing spreadsheet smells, however, revealed that the predictive power of individual smells can be limited. In this work we therefore propose a machine learning based approach which combines the predictions of individual smells by using an AdaBoost ensemble classifier. Experiments on two public datasets containing real-world spreadsheet faults show significant improvements in terms of fault prediction accuracy. © 2018 Association for Computing Machinery.",Fault Prediction | Spreadsheet QA | Spreadsheet Smells,Proceedings - International Conference on Software Engineering,2018-05-27,Conference Paper,"Koch, Patrick;Schekotihin, Konstantin;Jannach, Dietmar;Hofer, Birgit;Wotawa, Franz;Schmitz, Thomas",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84946574741,10.1109/SEATUC.2018.8788881,Software defect prediction using random forest algorithm,"The software defect can cause the unnecessary effects on the software such as cost and quality. The prediction of the software defect can be useful for the development of good quality software. For the prediction, the PROMISE public dataset will be used and random forest (RF) algorithm will be applied with the RAPIDMINER machine learning tool. This paper will compare the performance evaluation upon the different number of trees in RF. As the results, the accuracy will be slightly increased if the number of trees will be more. The maximum accuracy is up to 99.59 and the minimum accuracy is 85.96. Another comparison is based on AUC curve that represents the most informative indicator of predictive accuracy within the field of software defect prediction. All of the results show that RF algorithm is effective in this prediction which is more suitable with the usage of hundred trees in the RF. © 2018 IEEE.",Automatic bug triage | Developer recommendation | Developer relations | Software maintenance | Topic model,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",2014-01-01,Conference Paper,"Zhang, Tao;Yang, Geunseok;Lee, Byungjeong;Lua, Eng Keong",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84951840920,10.1109/JEC-ECC.2017.8305803,Predicting bug severity using customized weighted majority voting algorithms,"One of the crucial attributes of bug report is severity. Accurate prediction of bug severity can be a huge contribution towards optimized software maintenance. In this paper, a new model that combines classification techniques based on Customized Cascading Weighted Majority Voting has been proposed. The proposed technique has been evaluated using datasets from open-source projects. The results show that the proposed technique has superior performance compared to other classification techniques. © 2017 IEEE.",False alarm detection | Machine learning | Static analysis,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",2014-01-01,Conference Paper,"Yoon, Jongwon;Jin, Minsik;Jung, Yungbum",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85044395497,10.1145/3172871.3172872,Feature selection techniques to counter class imbalance problem for aging related bug prediction,"Aging-Related Bugs (ARBs) occur in long running systems due to error conditions caused because of accumulation of problems such as memory leakage or unreleased files and locks. Aging-Related Bugs are hard to discover during software testing and also challenging to replicate. Automatic identification and prediction of aging related fault-prone files and classes in an object oriented system can help the software quality assurance team to optimize their testing efforts. In this paper, we present a study on the application of static source code metrics and machine learning techniques to predict aging related bugs. We conduct a series of experiments on publicly available dataset from two large open-source software systems: Linux and MySQL. Class imbalance and high dimensionality are the two main technical challenges in building effective predictors for aging related bugs. We investigate the application of five different feature selection techniques (OneR, Information Gain, Gain Ratio, RELEIF and Symmetric Uncertainty) for dimensionality reduction and five different strategies (Random Under-sampling, Random Oversampling, SMOTE, SMOTEBoost and RUSBoost) to counter the effect of class imbalance in our proposed machine learning based solution approach. Experimental results reveal that the random under-sampling approach performs best followed by RUSBoost in-terms of the mean AUC metric. Statistical significance test demonstrates that there is a significant difference between the performance of the various feature selection techniques. Experimental results shows that Gain Ratio and RELEIF performs best in comparison to other strategies to address the class imbalance problem. We infer from the statistical significance test that there is no difference between the performances of the five different learning algorithms. © 2018 Association for Computing Machinery.",Aging Related Bugs | Empirical software engineering | Feature selection techniques | Imbalance learning | Machine learning | Predictive modeling | Software maintenance | Source code metrics,ACM International Conference Proceeding Series,2018-02-09,Conference Paper,"Kumar, Lov;Sureka, Ashish",Include,
10.1016/j.infsof.2022.107128,,10.14419/ijet.v7i3.12.16024,Assessment and analysis of software reliability using machine learning techniques,"Software reliability models access the reliability by fault prediction. Reliability is a real world phenomenon with many associated real time problems and to obtain solutions to problems quickly, accurately and acceptably a large no. of soft computing techniques has been developed. We attempt to address the software failure problems by modeling software failure data using the machine learning techniques such as support vector machine (SVM) regression and generalized additive models. The study of software reliability can be categorized into three parts: modeling, measurement, improvement. Programming unwavering quality demonstrating has developed to a point that important outcomes can be acquired by applying appropriate models to the issue; there is no single model all inclusive to every one of the circumstances. We propose different machine learning methods for the evaluation of programming unwavering quality, for example, artificial neural networks, support vector machine calculation approached. We at that point break down the outcomes from machine getting the hang of demonstrating, and contrast them with that of some summed up direct displaying procedures that are proportional to programming dependability models. © 2018 Authors.",Artificial neural networks; Machine learning techniques; Software-reliability; Support vector machine,International Journal of Engineering and Technology(UAE),2018,,"Krishna Mohan G., Yoshitha N., Lavanya M.L.N., Krishna Priya A.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85060451624,,Hypergraph learning with cost interval optimization,"In many classification tasks, the misclassification costs of different categories usually vary significantly. Under such circumstances, it is essential to identify the importance of different categories and thus assign different misclassification losses in many applications, such as medical diagnosis, saliency detection and software defect prediction. However, we note that it is infeasible to determine the accurate cost value without great domain knowledge. In most common cases, we may just have the information that which category is more important than the other categories, i.e., the identification of defect-prone softwares is more important than that of defect-free. To tackle these issues, in this paper, we propose a hypergraph learning method with cost interval optimization, which is able to handle cost interval when data is formulated using the high-order relationships. In this way, data correlations are modeled by a hypergraph structure, which has the merit to exploit the underlying relationships behind the data. With a cost-sensitive hypergraph structure, in order to improve the performance of the classifier without precise cost value, we further introduce cost interval optimization to hypergraph learning. In this process, the optimization on cost interval achieves better performance instead of choosing uncertain fixed cost in the learning process. To evaluate the effectiveness of the proposed method, we have conducted experiments on two groups of dataset, i.e., the NASA Metrics Data Program (NASA) dataset and UCI Machine Learning Repository (UCI) dataset. Experimental results and comparisons with state-of-the-art methods have exhibited better performance of our proposed method. Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"32nd AAAI Conference on Artificial Intelligence, AAAI 2018",2018,,"Zhao X., Wang N., Shi H., Wan H., Huang J., Gao Y.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85114049671,10.1109/ICACCI.2017.8126033,Evaluation of sampling techniques in software fault prediction using metrics and code smells,"The highly imbalanced nature of software fault datasets results in poor performance of machine leaning techniques used for software fault prediction. The objective of this paper is to evaluate sampling techniques and Meta-Cost learning in software fault prediction to alleviate problem of imbalanced data. We evaluate four sampling techniques in metrics as well as code smells based fault prediction on fault data sets of two open source systems ANT and POI. Our results indicate that Resample technique is best for metrics based fault prediction whereas Synthetic Minority Oversampling is best suited for code smells based fault prediction. The results are presented in terms of accuracy measures like G-Mean, Fmeasure and area under ROC curve. We also evaluate Meta-Cost learning and found that all sampling techniques outperform Meta-Cost learning. Our results also indicate that software metrics are better predictor of software faults than code smells. © 2017 IEEE.",Machine Learning | Natural Language Processing | Recurrent Neural Networks | Software Engineering,"2020 IEEE Congreso Bienal de Argentina, ARGENCON 2020 - 2020 IEEE Biennial Congress of Argentina, ARGENCON 2020",2020-12-01,Conference Paper,"Peña Veitía, Francisco J.;Roldán, Luciana;Vegetti, Marcela",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84982871189,10.1109/ISSRE.2017.30,Learning Feature Representations from Change Dependency Graphs for Defect Prediction,"Given the heterogeneity of the data that can be extracted from the software development process, defect prediction techniques have focused on associating different sources of data with the introduction of faulty code, usually relying on handcrafted features. While these efforts have generated considerable progress over the years, little attention has been given to the fact that the performance of any predictive model depends heavily on the representation of the data used, and that different representations can lead to different results. We consider this a relevant problem, as it could be affecting directly the efforts towards generating safer software systems. Therefore, we propose to study the impact of the representation of the data in defect prediction models. To this end, we focus on the use of developer activity data, from which we structure dependency graphs. Then, instead of manually generating features, such as network metrics, we propose two models inspired by recent advances in representation learning which are able to automatically generate feature representations from graph data. These new representations are compared against manually crafted features for defect prediction in real world software projects. Our results show that automatically learned features are competitive, reaching increments in prediction performance up to 13%. © 2017 IEEE.",,Proceedings - ASE 2002: 17th IEEE International Conference on Automated Software Engineering,2002-01-01,Conference Paper,"Owen, D.;Menzies, T.;Cukic, B.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85040542929,10.1109/ISSREW.2017.11,Getting defect prediction into industrial practice: The ELFF tool,Defect prediction has been the subject of a great deal of research over the last two decades. Despite this research it is increasingly clear that defect prediction has not transferred into industrial practice. One of the reasons defect prediction remains a largely academic activity is that there are no defect prediction tools that developers can use during their day-to-day development activities. In this paper we describe the defect prediction tool that we have developed for industrial use. Our ELFF tool seamlessly plugs into the IntelliJ IDE and enables developers to perform regular defect prediction on their Java code. We explain the state-of-art defect prediction that is encapsulated within the ELFF tool and describe our evaluation of ELFF in a large UK telecommunications company. © 2017 IEEE.,Defect prediction | Industry | Machine learning | Metrics | Tool,"Proceedings - 2017 IEEE 28th International Symposium on Software Reliability Engineering Workshops, ISSREW 2017",2017-11-14,Conference Paper,"Bowes, David;Counsell, Steve;Hall, Tracy;Petric, Jean;Shippey, Thomas",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85015905794,10.1007/978-981-10-3153-3_56,Evaluation of machine learning approaches for change-proneness prediction using code smells,"In the field of technology, software is an essential driver of business and industry. Software undergoes changes due to maintenance activities initiated by bug fixing, improved documentation, and new requirements of users. In software, code smells are indicators of a system which may give maintenance problem in future. This paper evaluates six types of machine learning algorithms to predict change-proneness using code smells as predictors for various versions of four Java-coded applications. Two approaches are used: method 1-random undersampling is done before Feature selection; method 2-feature selection is done prior to random undersampling. This paper concludes that gene expression programming (GEP) gives maximum AUC value, whereas cascade correlation network (CCR), treeboost, and PNN\GRNN algorithms are among top algorithms to predict F-measure, precision, recall, and accuracy. Also, GOD and L_M code smells are good predictors of software change-proneness. Results show that method 1 outperforms method 2. © Springer Nature Singapore Pte Ltd. 2017.",Code smells | Feature subset selection (FSS) | Machine learning algorithms | Software change-proneness | Undersampling,Advances in Intelligent Systems and Computing,2017-01-01,Conference Paper,"Kaur, Kamaldeep;Jain, Shilpi",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84963795380,10.1109/BigMM.2016.36,Empirical investigation of code and process metrics for defect prediction,"Data science is becoming more important for software engineering problems. Software defect prediction is a critical area which can help the development team allocate test resource efficiently and better understand the root cause of defects. Furthermore, it can help find the reason why a component or even a project is failure-prone. This paper deals with binary classification in predicting if a software component has a bug by using three widely used machine learning algorithms: Random Forest (RF), Neural Networks (NN), and Support Vector Machine (SVM). The paper investigates the applications of these algorithms to the challenging issue of predicting defects in software components. This paper combines code metrics and process metrics as indicators for the Eclipse environment using the aforementioned three algorithms for a sample of weekly Eclipse features. Feature reduction is also adopted using General Linear Model (GLM) to save computational time. The results confirm the predictive capabilities of using two features - NBD-max and Pre-defects - and are comparable to the results of using all 61 features. Additionally, this paper evaluates the performance of the three algorithms. NN and RF turn out to have the best fit. © 2016 IEEE.",Bug Localization | Bug Reports | Deep Learning | Deep Neural Network | Information Retrieval,"Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015",2016-01-04,Conference Paper,"Lam, An Ngoc;Nguyen, Anh Tuan;Nguyen, Hoan Anh;Nguyen, Tien N.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84962633635,10.20532/cit.2016.1002700,Improving the reliability of decision-support systems for nuclear emergency management by leveraging software design diversity,"This paper introduces a novel method of continuous verification of simulation software used in decision-supportsystems for nuclear emergency management (DSNE). The proposed approach builds on methods from the field of software reliability engineering, such as N-Version Programming, Recovery Blocks, and Consensus Recovery Blocks. We introduce a new acceptance test for dispersion simulation results and a new voting schemebased on taxonomies of simulation results rather than individual simulation results. The acceptance test and the voter are used in a new scheme, which extends the Consensus Recovery Block method by a database of result taxonomies to support machine-learning. This enables the system to learn how to distinguish correct from incorrect results, with respect to the implemented numerical schemes. Considering that decision-support systems for nuclear emergency management are used in a safety-critical application context, the methods introduced in this paper help improve the reliability of the system and the trustworthiness of the simulation results used by emergency managers in the decision making process. The effectiveness of the approach has been assessed using the atmospheric dispersion forecasts of two test versions of the widely used RODOS DSNE system.",Decision-support | Machine-learning | Safety-critical | Simulation | Software reliability,Journal of Computing and Information Technology,2016-01-01,Article,"Ionescu, Tudor B.;Scheuermann, Walter",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84961822768,10.1109/ICRITO.2015.7359258,Predicting Software Maintenance effort using neural networks,"Software Maintenance is an important phase of software development lifecycle, which starts once the software has been deployed at the customer's end. A lot of maintenance effort is required to change the software after it is in operation. Therefore, predicting the effort and cost associated with the maintenance activities such as correcting and fixing the defects has become one of the key issues that need to be analyzed for effective resource allocation and decision-making. In view of this issue, we have developed a model based on text mining techniques using machine learning method namely, Radial Basis Function of neural network. We apply text mining techniques to identify the relevant attributes from defect reports and relate these relevant attributes to software maintenance effort prediction. The proposed model is validated using 'Browser' application package of Android Operating System. Receiver Operating Characteristics (ROC) analysis is done to interpret the results obtained from model prediction by using the value of Area Under the Curve (AUC), sensitivity and a suitable threshold criterion known as the cut-off point. It is evident from the results that the performance of the model is dependent on the number of words considered for classification and therefore shows the best results with respect to top-100 words. The performance is irrespective of the type of effort category. © 2015 IEEE.",Defect reports | Machine Learning | Receiver Operating Characteristics | Software Maintenance Effort Prediction | Text mining,"2015 4th International Conference on Reliability, Infocom Technologies and Optimization: Trends and Future Directions, ICRITO 2015",2015-12-16,Conference Paper,"Jindal, Rajni;Malhotra, Ruchika;Jain, Abha",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84959288209,10.1109/ISMS.2015.20,Bug Detection Using Particle Swarm Optimization with Search Space Reduction,"A bug detection tool is an important tool in software engineering development. Many research papers have proposed techniques for detecting software bug, but there are certain semantic bugs that are not easy to detect. In our views, a bug can occur from incorrect logics that when a program is executed with a particular input, the program will behave in unexpected ways. In this paper, we propose a method and tool for software bugs detection by finding such input that causes an unexpected output guided by the fitness function. The method uses a Hierarchical Similarity Measurement Model (HSM) to help create the fitness function to examine a program behavior. Its tool uses Particle Swarm Optimization (PSO) with Search Space Reduction (SSR) to manipulate input by contracting and eliminating unfavorable areas of input search space. The programs under experiment were selected from four different domains such as financial, decision support system, algorithms and machine learning. The experimental result shows a significant percentage of success rate up to 93% in bug detection, compared to an estimated success rate of 28% without SSR. © 2015 IEEE.",Bug Detection | Fitness Function | Hierarchical Similarity Measurement Model (HSM) | Optimization | Particle Swarm Optimization (PSO),"Proceedings - International Conference on Intelligent Systems, Modelling and Simulation, ISMS",2015-10-28,Conference Paper,"Reungsinkonkarn, Arun;Apirukvorapinit, Paskorn",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85086316093,10.1007/978-3-319-12883-2_28,Measuring software reliability: A trend using machine learning techniques,"It has become inevitable for every software developer to understand, to follow that how and why software fails, and to express reliability in quantitative terms. This has led to a proliferation of software reliability models to estimate and predict reliability. The basic approach is to model past failure data to predict future behavior. Most of the models have three major components: assumptions, factors and a mathematical function, usually high order exponential or logarithmic used to relate factors to reliability. Software reliability models are used to forecast the curve of failure rate by statistical evidence available during testing phase. They also can indicate about the extra time required to carry out the test procedure in order to meet the specifications and deliver desired functionality with minimum number of defects. Therefore there are challenges whether, autonomous or machine learning techniques like other predictive methods could be able to forecast the reliability measures for a specific software application. This chapter contemplates reliability issue through a generic Machine Learning paradigm while referring the most common aspects of Support Vector Machine scenario. Couples of customized simulation and experimental results have been presented to support the proposed reliability measures and strategies. © Springer International Publishing Switzerland 2015.",Adaptive | AnyLogic | Machine learning | MAS | Microgrid | Multi-agent system | Protection | Simulink,"2019 29th Australasian Universities Power Engineering Conference, AUPEC 2019",2019-11-01,Conference Paper,"Uzair, Muhammad;Li, Li;Zhu, Jian Guo;Eskandari, Mohsen",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84958540895,10.1007/978-3-319-00948-3_10,How process enactment data affects product defectiveness prediction-a case study,"The quality of a software product is highly influenced by the software process used to develop it. However, abstract and dynamic nature of the software process makes its measurement difficult, and this difficulty has supported the assessment insight of indirectly measuring the performance of software process by using the characteristics of the developed product. In fact, enactment of the software process might have a significant effect on product characteristics and data, and therefore, on the use of measurement and analysis results. In this article, we report a case study that aimed to investigate the effect of process enactment data on product defectiveness in a small software organization. We carried out the study by defining and following a methodology that included the application of Goal-Question-Metric (GQM) approach to direct analysis, the utilization of a questionnaire to assess usability of metrics, and the application of machine learning methods to predict product defectiveness. The results of the case study showed that the accuracy of predictions varied according to the machine learning method used, but in the overall, about 3% accuracy improvement was achieved by including process enactment data in the analysis. © 2014 Springer International Publishing Switzerland.",defectiveness | machine learning | process enactment | software defect prediction | software measurement,Studies in Computational Intelligence,2014-01-01,Conference Paper,"Aslan, Damla;Tarhan, Ayça;Demirörs, Ve Onur",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85063898878,10.4018/978-1-4666-4490-8.ch019,Prediction of change-prone classes using machine learning and statistical techniques,"For software development, availability of resources is limited, thereby necessitating efficient and effective utilization of resources. This can be achieved through prediction of key attributes, which affect software quality such as fault proneness, change proneness, effort, maintainability, etc. The primary aim of this chapter is to investigate the relationship between object-oriented metrics and change proneness. Predicting the classes that are prone to changes can help in maintenance and testing. Developers can focus on the classes that are more change prone by appropriately allocating resources. This will help in reducing costs associated with software maintenance activities. The authors have constructed models to predict change proneness using various machine-learning methods and one statistical method. They have evaluated and compared the performance of these methods. The proposed models are validated using open source software, Frinika, and the results are evaluated using Receiver Operating Characteristic (ROC) analysis. The study shows that machine-learning methods are more efficient than regression techniques. Among the machine-learning methods, boosting technique (i.e. Logitboost) outperformed all the other models. Thus, the authors conclude that the developed models can be used to predict the change proneness of classes, leading to improved software quality. © 2013, IGI Global.",Machine learning | Predictive maintenance,"Proceedings - 16th IEEE International Symposium on Parallel and Distributed Processing with Applications, 17th IEEE International Conference on Ubiquitous Computing and Communications, 8th IEEE International Conference on Big Data and Cloud Computing, 11th IEEE International Conference on Social Computing and Networking and 8th IEEE International Conference on Sustainable Computing and Communications, ISPA/IUCC/BDCloud/SocialCom/SustainCom 2018",2018-07-02,Conference Paper,"Apiletti, Daniele;Barberis, Claudia;Cerquitelli, Tania;Macii, Alberto;Macii, Enrico;Poncino, Massimo;Ventura, Francesco",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84870653997,10.1145/2393216.2393332,Empirical study of software quality estimation,"Software Quality is an important nonfunctional requirement which is not satisfied by many software products. Prediction models using object oriented metrics can be used to identify the faulty classes. In this paper, we will empirically study the relationship between object oriented metrics and fault proneness of an open source project Emma. Twelve machine Learning classifiers have been used. Univariate and Multivariate analysis of Emma shows that Random Forest provides optimum values for accuracy, precision, sensitivity and specificity. Copyright © 2012 ACM.",Classifiers | Fault proneness | Object-orientedsoftware metrics | Quality metrics | ROC,ACM International Conference Proceeding Series,2012-12-12,Conference Paper,"Kaur, Inderpreet;Kaur, Arvinder",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85060656267,10.1007/978-3-642-27549-4_49,Applying heuristic approaches for predicting defect-prone software components,"Effective and efficient quality assurance has to focus on those parts of a software system that are most likely to fail. Defect prediction promises to indicate the defect-prone components of a software system. In this paper we investigate the viability of predicting defect-prone components in upcoming releases of a large industrial software system. Prediction models constructed with heuristic machine learning are used to classify the components of future versions of the software system as defective or defect-free. It could be shown that the accuracy of the predictions made for the next version is significantly higher (around 74%) than guessing even when taking only new or modified components into account. Furthermore, the results reveal that, depending on the specific prediction model, acceptable accuracy can be achieved for up to three versions in the future. © 2012 Springer-Verlag.",,"Proceedings - 2018 IEEE International Conference on Big Data, Big Data 2018",2018-07-02,Conference Paper,"Li, Tao;Choi, Minsoo;Guo, Yuntao;Lin, Lei",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85081409953,10.1007/978-3-642-17578-7_28,Bug forecast: A method for automatic bug prediction,"In this paper we present an approach and a toolset for automatic bug prediction during software development and maintenance. The toolset extends the Columbus source code quality framework, which is able to integrate into the regular builds, analyze the source code, calculate different quality attributes like product metrics and bad code smells; and monitor the changes of these attributes. The new bug forecast toolset connects to the bug tracking and version control systems and assigns the reported and fixed bugs to the source code classes from the past. It then applies machine learning methods to learn which values of which quality attributes typically characterized buggy classes. Based on this information it is able to predict bugs in current and future versions of the classes. The toolset was evaluated on an industrial software system developed by a large software company called evosoft. We studied the behavior of the toolset through a 1,5 year development period during which 128 snapshots of the software were analyzed. The toolset reached an average bug prediction precision of 72%, reaching many times 100%. We concentrated on high precision, as the primary purpose of the toolset is to aid software developers and testers in pointing out the classes which contain bugs with a high probability and keep the number of false positives relatively low. © 2010 Springer-Verlag Berlin Heidelberg.",cryptocurrency | machine learning | price prediction | real time | sentiment analysis | social media | software platform | Spark,"Proceedings - 2019 IEEE International Conference on Big Data, Big Data 2019",2019-12-01,Conference Paper,"Mohapatra, Shubhankar;Ahmed, Nauman;Alencar, Paulo",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84959488619,10.1109/TSE.2019.2957794,On the Costs and Profit of Software Defect Prediction,"Defect prediction can be a powerful tool to guide the use of quality assurance resources. However, while lots of research covered methods for defect prediction as well as methodological aspects of defect prediction research, the actual cost saving potential of defect prediction is still unclear. Within this article, we close this research gap and formulate a cost model for software defect prediction. We derive mathematically provable boundary conditions that must be fulfilled by defect prediction models such that there is a positive profit when the defect prediction model is used. Our cost model includes aspects like the costs for quality assurance, the costs of post-release defects, the possibility that quality assurance fails to reveal predicted defects, and the relationship between software artifacts and defects. We initialize the cost model using different assumptions, perform experiments to show trends of the behavior of costs on real projects. Our results show that the unrealistic assumption that defects only affect a single software artifact, which is a standard practice in the defect prediction literature, leads to inaccurate cost estimations. Moreover, the results indicate that thresholds for machine learning metrics are also not suited to define success criteria for software defect prediction. © 1976-2012 IEEE.",Big Data | GPGPU | Hadoop | Integrated Graphics | Machine Learning | Mahout | OpenCL,"Proceedings - 2015 IEEE 1st International Conference on Big Data Computing Service and Applications, BigDataService 2015",2015-08-10,Conference Paper,"Kim, Sungye;Bottleson, Jeremy;Jin, Jingyi;Bindu, Preeti;Sakhare, Snehal C.;Spisak, Joseph S.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85100543298,10.1002/stvr.1761,CSSG: A cost-sensitive stacked generalization approach for software defect prediction,"The prediction of software artifacts on defect-prone (DP) or non-defect-prone (NDP) classes during the testing phase helps minimize software business costs, which is a classification task in software defect prediction (SDP) field. Machine learning methods are helpful for the task, although they face the challenge of data imbalance distribution. The challenge leads to serious misclassification of artifacts, which will disrupt the predictor's performance. The previously developed stacking ensemble methods do not consider the cost issue to handle the class imbalance problem (CIP) over the training dataset in the SDP field. To bridge this research gap, in the cost-sensitive stacked generalization (CSSG) approach, we try to combine the staking ensemble learning method with cost-sensitive learning (CSL) since the CSL purpose is to reduce misclassification costs. In the cost-sensitive stacked generalization (CSSG) approach, logistic regression (LR) and extremely randomized trees classifiers in cases of CSL and cost-insensitive are used as a final classifier of stacking scheme. To evaluate the performance of CSSG, we use six performance measures. Several experiments are carried out to compare the CSSG with some cost-sensitive ensemble methods on 15 benchmark datasets with different imbalance levels. The results indicate that the CSSG can be an effective solution to the CIP than other compared methods. © 2021 John Wiley & Sons, Ltd.",class imbalance learning | cost of misclassification | cost-sensitive learning | data imbalance | ensemble learning | software defect prediction,Software Testing Verification and Reliability,2021-08-01,Article,"Eivazpour, Zeinab;Keyvanpour, Mohammad Reza",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85072909282,10.1016/j.scico.2021.102611,Adaptive selection of classifiers for bug prediction: A large-scale empirical analysis of its performances and a benchmark study,"Bug prediction aims at locating defective source code components relying on machine learning models. Although some previous work showed that selecting the machine-learning classifier is crucial, the results are contrasting. Therefore, several ensemble techniques, i.e., approaches able to mix the output of different classifiers, have been proposed. In this paper, we present a benchmark study in which we compare the performance of seven ensemble techniques on 21 open-source software projects. Our aim is twofold. On the one hand, we aim at bridging the limitations of previous empirical studies that compared the accuracy of ensemble approaches in bug prediction. On the other hand, our goal is to verify how ensemble techniques perform in different settings such as cross- and local-project defect prediction. Our empirical experimentation results show that ensemble techniques are not a silver bullet for bug prediction. In within-project bug prediction, using ensemble techniques improves the prediction performance with respect to the best stand-alone classifier. We confirm that the models based on VALIDATION AND VOTING achieve slightly better results. However, they are similar to those obtained by other ensemble techniques. Identifying buggy classes using external sources of information is still an open problem. In this setting, the use of ensemble techniques does not provide evident benefits with respect to stand-alone classifiers. The statistical analysis highlights that local and global models are mostly equivalent in terms of performance. Only one ensemble technique (i.e., ASCI) slightly exploits local learning to improve performance. © 2021 Elsevier B.V.",Azure | Bot | DevOps | Empirical software engineering | Infrastructure | Machine learning | Pull request | Scale | Software development life cycle,"Proceedings - 2019 IEEE/ACM 1st International Workshop on Bots in Software Engineering, BotSE 2019",2019-05-01,Conference Paper,"Kumar, Rahul;Bansal, Chetan;Maddila, Chandra;Sharma, Nitin;Martelock, Shawn;Bhargava, Ravi",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85103988801,10.3390/sym13040569,An intelligent fusion algorithm and its application based on subgroup migration and adaptive boosting,"Imbalanced data and feature redundancies are common problems in many fields, especially in software defect prediction, data mining, machine learning, and industrial big data application. To resolve these problems, we propose an intelligent fusion algorithm, SMPSO-HS-AdaBoost, which combines particle swarm optimization based on subgroup migration and adaptive boosting based on hybrid-sampling. In this paper, we apply the proposed intelligent fusion algorithm to software defect prediction to improve the prediction efficiency and accuracy by solving the issues caused by imbalanced data and feature redundancies. The results show that the proposed algorithm resolves the coexisting problems of imbalanced data and feature redundancies, and ensures the efficiency and accuracy of software defect prediction. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Adaptive boosting | Feature redundancies | Fusion algorithm | Imbalanced data | Subgroup migration,Symmetry,2021-04-01,Article,"Li, Timing;Yang, Lei;Li, Kewen;Zhai, Jiannan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85114475585,,Finding bugs using your own code: Detecting functionally-similar yet inconsistent code,"Probabilistic classification has shown success in detecting known types of software bugs. However, the works following this approach tend to require a large amount of specimens to train their models. We present a new machine learning-based bug detection technique that does not require any external code or samples for training. Instead, our technique learns from the very codebase on which the bug detection is performed, and therefore, obviates the need for the cumbersome task of gathering and cleansing training samples (e.g., buggy code of certain kinds). The key idea behind our technique is a novel two-step clustering process applied on a given codebase. This clustering process identifies code snippets in a project that are functionally-similar yet appear in inconsistent forms. Such inconsistencies are found to cause a wide range of bugs, anything from missing checks to unsafe type conversions. Unlike previous works, our technique is generic and not specific to one type of inconsistency or bug. We prototyped our technique and evaluated it using 5 popular open source software, including QEMU and OpenSSL. With a minimal amount of manual analysis on the inconsistencies detected by our tool, we discovered 22 new unique bugs, despite the fact that many of these programs are constantly undergoing bug scans and new bugs in them are believed to be rare. © 2021 by The USENIX Association. All rights reserved.",,Proceedings of the 30th USENIX Security Symposium,2021,,"Ahmadi M., Farkhani R.M., Williams R., Lu L.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85050980364,10.37190/E-INF210102,A deep-learning-based bug priority prediction using RNN-LSTM neural networks,"Context: Predicting the priority of bug reports is an important activity in software maintenance. Bug priority refers to the order in which a bug or defect should be resolved. A huge number of bug reports are submitted every day. Manual filtering of bug reports and assigning priority to each report is a heavy process, which requires time, resources, and expertise. In many cases mistakes happen when priority is assigned manually, which prevents the developers from finishing their tasks, fixing bugs, and improve the quality. Objective: Bugs are widespread and there is a noticeable increase in the number of bug reports that are submitted by the users and teams’ members with the presence of limited resources, which raises the fact that there is a need for a model that focuses on detecting the priority of bug reports, and allows developers to find the highest priority bug reports. This paper presents a model that focuses on predicting and assigning a priority level (high or low) for each bug report. Method: This model considers a set of factors (indicators) such as component name, summary, assignee, and reporter that possibly affect the priority level of a bug report. The factors are extracted as features from a dataset built using bug reports that are taken from closed-source projects stored in the JIRA bug tracking system, which are used then to train and test the framework. Also, this work presents a tool that helps developers to assign a priority level for the bug report automatically and based on the LSTM’s model prediction. Results: Our experiments consisted of applying a 5-layer deep learning RNN-LSTM neural network and comparing the results with Support Vector Machine (SVM) and K-nearest neighbors (KNN) to predict the priority of bug reports. The performance of the proposed RNN-LSTM model has been analyzed over the JIRA dataset with more than 2000 bug reports. The proposed model has been found 90% accurate in comparison with KNN (74%) and SVM (87%). On average, RNN-LSTM improves the F-measure by 3% compared to SVM and 15.2% compared to KNN. Conclusion: It concluded that LSTM predicts and assigns the priority of the bug more accurately and effectively than the other ML algorithms (KNN and SVM). LSTM significantly improves the average F-measure in comparison to the other classifiers. The study showed that LSTM reported the best performance results based on all performance measures (Accuracy = 0.908, AUC = 0.95, F-measure = 0.892). © 2021 Wroclaw University of Science and Technology. All rights reserved.",Fault tolerance | HPC | Machine learning | Modeling | Resilience | Scalability,"Proceedings - 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, CCGRID 2018",2018-07-13,Conference Paper,"Kestor, Gokcen;Peng, Ivy Bo;Gioiosa, Roberto;Krishnamoorthy, Sriram",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85100265796,10.1109/3ICT51146.2020.9312003,Software Risk Estimation through Bug Reports Analysis and Bug-fix Time Predictions,"Categorizing the level of software risk components is very important for software developers. This categorization allows the developers to increase software availability, security, and provide better project management process. This research proposes a novel approach risk estimation system that aims to help software internal stakeholders to evaluate the currently existing software risk by predicting a quantitative software risk value. This risk value is estimated using the earlier software bugs reports based on a comparison between current and upcoming bug-fix time, duplicated bugs records, and the software component priority level. The risk value is retrieved by using a machine learning on a Mozilla Core dataset (Networking: HTTP software component) using Tensorflow tool to predict a risk level value for specific software bugs. The total risk results ranged from 27.4% to 84% with maximum bug-fix time prediction accuracy of 35%. Also, the result showed a strong relationship for the risk values obtained from the bug-fix time prediction and showed a low relationship with the risk values from the duplicated bug records. © 2020 IEEE.",bug reports | bug-fix time | prediction | risk analysis | risk estimation | risk management,"2020 International Conference on Innovation and Intelligence for Informatics, Computing and Technologies, 3ICT 2020",2020-12-20,Conference Paper,"Mahfoodh, Hussain;Obediat, Qasem",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85103459360,10.1109/3ICT51146.2020.9311978,Software Change Proneness Prediction Using Machine Learning,"Software change-proneness is one of the vital quality metrics that represents the extent of change of a class across versions of the system. This change may occur due to evolving requirements, bug fixing, or code refactoring. Consequently, change-proneness may have a negative impact on software evolution. For instance, modules that are change-prone tend to produce more defects and accumulate more technical debt. This research work applies different Machine Learning (ML) techniques on a large dataset from a wide commercial software system to investigate the relationships between object-oriented (OO) metrics and change-proneness, and determine which OO metrics are necessary to predict change-prone classes. Moreover, several state-of-the-art combining methods were evaluated that were constructed by combining several heterogeneous single and ensemble classifiers with voting, Select-Best, and staking scheme. The result of the study indicates a high prediction performance of many of the ensemble classifiers as well as the combining methods selected and proved that ML methods are very beneficial for predicting change-prone classes in software. The study also proved that software metrics are significant indicators of class change-proneness and should be monitored regularly during software development and maintenance. © 2020 IEEE.",computational intelligence | document classification | enhancement reports | machine learning | natural language processing,"2021 IEEE 11th Annual Computing and Communication Workshop and Conference, CCWC 2021",2021-01-27,Conference Paper,"Arshad, Muhammad Ali;Huang, Zhiqiu;Riaz, Adnan;Hussain, Yasir",Include,
10.1016/j.infsof.2022.107128,,10.1145/3377816.3381738,Automatically Predicting Bug Severity Early in the Development Process,"Bug severity is an important factor in prioritizing which bugs to fix first. The process of triaging bug reports and assigning a severity requires developer expertise and knowledge of the underlying software. Methods to automate the assignment of bug severity have been developed to reduce the developer cost, however, many of these methods require 70-90% of the project's bug reports as training data and delay their use until later in the development process. Not being able to automatically predict a bug report's severity early in a project can greatly reduce the benefits of automation. We have developed a new bug report severity prediction method that leverages how bug reports are written rather than what the bug reports contain. Our method allows for the prediction of bug severity at the beginning of the project by using an organization's historical data, in the form of bug reports from past projects, to train the prediction classier. In validating our approach, we conducted over 1000 experiments on a dataset of five NASA robotic mission software projects. Our results demonstrate that our method was not only able to predict the severity of bugs earlier in development, but it was also able to outperform an existing keyword-based classifier for a majority of the NASA projects.Ccs Concepts• Software and its engineering ? Software maintenance tools; Maintaining software; Software testing and debugging; • Computing methodologies ? Machine learning. © 2020 ACM.",bug severity; machine learning; natural language processing,"Proceedings - 2020 ACM/IEEE 42nd International Conference on Software Engineering: New Ideas and Emerging Results, ICSE-NIER 2020",2020,,"Arokiam J., Bradbury J.S.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-77949742123,10.1109/ISSRE5003.2020.00038,Cross-project aging-related bug prediction based on joint distribution adaptation and improved subclass discriminant analysis,"Software aging, which is caused by Aging-Related Bugs (ARBs), refers to the phenomenon of performance degradation and eventual crash in long running systems. In order to discover and remove ARBs, ARB prediction is proposed. However, due to the low presence and reproducing difficulty of ARBs, it is usually difficult to collect sufficient ARB data within a project. Therefore, cross-project ARB prediction is proposed as a solution to build the target project's ARB predictor by using the labeled data from the source project. A key point for cross-project ARB prediction is to reduce distribution difference between source and target project. However, existing approaches mainly focus on the marginal distribution difference while somehow overlook the conditional distribution difference, and they mainly use random oversampling to alleviate the class imbalance which may lead to overfitting. To address these problems, we propose a new crossproject ARB prediction approach based on Joint Distribution Adaptation (JDA) and Improved Subclass Discriminant Analysis (ISDA), called JDA-ISDA. The key idea of JDA-ISDA is first to use JDA to reduce the marginal distribution and conditional distribution difference jointly and then apply ISDA to alleviate the severe class imbalance problem. A set of experiments are carried out on two large open-source projects with six different machine learning (ML) classifiers. The experimental results demonstrate that compared with the stateof- the-art Transfer Learning based Aging-related bug Prediction (TLAP) and Supervised Representation Learning Approach (SRLA), JDA-ISDA is much more robust to different ML classifiers than TLAP, and the average improvement in terms of the balance value can be achieved up to 31.8%, and JDA-ISDA also outperforms TLAP and SRLA on average when logistic regression is chosen as the classifier for best performance prediction. ©2020 IEEE.",Enterprise systems | Machine learning | System diagnostics,"Proceedings - 2009 International Conference on Computational Intelligence and Software Engineering, CiSE 2009",2009-12-01,Conference Paper,"Gaudin, Benoit;Nixon, Paddy;Bines, Keith;Busacca, Fulvio;Casey, Niall",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84959266084,10.1145/3324884.3416617,BiLO-CPDP: Bi-Level Programming for Automated Model Discovery in Cross-Project Defect Prediction,"Cross-Project Defect Prediction (CPDP), which borrows data from similar projects by combining a transfer learner with a classifier, have emerged as a promising way to predict software defects when the available data about the target project is insufficient. However, developing such a model is challenge because it is difficult to determine the right combination of transfer learner and classifier along with their optimal hyper-parameter settings. In this paper, we propose a tool, dubbed BiLO-CPDP, which is the first of its kind to formulate the automated CPDP model discovery from the perspective of bi-level programming. In particular, the bi-level programming proceeds the optimization with two nested levels in a hierarchical manner. Specifically, the upper-level optimization routine is designed to search for the right combination of transfer learner and classifier while the nested lower-level optimization routine aims to optimize the corresponding hyper-parameter settings. To evaluate BiLO-CPDP, we conduct experiments on 20 projects to compare it with a total of 21 existing CPDP techniques, along with its single-level optimization variant and Auto-Sklearn, a state-of-the-art automated machine learning tool. Empirical results show that BiLO-CPDP champions better prediction performance than all other 21 existing CPDP techniques on 70% of the projects, while being overwhelmingly superior to Auto-Sklearn and its single-level optimization variant on all cases. Furthermore, the unique bi-level formalization in BiLO-CPDP also permits to allocate more budget to the upper-level, which significantly boosts the performance. © 2020 ACM.",Accuracy | Context | Correlation | Decision trees | Error analysis | Predictive models | Sensitivity,"Proceedings - IEEE International Conference on Cluster Computing, ICCC",2015-10-26,Conference Paper,"Feng, Kun;Venkata, Manjunath Gorentla;Li, Dong;Sun, Xian He",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85074146880,10.1109/SCAM51674.2020.00016,"Failure of One, Fall of Many: An Exploratory Study of Software Features for Defect Prediction","Software defect prediction represents an area of interest in both academia and the software industry. Thus, software defects are prevalent in software development and might generate numerous difficulties for users and developers apart. The current literature offers multiple alternative approaches to predict the likelihood of defects in the source code. Most of these studies concentrate on predicting defects from a broad set of software features. As a result, the individual discriminating power of software features is still unknown as some perform well only with specific projects or metrics. In this study, we applied machine learning techniques in a popular dataset. This data has information about software defects in five Java projects, containing 5, 371 classes and 37 software features. To this aim, we convey an exploratory investigation that produced hundreds of thousands of machine learning models from a diverse collection of software features. These models are random in the sense that they promptly select the features from the entire pool of features. Even though the immense majority of models are ineffective, we could produce several models that yield accurate predictions, thus classifying defects from Java project classes. Among these accurate models, our results indicate that change metric features are more present than entropy or class-level metrics. We concentrated our analysis on models that rank a randomly chosen defective class higher than a casually selected clean class with over 80% accuracy. We also report and discuss some features contributing to the explanation of model decisions. Therefore, our study promotes reasoning on which features support predicting defects in these projects. Finally, we present the implications of our work to practitioners. © 2020 IEEE.",4S Quality Metrics | ACT (Automated Continuous Testing) | Auto Bug Logging and Tracking | Cloud | Continuous Delivery | Continuous Deployment | Continuous Integration | Continuous Testing | T-Model | TestBot | XaaS (Everything as a Service),"Proceedings of the International Conference on Machine Learning, Big Data, Cloud and Parallel Computing: Trends, Prespectives and Prospects, COMITCon 2019",2019-02-01,Conference Paper,"Chhillar, Dheeraj;Sharma, Kalpana",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85092278809,10.1109/AITEST49225.2020.00025,Neural Network Classification for Improving Continuous Regression Testing,"Continuous regression testing is a time constrained process aimed at detecting potential regressions introduced in source code integration. Test prioritization is an approach to increase the efficiency of continuous regression testing, by finding an order of tests that can detect regressions faster. The challenge for test prioritization in continuous integration is the scalability of continuous prioritization as more test execution data becomes available. In this paper we propose an approach that learns to prioritize regression tests by training a prediction model based on the existing fault detection ability of tests. We perform experiments in a case study of regression testing of industrial software developed in continuous integration. The initial results show that a learning-based approach can reduce test prioritization time, compared to a non-learning approach, while achieving a comparable fault detection effectiveness. © 2020 IEEE.",classification | continuous integration testing | Machine learning | neural networks | regression testing | software testing | test prioritization,"Proceedings - 2020 IEEE International Conference on Artificial Intelligence Testing, AITest 2020",2020-08-01,Conference Paper,"Marijan, Dusica;Gotlieb, Arnaud;Sapkota, Abhijeet",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85092738620,10.1109/ComPE49325.2020.9200076,Transfer Learning Code Vectorizer based Machine Learning Models for Software Defect Prediction,"Software development life cycle comprises of planning, design, implementation, testing and eventually, deployment. Software defect prediction can be used in the initial stages of the development life cycle for identifying defective modules. Researchers have devised various methods that can be used for effective software defect prediction. The prediction of the presence of defects or bugs in a software module can facilitate the testing process as it would enable developers and testers to allocate their time and resources on modules that are prone to defects. Transfer learning can be used for transferring knowledge obtained from one domain into the other. In this paper, we propose Transfer Learning Code Vectorizer, a novel method that derives features from the text of the software source code itself and uses those features for defect prediction. We focus on the software code and convert it into vectors using a pre-Trained deep learning language model. These code vectors are subsequently passed through machine and deep learning models. Further, we compare the results of using deep learning on the text of the software code versus the usage of software metrics for prediction of defects. In terms of weighted F1 scores, the experiments show that applying the proposed TLCV method outperforms the other machine learning techniques by 9.052%. © 2020 IEEE.",Machine Learning | Software Defect Prediction | Software Metrics | Transfer Learning,"2020 International Conference on Computational Performance Evaluation, ComPE 2020",2020-07-01,Conference Paper,"Singh, Rituraj;Singh, Jasmeet;Gill, Mehrab Singh;Malhotra, Ruchika;Garima, ",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85092048366,10.1109/ICIRCA48905.2020.9182869,An Efficient Software Defect Prediction Model Using Neuro Evalution Algorithm based on Genetic Algorithm,"The main aim of Software Defect Prediction (SDP) is to identify the defect prone in source code, therefore to reduce the effort and time taken as well the cost incurred by it with guaranteeing the quality of software. The machine learning algorithms is used both code and non-code metrics are trained to predict software defects. This paper explores a knowledge of using code profiles as an alternative to traditional metrics to predict software defects. This proposed novel evolution algorithm proves to be more promising than any traditional machine learning approaches. The objective is to derive an efficient machine learning model that can predict the number of bugs the software project can produce while reaching the Quality Assurance (QA) Stage. © 2020 IEEE.",classification | machine learning algorithm | neuro evolution algorithm | Software defect Prediction,"Proceedings of the 2nd International Conference on Inventive Research in Computing Applications, ICIRCA 2020",2020-07-01,Conference Paper,"Nalini, C.;Murali Krishna, T.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85091336081,10.1109/ICCES48766.2020.09137899,Noise filtering and imbalance class distribution Removal for optimizing software fault Prediction using best software metrics Suite,"Software fault detection is by far the most prevalent field of predictive analysis in the software engineering paradigm and several research centers have launched new ventures in this area. Predicting faults in software components before they are delivered to the end-users is of key importance, as it can save time, effort, and inconvenience associated with identifying and addressing these issues at later stages. This paper presents a software defect prediction technique to alleviate some basic problems of existing frameworks for predicting software defects. This study aims to combine simple noise removal, imbalanced class distribution, and software metrics selection techniques for optimizing defect prediction in software. The technique was tested on ten software fault prediction datasets. The experimental results including recall, precision, F-measure, accuracy, and ROC-AUC values show that the proposed method enhances fault prediction performance and the results obtained are better than or close to several comparative models. This proves the validity of our model. © 2020 IEEE.",Class Imbalance | Machine Learning | Noise Filtering | Performance evaluation parameters | Software Fault Prediction | Software Metrics,"Proceedings of the 5th International Conference on Communication and Electronics Systems, ICCES 2020",2020-06-01,Conference Paper,"Joon, Ankush;Tyagi, Rajesh Kumar;Kumar, Krishan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85090835222,10.1145/3383219.3383281,Cross-Project Software Fault Prediction Using Data Leveraging Technique to Improve Software Quality,"Software fault prediction is a process to detect bugs in software projects. Fault prediction in software engineering has attracted much attention from the last decade. The early prognostication of faults in software minimize the cost and effort of errors that come at later stages. Different machine learning techniques have been utilized for fault prediction, that is proven to be utilizable. Despite, the significance of fault prediction most of the companies do not consider fault prediction in practice and do not build useful models due to lack of data or lack of enough data to strengthen the power of fault predictors. However, models trained and tested on less amount of data are difficult to generalize, because they do not consider project size, project differences, and features selection. To overcome these issues, we proposed an instance-based transfer learning through data leveraging using logistic linear regression as a base proposed statistical methodology. In our study, we considered three software projects within the same domain. Finally, we performed a comparative analysis of three different experiments for building models (targeted project). The experimental results of the proposed approach show promising improvements in (SFP). © 2020 ACM.",Cross-project | data leveraging | Instance-based learning | Machine learning | Software fault prediction | Software Quality,ACM International Conference Proceeding Series,2020-04-15,Conference Paper,"Khan, Bilal;Iqbal, Danish;Badshah, Sher",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85093877092,10.4018/IJOSSP.2020040103,Ensemble techniques-based software fault prediction in an open-source project,"Software engineering repositories have been attracted by researchers to mine useful information about the different quality attributes of the software. These repositories have been helpful to software professionals to efficiently allocate various resources in the life cycle of software development. Software fault prediction is a quality assurance activity. In fault prediction, software faults are predicted before actual software testing. As exhaustive software testing is impossible, the use of software fault prediction models can help the proper allocation of testing resources. Various machine learning techniques have been applied to create software fault prediction models. In this study, ensemble models are used for software fault prediction. Change metrics-based data are collected for an open-source android project from GIT repository and code-based metrics data are obtained from PROMISE data repository and datasets kc1, kc2, cm1, and pc1 are used for experimental purpose. Results showed that ensemble models performed better compared to machine learning and hybrid search-based algorithms. Bagging ensemble was found to be more effective in the prediction of faults in comparison to soft and hard voting. © 2020, IGI Global.",Change Metrics | GIT Repository | Hybrid Search-Based Algorithms | Machine Learning | Quality | Software Metrics | Static Code Metrics | Testing,International Journal of Open Source Software and Processes,2020-04-01,Article,"Rhmann, Wasiur;Ansari, Gufran Ahmad",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85097352734,,COOBA: Cross-project bug localization via adversarial transfer learning,"Bug localization plays an important role in software quality control. Many supervised machine learning models have been developed based on historical bug-fix information. Despite being successful, these methods often require sufficient historical data (i.e., labels), which is not always available especially for newly developed software projects. In response, cross-project bug localization techniques have recently emerged whose key idea is to transferring knowledge from label-rich source project to locate bugs in the target project. However, a major limitation of these existing techniques lies in that they fail to capture the specificity of each individual project, and are thus prone to negative transfer. To address this issue, we propose an adversarial transfer learning bug localization approach, focusing on only transferring the common characteristics (i.e., public information) across projects. Specifically, our approach (COOBA) learns the indicative public information from cross-project bug reports through a shared encoder, and extracts the private information from code files by an individual feature extractor for each project. COOBA further incorporates adversarial learning to ensure that public information shared between multiple projects could be effectively extracted. Extensive experiments on four large-scale real-world data sets demonstrate that the proposed COOBA significantly outperforms the state of the art techniques. © 2020 Inst. Sci. inf., Univ. Defence in Belgrade. All rights reserved.",,IJCAI International Joint Conference on Artificial Intelligence,2020,,"Zhu Z., Li Y., Tong H., Wang Y.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85097088008,10.1155/2020/8852705,Software Defect Prediction Based on Fuzzy Weighted Extreme Learning Machine with Relative Density Information,"To identify software modules that are more likely to be defective, machine learning has been used to construct software defect prediction (SDP) models. However, several previous works have found that the imbalanced nature of software defective data can decrease the model performance. In this paper, we discussed the issue of how to improve imbalanced data distribution in the context of SDP, which can benefit software defect prediction with the aim of finding better methods. Firstly, a relative density was introduced to reflect the significance of each instance within its class, which is irrelevant to the scale of data distribution in feature space; hence, it can be more robust than the absolute distance information. Secondly, a K-nearest-neighbors-based probability density estimation (KNN-PDE) alike strategy was utilised to calculate the relative density of each training instance. Furthermore, the fuzzy memberships of sample were designed based on relative density in order to eliminate classification error coming from noise and outlier samples. Finally, two algorithms were proposed to train software defect prediction models based on the weighted extreme learning machine. This paper compared the proposed algorithms with traditional SDP methods on the benchmark data sets. It was proved that the proposed methods have much better overall performance in terms of the measures including G-mean, AUC, and Balance. The proposed algorithms are more robust and adaptive for SDP data distribution types and can more accurately estimate the significance of each instance and assign the identical total fuzzy coefficients for two different classes without considering the impact of data scale. © 2020 Shang Zheng et al.",,Scientific Programming,2020-01-01,Article,"Zheng, Shang;Gai, Jinjing;Yu, Hualong;Zou, Haitao;Gao, Shang",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85090904170,10.32604/CMC.2020.010117,KAEA: A novel three-stage ensemble model for software defect prediction,"Software defect prediction is a research hotspot in the field of software engineering. However, due to the limitations of current machine learning algorithms, we can’t achieve good effect for defect prediction by only using machine learning algorithms. In previous studies, some researchers used extreme learning machine (ELM) to conduct defect prediction. However, the initial weights and biases of the ELM are determined randomly, which reduces the prediction performance of ELM. Motivated by the idea of search based software engineering, we propose a novel software defect prediction model named KAEA based on kernel principal component analysis (KPCA), adaptive genetic algorithm, extreme learning machine and Adaboost algorithm, which has three main advantages: (1) KPCA can extract optimal representative features by leveraging a nonlinear mapping function; (2) We leverage adaptive genetic algorithm to optimize the initial weights and biases of ELM, so as to improve the generalization ability and prediction capacity of ELM; (3) We use the Adaboost algorithm to integrate multiple ELM basic predictors optimized by adaptive genetic algorithm into a strong predictor, which can further improve the effect of defect prediction. To effectively evaluate the performance of KAEA, we use eleven datasets from large open source projects, and compare the KAEA with four machine learning basic classifiers, ELM and its three variants. The experimental results show that KAEA is superior to these baseline models in most cases. © 2020 Tech Science Press. All rights reserved.",Adaboost | Adaptive genetic algorithm | Extreme learning machine | KPCA | Software defect prediction,"Computers, Materials and Continua",2020-01-01,Article,"Zhang, Nana;Zhu, Kun;Ying, Shi;Wang, Xu",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85089354147,10.1504/IJES.2020.108279,A feature selection model for prediction of software defects,"Software is a collection of computer programs written in a programming language. Software contains various modules which make it a complex entity and it can increase the defect probability at the time of development of the modules. In turn, cost and time to develop the software can be increased. Sometimes, these defects can lead to failure of entire software. It will lead to untimely delivery of the software to the customer. This untimely delivery can responsible for withdrawal or cancellation of project in future. Hence, in this research work, some machine learning algorithms are applied to ensure timely delivery and prediction of defects. Further, several feature selection techniques are also adopted to determine relevant features for defect prediction. Copyright © 2020 Inderscience Enterprises Ltd.",Classifier | Cognitive weight | Defect | Feature selection | Prediction | Software,International Journal of Embedded Systems,2020-01-01,Conference Paper,"Kumar, Amit;Kumar, Yugal;Kukkar, Ashima",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85084955052,10.1109/ACCESS.2020.2990645,Improving maintenance-consistency prediction during code clone creation,"Developers frequently introduce code clones into software through the copy-and-paste operations during the software development phase in order to shorten development time. Not all such clone creations are beneficial to software maintenance, as they may introduce extra effort at the software maintenance phase, where additional care is needed to ensure consistent change among these clones; i.e., changes made to a piece of code may need to be propagated to other clones. Failure in doing so may risk introducing bugs into software, which are usually called consistent defect. In response to the rampant maintenance cost caused by the introduction of new clones, some researchers have advocated the use of machine-learning approach to predict the likelihood of consistent change requirement when clones are freshly introduced. Leading in this approach is the work by Wang et al., which uses Bayesian Network to model maintenance-consistency of newly introduced clones. In this work, we leverage the success of the above-mentioned work by providing a revised set of attributes that has been shown to strengthen the predictive power of the Bayesian network model, as determined more quantitatively by the precision and recall levels. We firstly provide the definition of clone consistency-maintenance requirement, which can help transfer this problem to a classification problem. Then, based on collecting all clone creation operations through traversing clone genealogies, we redesign the attribute sets for representing clone creation with more information in code and context perspective. We evaluate the effectiveness of our approach on four open source projects with more quantitative analysis, and the experimental results show that our approach possesses a powerful ability in predicting clone consistency. To transfer this work into practice, we develop an Eclipse plug-in tool of this prediction to aid developers in software development and maintenance. © 2013 IEEE.",clone consistency prediction | clone consistent change | Code clones | machine learning | software maintenance,IEEE Access,2020-01-01,Article,"Zhang, Fanlong;Khoo, Siau Cheng;Su, Xiaohong",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84870834685,10.1016/j.procs.2020.03.274,A Novel technique for test case minimization in object oriented testing,"Software maintenance is the costliest phase of software life cycle and it consumes almost 70 percent of resources of development process. Software testing involves the examining the built software with the intension to find the defects in it. Exhaustive testing of the software for all possible test cases is not feasible as may take infinitely large amount of time and may consume large number of other resources. Researchers in the field of the software testing are exploring the different possibilities to reduce the required number of test cases to test given software. In case of an object oriented (OO) software, the complexities like inheritance, coupling and cohesion, make the software modules more prone to the faults. This problem gets augmented in case of very large software systems. Many researchers have solved the problem of test case minimization from the different perspectives like requirements coverage; statement coverage and risk coverage. But negligible research has been done of the basis of security metrics coverage. In this paper, a technique has been presented for minimization of test cases for the OO systems. The research reported in the paper has considered security as a perspective for test cases evaluation and minimization. Publically available data sets pertaining to open source software Camel 1.6.1 have been used for the evaluation of proposed methodology. Linear Regression (LR) model for the bugs present in the data and various object oriented metrics for security have been developed. The proposed model dissects the given metrics sets into effective and non-effective metrics. Effective metrics are then utilized for giving weights to the test cases, further with the help of weights obtained the test suite is minimized. The performance results of proposed approach are encouraging. © 2020 The Authors. Published by Elsevier B.V.",Bug Severity | Feature Selection | Machine Learning | Supervised Classification,"2012 CSI 6th International Conference on Software Engineering, CONSEG 2012",2012-12-14,Conference Paper,"Chaturvedi, K. K.;Singh, V. B.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85083309523,10.1007/978-3-030-25797-2_1,Cross projects defect prediction modeling,Software defect prediction has been much studied in the field of research in Software Engineering.Within project Software defect prediction works well as there is sufficient amount of data available to train anymodel. But rarely local training data of the projects is available for predictions. There are many public defect data repositories available from various organizations. This availability leads to the motivation for Cross projects defect prediction. This chapter cites on defect prediction using cross projects defect data. We proposed two experiments with cross projects homogeneous metric set data and within projects data on open source software projects with class level information. The machine learning models including the ensemble approaches are used for prediction. The class imbalance problem is addressed using oversampling techniques. An empirical analysis is carried out to validate the performance of the models. The results indicate that cross projects defect prediction with homogeneous metric sets are comparable to within project defect prediction with statistical significance. © Springer Nature Switzerland AG 2020.,Class imbalance | Cross projects | Defect prediction | Machine learning | Within-project,Lecture Notes on Data Engineering and Communications Technologies,2020-01-01,Book Chapter,"Goel, Lipika;Gupta, Sonam",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85106182864,10.1109/CICT48419.2019.9066206,Stacking based approach for prediction of faulty modules,"Determination of a software module, prone to fault is important before the defects are discovered; because it can be used for better prioritization of resources. Software fault prediction is one of such tasks that predicts the fault proneness of the developed modules by applying machine learning techniques on software defect data. State-of-art software defect prediction techniques suffer from achieving good accuracy due to the imbalanced nature of software defect datasets. To address this issue, here we present an approach for software defect prediction by combining imbalance removal and ensemble-model. As ensemble approach is very effective and provides better prediction results as compared to the individual techniques. The stacking-based framework is developed by considering the outperforming ensemble classifiers in order to predict the faulty software modules. All the experiments are performed over twelve benchmark NASA MDP datasets. The paper presents an improved ensemble-based stacking approach to classify the fault prediction for the software system in an effective way. © 2019 IEEE.",machine learning | software analytics | software metrics | software testability | Software testing,"26th International Computer Conference, Computer Society of Iran, CSICC 2021",2021-03-03,Conference Paper,"Nasrabadi, Morteza Zakeri;Parsa, Saeed",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85084071607,10.1109/ICCC47050.2019.9064412,Software Defect Prediction based on Machine Learning Algorithms,"Software defect prediction is very important in software engineering. It uses the defects found in historical software modules to predict defects in new software modules, and provides decision support for planning and process management in software project. Machine learning is one of the core research directions in the field of artificial intelligence and covers many disciplines. In the research of software defect prediction, various machine learning methods have been widely studied and applied, and have been verified to obtain good performance. In this paper, the basic framework of software defect prediction model based on machine learning is described, several important prediction models are studied and discussed. Finally, the main problems of software defect prediction are summarized, and corresponding solutions are proposed. © 2019 IEEE.",Cross Company Defect Prediction | Machine Learning | Software Defect Prediction,"2019 IEEE 5th International Conference on Computer and Communications, ICCC 2019",2019-12-01,Conference Paper,"Tian, Zhang;Xiang, Jing;Zhenxiao, Sun;Yi, Zhang;Yunqiang, Yan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85082652810,10.1109/AICCSA47632.2019.9035240,Investigating the use of deep neural networks for software defect prediction,"Many software projects are shipped to customers containing defects. Defective software cost money, time, and lives. To reduce this harm, software companies allocate testing and quality assurance budgets. The enormous sizes of modern software pose challenges to traditional testing approaches due to the need for scalability. Defect prediction models have been used to direct testing efforts to probable causes of defects in the software. Early approaches for software defect prediction relied on statistical approaches to classify software modules and decide whether each module is a defect-prone module or not. Lately, many researchers used machine learning techniques to train a model that can classify software modules to defect-prone modules and not defect-prone modules. Starting from the new millennium, neural networks and deep learning won many competitions in machine learning applications. However, the use of deep learning to build a software defect prediction model was not investigated thoroughly. In this paper, we used a deep neural network to build a software defect prediction model and compared our proposed model with other machine learning algorithms like random forests, decision trees, and naive Bayesian networks. The result shows small improvement over the other learning models in most of the comparisons. These results prove the value of using deep learning for defect prediction and open the door for more experiments. © 2019 IEEE.",Deep learning | Software defect prediction | Software metrics | Software quality,"Proceedings of IEEE/ACS International Conference on Computer Systems and Applications, AICCSA",2019-11-01,Conference Paper,"Samir, Mohamed;El-Ramly, Mohammad;Kamel, Amr",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85050694111,10.1109/ASEW.2019.00038,Predicting defects with latent and semantic features from commit logs in an industrial setting,"Software defect prediction is still a challenging task in industrial settings. Noisy data and/or lack of data make it hard to build successful prediction models. In this study, we aim to build a change-level defect prediction model for a software project in an industrial setting. We combine various probabilistic models, namely matrix factorization and topic modeling, with the expectation of overcoming the noisy and limited nature of industrial settings by extracting hidden features from multiple resources. Commit level process metrics, latent features from commits, and semantic features from commit messages are combined to build the defect predictors with the use of Log Filtering and feature selection techniques, and two machine learning algorithms Naive Bayes and Extreme Gradient Boosting (XGBoost). Collecting data from various sources and applying data pre-processing techniques show a statistically significant improvement in terms of probability of detection by up to 24% when compared to a base model with process metrics only. © 2019 IEEE.",machine learning algorithm | software defect prediction | Software metric | software testing,"2017 IEEE International Conference on Cybernetics and Computational Intelligence, CyberneticsCOM 2017 - Proceedings",2017-07-02,Conference Paper,"Meiliana, M.;Karim, Syaeful;Warnars, Harco Leslie Hendric Spits;Gaol, Ford Lumban;Abdurachman, Edi;Soewito, Benfano",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85077716808,10.1109/TENCON.2019.8929661,A Deep Introduction to AI Based Software Defect Prediction (SDP) and its Current Challenges,"Software systems are driving the quality of human life. Annual investment in software systems development is exceeding 4 trillion USD. Given the increasing complexity of software products and cognitive nature of software development process, the software defects are on the rise resulting in poor quality software that needs about 2.8 trillion USD to fix. To help improve software quality, the field of Software Defect Prediction (SDP) emerged. Its novel attempt to isolate defective software units enables defect removal as well as better utilization of resources in software development and maintenance activities. Having begun with a humble set of statistical models, SDP now employs sophisticated machine learning techniques delivering good results. SDP is now considering use of highly accurate search-based techniques for defects prediction. Also SDP is emerging as a formal methodology having its own process, model and evaluation criteria. Still, the SDP is troubled by a variety of problems such as diversity of datasets, difficulty in selecting ideal set of features to predict software defects, lack of robust methodology to build SDP models and non-availability of feature rich, cross-project industry (commercial) datasets to build ideal SDP models for use across multiple projects etc. There is a plenty of scope to strengthen SDP and also take measures to promote its practical use within the industry by simplifying SDP models and quantifying their outputs and benefits. © 2019 IEEE.",defect prediction | defect prediction strength | feature selection | feature sub-selection | Ideal SDP model | machine learning | metrics clusters | multi-co-linearity | performance measures | process features | product features | repository | SDP | SDP Issues | SDP model | SDP performance analysis | SDP process | search-based techniques | statistical techniques,"IEEE Region 10 Annual International Conference, Proceedings/TENCON",2019-10-01,Conference Paper,"Pandit, Mahesha Bangalore Ramalinga;Varma, Nitin",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85076201669,10.1109/UBMK.2019.8907170,Predicting Software Vulnerabilities Using Topic Modeling with Issues,"The existence of software vulnerabilities is an indicator of the reliability and safety of software products. Software vulnerabilities can be predicted using metrics derived from developers, organization, code and textual data. In this work, we aim to predict the software vulnerabilities using issue records in two different datasets. The first dataset consists of six-months of issue records collected in a corporate, whereas the second dataset consists of Wireshark project bug records from 2017 to 2018. Prediction models were established using six different machine learning for which textual descriptions of issue records were converted into topic models. A regression model was established for the corporate company in which textual description of issue records were used as the input, and the number of vulnerabilities were used as the output of the model. A classification model was established for Wireshark dataset in which textual descriptions of bug records were used as input of the model, and the class of vulnerable-prone or not is used as the output. The best regression model results are 0.23, 0.30, 0.44 MdMRE values, respectively. The best classification model result is 74% recall score. © 2019 IEEE.",bug report | issue record | software vulnerability prediction | textual description | topic modeling,"UBMK 2019 - Proceedings, 4th International Conference on Computer Science and Engineering",2019-09-01,Conference Paper,"Bulut, Fatma Gul;Altunel, Haluk;Tosun, Ayse",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85092737018,10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00244,Software defect prediction model based on KPCA-SVM,"Software defect prediction technology mainly relies on machine learning algorithm to learn the measurement data of existing software. There is some redundant data in the measurement element of software defect, which will reduce the accuracy of machine learning algorithm. This paper proposes a software defect prediction model based on KPCA-SVM.First, the dimension reduction pretreatment of software defect data sets is carried out.Then, This paper using support vector machines for classification.The accuracy of the model can be improved by keeping global features in the selection of the dimension reduction algorithm.Therefore, the kernel principal component analysis (KPCA) algorithm was selected for dimensionality reduction. For the selection of classification algorithm, this paper considering that the defect prediction data set has small samples and non-linear characteristics, the support vector machine has better advantages in this kind of data set, so SVM is selected as the classifier.In order to verify the performance of this model, this paper adopts the NASA MDP data set which is widely used in the field of software defect prediction.This paper use the CM1, JM1, PC1 and KC1 dataset to contrast KPCA -SVM model with a single SVM and LLE - SVM. This paper proved that KPCA - SVM model can better solve the problem of data redundancy of defect prediction data set.it can keep the global characteristics, and can have better prediction precision. © 2019 IEEE.",deep learning | defect prediction | effort estimation | fault localisation | software quality prediction metrics | systematic literature review,"2020 International Conference on Computational Performance Evaluation, ComPE 2020",2020-07-01,Conference Paper,"Malhotra, Ruchika;Gupta, Shreya;Singh, Tanishq",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85065797929,10.1109/ICTAS.2019.8703639,Using machine learning to prioritize automated testing in an agile environment,"Automated software testing is an integral part of most Agile methodologies. In the case of the Scrum Agile methodology, the definition of done includes the completion of tests. As a software project matures, however, the number of tests increases to such a point that the time required to run all the tests often hinders the speed in which artifacts can be deployed. This paper describes a technique of using machine learning to help prioritize automated testing to ensure that tests which have a higher probability of failing are executed early in the test run giving the programmers an early indication of problems. In order to do this, various metrics are collected about the software under test including Cyclomatic values, Halstead-based values, and Chidamber-Kemere values. In addition, the historical commit messages from the source code control system is accessed to see if there had been defects in the various source classes previously. From these two inputs, a data file can be created which contains various metrics and whether or not there had been defects in these source files previously. This data file can then be sent to Weka to create a decision tree indicating which measurements indicate potential defects. The model created by Weka can then then be used in future to attempt to predict where defects might be in the source files and then prioritize testing appropriately. © 2019 IEEE.",Agile | Automated testing | Chidamber-Kemere | Cyclomatic | Halstead | Machine learning | Weka,"2019 Conference on Information Communications Technology and Society, ICTAS 2019",2019-04-30,Conference Paper,"Butgereit, Laurie",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85064979003,10.1109/CCIS.2018.8691373,Manifold Learning for Cross-project Software Defect Prediction,"Traditional software defect prediction studies usually built models using within-project data. However, there are not enough local data repositories for us to build the software defect prediction model in practice. Recently, cross-project software defect prediction (CSDP) has been proposed. Due to distributions of source and target domains are different, the existing CSDP models do not investigate how to use mixed project data to predict target. In this paper, we propose a method termed geodesic flow kernel software defect prediction (GFKSDP) to solve the problem of different distributions between source domain and target domain. Our GFKSDP method shrinks the differences of source and target domains by integrating an infinite number of subspaces that characterize the changes of geometric and statistical properties from source domain to target domain. Our method can adaptively determine significant parameters to reduce computational complexity. Besides, this method does better than traditional studies in unsupervised learning. Experimental results in AEEEM and Relink datasets show that the proposed method can effectively improve the performance of cross-project software defect prediction. And the proposed method outperforms state-of-the-art methods in unsupervised learning. © 2018 IEEE.",Cross-project | Geodesic flow kernel | Software defect prediction | Within-project,"Proceedings of 2018 5th IEEE International Conference on Cloud Computing and Intelligence Systems, CCIS 2018",2019-04-12,Conference Paper,"Sun, Jing;Jing, Xiaoyuan;Dong, Xiwei",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85074175735,10.1109/COMITCon.2019.8862444,Evaluating Missing Values for Software Defect Prediction,Software defect prediction (SDP) models help software development teams to identify defected modules. SDP models use historical data collected from different software repositories. This data may contain certain missing values which make data unfit for SDP model training. This study identifies the best imputation technique used to handle missing values in SDP dataset. Also we investigated for the best imputation technique along with feature selection method. Results showed that the linear regression followed by correlation-based feature is the best combination for building SDP models. © 2019 IEEE.,feature selection | missing value imputation | Software defect prediction,"Proceedings of the International Conference on Machine Learning, Big Data, Cloud and Parallel Computing: Trends, Prespectives and Prospects, COMITCon 2019",2019-02-01,Conference Paper,"Kakkar, Misha;Jain, Sarika;Bansal, Abhay;Grover, P. S.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85081595392,10.1007/978-3-030-29551-6_23,Software Defect Prediction Using a Hybrid Model Based on Semantic Features Learned from the Source Code,"Software defect prediction has extensive applicability thus being a very active research area in Search-Based Software Engineering. A high proportion of the software defects are caused by violated couplings. In this paper, we investigate the relevance of semantic coupling in assessing the software proneness to defects. We propose a hybrid classification model combining Gradual Relational Association Rules with Artificial Neural Networks, which detects the defective software entities based on semantic features automatically learned from the source code. The experiments we have performed led to results that confirm the interplay between conceptual coupling and software defects proneness. © Springer Nature Switzerland AG 2019.",Conceptual coupling | Machine learning | Software defect prediction,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Miholca, Diana Lucia;Czibula, Gabriela",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85078736768,10.1109/ACCESS.2019.2958480,Improving Failure Prediction by Ensembling the Decisions of Machine Learning Models: A Case Study,"The complexity of software has grown considerably in recent years, making it nearly impossible to detect all faults before pushing to production. Such faults can ultimately lead to failures at runtime. Recent works have shown that using Machine Learning (ML) algorithms it is possible to create models that can accurately predict such failures. At the same time, methods that combine several independent learners (i.e., ensembles) have proved to outperform individual models in various problems. While some well-known ensemble algorithms (e.g Bagging) use the same base learners (i.e., homogeneous), using different algorithms (i.e., heterogeneous) may exploit the different biases of each algorithm. However, this is not a trivial task, as it requires finding and choosing the most adequate base learners and methods to combine their outputs. This paper presents a case study on using several ML techniques to create heterogeneous ensembles for Online Failure Prediction (OFP). More precisely, it attempts to assess the viability of combining different learners to improve performance and to understand how different combination techniques influence the results. The paper also explores whether the interactions between learners can be studied and leveraged. The results suggest that the combination of certain learners and techniques, not necessarily individually the best, can improve the overall ability to predict failures. Additionally, studying the synergies in the best ensembles provides interesting insights into why some are able to perform better. © 2013 IEEE.",Big Data Analytics | Broadband Access Network | Knowledge Graph | ML and AI,"Proceedings - 2019 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery, CyberC 2019",2019-10-01,Conference Paper,"Yang, Charlie Chen Yui;Li, Guangzhi;Liu, Xiang;Wu, Zonghuan;Zhang, Kaiyu",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85072821595,10.1007/978-3-030-29238-6_10,Empirical Analysis of Object-Oriented Metrics and Centrality Measures for Predicting Fault-Prone Classes in Object-Oriented Software,"A large number of metrics have been proposed in the literature to measure various structural properties of object-oriented software. Furthermore, many centrality measures have been introduced to identify central nodes in large networks. To the best of our knowledge, only few empirical software engineering studies have explored these metrics in the case of software systems. This paper aims at providing further evidence on the usefulness of centrality measures as indicators of software defect proneness by: (1) investigating the relationships between object-oriented metrics and centrality measures, and (2) exploring how they can be combined to improve the prediction of fault-prone classes in object-oriented software. We collected data from five different versions of one open source Java software system. We used the Principal Component Analysis technique to eliminate metrics (measures) providing redundant information. Then, we built different models to predict fault-prone classes using four machine learning algorithms. Results indicate that: (1) some centrality measures capture information that is not captured by traditional object-oriented metrics, and (2) combining centrality measures with object-oriented metrics improves the performance of fault-prone classes prediction. © Springer Nature Switzerland AG 2019.",Centrality measures | Empirical analysis | Fault-prone classes prediction | Machine learning algorithms | Object-oriented metrics | Principal Component Analysis,Communications in Computer and Information Science,2019-01-01,Conference Paper,"Ouellet, Alexandre;Badri, Mourad",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85071425763,10.5220/0006857802720279,A multi-source machine learning approach to predict defect prone components,"Software code life cycle is characterized by continuous changes requiring a great effort to perform the testing of all the components involved in the changes. Given the limited number of resources, the identification of the defect proneness of the software components becomes a critical issue allowing to improve the resources allocation and distributions. In the last years several approaches to evaluating the defect proneness of software components are proposed: these approaches exploit products metrics (like the Chidamber and Kemerer metrics suite) or process metrics (measuring specific aspect of the development process). In this paper, a multi-source machine learning approach based on a selection of both products and process metrics to predict defect proneness is proposed. With respect to the existing approaches, the proposed classifier allows predicting the defect proneness basing on the evolution of these features across the project development. The approach is tested on a real dataset composed of two well-known open source software systems on a total of 183 releases. The obtained results show that the proposed features have effective defect proneness prediction ability. Copyright © 2018 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved",Fault Prediction | Machine Learning | Software Metrics,ICSOFT 2018 - Proceedings of the 13th International Conference on Software Technologies,2019-01-01,Conference Paper,"Ardimento, Pasquale;Bernardi, Mario Luca;Cimitile, Marta",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85071380100,10.18293/SEKE2019-008,Software defect prediction model based on improved deep forest and AutoEncoder by forest,"Software defect prediction is an important way to make full use of software test resources and improve software performance. To deal with the problem that of the shallow machine learning based software defect prediction model can not deeply mine the software tool data, we propose software defect prediction model based on improved deep forest and autoencoder by forest. Firstly, the original input features are transformed by the data augmentation method to enhance the ability of feature expression, and the autoencoder by forest performs the data of dimensionality reduction on the features. Then, we use the improved deep forest algorithm and autoencoder by forest to build software defect prediction model. The experimental results show that the proposed algorithm has higher performance than the original deep forest (gcForest) algorithm and other existing start-of-art algorithms, and has higher performance and efficiency than other deep learning algorithms. © 2019 Knowledge Systems Institute Graduate School. All rights reserved.",AutoEncoder by forest | Data augmentation | Deep forest | Software defect prediction,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",2019-01-01,Conference Paper,"Zheng, Wenbo;Mo, Shaocong;Jin, Xin;Qu, Yili;Xie, Zefeng;Shuai, Jia",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85070479928,10.1504/IJCSE.2019.101343,Automated labelling and severity prediction of software bug reports,"Our main aim is to develop an intelligent classifier that is capable of predicting the severity and label (type) of a newly submitted bug report through a bug tracking system. For this purpose, we build two datasets that are based on 350 bug reports from the open-source community (Eclipse, Mozilla, and Gnome). These datasets are characterised with various textual features. Based on this information, we train variety of discriminative models that are used for automated labelling and severity prediction of a newly submitted bug report. A boosting algorithm is also implemented for an enhanced performance. The classification performance is measured using accuracy and a set of other measures. For automated labelling, the accuracy reaches around 91% with the AdaBoost algorithm and cross validation test. On the other hand, for severity prediction, the classification accuracy reaches around 67% with the AdaBoost algorithm and cross validation test. Overall, the results are encouraging. Copyright © 2019 Inderscience Enterprises Ltd.",Bug labelling | Machine learning | Severity prediction | Software bugs,International Journal of Computational Science and Engineering,2019-01-01,Article,"Otoom, Ahmed Fawzi;Al-Shdaifat, Doaa;Hammad, Maen;Abdallah, Emad E.;Aljammal, Ashraf",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85069956921,10.1109/RAMS.2019.8768923,A novel feature selection method for software fault prediction model,"Software fault prediction (SFP) is an active issue in software engineering (SE). At present, machine learning (ML) has been successfully applied to SFP classification problems. However, one of the challenges for building software fault prediction models (SFPM) is processing high dimensional datasets, which include many irrelevant and redundant features. To address this issue, feature selection techniques, mainly contain wrapper methods and filter methods, are used. In the paper, we report an empirical study aimed at providing a novel approach to select feature for SFP. © 2019 IEEE.",Classification | Data preprocessing | Feature selection (FS) | Machine learning | Software fault prediction model,Proceedings - Annual Reliability and Maintainability Symposium,2019-01-01,Conference Paper,"Cui, Can;Liu, Bin;Li, Guoqi",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85072017345,10.1504/IJCAT.2019.100297,Improved Bayesian regularisation using neural networks based on feature selection for software defect prediction,"Demand for software-based applications has grown drastically in various real-time applications. However, software testing schemes have been developed which include manual and automatic testing. Manual testing requires human effort and chances of error may still affect the quality of software. To overcome this issue, automatic software testing techniques based on machine learning techniques have been developed. In this work, we focus on the machine learning scheme for early prediction of software defects using Levenberg-Marquardt algorithm (LM), Back Propagation (BP) and Bayesian Regularisation (BR) techniques. Bayesian regularisation achieves better performance in terms of bug prediction. However, this performance can be enhanced further. Hence, we developed a novel approach for attribute selection-based feature selection technique to improve the performance of BR classification. An extensive study is carried out with the PROMISE repository where we considered KC1 and JM1 datasets. Experimental study shows that the proposed approach achieves better performance in predicting the defects in software. © 2019 Inderscience Enterprises Ltd.",Fault Injection | Gate-Level Netlist | k-Nearest Neighbors | Linear Least Squares | Machine Learning | Single Event Effects | Support Vector Regression | Transient Faults,"Proceedings - 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks - Supplemental Volume, DSN-S 2019",2019-06-01,Conference Paper,"Lange, Thomas;Balakrishnan, Aneesh;Glorieux, Maximilien;Alexandrescu, Dan;Sterpone, Luca",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85065542678,,Dynamic detection of software defects using supervised learning techniques,"In software testing, automatic detection of faults and defects in software is both complex and important. There are different techniques utilized to predict future defects. Machine learning is one of the most significant techniques used to build such prediction models. In this paper, we conduct a systematic review of the supervised machine learning techniques (classifiers) that are used for software defect prediction and evaluate their performance on several benchmark datasets. We experiment with different parameter values for the classifiers and explore the usefulness of employing dimensionality reduction techniques, such as Principle Component Analysis (PCA), and Ensemble Learning techniques. The results show the effectiveness of the considered classifiers in detecting bugs. Additionally, using PCA did not have a noticeable impact on prediction systems performance while parameter tuning positively impact classifies' accuracy, especially with Artificial Neural Network (ANN). The best results are obtained by using Ensemble Learning methods such as Bagging (which achieves 95.1% accuracy with Mozilla dataset) and Voting (which achieves 93.79% accuracy with kc1 dataset). © 2019 Kohat University of Science and Technology.",Artificial Neural Network; Machine learning; Software defects prediction; Supervised classifiers,International Journal of Communication Networks and Information Security,2019,,"Al-Nusirat A., Hanandeh F., Kharabsheh M., Al-Ayyoub M., Al-dhufairi N.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85053593770,10.1007/978-3-030-00211-4_2,Cost-sensitive learner on hybrid SMOTE-ensemble approach to predict software defects,"A software defect is a mistake in a computer program or system that causes to have incorrect or unexpected results, or to behave in unintended ways. Machine learning methods are helpful in software defect prediction, even though with the challenge of imbalanced software defect distribution, such that the non-defect modules are much higher than defective modules. In this paper we introduce an enhancement for the most resent hybrid SMOTE-Ensemble approach to deal with software defects problem, utilizing the Cost-Sensitive Learner (CSL) to improve handling imbalanced distribution issue. This paper utilizes four public available datasets of software defects with different imbalanced ratio, and provides comparative performance analysis with the most resent powerful hybrid SMOTE-Ensemble approach to predict software defects. Experimental results show that utilizing multiple machine learning techniques to cope with imbalanced datasets will improve the prediction of software defects. Also, experimental results reveal that cost-sensitive learner performs very well with highly imbalanced datasets than with low imbalanced datasets. © Springer Nature Switzerland AG. 2019.",Cost matrix | Cost-sensitive learner | Data mining | Ensemble approaches | Imbalanced dataset | SMOTE | Software defect prediction | Software engineering,Advances in Intelligent Systems and Computing,2019-01-01,Conference Paper,"Abuqaddom, Inas;Hudaib, Amjad",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85114899686,10.1109/ICIS.2018.8466535,A Learning-Based Bug Predicition Method for Object-Oriented Systems,"Because of the increase in size and complexity of todays advanced software systems; the number of structural defective software classes in projects also increases, when necessary precautions are not taken. In this study, we purpose a machine-learning-based approach to detect defective classes, which generate most of the errors in the tests. Our objective is helping software developers and testers to predict error-prone classes, eliminate design defects and reduce testing costs. In learning-based methods, the dataset that is used for training the model, strongly affects the accuracy of the detection system. Therefore, we focus on steps of constructing the proper dataset using different metrics collected from existing software projects. First, we consider the rate of errors generated by a class to label it as ""Clean"" or ""Buggy"". Secondly, we use CFS (Correlation-based Feature Selection) and the PCA (Principal Component Analysis) methods to obtain the most appropriate subset of metrics. This feature selection process increases the understandability and the detection performance of the model. Lastly, we apply the Random Forest classification method to determine error-prone classes. We evaluated our approach using five different datasets that include data collected from various open-source Eclipse subprojects. The results show that our approach is successful in building learning-based models for detecting error-prone classes. However, we also observed that different models should be created for different software systems, because each project has its own character. © 2018 IEEE.",AI Security | Fault Tolerance | Intel Software Guard Extensions | Machine Learning | Model Training | Non Volatile Memory | Persistent Memory,"Proceedings - 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks, DSN 2021",2021-06-01,Conference Paper,"Yuhala, Peterson;Felber, Pascal;Schiavoni, Valerio;Tchana, Alain",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85069508140,10.1109/SYNASC.2018.00074,An improved approach to software defect prediction using a hybrid machine learning model,"Software defect prediction is an intricate but essential software testing related activity. As a solution to it, we have recently proposed HyGRAR, a hybrid classification model which combines Gradual Relational Association Rules (GRARs) with ANNs. ANNs were used to learn gradual relations that were then considered in a mining process so as to discover the interesting GRARs characterizing the defective and non-defective software entities, respectively. The classification of a new entity based on the discriminative GRARs was made through a non-adaptive heuristic method. In current paper, we propose to enhance HyGRAR through autonomously learning the classification methodology. Evaluation experiments performed on two open-source data sets indicate that the enhanced HyGRAR classifier outperforms the related approaches evaluated on the same two data sets. © 2018 IEEE.",Artificial Neural Networks | Gradual Relational Association Rules | Machine Learning | Software defect prediction,"Proceedings - 2018 20th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing, SYNASC 2018",2018-09-01,Conference Paper,"Miholca, Diana Lucia",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85053241459,,Multi-classifier model for software fault prediction,"Prediction of fault prone module prior to testing is an emerging activity for software organizations to allocate targeted resource for development of reliable software. These software fault prediction depend on the quality of fault and related code extracted from previous versions of software. This paper, presents a novel framework by combining multiple expert machine learning systems. The proposed multi-classifier model takes the benefits of best classifiers in deciding the faulty modules of software system with consensus prior to testing. An experimental comparison is performed with various outperformer classifiers in the area of fault prediction. We evaluate our approach on 16 public dataset from promise repository which consists of National Aeronautics and Space Administration (NASA) Metric Data Program (MDP) projects and Turkish software projects. The experimental result shows that our multi classifier approach which is the combination of Support Vector Machine (SVM), Naive Bayes (NB) and Random forest machine significantly improves the performance of software fault prediction. © 2018, Zarka Private University. All rights reserved.",Machine learning; Software fault prediction; Software metrics,International Arab Journal of Information Technology,2018,,"Singh P., Verma S.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85055476807,10.1109/COMPSAC.2018.10273,High-Frequency Keywords to Predict Defects for Android Applications,"Android defect prediction has proved to be useful to reduce the manual testing effort for finding bugs. In recent years, researchers design metrics related to defects and analyze historical information to predict whether files contain defects using machine learning. However, those models learn to predict defects based on the characteristics of programs while ignoring the internal information, e.g., the functional and semantic information within the source code. This paper proposes a model, HIRER, to learn the functional and semantic information to predict whether files contain defects automatically for Android applications. Specifically, HIRER learns internal information within the source code based on the high-frequency keywords extracted from programs' Abstract Syntax Trees (ASTs). It gets rule-based programming patterns from high-frequency keywords and uses Deep Belief Network (DBN), a deep neutral network, to learn functional and semantic features from the programming patterns. We implement a defect testing system with five machine learning techniques based on HIRER to predict defective files in source code automatically. Then, we apply it on four open source Android applications. The results show that learned functional and semantic features can predict more defects than traditional metrics. In different versions of MMS, Gallery2, Bluetooth, Calendar open source applications, HIRER improves the AUC of the predicted results respectively in average. © 2018 IEEE.",Android | Defect prediction | High-frequency,Proceedings - International Computer Software and Applications Conference,2018-06-08,Conference Paper,"Fan, Yaqing;Cao, Xinya;Xu, Jing;Xu, Sihan;Yang, Hongji",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85123210088,10.1109/MALTESQUE.2018.8368452,The role of meta-learners in the adaptive selection of classifiers,"The use of machine learning techniques able to classify source code components in defective or not received a lot of attention by the research community in the last decades. Previous studies indicated that no machine learning classifier is capable of providing the best accuracy in any context, highlighting interesting complementarity among them. For these reasons ensemble methods, that combines several classifier models, have been proposed. Among these, it was proposed ASCI (Adaptive Selection of Classifiers in bug predIction), an adaptive method able to dynamically select among a set of machine learning classifiers the one that better predicts the bug proneness of a class based on its characteristics. In summary, ASCI experiments each classifier on the training set and then use a meta-learner (e.g., Random Forest) to select the most suitable classifier to use for each test set instance. In this work, we conduct an empirical investigation on 21 open source software systems with the aim of analyzing the performance of several classifiers used as meta-learner in combination with ASCI. The results show that the selection of the meta-learner has not strong influence in the results achieved by ASCI in the context of within-project bug prediction. Indeed, the use of lightweight classifiers such as Naive Bayes or Logistic Regression is suggested. © 2018 IEEE.",Analog Bench Validation | De-embedding | high-speed channel | insertion loss | Linear Regression | Machine Learning | Signal Integrity | support vector regression,EPEPS 2021 - IEEE 30th Conference on Electrical Performance of Electronic Packaging and Systems,2021-01-01,Conference Paper,"Goyal, Mohit;Pandey, Maneesh;Kumar, Sharad;Sharma, Rohit",Include,
10.1016/j.infsof.2022.107128,,10.1109/ICRITO.2017.8342442,Analysis of evolutionary algorithms to improve software defect prediction,"Defect prediction of software is necessary to determine defective parts of software. Defect prediction models are elaborated with the help of software metrics when combined with defective data to predict the classes that are defective. In this paper we have used datasets that statistically resolve the relationship among software metrics and defect vulnerability. The main intent of this paper are 1) Feature selection for defect prediction using proposed evolutionary algorithm 2) Comparing machine learning techniques 3) Use of precision and recall as performance measure for defect prediction 4) 10-fold validation is performed on every model. In this discourse, we predict defective class using 5 machine learning techniques and 2 evolutionary techniques for feature selection. In this work, we have applied evolutionary algorithms for feature selection suitable for each of the classification techniques applied on five open source android packages. Finally, for validation of calculated results, 10-fold validation is used. The results show that using evolutionary algorithms for feature selection can improve precision and recall for RF, DT and SVM. Precision and recall have best rise using SVM model. The use of evolutionary algorithms don't effect precision and recall for statistical classifier. The results obtained from evaluation thus confirm about the prediction of default classes using evolutionary algorithms is better than using only machine learning techniques. The analyzed and calculated results gave us the view about the usage of evolutionary algorithm with statistical classifier is of no use. © 2017 IEEE.",Defect Prediction; Evolutionary Algorithm; Machine Learning Techniques; Object-Oriented Metrics,"2017 6th International Conference on Reliability, Infocom Technologies and Optimization: Trends and Future Directions, ICRITO 2017",2018,,"Malhotra R., Khurana A.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84893243344,10.1109/ICRITO.2017.8342425,Assessment of machine learning algorithms for determining defective classes in an object-oriented software,"Software defect prediction is a well renowned field of software engineering. Determination of defective classes early in the lifecycle of a software product helps software practitioners in effective allocation of resources. More resources are allocated to probable defective classes so that defects can be removed in the initial phases of the software product. Such a practice would lead to a good quality software product. Although, hundreds of defect prediction models have been developed and validated by researchers, there is still a need to develop and evaluate more models to draw generalized conclusions. Literature studies have found Machine Learning (ML) algorithms to be effective classifiers in this domain. Thus, this study evaluates four ML algorithms on data collected from seven open source software projects for developing software defect prediction models. The results indicate superior performance of the Multilayer Perceptron algorithm over all the other investigated algorithms. The results of the study are also statistically evaluated to establish their effectiveness. © 2017 IEEE.",predictive models; defect prediction; effort estimation; software analytics; empirical software engineering,International Symposium on Empirical Software Engineering and Measurement,2013-12-01,Conference Paper,"Turhan, Burak;Wagner, Stefan;Bener, Ayse;Di Penta, Massimiliano;Yang, Ye",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85057448793,10.1007/978-3-030-04272-1_4,Evaluating and Integrating Diverse Bug Finders for Effective Program Analysis,"Many static analysis methods and tools have been developed for program bug detection. They are based on diverse theoretical principles, such as pattern matching, abstract interpretation, model checking and symbolic execution. Unfortunately, none of them can meet most requirements for bug finding. Individual tool always faces high false negatives and/or false positives, which is the main obstacle for using them in practice. A direct and promising way to improve the capability of static analysis is to integrate diverse bug finders. In this paper, we first selected five state-of-the-art C/C++ static analysis tools implemented with different theories. We then evaluated them over different defect types and code structures in detail. To increase the precision and recall for tool integration, we studied how to properly employ machine learning algorithms based on features of programs and tools. Evaluation results show that: (1) the abilities of diverse tools are quite different for defect types and code structures, and their overlaps are quite small; (2) the integration based on machine learning can obviously improve the overall performance of static analysis. Finally, we investigated the defect types and code structures which are still challenging for existing tools. They should be addressed in future research on static analysis. © 2018, Springer Nature Switzerland AG.",Machine learning | Static analysis | Tool integration,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,Conference Paper,"Lu, Bailin;Dong, Wei;Yin, Liangze;Zhang, Li",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85052014350,10.5220/0006600304400447,Anomaly detection in industrial software systems using variational autoencoders,"Industrial software systems are known to be used for performing critical tasks in numerous fields. Faulty conditions in such systems can cause system outages that could lead to losses. In order to prevent potential system faults, it is important that anomalous conditions that lead to these faults are detected effectively. Nevertheless, the high complexity of the system components makes anomaly detection a high dimensional machine learning problem. This paper presents the application of a deep learning neural network known as Variational Autoencoder (VAE), as the solution to this problem. We show that, when used in an unsupervised manner, VAE outperforms the well-known clustering technique DBSCAN. Moreover, this paper shows that higher recall can be achieved using the semi-supervised one class learning of VAE, which uses only the normal data to train the model. Additionally, we show that one class learning of VAE outperforms semi-supervised one class SVM when training data consist of only a very small amount of anomalous samples. When a tree based ensemble technique is adopted for feature selection, the obtained results evidently demonstrate that the performance of the VAE is highly positively correlated with the selected feature set. Copyright © 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved",Anomaly Detection | DBSCAN | Industrial Software Systems | VAE | Variational Autoencoder,ICPRAM 2018 - Proceedings of the 7th International Conference on Pattern Recognition Applications and Methods,2018-01-01,Conference Paper,"Kumarage, Tharindu;De Silva, Nadun;Ranawaka, Malsha;Kuruppu, Chamal;Ranathunga, Surangika",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85071192017,10.1007/978-981-10-5687-1_21,Is Open-Source Software Valuable for Software Defect Prediction of Proprietary Software and Vice Versa?,"Software Defect Prediction (SDP) models are used to identify the defect prone artifacts in a project to assist testing experts and better resource utilization. It is simple to build a SDP model for a project having historical data available. The problem arises when we want to build SDP model for a project, which has limited or no historical data available. In this paper, we have tried to find out whether data from Open-Source Software (OSS) projects is helpful in building SDP model for proprietary software having no or limited historical data. For collection of data for training SDP model a tool is developed which extract metric data from open-source software projects source codes. Three-benchmarked dataset from NASA projects are used as proprietary software dataset for which software defects are predicted. machine learning algorithms: LR, kNN, Naïve Bayes, Neural Network, SVM, and Random Forest are used to build SDP models. Using popular performance indicators such as precision, recall, F-measure, etc., the performances of these six SDP models are compared. The study concluded that when SDP models are trained using data from OSS projects then they are able to predict software defects for proprietary software with greater accuracy in comparison to SDP models predicting defects for OSS projects when trained using proprietary software data. © 2018, Springer Nature Singapore Pte Ltd.",,Proceedings of the European Test Workshop,2019-05-01,Conference Paper,"Mittal, Soumya;Shawn Blanton, R. D.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85030763412,10.1145/3106237.3117766,Screening heuristics for project gating systems,"Continuous Integration (CI) is a hot topic. Yet, little attention is payed to how CI systems work and what impacts their behavior. In parallel, bug prediction in software is gaining high attention. But this is done mostly in the context of software engineering, and the relation to the realm of CI and CI systems engineering has not been established yet. In this paper we describe how Project Gating systems operate, which are a specific type of CI systems used to keep the mainline of development always clean. We propose and evaluate three heuristics for improving Gating performance and demonstrate their trade-offs. The third heuristic, which leverages state-of-the-art bug prediction achieves the best performance across the entire spectrum of workload conditions. © 2017 Association for Computing Machinery.",Continuous Integration | Machine learning | Project gating,Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering,2017-08-21,Conference Paper,"Volf, Zahy;Shmueli, Edi",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85030991487,10.1109/IJCNN.2017.7965923,Analyzing and predicting concurrency bugs in open source systems,"Background Software systems are relying more and more on multi-core hardware requiring a parallel approach to address the problems and improve performances. Unfortunately, parallel development is error prone and many developers are not very experienced with this paradigm also because identifying, reproducing, and fixing bugs is often difficult. Objective The main goal of this paper is the identification of an approach able to: (i) identify solved concurrency-related bugs to characterize them and help retrospective activities; (ii) identify concurrency-related bugs as soon as they are entered in the bug management system to support bug triage phase and allocate them to more experienced developers. Approach To this end, the paper analyzes bugs related to concurrency looking at their specific characteristics using different machine learning methods to automatically distinguish them from other kinds of bugs based on the data available in the issue tracking systems and in the code repositories. Results The overall best models we developed for Apache HTTP Server and MariaDB have a precision of 0.985 and 0.814 and a recall of 0.876 and 0.629 when considering linked bugs (bug reports information in bug repository and the corresponding fix in the version control system) and a precision of 0.978 and 0.779 and a recall of 0.889 and 0.569 when considering only the information from bug reports. Conclusions Such results allow the development of an automated system able to classify such bugs and support developers in the bug triage process. © 2017 IEEE.",Concurrency | Defects | Open source | Prediction model,Proceedings of the International Joint Conference on Neural Networks,2017-06-30,Conference Paper,"Ciancarini, Paolo;Poggi, Francesco;Rossi, Davide;Sillitti, Alberto",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85048515865,10.1109/ICMLA.2017.00-89,MapReduce based classification for fault detection in big data applications,"Recently emerging software applications are large, complex, distributed and data-intensive, i.e., big data applications. That makes the monitoring of such applications a challenging task due to lack of standards and techniques for modeling and analysis of execution data (i.e., logs) produced by such applications. Another challenge imposed by big data applications is that the execution data produced by such applications also has high volume, velocity, variety, and require high veracity, value. In this paper, we present our monitoring solution that performs real-time fault detection in big data applications. Our solution is two-fold. First, we prescribe a standard model for structuring execution logs. Second, we prescribe a Bayesian classification based analysis solution that is MapReduce compliant, distributed, parallel, single pass and incremental. That makes it possible for our proposed solution to be deployed and executed on cloud computing platforms to process logs produced by big data applications. We have carried out complexity, scalability, and usability analysis of our proposed solution that how efficiently and effectively it can perform fault detection in big data applications. © 2017 IEEE.",Applications | Bayesian Classification | Big Data | Fault detection | MapReduce,"Proceedings - 16th IEEE International Conference on Machine Learning and Applications, ICMLA 2017",2017-01-01,Conference Paper,"Shafiq, M. Omair;Fekri, Maryam;Ibrahim, Rami",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85057754334,10.14232/actacyb.23.2.2017.7,Automatic calculation of process metrics and their bug prediction capabilities,"Identifying fault-prone code parts is useful for the developers to help re-duce the time required for locating bugs. It is usually done by characterizing the already known bugs with certain kinds of metrics and building a predictive model from the data. For the characterization of bugs, software product and process metrics are the most popular ones. The calculation of product metrics is supported by many free and commercial software products. However, tools that are capable of computing process metrics are quite rare. In this study, we present a method of computing software process metrics in a graph database. We describe the schema of the database created and we present a way to readily get the process metrics from it. With this technique, process metrics can be calculated at the file, class and method levels. We used GitHub as the source of the change history and we selected 5 open-source Java projects for processing. To retrieve positional information about the classes and methods, we used SourceMeter, a static source code analyzer tool. We used Neo4j as the graph database engine, and its query language-cypher-to get the process metrics. We published the tools we created as open-source projects on GitHub. To demonstrate the utility of our tools, we selected 25 release versions of the 5 Java projects and calculated the process metrics for all of the source code elements (files, classes and methods) in these versions. Using our previous published bug database, we built bug databases for the selected projects that contain the computed process metrics and the corresponding bug numbers for files and classes. (We published these databases as an online appendix.) Then we applied 13 machine learning algorithms on the database we created to find out if it is feasible for bug prediction purposes. We achieved F-measure values on average of around 0.7 at the class level, and slightly better values of between 0.7 and 0.75 at the file level. The best performing algorithm was the RandomForest method for both cases.",Cloud Computing | Reliability | ReMot | Reputation | Volunteer Cloud,"Proceedings - 2018 IEEE 6th International Conference on Future Internet of Things and Cloud, FiCloud 2018",2018-09-07,Conference Paper,"Alsenani, Yousef;Crosby, Garth V.;Velasco, Tomas;Alahmadi, Abdulrahman",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85123364715,10.1145/3028842.3028859,Software defect prediction based on manifold learning in subspace selection,"Software defects will lead to software running error and system crashes. In order to detect software defect as early as possible at early stage of software development, a series of machine learning approaches have been studied and applied to predict defects in software modules. Unfortunately, the imbalanceof software defect datasets brings great challenge to software defect prediction model training. In this paper, a new manifold learning based subspace learning algorithm, Discriminative Locality Alignment(DLA), is introduced into software defects prediction. Experimental results demonstrate that DLA is consistently superior to LDA (Linear Discriminant Analysis) and PCA (Principal Component Analysis) in terms of discriminate information extraction and prediction performance.In addition, DLA reveals some attractive intrinsic properties for numeric calculation, e.g. it can overcome the matrix singular problem and small sample size problem in software defect prediction. © 2016 ACM.",Adversarial Learning | Bug Fixing Time Prediction | Bug Report Analysis | Multi-task Learning,"2021 IEEE 10th Global Conference on Consumer Electronics, GCCE 2021",2021-01-01,Conference Paper,"Liu, Qicong;Washizaki, Hironori;Fukazawa, Yoshiaki",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85018614021,10.4018/978-1-5225-1759-7.ch059,Machine learning techniques to predict software defect,"The past 10 years have seen the prediction of software defects proposed by many researchers using various metrics based on measurable aspects of source code entities (e.g. methods, classes, files or modules) and the social structure of software project in an effort to predict the software defects. However, these metrics could not predict very high accuracies in terms of sensitivity, specificity and accuracy. In this chapter, we propose the use of machine learning techniques to predict software defects. The effectiveness of all these techniques is demonstrated on ten datasets taken from literature. Based on an experiment, it is observed that PNN outperformed all other techniques in terms of accuracy and sensitivity in all the software defects datasets followed by CART and Group Method of data handling. We also performed feature selection by t-statistics based approach for selecting feature subsets across different folds for a given technique and followed by the feature subset selection. By taking the most important variables, we invoked the classifiers again and observed that PNN outperformed other classifiers in terms of sensitivity and accuracy. Moreover, the set of 'if- then rules yielded by J48 and CART can be used as an expert system for prediction of software defects. © 2017 by IGI Global. All rights reserved.",,"Artificial Intelligence: Concepts, Methodologies, Tools, and Applications",2016-12-12,Book Chapter,"Mohanty, Ramakanta;Ravi, Vadlamani",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85017034869,10.1109/ICSESS.2016.7883044,Decision support for global software development with pattern discovery,"Background: Software development process nowadays is becoming more globalized than ever before. Global Software Development (GSD) implies that the software development process is spread across countries and geographic boundaries. GSD brings challenges to software project leaders / managers because of the increase in management difficulty. As a result, utilizing data mining and machine learning techniques to provide quantitative, objective and predictive solution for project management is essential. Aim: To facilitate software project management to make decisions by mining embedded knowledge from data and providing meaningful results. Method: In this paper we propose to adopt a pattern discovery technique which has been successfully applied in the field of computational Biology. The technique discovers association patterns inherited in the data which can provide insightful information for domain experts (e.g., project leaders), therefore increasing their confidence in making decisions. We apply the technique in the software defect datasets from the NASA MDP repository to predict whether a software project is defective or not and find out important factors in the data that signaled the prediction. Results: For the tested datasets, statistically significant patterns are produced with good classification performance. The experiment results also reveal the effect of different discretization techniques on performance. Conclusions: To the best of our knowledge, this is the first study to employ the specific pattern mining technique in Software Engineering for defective software detection and the results showed the potential of such a technique in which it can provide not only good classification results but also meaningful information for project leaders to make decisions. © 2016 IEEE.",association pattern | attribute clustering | discretization | pattern discovery,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",2016-07-02,Conference Paper,"Wu, Jack H.C.;Keung, Jacky",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84973910944,10.1145/2915970.2915979,Using different characteristics of machine learners to identify different defect families,"Background: Software defect prediction has been an active area of research for the last few decades. Many models have been developed with aim to find locations in code likely to contain defects. As of yet, these prediction models are of limited use and rarely used in the software industry. Problem: Current modelling techniques are too coarse grained and fail in finding some defects. Most of the prediction models do not look for targeted defect characteristics, but rather treat them as a black box and homogeneous. No study has investigated in greater detail how well certain defect characteristics work with different prediction modelling techniques. Methodology: This PhD will address three major tasks. First, the relation among software defects, prediction models and static code metrics will be analysed. Second, the possibility of a mapping function between prediction models and defect characteristics shall be investigated. Third, an optimised ensemble model that searches for targeted defects will be developed. Contribution: A few contributions will yield from this work. Characteristics of defects will be identified, allowing other researchers to build on this work to produce more efficient prediction models in future. New modelling techniques that better suit state-of-the-art knowledge in defect prediction shall be designed. Such prediction models should be transformed in a tool that can be used by our industrial collaborator in the real industry environment. © 2016 ACM.",Adaboost | machine learning | severity predection | software bugs,"2016 7th International Conference on Information and Communication Systems, ICICS 2016",2016-05-20,Conference Paper,"Otoom, Ahmed Fawzi;Al-Shdaifat, Doaa;Hammad, Maen;Abdallah, Emad E.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84966642611,10.1109/DCIT.2015.21,Software Defect Prediction Based on Conditional Random Field in Imbalance Distribution,"To contribute software testing, and save testing costs, a wide range of machine learning approachs have been studied to predict defects in software modules. Unfortunately, the imbalanced nature of this type of data increases the learning difficulty of such a task. In this paper, we present UCRF, a method based on undersampling technique and conditional random field (CRF) for software defect prediction in imbalance distribution. In our proposed method, firstly, we leverage meanshift clustering method to reduce the samples of majority clab for balancing the train data set. Secondly, we propose to apply CRF model in the above balanced train data set because the CRF model can handle complex features without any change in training procedure. Interestingly, we find that the UCRF method achieves much better final results than the other approach as shown in the software defect data clabification task. © 2015 IEEE.",conditional random field | imbalance distribution | Meanshift clustering method | Software defect prediction,"Proceedings - 2015 2nd International Symposium on Dependable Computing and Internet of Things, DCIT 2015",2016-03-15,Conference Paper,"Yang, Chunhui;Gao, Yan;Xiang, Jianwen;Liang, Lixin",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85018501119,10.1109/ICOSST.2015.7396406,Fault prediction model for software using soft computing techniques,"Faulty modules of any software can be problematic in terms of accuracy, hence may encounter more costly redevelopment efforts in later phases. These problems could be addressed by incorporating the ability of accurate prediction of fault prone modules in the development process. Such ability of the software enables developers to reduce the faults in the whole life cycle of software development, at the same time it benefits automation process, and reduces the overall cost and efforts of the software maintenance. In this paper, we propose to design fault prediction model by using a set of code and design metrics; applying various machine learning (ML) classifiers; also used transformation techniques for feature reduction and dealing class imbalance data to improve fault prediction model. The data sets were obtained from publicly available PROMISE repositories. The results of the study revealed that there was no significant impact on the ability to accurately predict the fault-proneness of modules by applying PCA in reducing the dimensions; the results were improved after balancing data by SMOTE, Resample techniques, and by applying PCA with Resample in combination. It has also been seen that Random Forest, Random Tree, Logistic Regression, and Kstar machine learning classifiers have relatively better consistency in prediction accuracy as compared to other techniques. © 2015 IEEE.",Activity profiling of developers | Bug assignment | Bug triaging | Mining software repositories | Time based expertise,"2016 9th International Conference on Contemporary Computing, IC3 2016",2017-03-16,Conference Paper,"Anjali, ;Mohan, Devina;Sardana, Neetu",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85009275163,10.14257/ijseia.2016.10.12.27,Software module fault prediction using convolutional neural network with feature selection,"Software plays a significant role in technological and economic development due to its utmost importance in day to day activities. A sequence of rigorous activities under certain constraints is followed to come up with reliable software. Various measures are taken during the process of software development to ensure high quality software. One such method is software module fault prediction for quality assurance to discover defects in the software prior to testing. It aids in predicting the software module faults earlier in the development of the software which predicts fault prone modules so that these can be given special attention to avoid any future risk which eventually curbs the testing along with maintenance cost and effort. The literature survey uncovers many findings that had never been focused like dimensionality reduction and feature selection based on individual feature importance which leads to increase in time complexity and chances of false information. This paper addresses these issues and proposes a supervised machine learning based software module fault prediction technique by implementing Convolutional Neural Network (CNN) as classifier model. Feature selection methods used are InfoGain and Correlation. The results obtained are compared with the existing method HySOM (SOM Clustering with Artificial Neural Network Classification) by considering three different feature sets (Fifteen features, Eighteen features and Twenty one features) of PC1 dataset from NASA. The comparative analysis is performed on the basis of accuracy, precision, recall and F1-measure. The results clearly show better performance of the proposed CNN based technique than HySOM. This paper will contribute towards improvement of quality assurance models utilized for software fault prediction by automating this process using machine learning which enhances True Positive Rate and reduces the detection error. This in turn will help project managers, testers and developers to locate and keep track of fault prone modules so that final software is more accurate, consistent and reliable without consuming much of the testing and maintenance resources. © 2016 SERSC.",Convolutional neural network | Feature selection | Software fault prediction | Software faults,International Journal of Software Engineering and its Applications,2016-01-01,Article,"Sharma, Rupali;Kakkar, Parveen",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85073230143,10.1007/978-3-319-47955-2_19,An empirical validation of learning schemes using an automated genetic defect prediction framework,"Today, it is common for software projects to collect measurement data through development processes. With these data, defect prediction software can try to estimate the defect proneness of a software module, with the objective of assisting and guiding software practitioners. With timely and accurate defect predictions, practitioners can focus their limited testing resources on higher risk areas. This paper reports a benchmarking study that uses a genetic algorithm that automatically generates and compares different learning schemes (preprocessing + attribute selection + learning algorithms). Performance of the software development defect prediction models (using AUC, Area Under the Curve) was validated using NASA-MDP and PROMISE data sets. Twelve data sets from NASA-MDP (8) and PROMISE (4) projects were analyzed running a M × N-fold cross-validation. We used a genetic algorithm to select the components of the learning schemes automatically, and to evaluate and report those with the best performance. In all, 864 learning schemes were studied. The most common learning schemes were: data preprocessors: Log and CoxBox + attribute selectors: Backward Elimination, Best First and Linear Forward Selection + learning algorithms: Naïve Bayes, Naïve Bayes Simple, Simple Logistic, Multilayer Perceptron, Logistic, Logit Boost, Bayes Net, and One R. The genetic algorithm reported steady performance and runtime among data sets, according to statistical analysis. © Springer International Publishing AG 2016.",Bug Report | Bug Repository | Classification | Developer Selection | Ensemble Learning | Machine Learning | Mining Software Repositories | Triaging,"2019 12th International Conference on Contemporary Computing, IC3 2019",2019-08-01,Conference Paper,"Goyal, Anjali;Sardana, Neetu",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85069188180,10.1007/978-3-319-30933-0_24,"Micro-interaction metrics based software defect prediction with machine learning, immune inspired and evolutionary classifiers: An empirical study","Software developer’s pattern of activities, level of understanding of the source code and work practices are important factors that impact the defects introduced in software during development and its post-release quality. In very recent previous research (Lee et al. in Micro interaction metrics for defect prediction, pp 311–321, 2011), process metrics and micro-interaction metrics (Lee et al. in Micro interaction metrics for defect prediction, pp 311–321, 2011) that capture developer’s interaction with the source code have been shown to be influential on software defects introduced during development. Evaluation and selection of suitable classifiers in an unbiased manner is another conspicuous research issue in metrics based software defect prediction This study investigates software defect prediction models where micro-interactions metrics (Lee et al. in Micro interaction metrics for defect prediction, pp 311–321, 2011) are used as predictors for ten Machine Leaning (ML), fifteen Evolutionary Computation (EC) and eight Artificial Immune recognition system (AIRS) classifiers to predict defective files of three sub-projects of Java project Eclipse. They are -etc, mylyn and team. While no single best classifier could be obtained with respect to various accuracy measures on all datasets, we recommend a list of learning classifiers with respect to different goals of software defect prediction (SDP). For overall better quality of classification of defective and non-defective files, measured by F-measure, ensemble methods-Random Forests, Rotation Forests, a decision tree classifier J48 and UCS an evolutionary learning classifier system are recommended. For risk-averse and mission critical software projects defect prediction, we recommend logistic, J48, UCS and Immunos-1, an artificial immune recognition system classifier. For minimizing testing of non-defective files, we recommend Random Forests, Rotation Forests, MPLCS (Memetic Pittsburgh Learning Classifier) and Generational Genetic Algorithm (GGA) classifier. © Springer International Publishing Switzerland 2016.",Bug reports | Machine Learning techniques | Severity Prediction | Text Mining,"Proceedings - IEEE 2018 International Conference on Advances in Computing, Communication Control and Networking, ICACCCN 2018",2018-10-01,Conference Paper,"Kaur, Arvinder;Goyal Jindal, Shubhra",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84962079877,10.1109/COMPSAC.2015.143,An implementation of just-in-time fault-prone prediction technique using text classifier,"Since the fault prediction is an important technique to help allocating software maintenance effort, much research on fault prediction has been proposed so far. The goal of these studies is applying their prediction technique to actual software development. In this paper, we implemented a prototype fault-prone module prediction tool using a text-filtering based technique named 'Fault-Prone Filtering'. Our tool aims to show the result of fault prediction for each change (i.e., Commits) as a probability that a source code file to be faulty. The result is shown on a Web page and easy to track the histories of prediction. A case study performed on three open source projects shows that our tool could detect 90 percent of the actual fault modules (i.e., The recall of 0.9) with the accuracy of 0.67 and the precision of 0.63 on average. © 2015 IEEE.",Fault prediction | Machine learning | Mining software repository | Software development support tool | Software maintenance | Spam filter,Proceedings - International Computer Software and Applications Conference,2015-09-21,Conference Paper,"Mori, Keita;Mizuno, Osamu",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84951334965,10.1007/978-3-319-22183-0_33,Software defect classification with a variant of NSGA-II and simple voting strategies,"Software Defect Prediction is based on datasets that are imbalanced and therefore limit the use of machine learning based classification. Ensembles of genetic classifiers indicate good performance and provide a promising solution to this problem. To further examine this solution, we performed additional experiments in that direction. In this paper we report preliminary results obtained by using a Matlab variant of NSGA-II in combination with four simple voting strategies on three subsequent releases of the Eclipse Plug-in Development Environment (PDE) project. Preliminary results indicate that the voting procedure might influence software defect prediction performances. © Springer International Publishing Switzerland 2015.",Multi-objective optimisation | NSGA-II | SBSE | SDP,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2015-01-01,Conference Paper,"Rubinić, Emil;Mauša, Goran;Grbac, Tihana Galinac",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84907389222,,Analysis of software project reports for defect prediction using KNN,"Defect severity assessment is highly essential for the software practitioners so that they can focus their attention and resources on the defects having a higher priority than the other defects. This would directly impact resource allocation and planning of subsequent defect fixing activities. In this paper, we intend to predict a model which will be used to assign a severity level to each of the defect found during testing. The model is based on text mining and machine learning technique. We have used KNN machine learning method to predict the model employed on an open source NASA dataset available in the PITS database. Area Under the Curve (AUC) obtained from Receiver Operating Characteristics (ROC) analysis is used as the performance measure to validate and analyze the results. The obtained results show that the performance of KNN technique is exceptionally well in predicting the defects corresponding to top 100 words for all the severity levels. Its performance is less for top 5 words, better for top 25 words and still better for top 50 words. Hence, with these results, it is reasonable to claim that the performance of KNN is dependent on the number of words selected as independent features. As the number of words increases, the performance of KNN also gets better. Apart from this, it has been noted that KNN method works best for medium severity defects as compared to the other severity defects.",Defect; K-Nearest neighbour; Machine learning; Receiver operating characteristics; Severity; Text mining,Lecture Notes in Engineering and Computer Science,2014,,"Jindal R., Malhotra R., Jain A.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-79953069937,10.1109/ICECS.2010.5724589,Estimating design quality of digital systems via machine learning,"Although the term design quality of digital systems can be assessed from many aspects, the distribution and density of bugs are two decisive factors. This paper presents the application of machine learning techniques to model the relationship between specified metrics of high-level design and its associated bug information. By employing the project repository (i.e., high level design and bug repository), the resultant models can be used to estimate the quality of associated designs, which is very beneficial for design, verification and even maintenance processes of digital systems. A real industrial microprocessor is employed to validate our approach. We hope that our work can shed some light on the application of software techniques to help improve the reliability of various digital designs. ©2010 IEEE.",Bug repository | Design quality | Machine learning | Software-aided,"2010 IEEE International Conference on Electronics, Circuits, and Systems, ICECS 2010 - Proceedings",2010-12-01,Conference Paper,"Guo, Qi;Chen, Tianshi;Shen, Haihua;Chen, Yunji",Include,
10.1016/j.infsof.2022.107128,2-s2.0-77950601505,10.1109/ICNC.2009.501,A dynamic probability fault localization algorithm using digraph,"Analyzed here is a probability learning fault localization algorithm based on directed graph and setcovering. The digraph is constituted as following: get the deployment graph of managed business from the topography of network and software environment; generate the adjacency matrix (Ma); compute the transitive matrix (Ma2) and transitive closure (M t) and obtain dependency matrix (R). When faults occur, the possible symptoms will be reflected in R with high probability in fault itself, less probability in Ma, much less in Ma and least in MP MCA+ is a probability max covering algorithm taking lost and spurious symptom into account. DMCA+ is dynamic probability updating algorithm through learning run-time fault localization experience. When fail to localize the faults, probabilities of real faults will be updated with an increment. The simulation results show the validity and efficiency of DMCA+ under complex network. In order to promote detection rate, multi-recommendation strategy is also investigated in MCA+ and DMCA+. © 2009 IEEE.",Fault localization | Fault propagation model | Machine learning | Transitive closure | Uncertainty reasoning,"5th International Conference on Natural Computation, ICNC 2009",2009-12-01,Conference Paper,"Li, Chunfang;Liu, Lianzhong;Pang, Xiaojie",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85112462419,10.1007/s00500-021-06096-3,An empirical study toward dealing with noise and class imbalance issues in software defect prediction,"The quality of the defect datasets is a critical issue in the domain of software defect prediction (SDP). These datasets are obtained through the mining of software repositories. Recent studies claim over the quality of the defect dataset. It is because of inconsistency between bug/clean fix keyword in fault reports and the corresponding link in the change management logs. Class Imbalance (CI) problem is also a big challenging issue in SDP models. The defect prediction method trained using noisy and imbalanced data leads to inconsistent and unsatisfactory results. Combined analysis over noisy instances and CI problem needs to be required. To the best of our knowledge, there are insufficient studies that have been done over such aspects. In this paper, we deal with the impact of noise and CI problem on five baseline SDP models; we manually added the various noise level (0–80%) and identified its impact on the performance of those SDP models. Moreover, we further provide guidelines for the possible range of tolerable noise for baseline models. We have also suggested the SDP model, which has the highest noise tolerable ability and outperforms over other classical methods. The True Positive Rate (TPR) and False Positive Rate (FPR) values of the baseline models reduce between 20–30% after adding 10–40% noisy instances. Similarly, the ROC (Receiver Operating Characteristics) values of SDP models reduce to 40–50%. The suggested model leads to avoid noise between 40–60% as compared to other traditional models. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Class imbalance | Fault proneness | Machine learning | Noisy instance | Software fault prediction | Software metrics | Software testing,Soft Computing,2021-11-01,Article,"Pandey, Sushant Kumar;Tripathi, Anil Kumar",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85109767714,10.1007/s10515-021-00287-w,Traceability recovery between bug reports and test cases-a Mozilla Firefox case study,"Automatic recovery of traceability between software artifacts may promote early detection of issues and better calculate change impact. Information Retrieval (IR) techniques have been proposed for the task, but they differ considerably in input parameters and results. It is difficult to assess results when those techniques are applied in isolation, usually in small or medium-sized software projects. Recently, multilayered approaches to machine learning, in special Deep Learning (DL), have achieved success in text classification through their capacity to model complex relationships among data. In this article, we apply several IR and DL techniques for investing automatic traceability between bug reports and manual test cases, using historical data from the Mozilla Firefox’s Quality Assurance (QA) team. In this case study, we assess the following IR techniques: LSI, LDA, and BM25, in addition to a DL architecture called Convolutional Neural Networks (CNNs), through the use of Word Embeddings. In this context of traceability, we observe poor performances from three out of the four studied techniques. Only the LSI technique presented acceptable results, standing out even over the state-of-the-art BM25 technique. The obtained results suggest that the semi-automatic application of the LSI technique – with an appropriate combination of thresholds – may be feasible for real-world software projects. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Bug reports | Deep learning | Information retrieval | System features | Test cases | Traceability,Automated Software Engineering,2021-11-01,Article,"Gadelha, Guilherme;Ramalho, Franklin;Massoni, Tiago",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85070599364,10.1016/j.infsof.2021.106605,Leveraging developer information for efficient effort-aware bug prediction,"Context: Software bug prediction techniques can provide informative guidance in software engineering practices. Over the past 15 years, developer information has been intensively used in bug prediction as features or basic data source to construct other useful models. Objective: Further leverage developer information from a new and straightforward perspective to improve effort-aware bug prediction. Methods: We propose to investigate the direct relations between the number of developers and the probability for a file to be buggy. Based on an empirical study on nine open-source Java systems with 32 versions, we observe a widely-existed and interesting tendency: when there are more developers working on a source file, there will be a stronger possibility for this file to be buggy. Based on the observed tendency, we propose an unsupervised algorithm and a supervised equation both called top-dev to improve effort-aware bug prediction. The key idea is to prioritize the ranking of files, whose number of developers is large, in the suspicious file list generated by effort-aware models. Results: Experimental results show that the proposed top-dev algorithm and equation significantly outperform the unsupervised and supervised baseline models (ManualUp, Rad, Rdd, Ree, CBS+, and top-core). Moreover, the unsupervised top-dev algorithm is comparable or superior to existing supervised baseline models. Conclusion: The proposed approaches are very useful in effort-aware bug prediction practices. Practitioners can use the top-dev algorithm to generate a high-quality and informative suspicious file list without training complex machine learning classifiers. On the other hand, when building supervised bug prediction model, the best practice is to combine existing models with the top-dev equation. © 2021 Elsevier B.V.",Failures | Faults | Feature Selection | Intelligent Software | Machine Learning Techniques | Software Reliability,"2019 International Conference on Automation, Computational and Technology Management, ICACTM 2019",2019-04-01,Conference Paper,"Banga, Manu;Bansal, Abhay;Singh, Archana",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85100096979,10.1007/s10586-021-03282-8,Principal component based support vector machine (PC-SVM): a hybrid technique for software defect detection,"Defects are the major problems in the current situation and predicting them is also a difficult task. Researchers and scientists have developed many software defects prediction techniques to overcome this very helpful issue. But to some extend there is a need for an algorithm/method to predict defects with more accuracy, reduce time and space complexities. All the previous research conducted on the data without feature reduction lead to the curse of dimensionality. We brought up a machine learning hybrid approach by combining Principal component Analysis (PCA) and Support vector machines (SVM) to overcome the ongoing problem. We have employed PROMISE (CM1: 344 observations, KC1: 2109 observations) data from the directory of NASA to conduct our research. We split the dataset into training (CM1: 240 observations, KC1: 1476 observations) dataset and testing (CM1: 104 observations, KC1: 633 observations) datasets. Using PCA, we find the principal components for feature optimization which reduce the time complexity. Then, we applied SVM for classification due to very native qualities over traditional and conventional methods. We also employed the GridSearchCV method for hyperparameter tuning. In the proposed hybrid model we have found better accuracy (CM1: 95.2%, KC1: 86.6%) than other methods. The proposed model also presents higher evaluation in the terms of other criteria. As a limitation, the only problem with SVM is there is no probabilistic explanation for classification which may very rigid towards classifications. In the future, some other method may also introduce which can overcome this limitation and keep a soft probabilistic based margin for classification on the optimal hyperplane. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",load balancing | machine learning | segment routing | software defined networks,"2020 International Conference Automatics and Informatics, ICAI 2020 - Proceedings",2020-10-01,Conference Paper,"Todorov, Dimitar;Valchanov, Hristo;Aleksieva, Veneta",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85089345974,10.1007/s00521-021-05811-3,Improved prediction of software defects using ensemble machine learning techniques,"Software testing process is a crucial part in software development. Generally the errors made by developers get fixed at a later stage of the software development process. This increases the impact of the defect. To prevent this, defects need to be predicted during the initial days of the software development, which in turn helps in efficient utilization of the testing resources. Defect prediction process involves classification of software modules into defect prone and non-defect prone. This paper aims to reduce the impact of two major issues faced during defect prediction, i.e., data imbalance and high dimensionality of the defect datasets. In this research work, various software metrics are evaluated using feature selection techniques such as Recursive Feature Elimination (RFE), Correlation-based feature selection, Lasso, Ridge, ElasticNet and Boruta. Logistic Regression, Decision Trees, K-nearest neighbor, Support Vector Machines and Ensemble Learning are some of the algorithms in machine learning that have been used in combination with the feature extraction and feature selection techniques for classifying the modules in software as defect prone and non-defect prone. The proposed model uses combination of Partial Least Square (PLS) Regression and RFE for dimension reduction which is further combined with Synthetic Minority Oversampling Technique due to the imbalanced nature of the used datasets. It has been observed that XGBoost and Stacking Ensemble technique gave best results for all the datasets with defect prediction accuracy more than 0.9 as compared to algorithms used in the research work. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.",big data | digital twin | machine learning | predictive maintenance | smart power distribution system,"2020 3rd International Conference on Artificial Intelligence and Big Data, ICAIBD 2020",2020-05-01,Conference Paper,"Zhang, Ganghong;Huo, Chao;Zheng, Libin;Li, Xinjun",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85114557401,10.1109/DSN-W52860.2021.00037,Byzantine Fault-Tolerant Distributed Machine Learning with Norm-Based Comparative Gradient Elimination,"This paper considers the Byzantine fault-tolerance problem in distributed stochastic gradient descent (D-SGD) method - a popular algorithm for distributed multi-agent machine learning. In this problem, each agent samples data points independently from a certain data-generating distribution. In the fault-free case, the D-SGD method allows all the agents to learn a mathematical model best fitting the data collectively sampled by all agents. We consider the case when a fraction of agents may be Byzantine faulty. Such faulty agents may not follow a prescribed algorithm correctly, and may render traditional D-SGD method ineffective by sharing arbitrary incorrect stochastic gradients. We propose a norm-based gradient-filter, named comparative gradient elimination (CGE), that robustifies the D-SGD method against Byzantine agents. We show that the CGE gradient-filter guarantees fault-tolerance against a bounded fraction of Byzantine agents under standard stochastic assumptions, and is computationally simpler compared to many existing gradient-filters such as multi-KRUM, geometric median-of-means, and the spectral filters. We empirically show, by simulating distributed learning on neural networks, that the fault-tolerance of CGE is comparable to that of existing gradient-filters. We also empirically show that exponential averaging of stochastic gradients improves the fault-tolerance of a generic gradient-filter. © 2021 IEEE.",,"Proceedings - 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks Workshops, DSN-W 2021",2021-06-01,Conference Paper,"Gupta, Nirupam;Liu, Shuo;Vaidya, Nitin",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85123530378,10.1007/s00500-021-05689-2,On the classification of bug reports to improve bug localization,"Bug localization is the automated process of finding the possible faulty files in a software project. Bug localization allows developers to concentrate on vital files. Information retrieval (IR)-based approaches have been proposed to assist automatically identify software defects by using bug report information. However, some bug reports that are not semantically related to the relevant code are not helpful to IR-based systems. Running an IR-based reporting system can lead to false-positive results. In this paper, we propose a classification model for classifying a bug report as either uninformative or informative. Our approach helps to lower false positives and increase ranking performances by filtering uninformative information before running an IR-based bug location system. The model is based on implicit features learned from bug reports that use neural networks and explicit features defined manually. We test our proposed model on three open-source software projects that contain over 9000 bug reports. The results of the evaluation show that our model enhances the efficiency of a developed IR-based system in the trade-off between precision and recall. For implicit features, our tests with comparisons show that the LSTM network performs better than the CNN and multilayer perceptron with respect to the F-measurements. Combining both implicit and explicit features outperforms using only implicit features. Our classification model helps improve precision in bug localization tasks when precision is considered more important than recall. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Accuracy | Artificial intelligence | Deep neural networks | Edge AI | Edge computing | Energy efficiency | Latency | Machine learning | Reliability | Robustness | Security | Spiking neural networks | TinyML,"IEEE/ACM International Conference on Computer-Aided Design, Digest of Technical Papers, ICCAD",2021-01-01,Conference Paper,"Shafique, Muhammad;Marchisio, Alberto;Putra, Rachmad Vidya Wicaksana;Hanif, Muhammad Abdullah",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85096708190,10.1002/spe.2927,Predicting just-in-time software defects to reduce post-release quality costs in the maritime industry,"Background: The importance of software in maritime transportation is rapidly increasing as the industry seeks to develop and utilize innovative future ships, which can be realized using software technology. Due to the safety-critical nature of ships, software quality assurance (SQA) has become an essential prerequisite for such development. Objective: Based on the unique characteristics of the maritime domain, the purpose of this study was to achieve effective SQA resource allocation to reduce post-release quality costs. Method: Software defect prediction (SDP) is employed to predict defects in newly developed software based on models trained with past software defects and to update information using machine learning. This study demonstrated that just-in-time SDP is applicable to maritime domain practice and can reduce post-release quality costs via combination with an estimation model, qCOPLIMO. Results: Using real-world datasets collected from the maritime industry, performance and cost-benefit analyses of SDP were performed. A successful model was obtained that meets the performance criterion of 0.75 in within-project defect prediction (WPDP) but not cross-project defect prediction (CPDP). In addition, the cost-benefit analysis results showed that 20% effort enables the detection of 56% of defects on average and that the post-release quality cost can be reduced by 37.3% in the maritime domain. Conclusion: SDP can be successfully applied to the maritime domain. Further, it is desirable to utilize WPDP instead of CPDP once minimum high-quality commits are available that can be identified as defective or not. Finally, SDP can help reduce review effort and post-release quality costs. © 2020 John Wiley & Sons Ltd",industrial application | just-in-time prediction | maintenance | maritime transportation | software defect prediction | software quality assurance,Software - Practice and Experience,2021-04-01,Conference Paper,"Kang, Jonggu;Ryu, Duksan;Baik, Jongmoon",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85106576531,10.1109/SANER50967.2021.00056,Enhancing Just-in-Time Defect Prediction Using Change Request-based Metrics,"Identifying defective software components as early as their commit helps to reduce significant software development and maintenance costs. In recent years, several studies propose to use just-in-time (JIT) defect prediction techniques to identify changes that could introduce defects at check-in time. To predict defect introducing changes, JIT defect prediction approaches use change metrics collected from software repositories. These change metrics, however, capture code and code change related information. Information related to the change requests (e.g., clarity of change request and difficulty to implement the change) that could determine the change's proneness to introducing new defects are not studied. In this study, we propose to augment the publicly available change metrics dataset with six change request- based metrics collected from issue tracking systems. To build the prediction model, we used five machine learning algorithms: AdaBoost, XGBoost, Deep Neural Network, Random Forest and Logistic Regression. The proposed approach is evaluated using a dataset collected from four open source software systems, i.e., Eclipse platform, Eclipse JDT, Bugzilla and Mozilla. The results show that the augmented dataset improves the performance of JIT defect prediction in 19 out of 20 cases. F1-score of JIT defect prediction in the four systems is improved by an average of 4.8%, 3.4%, 1.7%, 1.1% and 1.1% while using AdaBoost, XGBoost, Deep Neural Network, Random Forest and Logistic Regression, respectively. © 2021 IEEE.",Issue Tracking Systems | Just-in-Time Software Defect Prediction | Software Bugs | Software Metrics,"Proceedings - 2021 IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2021",2021-03-01,Conference Paper,"Tessema, Hailemelekot Demtse;Abebe, Surafel Lemma",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85105020930,10.1016/j.micpro.2020.103538,Software Reliability Growth Fault Correction Model Based on Machine Learning and Neural Network Algorithm,"Dependence is one of programming quality points can be quantified. Software Reliability Growth Model (SRGM) used to study the reliability in various difficult times available. In all cases the test execution over time, the traditional time-sensitive SRGM may not be accurate. To overcome this problem, SRGM practice rather than using the test of time. Previously, the proposed limited testing athletic ability, but with unlimited test time, test motion becomes infinite and may not make sense. Then, this method presents endless test execution work related to older models of Neural Heterogeneous Poisson Process (NHPP) of. Programming model disappointment with the proposed information to prepare Artificial Neural Network (ANN). They can imagine a lot of the load placed on a similar model to represent the previous disappointment information. Use AI model selection method can adequately represent the past and future load fitting location information. Consider using common-sense information programming disappointed indicators to show the proposed number of Testing Exertion Flow (TEF) and SRGM also successfully depicted disappointment extensive information, Machine Learning (ML) and ANN improved the accuracy of the conventional boundary estimation as compared with the discharge time and can be used to ensure that programming. © 2020",Big Data | Binary Class Imbalanced Data | Imbalanced Clustering | Imbalanced data | Machine learning | Multi-Class Imbalanced Data,"2021 International Conference on Computer Communication and Informatics, ICCCI 2021",2021-01-27,Conference Paper,"Rekha, G.;Tyagi, Amit Kumar;Sreenath, N.;Mishra, Shashvi",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85103826746,10.1109/Confluence51648.2021.9377116,Application of particle swarm optimization for software defect prediction using object oriented metrics,"With the growing number of software applications being developed for every small challenge, the importance of devising efficient software defect prediction models is imperative. Over the years, various machine learning techniques have been utilized to develop defect prediction model and have managed to achieve good results. In all defect prediction models, the task of correcting unbalanced data and feature selection has been of great significance. In this paper we have tried to analyze the working of the oversampling technique S M O T E along with feature selection using Particle Swarm Optimization on Object Oriented metrics. The selected features were then used to train the datasets one of the most popular classification techniques- Support Vector Machine to predict defects. The four datasets used for this study are of different Apache applications whose source code was obtained from open-source platforms and the raw data was pre-processed to obtain Object Oriented metrics. The performance measures used to record the results were Area Under R O C Curve, Recall and F-Measure values which showed that the Support Vector Classifier performed better on the dataset that had been balanced using S M O T E and acted upon by Particle Swarm Optimization for selecting feature set. © 2021 IEEE",Object Orientedmetrics | Particle Swarm Optimization | SMOTE | Software Defect Prediction | Support Vector Machine,"Proceedings of the Confluence 2021: 11th International Conference on Cloud Computing, Data Science and Engineering",2021-01-28,Conference Paper,"Malhotra, Ruchika;Nishant, ;Gurha, Spandun;Rathi, Vishal",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85103803248,10.1007/978-981-15-7527-3_19,Evaluation of LMT and DNN Algorithms in Software Defect Prediction for Open-Source Software,"Software defect prediction has achieved considerable salability in the last years. Defect prediction is the first important step in dependability assessment of complex software systems, because it can immediately be impacted on the dependability of those systems, improves performance, and decreases the software cost. Among the most important approaches used primarily in software defect prediction are the algorithms of machine learning classification. In this research, LMT machine learning and deep learning algorithms are presented for the software bug prediction model process on a general dataset obtained by a combination of several datasets for four open-source software and applications. These are Linux kernel, MySQL DBMS, Apache HTTPD web server, and Apache AXIS WS. A new dataset has represented four defect classes: Bohrbug, Mandelbug, Aging-Related Bug, and Unknown bugs. The new classification approach depends on defect location in OSSs. The system behavior is categorized to assess system dependability in the future. Performance measurement is implemented in JAVA using NETBEANS version 8.0.2 for LMT classifier and in Python for Deep Learning classifier. In the results of experiments, fault predictors using the DNN classifier perform better than LMT, where accuracy weighted average for each class using the LMT classifier is 0.849, while accuracy weighted average for each class using DNN is 0.87. © 2021, Springer Nature Singapore Pte Ltd.",Bugs | Deep learning | Defects | Dependability | Fault predication | Machine learning,Advances in Intelligent Systems and Computing,2021-01-01,Conference Paper,"Alazawi, Sundos Abdulameer;Salam, Mohammed Najim Al",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85070530233,10.1080/17445760.2019.1650039,Cross-project defect prediction using data sampling for class imbalance learning: an empirical study,"The presence of defect data related to different projects leads to cross-project defect prediction an open issue in the field of research in software engineering. In cross-project defect prediction, the source and the target projects are different. The prediction model is trained by using the data sources of the different projects and then it is tested on the target data source. The data source from the varying projects leads to a highly imbalanced source dataset. The performance of the predictive model degrades due to this imbalance nature of the dataset. This is termed as the class imbalance problem in machine learning. This paper conducts an empirical analysis in a bi-fold manner. It evaluates whether data sampling techniques can handle the class imbalance problem and improve the performance of the predictive model for cross-project defect prediction (CPDP). Secondly, it also evaluates whether the results of CPDP after data sampling are comparable to within project defect prediction (WPDP). Ensemble learning classifiers are used as the predictive model over 12 publically available object-oriented project datasets. The experimental results infer that SMOTE oversampling can be applied to overcome the problem of class imbalance on CPDP. It also gives comparable results to WPDP with statistical significance. © 2019 Informa UK Limited, trading as Taylor & Francis Group.",class imbalance learning | Cross project defect prediction (CPDP) | data sampling | ensemble learning | synthetic minority oversampling technique (SMOTE) | within project defect prediction (WPDP),"International Journal of Parallel, Emergent and Distributed Systems",2021-01-01,Article,"Goel, Lipika;Sharma, Mayank;Khatri, Sunil Kumar;Damodaran, D.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85102859382,10.1145/3439961.3439979,Predicting Software Defects with Explainable Machine Learning,"Most software systems must evolve to cope with stakeholders' requirements and fix existing defects. Hence, software defect prediction represents an area of interest in both academia and the software industry. As a result, predicting software defects can help the development team to maintain substantial levels of software quality. For this reason, machine learning models have increased in popularity for software defect prediction and have demonstrated effectiveness in many scenarios. In this paper, we evaluate a machine learning approach for selecting features to predict software module defects. We use a tree boosting algorithm that receives as input a training set comprising records of software features encoding characteristics of each module and outputs whether the corresponding module is defective prone. For nine projects within the widely known NASA data program, we build prediction models from a set of easy-To-compute module features. We then sample this sizable model space by randomly selecting software features to compose each model. This significant number of models allows us to structure our work along model understandability and predictive accuracy. We argue that explaining model predictions is meaningful to provide information to developers on features related to each module defective-prone. We show that (i) features that contribute most to finding the best models may vary depending on the project, and (ii) effective models are highly understandable based on a survey with 40 developers. © 2020 ACM.",explainable models | NASA datasets | SHAP values | software defects,ACM International Conference Proceeding Series,2020-12-01,Conference Paper,"Santos, Geanderson;Figueiredo, Eduardo;Veloso, Adriano;Viggiato, Markos;Ziviani, Nivio",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85102389452,10.1109/APSEC51365.2020.00016,Software defect prediction and localization with attention-based models and ensemble learning,"Software defect prediction (SDP) utilizes a trained prediction model to predict the defect proneness of code modules in a software system by mining the inherent characteristics of historical defect data. An effective model can optimize the allocation of testing resources, thus improving the quality of software products. Most previous studies use handcrafted features to represent code snippets, but the main problem is that it is difficult to capture the semantic and structural information of the code context, which is often crucial for software defect prediction. Meanwhile, most of the existing software defect prediction models cannot make predictions at the code line level, which makes it extremely arduous to provide developers with more detailed reference information. To address these issues, in this paper, we propose a model based on ensemble learning techniques and attention mechanisms to offer more comprehensive prediction information to developers by locating suspect lines of code when making method-level defect predictions. This model leverages abstract syntax trees (ASTs) as the intermediate representation of code snippets. Since the historical defect data has a striking characteristic of class-imbalance, an approach based on Self-organizing Map (SOM) clustering is employed to handle noisy data. Experimental results show that, on average, the proposed model improves the F-measure by 17.7% and AUC by 37.8%, compared with the other four machine learning algorithms. © 2020 IEEE.",Attention model | Deep learning | Ensemble learning | Software defect prediction,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",2020-12-01,Conference Paper,"Zhang, Tianhang;Du, Qingfeng;Xu, Jincheng;Li, Jiechu;Li, Xiaojun",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85099318439,10.1109/QRS51102.2020.00056,On the Defect Prediction for Large Scale Software Systems-From Defect Density to Machine Learning,"As the software industry transitions to software-As-A-service (SAAS) model, there has been tremendous competitive pressure on companies to improve software quality at a much faster rate than before. The software defect prediction (SDP) plays an important role in this effort by enabling predictive quality management during the entire software development lifecycle (SDLC). The SDP has traditionally used defect density and other parametric models. However, recent advances in machine learning and artificial intelligence (ML/AI) have created a renewed interest in ML-based defect prediction among academic researchers and industry practitioners. Published studies on this subject have focused on two areas, i.e. model attributes and ML algorithms, to develop SDP models for small to medium sized software (mostly opensource). However, as we present in this paper, ML-based SDP for large scale software with hundreds of millions of lines of code (LOC) needs to address challenges in additional areas called ""Data Definition""and ""SDP Lifecycle.""We have proposed solutions for these challenges and used the example of a large-scale software (IOS-XE) developed by Cisco Systems to show the validity of our solutions. © 2020 IEEE.",large scale software | machine learning | Software defect prediction | software quality | software quality analytics,"Proceedings - 2020 IEEE 20th International Conference on Software Quality, Reliability, and Security, QRS 2020",2020-12-01,Conference Paper,"Pradhan, Satya;Nanniyur, Venky;Vissapragada, Pavan K.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85102188506,10.1109/ICIMCIS51567.2020.9354282,Tackling Feature Selection Problems with Genetic Algorithms in Software Defect Prediction for Optimization,"Software defect prediction is a way to improve quality by finding and tracking defective modules in the software which helps reduce costs during the software testing process. The use of machine learning methods for predicting software defects can be applied to predict defects in each software module. However, basically the software defect prediction dataset has two problems, namely class imbalance with very few defective modules compared to non-defective modules and contains noisy attributes due to irrelevant features. With these two problems, it will result in overfitting and lead to biased classification results so that it will have an impact on significantly reducing the performance of the machine learning model. In this study, we propose the implementation of bagging techniques and genetic algorithms to improve the classification performance of machine learning models in predicting software defects based Logistic Regression, Naive Bayes, SVM, KNN, Decision Tree. Bagging techniques and Genetic algorithms are approaches that can handle two main problems in software defects prediction, each of which can handle the class imbalance and feature selection problem. We used 6 NASA Promise datasets to evaluate the classification performance results based on AUC and G-Means values. The results using 10 cross-validations show that the proposed method can improve classification performance when compared to the original algorithm. The Decision Tree shows the highest performance of the 3 datasets tested, with the highest value of 94.61 % on the KC4 dataset. We also compare GA performance with another natural algorithm, Particle Swarm Optimization (PSO). The results show that the performance of all machine learning models with GA can outperform the algorithms with PSO. © 2020 IEEE.",Bagging | Class Imbalance | Feature Selection | Genetic Algorithm | Particle Swarm Optimization | Software Defect Prediction,"Proceedings - 2nd International Conference on Informatics, Multimedia, Cyber, and Information System, ICIMCIS 2020",2020-11-19,Conference Paper,"Bahaweres, Rizal Broer;Imam Suroso, Arif;Wahyu Hutomo, Alam;Permana Solihin, Indra;Hermadi, Irman;Arkeman, Yandra",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85099562883,10.1109/INOCON50539.2020.9298203,Predicting Bug in a Software using ANN Based Machine Learning Techniques,"To achieve a successful software, bug prediction mainly place an important role. Nowadays, bug prediction has become an essential argument in the maintenance and development phase. So, it's necessary to predict bugs in earlier stages of software development life cycle. The challenge here is to develop a model that helps in predicting bugs leading to good quality, reliable, efficient and cost-effective software. For complex software projects, bugs are major issues. The proposed model provides the comparative analysis on various machine learning algorithms that are developed to predict bugs namely Random Forest, Logistic regression, Decision Tree, Artificial Neural Network and Naïve Bayes. Here Artificial Neural Network is used along with other algorithms such that the model can be trained for large datasets, in order to get more accurate results working effectively for various scenarios. The performance of each model is evaluated, and cross validation is performed followed by visualizing the results. Finally, when all the models are compared Artificial Neural Network appears to be the best model by providing 82.77%. © 2020 IEEE.",Artificial Neural Network | Bug | Cross Validation | Decision Tree | Logistic Regression | Machine Learning | Model Performance | Naive Bayes | Random Forest | Software Bug Prediction,"2020 IEEE International Conference for Innovation in Technology, INOCON 2020",2020-11-06,Conference Paper,"Rashmi, P.;Kambli, Prashanth",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84992650855,10.1109/ICSME46990.2020.00044,CounterFault: Value-Based Fault Localization by Modeling and Predicting Counterfactual Outcomes,"This paper presents a new, flexible approach to automatically localizing faults in software, named CounterFault. It uses a form of causal inference called counterfactual prediction to predict the effect, on the success or failure of an execution Ex, of intervening at a statement s to set an assignment target A to a value a that is not actually assigned to A in Ex but that could be if s or Ex was modified. CounterFault generates this prediction without actually modifying s or Ex, by employing a very flexible non-parametric statistical or machine learning model (e.g., a random forest). CounterFault applies this basic idea to estimate, with minimal confounding bias, the average causal effects on program failures of different changes in the values assigned to program variables, and these estimates are then employed to derive suspiciousness scores, which are used to assist developers in localizing faults. This paper also reports on an empirical evaluation of CounterFault involving the widely used Defects4J evaluation framework, which contains real software faults, as well as several other Java numerical programs. CounterFault is compared empirically with two other value-based fault localization techniques and four of the best performing coverage-based techniques. The results indicate that CounterFault is more effective than the competing techniques. © 2020 IEEE.",feature selection | instance selection | machine learning classifier | Open source projects | representative values | selection techniques,"2016 International Conference on Computation of Power, Energy, Information and Communication, ICCPEIC 2016",2016-08-31,Conference Paper,"Govindasamy, V.;Akila, V.;Anjanadevi, G.;Deepika, H.;Sivasankari, G.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85104600959,10.1109/UBMK50275.2020.9219531,Performance evaluation of some machine learning algorithms in NASA defect prediction data sets,"The main purpose of machine learning is to model the systems making predictions by using some mathematical and operational features on the data with computers [1]. Today, there are many studies on machine learning in all areas of the software world. Software Defect Prediction is a sub-branch that progresses rapidly in machine learning. In this study, five of machine learning classification algorithms were conducted on with PYTHON programming language on defect prediction data sets which are JM1, KC1, CM1, PC1 in the PROMISE repository. These data sets are created within the scope of the publicly available NASA institution's Metric Data Program. The accuracy, recall, precision and F-measure and support values of the algorithms on the data are compared. When the results are examined in terms of the accuracy of machine learning algorithms, the accuracy rates of the algorithms are quite high in all 4 data sets. The highest success rates were obtained from the classification algorithms applied in 4 data sets in CMl and PCI data sets. In 4 data sets, the highest success rates were seen with Random Forest algorithm. © 2020 IEEE.",,"2021 International Conference on Code Quality, ICCQ 2021",2021-03-27,Conference Paper,"Raychev, Veselin",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85090125949,,Deep hybrid features for code smells detection,"Code smells are symptoms of poor software design and implementation choices. Previous empirical studies have underlined their negative effect on software comprehension, fault-proneness and maintainability. A number of approaches have been proposed to identify the existence of code smells in the source code; recent studies have shown the potential of machine learning models in this context. However, previous approaches did not exploit the lexical and syntactical features of the source code; they instead modelled the source code using software metrics only. This paper proposes an approach for detecting the occurrence of the God class smell which utilizes both, the source code textual features and metrics to train three deep learning networks (i) Long short term memory, (ii) Gated recurrent unit and (iii) Convolutional neural network. We proposed utilizing deep leaning networks as they are reported to outperform traditional machine learning models in several domains including software engineering. To assess the proposed approach, a dataset for the God class smell was built using source codes acquired from the ""Qualitas Corpus"". Experimental results demonstrated that, the three deep learning networks outperformed three traditional machine learning models: Naïve Bayes, Random forests and Decision trees. Additionally, of the three deep learning networks the Gated recurrent unit model is the superior in this context. Furthermore, combining both, the source code metrics and textual features enhanced the accuracy of detecting the God class smell. © 2020 Little Lion Scientific. All rights reserved.",CNN; Code smells; Deep learning; God class; GRU; IR; LSTM; Software maintenance; Text mining; VSM,Journal of Theoretical and Applied Information Technology,2020,,"Hamdy A., Tazy M.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85092019543,10.1109/ICIRCA48905.2020.9183110,Software Defect Density Prediction based on Multiple Linear Regression,"The software quality is an indicator of the ability of a software system to function in accordance with the customer requirements. In maintaining an acceptable software quality level, defect density plays a major role. With the increasing complexity of the software, the chances of defects are also increasing. In this paper, a machine learning-based model for the prediction of the defect density of the future version of an open-source software (OSS) using the multiple linear regression technique is presented. The data of the OSS versions has been collected from the version control system, Git. Software metrics such as source lines of code, number of developers, number of commits, and code churn affect the defect density of the software, therefore, these metrics are selected as the predictor variables. The normality test of each of the variables is performed to show that the variable data belongs to a normal distribution. The results show that how much variability of defect density is predicted by these software metrics. © 2020 IEEE.",Defect Density | Multiple Linear Regression | Python | Software metrics | SPSS Tool,"Proceedings of the 2nd International Conference on Inventive Research in Computing Applications, ICIRCA 2020",2020-07-01,Conference Paper,"Rathaur, Suraj;Kamath, Narayan;Ghanekar, Umesh",Include,
10.1016/j.infsof.2022.107128,,10.1145/3377812.3381396,A composed technical debt identification methodology to predict software vulnerabilities,"Technical debt (TD), its impact on development and its consequences such as defects and vulnerabilities, are of common interestand great importance to software researchers and practitioners.Although there exist many studies investigating TD, the majorityof them focuses on identifying and detecting TD from a single stageof development. There are also studies that analyze vulnerabilities focusing on some phases of the life cycle. Moreover, severalapproaches have investigated the relationship between TD and vulnerabilities, however, the generalizability and validity of findingsare limited due to small dataset. In this study, we aim to identifyTD through multiple phases of development, and to automaticallymeasure it through data and text mining techniques to form acomprehensive feature model. We plan to utilize neural networkbased classifiers that will incorporate evolutionary changes on TDmeasures into predicting vulnerabilities. Our approach will be empirically assessed on open source and industrial projects. © 2020 Copyright held by the owner/author(s).",Feature engineering; Machine learning; Software security; Technical debt,Proceedings - International Conference on Software Engineering,2020,,Halepmollasi R.,Include,
10.1016/j.infsof.2022.107128,2-s2.0-85070247711,10.1145/3387940.3391464,OffSide: Learning to Identify Mistakes in Boundary Conditions,"Mistakes in boundary conditions are the cause of many bugs in software. These mistakes happen when, e.g., developers make use of '<' or '>' in cases where they should have used '<=' or '>='. Mistakes in boundary conditions are often hard to find and manually detecting them might be very time-consuming for developers. While researchers have been proposing techniques to cope with mistakes in the boundaries for a long time, the automated detection of such bugs still remains a challenge. We conjecture that, for a tool to be able to precisely identify mistakes in boundary conditions, it should be able to capture the overall context of the source code under analysis. In this work, we propose a deep learning model that learn mistakes in boundary conditions and, later, is able to identify them in unseen code snippets. We train and test a model on over 1.5 million code snippets, with and without mistakes in different boundary conditions. Our model shows an accuracy from 55% up to 87%. The model is also able to detect 24 out of 41 real-world bugs; however, with a high false positive rate. The existing state-of-the-practice linter tools are not able to detect any of the bugs. We hope this paper can pave the road towards deep learning models that will be able to support developers in detecting mistakes in boundary conditions. © 2020 ACM.",Failure recovery | Fault tolerance | Machine learning | Proactive rerouting | Software defined networking,"2019 IEEE International Conference on Communications Workshops, ICC Workshops 2019 - Proceedings",2019-05-01,Conference Paper,"Truong-Huu, Tram;Prathap, Prarthana;Mohan, Purnima Murali;Gurusamy, Mohan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85087651182,10.1109/ISDFS49300.2020.9116209,Classifying Software Vulnerabilities by Using the Bugs Framework,"Software vulnerabilities, specific type of software bugs, are defined as occurrences of a software weakness, which can be exploited by an agent to cause various consequences such as modifying or accessing unintended data. Identifying and fixing software vulnerabilities thus plays an important role in software security and software engineering. A reliable body of knowledge on categories of vulnerabilities is critical to identify software vulnerabilities. In this paper, we use data-mining techniques to identify software vulnerabilities, classify them into different categories by using the Bugs Framework proposed by the National Institute of Standards and Technology (NIST), and design a model to predict the weakness of future vulnerabilities. Knowledge about vulnerability types helps software engineers save time and energy, develop programs by avoiding security vulnerabilities, and program with precaution. © 2020 IEEE.",Bugs Framework | Machine Learning | Software Vulnerabilities,"8th International Symposium on Digital Forensics and Security, ISDFS 2020",2020-06-01,Conference Paper,"Adhikari, Thamali Madhushani;Wu, Yan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85083030688,10.1145/3341105.3374008,Detecting architectural integrity violation patterns using machine learning,"Recent1 years have seen a surge of research into new ways of analyzing software quality. Specifically, a set of studies has been devoted to the impact the architectural relations among files have on system maintainability and file bug-proneness. The literature has proposed a set of rules for determining recurring architectural design flaws that occur in most complex systems, are associated with bugs, and thus incur high maintenance costs. In the present paper we advocate for using machine learning as the means of refining the approach and revealing new patterns of architectural integrity violations. Having trained a machine learning model on the combination of structural and historical information acquired from the Tiki open source project, we have been able to replicate three of the six known types of architectural violations and discover one new type, the Reverse Unstable Interface pattern. The implication of our study is that machine learning can provide valuable insights into the problem and discover novel patterns which would help software analysts to pinpoint specific architectural problems that may be the root causes of elevated bug- and change-proneness. © 2020 ACM.",Architectural flaws | Bug-proneness | Hotspot patterns | Machine learning | Software architecture,Proceedings of the ACM Symposium on Applied Computing,2020-03-30,Conference Paper,"Zakurdaeva, Alla;Weiss, Michael;Muegge, Steven",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84980416420,10.1007/978-3-030-63823-8_75,Software Defect Prediction with Spiking Neural Networks,"Software defect prediction is one of the most active research areas in software engineering and plays an important role in software quality assurance. In recent years, many new defect prediction studies have been proposed. There are four main aspects of research: machine learning-based prediction algorithms, manipulating the data, effor-softaware prediction and empirical studies. The research community is still facing many challenges in constructing methods, and there are also many research opportunities in the meantime. This paper proposes a method of applying spiking neural network to software defect prediction. The software defect prediction model is constructed by feed-forward spiking neural networks and trained by spike train learning algorithm. This model uses the existing project data sets to predict software defects projects. Extensive experiments on 28 public projects from five data sources indicate that the effectiveness of the proposed approach with respect to the considered metrics. © 2020, Springer Nature Switzerland AG.",,"2016 IEEE 32nd International Conference on Data Engineering, ICDE 2016",2016-06-22,Conference Paper,"Fernandez, Raul Castro;Garefalakis, Panagiotis;Pietzuch, Peter",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85116573570,10.1016/j.procs.2020.08.004,CoMET: A conceptual coupling based metrics suite for software defect prediction,"Identifying defective software components is an essential activity during software development which contributes to continuously improving the software quality. Since relatively numerous defects are due to violated software dependencies, coupling metrics could increase the performance of software defect prediction. Among various measures expressing the coupling between software components, the conceptual coupling metrics capture similarities based on the semantic information contained in the source code. We are introducing a new conceptual coupling based metric suite, named COMET, for software defect prediction. Experiments conducted on publicly available data sets, using both unsupervised and supervised learning models, emphasize that COMET metrics suite is superior to the software metrics widely used in the defect prediction literature. © 2020 The Authors. Published by Elsevier B.V.",Algorithms | BigData | IoT | Machine Learning | Raspberry | Spark,"Proceedings - 2021 International Conference on Design Innovations for 3Cs Compute Communicate Control, ICDI3C 2021",2021-06-01,Conference Paper,"Sugathan, Divya;Revanth, R. M.;Kumar, V. Praveen;Desai, Sachin;Keshava, L.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85123420167,10.1007/978-3-030-52856-0_19,Training data selection using ensemble dataset approach for software defect prediction,"Cross-project defect prediction (CPDP) is using due to the limitation of within project defect prediction (WPDP) in Software Defect Prediction (SDP) research. CPDP aims to train one project data to predict another project using the machine learning technique. The source and target projects are different in the CPDP setting, because of various structured source-target projects, sometimes it may not be a perfect combination. This study represents a categorical data set ensemble technique, where multiple data sets have been aggregated for source data instead of using a single data set. The method has been evaluated on nine data sets, taken from the publicly accessible repository with two performance indicators. The results of this data set ensemble approach show the improvement of the prediction performance over 65% combinations compared with traditional CPDP models. The results also show that same categories (homogeneous) train-test data set pairs give high performance; otherwise, the prediction performances of different category data sets are mostly collapsed. Therefore, the proposed scheme is recommended as an alternative to predict defects that can improve the prediction of most of the cases compared with traditional cross-project SDP models. © ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2020.",Long Short-Term Memory | Low Density Parity Checks | Memory leaks | Software defects | virtualization,"2021 4th International Conference on Electrical, Computer and Communication Technologies, ICECCT 2021",2021-01-01,Conference Paper,"Turikumwe, Jean Paul;Wilson, Cheruiyot;Kibe, Anne",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85079675720,10.1007/978-981-15-0199-9_25,Heterogeneous Defect Prediction Using Ensemble Learning Technique,"One of the quite frequently used approaches that programmers adhere to during the testing phase is the software defect prediction of the life cycle of the software development, this testing becomes utmost important as it identifies potential error before the product is delivered to the clients or released in the market. Our primary concern is to forecast the errors by using an advanced heterogeneous defect prediction model based on ensemble learning technique which incorporates precisely eleven classifiers. Our approach focuses on the inculcation of supervised machine learning algorithms which paves the way in predicting the defect proneness of the software modules. This approach has been applied on historical metrics dataset of various projects of NASA, AEEEM and ReLink. The dataset has been taken from the PROMISE repository. The assessment of the models is done by using the area under the curve, recall, precision and F-measure. The results obtained are then compared to the methods that exist for predicting the faults. © Springer Nature Singapore Pte Ltd 2020.",Cross-project defect prediction (CPDP) | Ensemble learning | Heterogeneous defect prediction (HDP) | Software defect prediction (SDP) | Within-project defect prediction (WPDP),Advances in Intelligent Systems and Computing,2020-01-01,Conference Paper,"Ansari, Arsalan Ahmed;Iqbal, Amaan;Sahoo, Bibhudatta",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85081362707,,Gradient tree boosting approach for software defect prediction,"Prediction of the actual defect in software has remained a challenging task for the software developers. Any deviation in counting the no. of defects or estimating the defects may lead to serious problems like the unexpected outcome. Majorly, defects like time, cost and effort have to be computed effectively at the initial phase of software development. Some of the early developed data mining approaches are developed for quality analysis and defect prediction. But for large scale software development, these techniques are not performing well due to the high nonlinearity nature of data. This paper proposes a novel gradient boosting based machine learning approach for the effective prediction of a software defect. The proposed method has been analyzed with various performance-related factors and found to be superior among nine competitive machine learning approaches. © 2019 SERSC.",Gradient boosting; Machine learning; Software defect prediction; Software quality,International Journal of Advanced Science and Technology,2019,,"Eswara Rao K., Appa Rao G., Anuradha S.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85077813526,10.1145/3368926.3369711,Unsupervised methods for Software Defect Prediction,"Software Defect Prediction (SDP) aims to assess software quality by using machine learning techniques. Recently, by proposing the connectivity-based unsupervised learning method, Zhang et al. have been proven that unsupervised classification has great potential to apply to this problem. Inspiring by this idea, in our work we try to replicate the results of Zhang et al.'s experiment and attempt to improve the performance by examining different techniques at each step of the approach using unsupervised learning methods to solve the SDP problem. Specifically, we try to follow the steps of the experiment described in their work strictly and examine three other clustering methods with four other ways for feature selection besides using all. To the best of our knowledge, these methods are first applied in SDP to evaluate their predictive power. For replicating the results, generally results in our experiments are not as good as the previous work. It may be due to we do not know which features are used in their experiment exactly. Fluid clustering and spectral clustering give better results than Newman clustering and CNM clustering in our experiments. Additionally, the experiments also show that using Kernel Principal Component Analysis (KPCA) or Non-Negative Matrix Factorization (NMF) for feature selection step gives better performance than using all features in the case of unlabeled data. Lastly, to make replicating our work easy, a lightweight framework is created and released on Github. © 2019 Association for Computing Machinery.",Community structure detection | Machine learning | Software Defect Prediction | Software engineering | Unsupervised learning,ACM International Conference Proceeding Series,2019-12-04,Conference Paper,"Ha, Duy An;Chen, Ting Hsuan;Yuan, Shyan Ming",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85078954844,10.1109/ICICAS48597.2019.00160,Analysis of Attribute Selection Method Based on IDOE in Software Fault Prediction,"This paper presents a new attribute selection method based on Importance Descending Order Exclusion (IDOE) to predict software fault. By using public datasets CM1 and JM1 from PROMISE repository, six machine learning algorithms-Naïve Bayes, LibSVM, Logistic, Multilayer Perceptron, SMO and Random forests integrated in Weka are used for analysis. Firstly, the GeneticSearch and Ranker search method is chosen to select attributes. Secondly, these machine learning algorithms are used in order to predict precision, recall and F-measure of software modules. Then the IDOE method is used to update the attribute datasets. Finally, the different iteration experiment results are compared and analyzed. © 2019 IEEE.",feature selection | prediction | software defect | software maintainability | Systematic literature review,"2019 International Conference on Electrical and Computing Technologies and Applications, ICECTA 2019",2019-11-01,Conference Paper,"Alsolai, Hadeel;Roper, Marc",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85089242743,10.1109/MACS48846.2019.9024799,Software Defect-Prone Classification using Machine Learning: A Virtual Classification Study between LibSVM & LibLinear,"The field of machine learning has been developing quickly, delivering an assortment of learning algorithms for various software applications. A definitive estimation of those algorithms is, all things considered, made a decision by their accomplishment in taking care of genuine issues. Machine Learning systems can be utilized to dissect data from alternate points of view and empower engineers to recover valuable data. Machine learning has been effectively applied to make forecasts in different datasets. Given the tremendous number of bug datasets accessible today, foreseeing the nearness of bugs also should be possible utilizing different machine learning procedures. The machine learning systems that can be utilized to identify bugs in software datasets by use of classification technique. Classification is a data mining and machine learning approach, helpful in software Defect-Prone model. It includes order of software modules into buggy, faulty or non-buggy, non-faulty that is signified by a lot of software intricacy measurements by using a classification model that is gotten from before improvement ventures data. we classifying effectiveness accuracy and efficiency software defect-prone model dependent on classification technique where we have utilized LibSVM and LibLinear classification. In our examination, we have seen that the during classification, LibSVM have increased the accuracy and efficiency special in train-set way. The TP-Rate and F-Measure positive accuracy is highly increased rather than other techniques. The area under curve is also enhanced using LibSVM in training datasets. But the correctly classified instances rate also increased in all classification. Bute with the use of % split, LibLinear an SVM are also good in few evaluation measures for their enhancement accuracy and efficiency. © 2019 IEEE.",Diversified Ensemble | Evolutionary Classifier of Ensemble | Reusability Software Prediction | Web-Service,"4th International Conference on Electrical, Electronics, Communication, Computer Technologies and Optimization Techniques, ICEECCOT 2019",2019-12-01,Conference Paper,"Parande, Prakash V.;Banga, M. K.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85080923042,10.1109/ICMLA.2019.00195,Software fault prediction based on fault probability and impact,"Nowadays, software tests prioritization is a crucial task. Indeed, testing exhaustively the whole software system can be very difficult, heavily time and resources consuming. Using machine learning algorithms to predict which parts of a software system are fault-prone can help testers to focus on high-risk parts of the code and improve resources allocation. This paper aims to investigate the potential of a risk-based model to predict fault-prone classes. The risk of classes is evaluated based on two factors: the probability that a class is fault-prone and its impact on the rest of the system. We used object-oriented metrics to capture the two risk factors. The risk of a class is modeled using the Euclidean distance. We built various variants of the risk-based model using a data-set from five versions of the ANT system. We used different machine learning algorithms (Naive Bayes, J48, Random Forest, Support Vector Machines, Multilayer Perceptron and Logistic Regression) to construct various models for fault and level of severity prediction. The objective was to distinguish between classes containing trivial and high severity faults. The considered model achieves good results for binary fault prediction. In addition, the overall multi-classification of severity levels is more than acceptable. © 2019 IEEE.",Fault Severity | Fault-Proneness | Machine Learning Algorithms | Object-Oriented Metrics | Prediction | Risk | Risk Based Model,"Proceedings - 18th IEEE International Conference on Machine Learning and Applications, ICMLA 2019",2019-12-01,Conference Paper,"Moudache, Salim;Badri, Mourad",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85083106752,10.1109/ICMLA.2019.00176,A study on software metric selection for software fault prediction,"For most software systems, superfluous software metrics are often collected. Sometimes, metrics that are collected may be redundant or irrelevant to fault prediction results. Feature (software metric) selection helps separating relevant software metrics from irrelevant or redundant ones, thereby identifying the small set of software metrics that are best predictors of fault proneness for new components, modules, or releases. In this study, we compare three forms of feature selection techniques (filter-and wrapper-based subset evaluators along with two search techniques (Best First (BF) and Greedy Stepwise (GS)), and feature ranking on four datasets from a real world software project. Five learners are used to build fault prediction models with the selected software metrics. Each model is assessed using the Area Under the Receiver Operating Characteristic Curve (AUC). We find that wrapper-based subset evaluators performed best and feature ranking performed worst. In addition, the model built with the logistic regression (LR) learner performs best in terms of the AUC performance metric. This leads us to recommend the use of the wrapper-based subset evaluators to select software metric subsets and the LR learner for building software fault prediction models. © 2019 IEEE.",Context Based Clearing (NGA) | Enhanced feature selection | Finger Vein | Gabor Transformation,ICENCO 2019 - 2019 15th International Computer Engineering Conference: Utilizing Machine Intelligence for a Better World,2019-12-01,Conference Paper,"Alhadethy, Ahmed H.;Darwish, Saad",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85078153846,10.1109/APSEC48747.2019.00045,Class Imbalance Data-Generation for Software Defect Prediction,"The imbalanced nature of class in software defect data, which including intra-class imbalance and inter-classes imbalance, increases the difficulty of learning an effective defect prediction model. Most of sampling and example generation approaches just focused on inter-class imbalanced defect data, and they are not effective to handle the issue of intra-class imbalance. This paper proposed a distribution based data generation approach for software defect prediction to deal with inter-class and intra-class imbalanced data simultaneously. First, the classified sub-regions are clustered according to the distribution in the sample feature space. Second, the data are generated by corresponding strategies according to different distribution in sub-regions, where the inter-class balance is achieved by increasing the number of defective samples, and the intra-class balance is achieved by generating different density of data in different sub-regions. Experiment results show that the proposed method can reduce the impact of data imbalance on defect prediction and improve the accuracy of software defect prediction model effectively by generating inter-class and intra-class balanced defects data. © 2019 IEEE.",data generation | imbalanced data | machine learning | software defect prediction,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",2019-12-01,Conference Paper,"Li, Zheng;Zhang, Xingyao;Guo, Junxia;Shang, Ying",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85091536679,10.1109/ISKE47853.2019.9170328,A Hybrid Data Preprocessing Technique based on Maximum Likelihood Logistic Regression with Filtering for Enhancing Software Defect Prediction,"Software Defect Prediction (SDP) is critical to ensure product reliability and customer satisfaction. Many studies conducted to predict defective modules in the software development process applied historical software defect data mined from online depositories. However, data obtained from online depositories have problems associated with high dimensionality and noisy. High dimensionality comes when the data contains a huge number of independent attributes for developing models of classification. Noisy data refer to those located deep inside the region of different class other than their own. The adverse effect of these data problems for the prediction performance of most statistical learning methodologies is self-evident. To this end, we suggest a hybrid preprocessing approach in which iterative partitioning filtering is conducted prior to feature selection (FS) and the technique is validated using selected FS methods including Chisquare (CS), information gain (IG), gain ratio (GR), relief (RF) and symmetric uncertainty (SU) and maximum likelihood logistic regression (MLLR). It is important to note that the idea of MLLR for FS in the context of software defect prediction have not been fully investigated despite the potential usefulness. For the combined use of variable selection and noise filtering (NF), two cases are considered: (1) variable selection technique applied on original dataset and relevant variables selected from original data (2) variable selection approach applied on a cleaned data and variables selected from cleaned data. We apply this method on six software defect datasets in their clean and noisy form to select useful features for classification modeling of Software Defect Prediction (SDP). The Random Forest (RaF), K-Nearest Neighbors (KNN) and Multilayer Perceptron (MLP) classifiers are tested on the selected feature. The performance of the models captured using three indicators are compared. The results confirm that the MLLR can be useful in selecting the optimal feature subset for more accurate prediction of defective modules in the software development process. © 2019 IEEE.",Machine learning | Maximum-likelihood logistic regression | Noise filtering | Software defect prediction,"Proceedings of IEEE 14th International Conference on Intelligent Systems and Knowledge Engineering, ISKE 2019",2019-11-01,Conference Paper,"Bashir, Kamal;Bashir, Kamal;Yahaya, Mahama;Hussein, Ahmed Saad",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84985902350,10.1109/AICCSA47632.2019.9035342,The impact of SMOTE and grid search on maintainability prediction models,"Software maintainability has gained more attention in recent years. It can be defined as the ease with which modifications can be performed. In this study, we are interested in the changes of a class due to bug fixing. We performed an experimental study using five Machine learning techniques; K-Nearest Neighbor (KNN), Support Vector Machine (SVM), Decision Trees (DT), Multilayer Perceptron (MLP), and Naïve Bayes (NB). The main focus in this study is to propose the use of Grid search method for tuning hyper-parameters and to balance datasets using SMOTE technique. The results show that there is no evidence concerning the best ML technique in all datasets. However, we found that balanced data and tuning parameters are suitable in order to obtain the best performance of ML techniques. © 2019 IEEE.",Empirical Validation | Machine Learning | Security Vulnerabilities | Security-Related Defects | Statistical Methods,"2016 1st International Conference on Innovation and Challenges in Cyber Security, ICICCS 2016",2016-08-11,Conference Paper,"Bansal, Ankita;Malhotra, Ruchika;Raje, Kimaya",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85083579430,10.1109/NSS/MIC42101.2019.9059737,Software Defect Prediction on Unlabelled Dataset with Machine Learning Techniques,"In this work, we are going to explore how to conduct defect software prediction on unlabelled datasets by exploiting unsupervised machine learning techniques. In previous literature, various approaches have been proposed over time with the aim of labelling an unlabelled dataset (i.e. classifying dataset instances/modules in terms of their defectiveness), they can usually rely on other software datasets, software experts or metrics' thresholds. In this study, we intend to show the results obtained by exploiting CLAMI and CLAMI+, two approaches that overcome the various limitation of the previous ones adopted by researches, since they are independent on metrics' threshold, do not need experts software knowledge and can be easily automated.Our prediction model takes as input a set of unlabelled datasets and machine learning techniques. Its output is composed of the values of several performance indicators on training datasets and predictions on test datasets. The latter can be employed to deduce information on the status of software code and, consequently, concentrate the software developers' effort only where necessary. © 2019 IEEE.",Sentiment Analysis | Software Quality | Text Analysis,"2019 IEEE Nuclear Science Symposium and Medical Imaging Conference, NSS/MIC 2019",2019-10-01,Conference Paper,"Ronchieri, Elisabetta;Canaparo, Marco;Belgiovine, Mauro;Salomoni, Davide",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84879874384,10.1109/KSE.2019.8919429,Experimental study on software fault prediction using machine learning model,"Faults are the leading cause of time consuming and cost wasting during software life cycle. Predicting faults in early stage improves the quality and reliability of the system and also reduces cost for software development. Many researches proved that software metrics are effective elements for software fault prediction. In addition, many machine learning techniques have been developed for software fault prediction. It is important to determine which set of metrics are effective for predicting fault by using machine learning techniques. In this paper, we conduct an experimental study to evaluate the performance of seven popular techniques including Logistic Regression, K-nearest Neighbors, Decision Tree, Random Forest, Naïve Bayes, Support Vector Machine and Multilayer Perceptron using software metrics from Promise repository dataset usage. Our experiment is performed on both method-level and class-level datasets. The experimental results show that Support Vector Machine archives a higher performance in class-level datasets and Multilayer Perception produces a better accuracy in method-level datasets among seven techniques above. © 2019 IEEE.",machine learning | scheme evaluation | Software defect prediction | software defectproneness prediction,"2013 International Conference on Information Communication and Embedded Systems, ICICES 2013",2013-07-12,Conference Paper,"Punitha, K.;Chitra, S.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85071910258,10.1145/3338906.3341466,Machine-learning supported vulnerability detection in source code,"The awareness of writing secure code rises with the increasing number of attacks and their resultant damage. But often, software developers are no security experts and vulnerabilities arise unconsciously during the development process. They use static analysis tools for bug detection, which often come with a high false positive rate. The developers, therefore, need a lot of resources to mind about all alarms, if they want to consistently take care of the security of their software project. We want to investigate, if machine learning techniques could point the user to the position of a security weak point in the source code with a higher accuracy than ordinary methods with static analysis. For this purpose, we focus on current machine learning on code approaches for our initial studies to evolve an efficient way for finding security-related software bugs. We will create a configuration interface to discover certain vulnerabilities, categorized in CWEs. We want to create a benchmark tool to compare existing source code representations and machine learning architectures for vulnerability detection and develop a customizable feature model. At the end of this PhD project, we want to have an easy-to-use vulnerability detection tool based on machine learning on code. © 2019 ACM.",Machine learning on code | Software security | Source code analysis | Vulnerabilities | Vulnerability detection,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Sonnekalb, Tim",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85073224084,10.1145/3348445.3348453,Predicting and classifying software faults: A data mining approach,"In the field of software engineering, the detection of fault in the software has become a major topic to explore. With the help of data mining and machine learning approaches, this paper aims to denote whether a software is fault prone or not. In order to accomplish that this paper gives importance to compare between different machine learning approaches and by observing their performances we can conclude which models perform better to detect fault in the selected software modules. The dataset we have chosen to work on has imbalanced data. This paper research also worked with the imbalanced dataset and what results the imbalanced dataset gave when examined. The accuracy comparison, the performance of the different metrics can broadly help in software defect detection mechanism. © 2019 Association for Computing Machinery.",Adaboost | Association rules | Data mining | Prediction | Software faults | SVM,ACM International Conference Proceeding Series,2019-07-27,Conference Paper,"Cynthia, Shamse Tasnim;Ripon, Shamim H.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85071535143,10.1145/3342999.3343010,Study on the influence of the number of features on the performance of software defect prediction model,"The software defect prediction model based on machine learning technology is the key to improve the reliability of software. The influence of the number of features on the performance of different software defect prediction models was proposed in this paper. First, a new data sets was built, which is increasing by the number of features based on the NASA public data sets. Then, the eight predictive models are experimented based on these data sets. Next, the influence of the number of features on the performance of different prediction models was analyzed based on the experimental results. Next, the AUC values obtained from the experiment were used to evaluate the performance of different prediction models, and the coefficient of variation C·V values was used to evaluate the performance stability of different prediction models while the number of features changed. In the end, the experiments show that the performance of the predictive model C4.5 is highly susceptible to changes in the number of features, while the performance of the predictive model SMO is relatively stable. © 2019 Association for Computing Machinery.",Feature selection | Machine learning | Number of features | Software defect prediction,ACM International Conference Proceeding Series,2019-07-05,Conference Paper,"Cui, Mengtian;Sun, Yue;Lu, Yang;Jiang, Yue",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85074961762,10.35940/ijitee.i8979.078919,Machine learning algorithms in software defect prediction analysis,"Programming deformity forecast assumes a vital job in keeping up great programming and decreasing the expense of programming improvement. It encourages venture directors to assign time and assets to desert inclined modules through early imperfection identification. Programming imperfection expectation is a paired characterization issue which arranges modules of programming into both of the 2 classifications: Defect– inclined and not-deformity inclined modules. Misclassifying imperfection inclined modules as not-deformity inclined modules prompts a higher misclassification cost than misclassifying not-imperfection inclined modules as deformity inclined ones. The machine learning calculation utilized in this paper is a blend of Cost-Sensitive Variance Score (CSVS), Cost-Sensitive Laplace Score (CSLS) and Cost-Sensitive Constraint Score (CSCS). The proposed Algorithm is assessed and indicates better execution and low misclassification cost when contrasted and the 3 algorithms executed independently. © BEIESP.",Cost-Sensitive learning | Feature selection | Software defect prediction,International Journal of Innovative Technology and Exploring Engineering,2019-07-01,Article,"Yalla, Prasanth;Meghana, Pasam;Sravanthi, Regula Chaitanya;Mandhala, Venkata Naresh",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85101263959,10.1109/MSR.2019.00018,Empirical study in using version histories for change risk classification,"Many techniques have been proposed for mining software repositories, predicting code quality and evaluating code changes. Prior work has established links between code ownership and churn metrics, and software quality at file and directory level based on changes that fix bugs. Other metrics have been used to evaluate individual code changes based on preceding changes that induce fixes. This paper combines the two approaches in an empirical study of assessing risk of code changes using established code ownership and churn metrics with fix inducing changes on a large proprietary code repository. We establish a machine learning model for change risk classification which achieves average precision of 0.76 using metrics from prior works and 0.90 using a wider array of metrics. Our results suggest that code ownership metrics can be applied in change risk classification models based on fix inducing changes. © 2019 IEEE.",Deep Convolutional Neural Networks | Medical Imaging | Quality Enhancement,"2020 IEEE 15th International Conference on Industrial and Information Systems, ICIIS 2020 - Proceedings",2020-11-26,Conference Paper,"Karthik, K.;Sowmya Kamath, S.;Kamath, Surendra U.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85068334437,10.1109/KBEI.2019.8734915,Improving Performance in Software Defect Prediction Using Variational Autoencoder,"Software defect prediction (SDP) is a beneficial task to save limited resources in the software testing stage for improving software quality. However, the imbalanced distribution in defect datasets could be a challenge for often machine learning algorithms, an effect on the performance of the algorithms. To overcome this issue, oversampling techniques from the minority class has been adopted. In this work, we suggest a new oversampling method, which trained a variational autoencoder (VAE) to generate synthesized samples aimed for output mimicked minority samples that were then combined with training dataset into an augmented training dataset. In the experiments, we explored ten SDP datasets from the PROMISE freely accessible repository. We measured the performance of the proposed method by comparing it with state-of-the-art oversampling techniques including Random Over-Sampling, SMOTE, Borderline-SMOTE, and ADASYN. Based on the investigation results, the proposed method provides better mean performance of SDP models between all examined techniques. © 2019 IEEE.",Class Imbalance | Over-sampling | Software Defect Prediction | Variational Autoencoder,"2019 IEEE 5th Conference on Knowledge Based Engineering and Innovation, KBEI 2019",2019-02-01,Conference Paper,"Eivazpour, Z.;Keyvanpour, Mohammad Reza",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85097651183,10.1007/978-3-030-34885-4_27,Contributing Features-Based Schemes for Software Defect Prediction,"Automated defect prediction of large and complex software systems is a challenging task. However, by utilising correlated quality metrics, a defect prediction model can be devised to automatically predict the defects in a software system. The robustness and accuracy of a prediction model is highly dependent on the selection of contributing and non-contributing features. Hence, in this regard, the contribution of this paper is twofold, first it separates those features which are contributing towards the development of a defect in a software component from those which are non-contributing features. Secondly, a logistic regression and Ensemble Bagged Trees-based prediction model are applied on the contributing features for accurately predicting a defect in a software component. The proposed models are compared with the most recent scheme in the literature in terms of accuracy and area under the curve (AUC). It is evident from the results and analysis that the performance of the proposed prediction models outperforms the schemes in the literature. © 2019, Springer Nature Switzerland AG.",Assessment | formatting | Machine Learning Technique | Risk | Software Project Management,"2020 8th International Conference on Information Technology and Multimedia, ICIMU 2020",2020-08-24,Conference Paper,"Mahdi, Mohamed Najah;Mohamed, Mohamed Zabil;Yusof, Azlan;Cheng, Lim Kok;Mohd Azmi, Muhammad Sufyian;Ahmad, Abdul Rahim",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85072243307,10.1504/IJAIP.2019.101983,Severity of defect: An optimised prediction,"To assure the quality of software an important activity is performed namely software defect prediction (SDP). Historical databases are used to detect software defects using different machine learning techniques. Conversely, there are disadvantages like testing becomes expensive, poor quality and so the product is unreliable for use. This paper classifies the severity of defects by using a method based on optimised neural network (NN). In full search space, a solution is found by many meta-heuristic optimisations and global search ability has been used. Hence, high-quality solutions are finding within a reasonable period of time. SDP performance is improved by the combination of meta-heuristic optimisation methods. For class imbalance problem, meta-heuristic optimisation methods such as genetic algorithm (GA) and shuffled frog leaping algorithm (SFLA) are applied. The above method is based on SFLA and the experimental outputs show that it can do better than Leven berg Marquardt based NN system (LM-NN). © 2019 Inderscience Publishers. All rights reserved.",Levenberg Marquardt | LM | Neural network | SDP | Severity | Shuffled frog and fuzzy classifier | software defect prediction,International Journal of Advanced Intelligence Paradigms,2019-01-01,Article,"Kumar, Reddi Kiran;Achuta Rao, S. V.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85067601854,10.3233/JIFS-18473,Empirical assessment of feature selection techniques in defect prediction models using web applications,"In order to minimize the over-fitting and related factors that are caused by the high dimensionality of the input data in software defect prediction, the attributes are often optimized using various feature selection techniques. However, the comparative performance of these selection techniques in combination with machine learning algorithms remains largely unexplored using web applications. In this work, we investigate the best possible combination of feature selection technique with machine learning algorithms, with the sample space chosen from open source Apache Click and Rave data sets. Our results are based on 945 defect prediction models derived from parametric, non-parametric and ensemble-based machine learning algorithms, for which the metrics are derived from the various filter and threshold-based ranking techniques. Friedman and Nemenyi post-hoc statistical tests are adopted to identify the performance difference of these models.We find that filter-based feature selection in combination with ensemble-based machine learning algorithms not only poise as the best strategy but also yields a maximum feature set redundancy by 94%, with little or no comprise on the performance index. © 2019 IOS Press and the authors. All rights reserved.",Feature ranking | Feature selection | Machine learning | Web application quality,Journal of Intelligent and Fuzzy Systems,2019-01-01,Article,"Malhotra, Ruchika;Sharma, Anjali",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85065025289,10.1007/978-3-030-16142-2_17,Towards one reusable model for various software defect mining tasks,"Software defect mining is playing an important role in software quality assurance. Many deep neural network based models have been proposed for software defect mining tasks, and have pushed forward the state-of-the-art mining performance. These deep models usually require a huge amount of task-specific source code for training to capture the code functionality to mine the defects. But such requirement is often hard to be satisfied in practice. On the other hand, lots of free source code and corresponding textual explanations are publicly available in the open source software repositories, which is potentially useful in modeling code functionality. However, no previous studies ever leverage these resources to help defect mining tasks. In this paper, we propose a novel framework to learn one reusable deep model for code functional representation using the huge amount of publicly available task-free source code as well as their textual explanations. And then reuse it for various software defect mining tasks. Experimental results on three major defect mining tasks with real world datasets indicate that by reusing this model in specific tasks, the mining performance outperforms its counterpart that learns deep models from scratch, especially when the training data is insufficient. © Springer Nature Switzerland AG 2019.",Machine learning | Model reuse | Software defect mining,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Li, Heng Yi;Li, Ming;Zhou, Zhi Hua",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84873601067,10.5220/0006886503200327,Cross project software defect prediction using extreme learning machine: An ensemble based study,"Cross project defect prediction, involves predicting software defects in the new software project based on the historical data of another project. Many researchers have successfully developed defect prediction models using conventional machine learning techniques and statistical techniques for within project defect prediction. Furthermore, some researchers also proposed defect prediction models for cross project defect prediction. However, it is observed that the performance of these defect prediction models degrade on different datasets. The completeness of these models are very poor. We have investigated the use of extreme learning machine (ELM) for cross project defect prediction. Further, this paper investigates the use of ELM in non linear heterogeneous ensemble for defect prediction. So, we have presented an efficient nonlinear heterogeneous extreme learning machine ensemble (NH ELM) model for cross project defect prediction to alleviate these mentioned issues. To validate this ensemble model, we have leveraged twelve PROMISE and five eclipse datasets for experimentation. From experimental results and analysis, it is observed that the presented nonlinear heterogeneous ensemble model provides better prediction accuracy as compared to other single defect prediction models. The evidences from completeness analysis also proved that the ensemble model shows improved completeness as compared to other single prediction models for both PROMISE and eclipse datasets. Copyright © 2018 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved",experimental techniques | fault prediction | machine learning | methodology | software engineering,"Proceedings - 2012 11th International Conference on Machine Learning and Applications, ICMLA 2012",2012-12-01,Conference Paper,"Hall, Tracy;Bowes, David",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84899418230,10.5220/0006839500690078,Extreme learning machine based linear homogeneous ensemble for software fault prediction,"Many recent studies have experimented the software fault prediction models to predict the number of software faults using statistical and traditional machine learning techniques. However, it is observed that the performance of traditional software fault prediction models vary from dataset to dataset. In addition, the performance of the traditional models degrade for inter release prediction. To address these issues, we have proposed linear homogeneous ensemble methods based on two variations of extreme learning machine, Differentiable Extreme Learning Machine Ensemble (DELME) and Non-differentiable Extreme Learning Machine Ensemble (NELME), to predict the number of software faults. We have used seventeen PROMISE datasets and five eclipse datasets to validate these software fault prediction models. We have performed two types of predictions, within project defect prediction and inter release prediction, to validate our proposed fault prediction model. The experimental result shows consistently better performance across all datasets. Copyright © 2018 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved",Bug priority | bug triaging | predictive model | text classification,"Proceedings - 2013 12th International Conference on Machine Learning and Applications, ICMLA 2013",2013-01-01,Conference Paper,"Alenezi, Mamdouh;Banitaan, Shadi",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84969651482,10.1007/978-3-319-90893-9_35,Applying Weighted Particle Swarm Optimization to Imbalanced Data in Software Defect Prediction,"Imbalanced data typically refers to class distribution skews and underrepresented data, which affect the performance of learning algorithms. Such data are well-known in real-life situations, such as behavior analysis, cancer malignancy grading, industrial systems’ monitoring and software defect prediction. In this paper, we present a W-PSO method, which comprises weighting of instances in a dataset and the Particle Swarm Optimization algorithm. The presented method was combined with classification methods C4.5 and Naive Bayes, respectively, and tested experimentally on ten freely accessible software defect prediction datasets. Based on the results achieved, the presented W-PSO method creates better classification models than classification methods C4.5 and Naive Bayes in the majority of the cases. © 2019, Springer International Publishing AG, part of Springer Nature.",Bagging | Boosting | Ensemble method | Interpretability | Model transformatiomn | Naive Bayes classifier | Performance,"Proceedings - 2015 IEEE 14th International Conference on Machine Learning and Applications, ICMLA 2015",2016-03-02,Conference Paper,"Mori, Toshiki",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85061104434,10.1007/978-3-030-12146-4_22,Recurrent neural networks for fuzz testing web browsers,"Generation-based fuzzing is a software testing approach which is able to discover different types of bugs and vulnerabilities in software. It is, however, known to be very time consuming to design and fine tune classical fuzzers to achieve acceptable coverage, even for small-scale software systems. To address this issue, we investigate a machine learning-based approach to fuzz testing in which we outline a family of test-case generators based on Recurrent Neural Networks (RNNs) and train those on readily available datasets with a minimum of human fine tuning. The proposed generators do, in contrast to previous work, not rely on heuristic sampling strategies but principled sampling from the predictive distributions. We provide a detailed analysis to demonstrate the characteristics and efficacy of the proposed generators in a challenging web browser testing scenario. The empirical results show that the RNN-based generators are able to provide better coverage than a mutation based method and are able to discover paths not discovered by a classical fuzzer. Our results supplement findings in other domains suggesting that generation based fuzzing with RNNs is a viable route to better software quality conditioned on the use of a suitable model selection/analysis procedure. © 2019, Springer Nature Switzerland AG.",Browser security | Fuzz testing | Software security,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Sablotny, Martin;Jensen, Bjørn Sand;Johnson, Chris W.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85048467519,10.1007/978-3-030-05767-1_8,Improving Defect Localization by Classifying the Affected Asset Using Machine Learning,"A vital part of a defect’s resolution is the task of defect localization. Defect localization is the task of finding the exact location of the defect in the system. The defect report, in particular, the asset attribute, helps the person assigned to handle the problem to limit the search space when investigating the exact location of the defect. However, research has shown that oftentimes reporters initially assign values to these attributes that provide incorrect information. In this paper, we propose and evaluate the way of automatically identifying the location of a defect using machine learning to classify the source asset. By training an Support-Vector-Machine (SVM) classifier with features constructed from both categorical and textual attributes of the defect reports we achieved an accuracy of 58.52% predicting the source asset. However, when we trained an SVM to provide a list of recommendations rather than a single prediction, the recall increased to up to 92.34%. Given these results, we conclude that software development teams can use these algorithms to predict up to ten potential locations, but already with three predicted locations, the teams can get useful results with the accuracy of over 70%. © 2019, Springer Nature Switzerland AG.",boosting | computer applications | machine learning | ranking,"Proceedings - 16th IEEE International Conference on Machine Learning and Applications, ICMLA 2017",2017-01-01,Conference Paper,"Fenu, Stefano;Rozell, Chris",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85061147574,10.1109/CloudCom2018.2018.00062,PReT: A tool for automatic phase-based regression testing,"In this paper, we present our tool PReT, which performs automatic performance regression testing on software. PReT does non-intrusive profiling based on application snapshots to learn behaviour for performance regression tests and can identify any changes in the testing behaviour by comparing the current behaviour against a learned model. PReT annotates resource usage profiles with application stacktraces and uses a variation of k-means to learn the models per regression test online. On top of that, PReT uses version information of the software to identify change(s) that introduce(s) performance issue(s) if any. We show the usefulness of PReT in correctly identifying two real world performance bugs in Cassandra database server. We show that PReT is able to characterize the performance tests being run for the software with higher accuracy than a purely resource utilization based characterization technique. © 2018 IEEE.",Bug Detection | Machine Learning | Performance Regression,"Proceedings of the International Conference on Cloud Computing Technology and Science, CloudCom",2018-12-26,Conference Paper,"Bhattacharyya, Arnamoy;Amza, Cristiana",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85062245280,10.1109/BRACIS.2018.00101,Instance selection and class balancing techniques for cross project defect prediction,"Various software metrics and statistical models have been developed to help companies to predict software defects. Traditional software defect prediction approaches use historical data about previous bugs on a project in order to build predictive machine learning models. However, in many cases the historical testing data available in a project is scarce, i.e., very few or even no labeled training instances are available, which will result on a low quality defect prediction model. In order to overcome this limitation, Cross-Project Defect Prediction (CPDP) can be adopted to learn a defect prediction model for a project of interest (i.e., a target project) by reusing (transferring) data collected from several previous projects (i.e., source projects). In this paper, we focused on neighborhood-based instance selection techniques for CPDP which select labeled instances in the source projects that are similar to the unlabeled instances available in the target project. Despite its simplicity, these techniques have limitations which were addressed in our work. First, although they can select representative source instances, the quality of the selected instances is usually not addressed. Additionally, bug prediction datasets are normally unbalanced (i.e., there are more nondefect instances than defect ones), which can harm learning performance. In this paper, we proposed a new transfer learning approach for CPDP, in which instances selected by a neighborhood-based technique are filtered by the FuzzyRough Instance Selection (FRIS) technique in order to remove noisy instances in the training set. Following, in order to solve class balancing problems, the Synthetic Minority Oversampling Technique (SMOTE) technique is adopted to oversample the minority (defect-prone) class, thus increasing the chance of finding bugs correctly. Experiments were performed on a benchmark set of Java projects, achieving promising results. © 2018 IEEE.",Bug locating | Bug report | Convolutional neural network | Long short-term memory,"Proceedings - 17th IEEE International Conference on Machine Learning and Applications, ICMLA 2018",2018-07-02,Conference Paper,"Ye, Xin;Fang, Fan;Wu, John;Bunescu, Razvan;Liu, Chang",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85080877813,10.1109/BDAI.2018.8546671,Parameter Tuning on Software Defect Prediction Using Differential Evolution & Simulated Annealing,"Machine Learning algorithms are used in Software Engineering to predict defects. These defect predictors are powerful in comparison to manual methods. They are also pretty simple to grasp and use. But an important thing, which is often ignored is the tuning of these defect predictors to optimize their performance. We try to find simple and easy to implement methods for tuning the defect predictors and also compare the performances of these methods. We ran Differential Evolution and Simulated Annealing as optimizers using different datasets from open-source JAVA systems to explore the tuning space. Finally, we tested the tunings and compared the results obtained from both the methods. We found that tuning improved the performance in majority of cases. It was also found that not all optimisation algorithms used for tuning produced the same results. As (1) there is significant improvement in performance after parameter tuning, there is a need to change standard methods used in software analytics. It is not sufficient to present the result without performing a proper tuning optimization study, especially in the case of defect prediction. (2) Differential Evolution and Simulated Annealing didn't give similar results for majority of the datasets, so it is necessary to perform tuning using different optimization algorithms to obtain the best possible results. © 2018 IEEE.",Bug | Group | Recommendation | Triage,"Proceedings - 18th IEEE International Conference on Machine Learning and Applications, ICMLA 2019",2019-12-01,Conference Paper,"De Lara Pahins, Cicero Augusto;D'Morison, Fabricio;Rocha, Thiago M.;Almeida, Larissa M.;Batista, Arthur F.;Souza, Diego F.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85080913349,10.1109/ISCAIE.2018.8405511,Bug localization in software using NSGA-II,"Finding bugs in a software is a cumbersome and tedious task. When a new bug is reported, the developers find it challenging to replicate the unexpected behavior of the software, in order to fix the original fault. In this paper, an automated model is presented to find and sort the classes present in the source code according to their proneness of containing a bug, depending upon the bug reports. The model uses a text mining approach and recommends a list of classes based upon the lexical similarity between bug reports and the API descriptions, and also the changes previously recommended during bug fixing. To maximize the similarity index and at the same time reduce the number of classes recommended, a Non-dominant Sorting Genetic Algorithm (NSGA-II) is employed. This model was evaluated on three Java based open source applications and it is observed that the model created using multi-objective NSGA-II outperforms the traditional methods of bug localization. © 2018 IEEE.",Bug triaging | Computer science | Deep neural network | Machine learning | Multi label | Software engineering | Statistics,"Proceedings - 18th IEEE International Conference on Machine Learning and Applications, ICMLA 2019",2019-12-01,Conference Paper,"Choquette-Choo, Christopher A.;Sheldon, David;Proppe, Jonny;Alphonso-Gibbs, John;Gupta, Harsha",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85046344961,10.1109/IC3.2017.8284297,Analyzing fault prediction usefulness from cost perspective using source code metrics,Software fault prediction techniques are useful for the purpose of optimizing test resource allocation. Software fault prediction based on source code metrics and machine learning models consists of using static program features as input predictors to estimate the fault proneness of a class or module. We conduct a comparison of five machine learning algorithms on their fault prediction performance based on experiments on 56 open source projects. Several researchers have argued on the application of software engineering economics and testing cost for the purpose of evaluating a software quality assurance activity. We evaluate the performance and usefulness of fault prediction models within the context of a cost evaluation framework and present the results of our experiments. We propose a novel approach using decision trees to predict the usefulness of fault prediction based on distributional characteristics of source code metrics by fusing information from the output of the fault prediction usefulness using cost evaluation framework and distributional source code metrics. © 2017 IEEE.,Fault Prediction | Machine Learning | Software Engineering | Software Engineering Economics | Source Code Metrics,"2017 10th International Conference on Contemporary Computing, IC3 2017",2017-07-02,Conference Paper,"Kumar, Lov;Sureka, Ashish",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85058296514,10.1049/iet-sen.2017.0198,Low-rank representation for semi-supervised software defect prediction,"Software defect prediction based on machine learning is an active research topic in the field of software engineering. The historical defect data in software repositories may contain noises because automatic defect collection is based on modified logs and defect reports. When the previous defect labels of modules are limited, predicting the defect-prone modules becomes a challenging problem. In this study, the authors propose a graph-based semi-supervised defect prediction approach to solve the problems of insufficient labelled data and noisy data. Graph-based semi-supervised learning methods used the labelled and unlabelled data simultaneously and consider them as the nodes of the graph at the training phase. Therefore, they solve the problem of insufficient labelled samples. To improve the stability of noisy defect data, a powerful clustering method, low-rank representation (LRR), and neighbourhood distance are used to construct the relationship graph of samples. Therefore, they propose a new semi-supervised defect prediction approach, named low-rank representation-based semi-supervised software defect prediction (LRRSSDP). The widely used datasets from NASA projects and noisy datasets are employed as test data to evaluate the performance. Experimental results show that (i) LRRSSDP outperforms several representative state-of-the-art semi-supervised defect prediction methods; and (ii) LRRSSDP can maintain robustness in noisy environments. © The Institution of Engineering and Technology 2018.",,IET Software,2018-01-01,Article,"Zhang, Zhi Wu;Jing, Xiao Yuan;Wu, Fei",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85028598796,10.1007/978-3-319-60618-7_55,On the effectiveness of cost sensitive neural networks for software defect prediction,"The cost of fixing a software defect varies with the phase in which it is uncovered. Defect found during post-release phase costs much more than the defect that is uncovered in pre-release phase. Hence defect prediction models have been proposed to predict bugs in pre-release phase. For any prediction model, there are two kinds of misclassification errors - Type I and Type II errors. Type II errors are found to be more costly than Type I errors for defect prediction problem. However there have been only few studies that have considered misclassifications costs while building or evaluating defect predictions models. We have built classification models using three cost-sensitive boosting Neural Network methods, namely, CSBNN-TM, CSBNN-WU1 and CSBNN-WU2. We have compared the performance of these cost sensitive Neural Networks with the traditional machine learning algorithms like Logistic Regression, Naive Bayes, Random Forest, Bayesian Network, Neural Networks, k-Nearest Neighbors and Decision Tree. We have compared the performance of the resultant models using cost centric measure - Normalized Expected Cost of Misclassification (NECM). © Springer International Publishing AG 2018.",Cost-sensitive neural networks | Misclassification cost | Software defect prediction,Advances in Intelligent Systems and Computing,2018-01-01,Conference Paper,"Muthukumaran, K.;Dasgupta, Amrita;Abhidnya, Shirode;Neti, Lalita Bhanu Murthy",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85102511116,10.1007/978-3-319-60618-7_28,Software defect prediction using augmented bayesian networks,"Prediction models are built with various machine learning algorithms to identify defects prior to release to facilitate software testing, and save testing costs. Naïve Bayes classifier is one of the best performing classification techniques in defect prediction. It assumes conditional independence of features and for defect prediction problem some of the features are not actually conditionally independent. The interesting problem is to relax these conditional independence assumptions and to check whether there is any improvement in performance of classifiers. We have built Bayesian Network structures using different classes of algorithms namely score-based, constraint-based and hybrid algorithms. We propose an approach to augment these Bayesian Network structures with class node. Bayesian Network classifiers along with Random Forests, Logistic Regression and Naïve Bayes classifiers are then evaluated using measures like AUC and H-measure. We observe that RSMAX2 and Grow-Shrink classifiers (after augmentation) perform consistently better in defect prediction. © Springer International Publishing AG 2018.",Binarized Neural Network Accelerator | Black/white -box Attack | Fault Injection | Machine Learning | ML/DL -powered IoT | Soft Error,"Proceedings - 19th IEEE International Conference on Machine Learning and Applications, ICMLA 2020",2020-12-01,Conference Paper,"Khoshavi, Navid;Broyles, Connor;Bi, Yu;Roohi, Arman",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85039907708,,Implications of discretization towards improving classification accuracy for software defect data,"Since the advent of new software architectures, paradigms and technologies the software design and development has developed a cutting edge requirements of being on the right track in terms of software quality and reliability. This leads the prediction of defects in software at its early stages of its development. Implications of machine learning algorithms are now playing a very crucial role in classification and prediction of the possible bugs during the systems design phase. In this research work a discretization method is proposed based on the Object Oriented metrics threshold values in order to gain better classification accuracy on a given data set. For the experimentation purpose, Jedit, Lucene, tomcat, velocity, xalan and xerces software systems from NASA repositories have been considered and classification accuracies have been compared with the existing approaches with the help of open source WEKA tool. For this study, the Object Oriented CK metrics suite has been considered due to its wide applicability in software industry for software quality prediction. After experimentation it is found that Naive Bayes and Voted Perceptron, classifiers are performing well and provide highest accuracy level with the discretized dataset values. The performance of these classifiers are checked and analyzed on different performance measures like ROC, RMSE, Precision, Recall values in this research work. Result shows significant performance improvements towards classification accuracy if used with discrete features of the individual software systems. © 2005 – ongoing JATIT & LLS.",CK metrics; Classification; Discretization; Software defect prediction,Journal of Theoretical and Applied Information Technology,2017,,"Kapoor P., Arora D., Kumar A.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85045942717,10.1145/3178212.3178221,Empirical study on software bug prediction,"Software defect prediction is a vital research direction in software engineering field. Software defect prediction predicts whether software errors are present in the software by using machine learning analysis on software metrics. It can help software developers to improve the quality of the software. Software defect prediction is usually a binary classification problem, which relies on software metrics and the use of classifiers. There have been many research efforts to improve accuracy in software defect prediction using a variety of classifiers and data preprocessing techniques. However, the ""classic classifier validity"" and ""data preprocessing techniques can enhance the functionality of software defect prediction"" has not yet been answered explicitly. Therefore, it is necessary to conduct an empirical analysis tocompare these studies. In software defect prediction, the category of interest is a defective module, and the number of defective modules is much less than that of a non-defective module in data. This leads to a category of imbalance problem that reduces the accuracy of the prediction. Therefore, the problem of imbalance is a key problem that needs to be solved in software defect prediction. In this paper, we proposed an experimental model and used the NASA MDP data set to analyze the software defect prediction. Five research questions were defined and analyzed experimentally. In addition to experimental analysis, this paperfocuses on the improvement of SMOTE. SMOTE ASMO algorithm has been proposed to overcome the shortcomings of SMOTE. Copyright 2010 ACM.",Classification | Data preprocessing | Defect prediction | SMOTE,ACM International Conference Proceeding Series,2017-12-28,Conference Paper,"Rizwan, Syed;Tiantian, Wang;Xiaohong, Su;Salahuddin, ",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85113726700,10.1109/EIConRus.2017.7910598,Machine learning for embedded devices software analysis via hardware platform emulation,"Nowadays commercial-off-the-shelf (COTS) embedded devices are widely used in many security-critical systems like nuclear stations and traffic control systems. Most of these devices has proprietary hardware and software (frequently called firmware) with little documentation available. Another common feature is the use of 'binary blob' firmware, where hardware specific and high level layers can't be easily separated and, therefore are forced to be analyzed together. All these facts make firmware analysis quite a challenging task. In this paper, we'll suggest an approach for the firmware analysis with poorly documented hardware platform emulation. The advantages of this approach are almost full control under firmware state, achieving easy to scale fuzzing and manual bug hunting facilitation. For the purpose of successful realization, an identification of communication between hardware components (e.g. communication between main SoC and Bluetooth SoC) should be done. To address this issue, we suggest the use of machine learning, which, because of its nature, enables construction of algorithms that can learn from and make predictions on data. © 2017 IEEE.",Big data | Clustering | Ensemble | Imbalanced big data | Instance selection,Proceedings - International Conference on Machine Learning and Cybernetics,2020-12-02,Conference Paper,"Zhai, Jun Hai;Zhang, Su Fang;Wang, Mo Han;Li, Yan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85054259406,10.1145/3021460.3021491,Software quality predictive modeling: An effective assessment of experimental data,"A major problem faced by software project managers is to develop good quality software products within tight schedules and budget constraints [1]. Predictive modeling, in the context of software engineering relates to construction of models for estimation of software quality attributes such as defect-proneness, maintainability and effort amongst others. For developing such models, software metrics act as predictor variables as they signify various design characteristics of a software such as coupling, cohesion, inheritance and polymorphism. A number of techniques such as statistical and machine learning are available for developing predictive models. However, conducting effective empirical studies, which develop successful predictive models, is not possible if proper research methodology and steps are not followed. This work introduces a successful stepwise procedure for efficient application of various techniques to predictive modeling. A number of research issues which are important to be addressed while conducting empirical studies such as data collection, validation method, use of statistical tests, use of an effective performance evaluator etc. are also discussed with the help of an example. The tutorial presents an overview of the research process and methodology followed in an empirical research [2]. All steps that are needed to perform an effective empirical study are described. The tutorial would demonstrate the research methodology with the help of an example based on a data set for defect prediction. In this work we focus on various research issues that are stated below: RQ1: Which repositories are available for extracting software engineering data? RQ2: What type of data pre-processing and feature selection techniques should be used before developing predictive models? RQ3: Which possible tools are freely available for mining and analysis of data for developing software quality predictive models? RQ4: Which techniques are available for developing software quality predictive models? RQ5: Which metrics should be used for performance evaluation for models developed for software? RQ6: Which statistical tests can be effectively used for hypothesis testing using search-based techniques? RQ7: How can we effectively use search-based techniques for predictive modeling? RQ8: What are possible fitness functions while using search-based techniques for predictive modeling? RQ9: How would researchers account for the stochastic nature of search-based techniques? The reasons for relevance of this study are manifold. Empirical validation of OO metrics is a critical research area in the present day scenario, with a large number of academicians and research practitioners working towards this direction to predict software quality attributes in the early phases of software development. Thus, we explore the various steps involved in development of an effective software quality predictive model using a modeling technique with an example data set. Performing successful empirical studies in software engineering is important for the following reasons: To identify defective classes at the initial phases of software development so that more resources can be allocated to these classes to remove errors. To analyze the metrics which are important for predicting software quality attributes and to use them as quality benchmarks so that the software process can be standardized and delivers effective products. To efficiently plan testing, walkthroughs, reviews and inspection activities so that limited resources can be properly planned to provide good quality software. To use and adapt different techniques (statistical, machine learning & search-based) in predicting software quality attributes. To analyze existing trends for software quality predictive modeling and suggest future directions for researchers. To document the research methodology so that effective replicated studies can be performed with ease. © 2017 ACM.",Empirical validation | Object-oriented metrics | Search-based techniques | Software quality predictive modeling,ACM International Conference Proceeding Series,2017-02-05,Conference Paper,"Malhotra, Ruchika",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85048955483,,Novel cluster based cross defect classification technique for multiple software defect databases,"Bug detection is essential to developers especially in large-scale software systems due to high dimensionality and high error rate. The identification of a new bug comes along with two vital tasks. The first task gauges the level of seriousness of that bug. This task indicates the severity of having that bug identified and fixed in large data repositories. The second task is known as bug location tracking and finding similar type of bugs in large corpus. Inspecting software bugs through manual checking is a difficult task as a result of the increasing complexity and size of software. Most of the traditional bug predictors are used to assist the software bugs from the bug reports. Databases from historical defects are the basis upon which these tools are built, and statistical patterns for unseen projects should be identified and generalized by the tools. In this proposed model, a novel pre-processing based classification model was implemented on large multiple bug datasets for cross defect detection and ranking the high rated bugs. Experimental results proved that the proposed model has high computational efficiency in terms of true positivity and low error rate are concerned. © 2018, Institute of Advanced Scientific Research, Inc.. All rights reserved.",Bug classification; Machine learning; Software defects,Journal of Advanced Research in Dynamical and Control Systems,2017,,"Kumar P.R., Varma G.P.S.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85041005946,,An adaptive Neuro-Fuzzy model to predict the reliability before testing phase,"Reliability of a system has a direct impact on the success of any software system. Predicting the reliability at an early stage helps to optimize the testing and maintenance of the software. Rapid changes in hardware and software technologies lead to inventions of new methodologies and needs developing and validating a reliability predicting model for each method, Machine Learning approach can provide a solution to this problem. Selecting suitable metrics that affect the reliability of the system input to a model, and output of the model should reflect whether a system is reliable or not. In this paper, software is developed based on the design of Service-Oriented Architecture (SOA) methodology, is considered where a set of components interact for autonomous services. Further, to predict the reliability of a component, the probability that a component executes without fault, Adaptive Neuro-Fuzzy Inference System (ANFIS) approach is used. The coefficient of determination is evaluated to validate the model. © MIR Labs.",ANFIS; Faults; FIS; Machine Learning; Reliability; Service-Oriented Architecture (SOA),International Journal of Computer Information Systems and Industrial Management Applications,2017,,"Sehgal R., Mehrotra D., Bala M.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85008178951,,A novel software quality prediction system based on incorporating ISO 9126 with machine learning,"To begin with, this research defines Software Quality Prediction System (SQPS) as a system composed of a Classification Algorithm (CA) and a Software Quality Measurement Model (SQMM). Machine Learning applications in software quality measurement are expanding as research intensifies in two directions, the first direction focuses on improving the performance of CAs while the other direction concentrates on improving SQMMs. Despite of the increasing attention in this area, some well-designed SQPSs showed considerable false predictions, which could be explained by faults in the design of the CA, the SQMM, or the SQPS as a whole. In this context, there is a debate on which CA is better for measuring software quality, as well as there is a debate on which SQMM to follow. To start with, the research studied an original dataset of 7311 software projects. Then, the research derived quality measurements from the ISO 9126 Quality Model and developed the SQMM accordingly. Next, the research compared statistical measures of performance of four CAs, using WEKA and SPSS. Our experimental results showed that ISO 9126 is general, but flexible enough to act as a SQMM. Despite of their convergent performance, our experiments showed that Multilayer Perceptron Network (MLPN) have more balanced predictions than Naïve Bayes does. Following a rarely researched approach, the SQPS predicted five levels of software quality instead of making a binary prediction, limited with defect or non-defect software. © 2005 - 2016 JATIT & LLS. All rights reserved.",Classification algorithm (CA); ISO 9126 software quality model; Machine learning; Multilayer perceptron network (MLPN); Software quality measurement model (SQMM); Software quality prediction system (SQPS),Journal of Theoretical and Applied Information Technology,2016,,"Alshareet O., Itradat A., Doush I.A., Quttoum A.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84997017896,10.1142/S021853931640009X,Predicting Software Maintenance Effort by Mining Software Project Reports Using Inter-Version Validation,"Changes in the software are unavoidable due to an ever changing dynamic and active environment wherein expectations and requirements of the users tend to change rapidly. As a result, software needs to upgrade itself from its previous version to the next version in order to meet expectations of the user. The upgradation of the software is in terms of total number of Lines of Code (LOC) that might have been inserted, deleted or modified in moving from one version of software to the next. These changes are maintained in the change reports which constitute of the defect ID and defect description. Defect description describes the cause of defect which might have occurred in the previous version of the software due to which either new LOC needs to be inserted or existing LOC need to be deleted or modified. A lot of effort is required to correct the defects identified in software at the maintenance phase i.e., when software is delivered at the customers end. Thus, in this paper, we intend to predict maintenance effort by analyzing the defect reports using text mining techniques and thereafter developing the prediction models using suitable machine learning algorithms viz. Multi-Layer Perceptron (MLP), Radial-Basis Function (RBF) network and Decision Tree (DT). We have considered the changes between three successive versions of 'MMS' application package of Android operating system and have performed inter-version validation where the model predicted using the version 'v' is validated on the subsequent version i.e., 'v+1'. The performance of the model was evaluated using Receiver Operating Characteristics (ROC) analysis. The results indicated that the model predicted on 'MMS' 4.0 version using MLP algorithm has shown good results when validated on 'MMS' 4.1 version. On the other hand, the performance of RBF and DT algorithms has been consistently average in predicting the maintenance effort. © 2016 World Scientific Publishing Company.",Defect reports | machine learning | receiver operating characteristics | software maintenance effort prediction | text mining,"International Journal of Reliability, Quality and Safety Engineering",2016-12-01,Conference Paper,"Jindal, Rajni;Malhotra, Ruchika;Jain, Abha",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84997428892,,Cross project source code based risk analysis to identify fault prone modules,"Manual software testing requires a lot of resources and time. Fault prone module prediction is an emerging approach to efficiently use the available resources. The accurate prediction of fault prone modules will enable the testing process focus on the complex software modules. In this paper, the authors have studied various emerging metrics and techniques to predict the fault prone modules and their advantages and disadvantages. Also, a new approach has been proposed to solve the disadvantages found in the studied techniques. © 2016 IEEE.",Cross project prediction; Fault prone modules; Machine learning; Software metrics; Software testing,"Proceedings of the 10th INDIACom; 2016 3rd International Conference on Computing for Sustainable Global Development, INDIACom 2016",2016,,"Kaur I., Kapoor N.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85016135213,,On the non-generalizability in bug prediction,"Bug prediction is a technique used to estimate the most bug-prone entities in software systems. Bug prediction approaches vary in many design options, such as dependent variables, independent variables, and machine learning models. Choosing the right combination of design options to build an effective bug predictor is hard. Previous studies do not consider this complexity and draw conclusions based on fewer-than-necessary experiments. We argue that each software project is unique from the perspective of its development process. Consequently, metrics and machine learning models perform differently on different projects, in the context of bug prediction. We confirm our hypothesis empirically by running different bug predictors on different systems. We show there are no universal bug prediction configurations that work on all projects. Copyright © by the paper's authors.",,CEUR Workshop Proceedings,2016,,Osman H.,Include,
10.1016/j.infsof.2022.107128,2-s2.0-85006968329,,Maintainability of classes in terms of bug prediction,"Measuring software product maintainability is a central issue in software engineering which led to a number of different practical quality models. Besides system level assessments it is also desirable that these models provide technical quality information at source code element level (e.g. classes, methods) to aid the improvement of the software. Although many existing models give an ordered list of source code elements that should be improved, it is unclear how these elements are affected by other important quality indicators of the system, e.g. bug density. In this paper we empirically investigate the bug prediction capabilities of the class level maintainability measures of our ColumbusQM probabilistic quality model using open-access PROMSIE bug dataset. We show that in terms of correctness and completeness, ColumbusQM competes with statistical and machine learning prediction models especially trained on the bug data using product metrics as predictors. This is a great achievement in the light of that our model needs no training and its purpose is different (e.g. to estimate testability, or development costs) than those of the bug prediction models. © 2016, Eszterhazy Karoly College. All rights reserved.",Bug; Bug prediction; Class level maintainability; ColumbusQM; ISO/IEC 25010; PROMISE; Software maintainability,Annales Mathematicae et Informaticae,2016,,Ladányi G.,Include,
10.1016/j.infsof.2022.107128,2-s2.0-84961370853,10.1007/978-81-322-2517-1_26,Application of locally weighted regression for predicting faults using software entropy metrics,"There are numerous approaches for predicting faults in the software engineering research field. Software entropy metrics introduced by Hassan (Predicting faults using the complexity of code changes, 78–88, 2009) [1] are also popularly used for fault prediction. In previous studies, statistical linear regression (SLR) and support vector regression (SVR) for predicting faults using software entropy metrics have been validated. However, other machine learning approaches have not yet been explored. This study explores the applicability of locally weighted regression (LWR) approach for predicting faults using the software entropy metrics and compares it with SVR. It is noticed that the LWR performs better than SVR in most of the cases. © Springer India 2016.",active learning | clustering | defect categorization | semi supervised learning | support vector machine,IEEE International Conference on Program Comprehension,2015-08-05,Conference Paper,"Thung, Ferdian;Le, Xuan Bach D.;Lo, David",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84958545174,10.1007/978-3-319-19644-2_34,Detecting anomalies in embedded computing systems via a novel hmm-based machine learning approach,"Computing systems are vulnerable to anomalies that might occur during execution of deployed software: E.g., faults, bugs or deadlocks. When occurring on embedded computing systems, these anomalies may severely hamper the corresponding devices; on the other hand, embedded systems are designed to perform autonomously, i.e., without any human intervention, and thus it is difficult to debug an application to manage the anomaly. Runtime anomaly detection techniques are the primary means of being aware of anomalous conditions. In this paper, we describe a novel approach to detect an anomaly during the execution of one or more applications. Our approach describes the behaviour of the applications using the sequences of memory references generated during runtime. The memory references are seen as signals: They are divided in overlapping frames, then parametrized and finally described with Hidden Markov Models (HMM) for detecting anomalies. The motivations of using such methodology for embedded systems are the following: first, the memory references could be extracted with very low overhead with software or architectural tools. Second, the device HMM analysis framework, while being very powerful in gathering high level information, has low computational complexity and thus is suitable to the rather low memory and computational capabilities of embedded systems. We experimentally evaluated our proposal on a ARM9, Linux based, embedded system using the SPEC 2006 CPU benchmark suite and found that it shows very low error rates for some artificially injected anomalies, namely a malware, an infinite loop and random errors during execution. © Springer International Publishing Switzerland 2015.",,Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science),2015-01-01,Conference Paper,"Cuzzocrea, Alfredo;Medvet, Eric;Mumolo, Enzo;Cecolin, Riccardo",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84899338410,10.4018/978-1-61520-809-8.ch005,Application of artificial immune systems paradigm for developing software fault prediction models,"Artificial Immune Systems, a biologically inspired computing paradigm such as Artificial Neural Networks, Genetic Algorithms, and Swarm Intelligence, embody the principles and advantages of vertebrate immune systems. It has been applied to solve several complex problems in different areas such as data mining, computer security, robotics, aircraft control, scheduling, optimization, and pattern recognition. There is an increasing interest in the use of this paradigm and they are widely used in conjunction with other methods such as Artificial Neural Networks, Swarm Intelligence and Fuzzy Logic. In this chapter, we demonstrate the procedure for applying this paradigm and bio-inspired algorithm for developing software fault prediction models. The fault prediction unit is to identify the modules, which are likely to contain the faults at the next release in a large software system. Software metrics and fault data belonging to a previous software version are used to build the model. Fault-prone modules of the next release are predicted by using this model and current software metrics. From machine learning perspective, this type of modeling approach is called supervised learning. A sample fault dataset is used to show the elaborated approach of working of Artificial Immune Recognition Systems (AIRS). © 2010, IGI Global.",,Evolutionary Computation and Optimization Algorithms in Software Engineering: Applications and Techniques,2010-12-01,Book Chapter,"Catal, Cagatay;Banerjee, Soumya",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85120166732,10.1007/s10515-021-00311-z,Prediction of software fault-prone classes using ensemble random forest with adaptive synthetic sampling algorithm,"The process of predicting fault module in software is known as Software Fault Prediction (SFP) which is important for releasing software versions that are dependent on the predefined metrics due to historical faults in software. The fault prediction in software such as components, classes and modules, at an early stage in the development cycle, is important as it significantly contributes to time reduction and cost reduction. Therefore, the modules that are used for processing each step is reduced by the unnecessary efforts eliminated the faults during development process. However, the problem of imbalanced dataset becomes a significant challenge during SFP for software fault prediction at an early stage. The limitations such as inclusion of software metric for SFP models, cost effectiveness of the fault and the fault density prediction, are still few obstacles faced by research. The proposed Butterfly optimization performs feature selection that helps to predict meticulous and remarkable results by developing the applications of Machine Learning techniques. The present research uses Ensemble Random Forest with Adaptive Synthetic Sampling (E-RF-ADASYN) for fault prediction by using various classifiers which is mentioned in the proposed method section. The proposed E-RF-ADASYN obtained Area Under Curve (AUC) of 0.854767 better when compared with the existing method Rough-KNN Noise-Filtered Easy Ensemble (RKEE) of 0.771. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Adaptive synthetic sampling | Butterfly optimization | Ensemble random forest | Imbalanced data | Software fault prediction,Automated Software Engineering,2022-05-01,Article,"Balaram, A.;Vasundra, S.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85124226914,10.1016/j.knosys.2022.108293,Domain knowledge-based security bug reports prediction,"To eliminate security attack risks of software products, the security bug report (SBR) prediction has been increasingly investigated. However, there is still much room for improving the performance of automatic SBR prediction. This work is inspired by the work of two recent studies proposed by Peters et al. and Wu et al., which focused on SBR prediction and have been published on the top tier journal TSE (IEEE Transactions on Software Engineering). The goal of this work is to improve the effectiveness of supervised machine learning-based SBR prediction with the help of software security domain knowledge. First, we split the words in summary and description fields of the SBRs. Then, we use customized relationships to label entities and build a rule-based entity recognition corpus. After that, we establish relationships between entities and construct knowledge graphs. The information of CWE (Common Weakness Enumeration) is used to expand our corpus and the security-related words and phrases are integrated. Finally, we predict SBRs from target project by calculating the cosine similarity between our integrated corpus and the target bug reports. Our experimental evaluation on 5 open-source SBR datasets shows that our domain knowledge-guided approach could improve the effectiveness of SBRs prediction by 52% in terms of F1-score on average. © 2022 Elsevier B.V.",Domain knowledge | Entity recognition | Knowledge graph | Security bug report prediction | Software security,Knowledge-Based Systems,2022-04-06,Article,"Zheng, Wei;Cheng, Jing Yuan;Wu, Xiaoxue;Sun, Ruiyang;Wang, Xiaolong;Sun, Xiaobing",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85120805547,10.1016/j.eswa.2021.116217,Empirical investigation of hyperparameter optimization for software defect count prediction,"Prior identification of defects in software modules can help testers to allocate limited resources efficiently. Defect prediction techniques are helpful for this situation because they allow testers to identify and focus on defect prone parts of the software. The regression approach is a machine learning technique used to find the defect count in software segments. These regression techniques are more effective if their hyperparameters are adjusted. However, limited studies are available for hyperparameter optimization of regression techniques. In this paper, we investigated the impact of hyperparameter optimization on defect count prediction. In an empirical analysis on 15 software defect datasets, we find that hyperparameter optimization of learning techniques: (1) improves the prediction performance for MLPR, Lasso, DTR, Hubber, and SVR, by 16.96%, 8.31%, 8.16%, 6.01% and 5.22%, respectively; (2) linear regression is not optimization-sensitive; (3) overall grid search optimization improved the prediction performance by 4.42% while random search optimization by 3.36%.; (4) non-significant classifier have also changed their ranking substantially, and (5) logistic regression obtained the highest ranking concerning hyperparameter optimization. While both random and grid search performed well, but concerning the default parameter, the grid search always obtained better outcomes; however, it may not with random search. This emphasizes the importance of exploring the parameter space when using parameter-sensitive regression techniques. © 2021 Elsevier Ltd",Defect count prediction | Hyperparameter tuning | Machine learning,Expert Systems with Applications,2022-04-01,Article,"Nevendra, Meetesh;Singh, Pradeep",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85123756783,10.1007/s10664-021-10103-4,An extended study on applicability and performance of homogeneous cross-project defect prediction approaches under homogeneous cross-company effort estimation situation,"Software effort estimation (SEE) models have been studied for decades. One of serious but typical situations for data-oriented models is the availability of datasets for training models. Cross-company software effort estimation (CCSEE) is a research topic to utilize project data outside an organization. The same problem was identified in software defect prediction research, and Cross-project defect prediction (CPDP) has been studied. CPDP and CCSEE shared the motivation and developed approaches for the same problem. A question arisen that CPDP approaches were applicable to CCSEE. This study explored this question with a survey and an empirical study focusing on homogeneous approaches. We first examined 24 homogeneous CPDP approach implementations provided in a CPDP framework and found more than half of the approaches were applicable. Next, we used the results of two past studies to check whether the applicable CPDP approaches covered strategies taken in past CPDP studies. The empirical experiment was then conducted to evaluate the estimation performance of the applicable CPDP approaches. CPDP approaches were configured with some machine learning techniques if available, and ten CCSEE dataset configurations were supplied for evaluation. The result was also compared with the results of those two past studies to find the commonalities and the differences between CPDP and CCSEE. The answers to the research questions revealed the our CPDP selection covered the strategies that CPDP approaches had taken. The empirical results supported the simple merge with no instance weighting, no feature selection, no data transformation, nor the simple voting. It was not necessarily surprising according CPDP and CCSEE studies, but had not been explored with CPDP approaches under CCSEE situation. A practical implication is: Combine cross-company data to make effort estimates. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Cross-company effort estimation | Cross-project defect prediction | Empirical evaluation,Empirical Software Engineering,2022-03-01,Article,"Amasaki, Sousuke;Aman, Hirohisa;Yokogawa, Tomoyuki",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85123524404,10.3390/app12031387,Causally Remove Negative Confound Effects of Size Metric for Software Defect Prediction,"Software defect prediction technology can effectively detect potential defects in the software system. The most common method is to establish machine learning models based on software metrics for prediction. However, most of the prediction models are proposed without considering the confounding effects of size metric. The size metric has unexpected correlations with other software metrics and introduces biases into prediction results. Suitably removing these confounding effects to improve the prediction model’s performance is an issue that is still largely unexplored. This paper proposes a method that can causally remove the negative confounding effects of size metric. First, we quantify the confounding effects based on a causal graph. Then, we analyze each confounding effect to determine whether they are positive or negative, and only the negative confounding effects are removed. Extensive experimental results on eight data sets demonstrate the effectiveness of our proposed method. The prediction model’s performance can, in general, be improved after removing the negative confounding effects of size metric. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",Casual graph | Confounding effect | Defect prediction | Generalized additive models | Logistic regression | Size metric,Applied Sciences (Switzerland),2022-02-01,Article,"Li, Chenlong;Yuan, Yuyu;Yang, Jincui",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85124668256,10.1007/s12530-022-09423-7,Correlation-based modified long short-term memory network approach for software defect prediction,"Developing software applications has become more perplexing nowadays due to the huge usage of software applications. Under such circumstances, developing software without defects is a very challenging task. So, detecting defects in software modules is necessary for the developers to allocate appropriate sources for the project. Knowing the defects in advance increases the software quality at a low cost. This article aims to develop a correlation-based neural network model for identifying defects in software projects. A novel correlation-based modified long short-term memory neural network (CM-LSTM) is proposed to estimate the software defects in software modules with modeled data. Based on the positive correlation between the features and the target variable, target variables have been changed. The prepared data is fed to the LSTM model to overcome the imbalance issue in the software defect prediction data. The adequacy of the proposed method is tested with a JM1 software defect prediction dataset with various performance parameters. It is observed that the proposed correlation-based modified LSTM technique is effective in detecting defects in software projects. The proposed technique employs correlation-based feature selection for long-short term memory neural networks to identify defects in software projects, and it is found to be more efficient than other existing approaches such as correlation-based LSTM, K-nearest neighbor, Stochastic gradient descent, Random forest, Gaussian Naive Bays, Logistic regression, Decision trees, Linear discriminant analysis, Multi-layer perceptron. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Deep learning | Long short-term memory | Machine learning | Neural networks | Software defect prediction,Evolving Systems,2022-12-01,Article,"Pemmada, Suresh Kumar;Behera, H. S.;Nayak, Janmenjoy;Naik, Bighnaraj",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85124032503,10.1142/S2196888822500142,Heterogeneous Fault Prediction Using Feature Selection and Supervised Learning Algorithms,"Software Fault Prediction (SFP) is the most persuasive research area of software engineering. Software Fault Prediction which is carried out within the same software project is known as With-In Fault Prediction. However, local data repositories are not enough to build the model of With-in software Fault prediction. The idea of cross-project fault prediction (CPFP) has been suggested in recent years, which aims to construct a prediction model on one project, and use that model to predict the other project. However, CPFP requires that both the training and testing datasets use the same set of metrics. As a consequence, traditional CPFP approaches are challenging to implement through projects with diverse metric sets. The speci¯c case of CPFP is Heterogeneous Fault Prediction (HFP), which allows the program to predict faults among projects with diverse metrics. The proposed framework aims to achieve an HFP model by implementing Feature Selection on both the source and target datasets to build an e±cient prediction model using supervised machine learning techniques. Our approach is applied on two open-source projects, Linux and MySQL, and prediction is evaluated based on Area Under Curve (AUC) performance measure. The key results of the proposed approach are as follows: It signi¯cantly gives better results of prediction performance for heterogeneous projects as compared with cross projects. Also, it demonstrates that feature selection with feature mapping has a signi¯cant e®ect on HFP models. Non-parametric statistical analyses, such as the Friedman and Nemenyi Post-hoc Tests, are applied, demonstrating that Logistic Regression performed signi¯cantly better than other supervised learning algorithms in HFP models. © Vietnam Journal of Computer Science.All right reserved.",Feature mapping | Feature selection | Heterogeneous fault prediction | Software engineering | Supervised machine learning | Transfer learning,Vietnam Journal of Computer Science,2022-01-01,Article,"Arora, Rashmi;Kaur, Arvinder",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85122445650,10.1007/978-981-16-6289-8_31,Latent Dirichlet Allocation (LDA) Based on Automated Bug Severity Prediction Model,"The software systems prevail various types of bugs reported by users. On the basis of severity levels of bugs, it is decided which bug needs to be fixed first. If the bug is crucial, then it is fixed immediately, and if the bug is minor, then its fixing procedure can be postponed. This research seeks to present an automated bug severity prediction model by using topic modelling and classification techniques. The summary of historical bug reports of the Eclipse project is pre-processed to extract features. Later feature extraction is done by using term frequency and latent Dirichlet allocation (LDA): a topic modelling technique. The topics extracted from LDA are then used to train the classifiers. Our approach also endorses the severity level for the newly disclosed bug. After training the model, the results actively exhibit admissible accuracy scores. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Bug severity prediction | Feature extraction | Latent dirichlet allocation (LDA) | Machine learning | Topic modelling,Lecture Notes on Data Engineering and Communications Technologies,2022-01-01,Book Chapter,"Bibyan, Ritu;Anand, Sameer;Jaiswal, Ajay",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85119415947,10.32604/csse.2022.021750,Defect prediction using Akaike and Bayesian information criterion,"Data available in software engineering for many applications contains variability and it is not possible to say which variable helps in the process of the prediction. Most of the work present in software defect prediction is focused on the selection of best prediction techniques. For this purpose, deep learning and ensemble models have shown promising results. In contrast, there are very few researches that deals with cleaning the training data and selection of best parameter values from the data. Sometimes data available for training the models have high variability and this variability may cause a decrease in model accuracy. To deal with this problem we used the Akaike information criterion (AIC) and the Bayesian information criterion (BIC) for selection of the best variables to train the model. A simple ANN model with one input, one output and two hidden layers was used for the training instead of a very deep and complex model. AIC and BIC values are calculated and combination for minimum AIC and BIC values to be selected for the best model. At first, variables were narrowed down to a smaller number using correlation values. Then subsets for all the possible variable combinations were formed. In the end, an artificial neural network (ANN) model was trained for each subset and the best model was selected on the basis of the smallest AIC and BIC value. It was found that combination of only two variables’ ns and entropy are best for software defect prediction as it gives minimum AIC and BIC values. While, nm and npt is the worst combination and gives maximum AIC and BIC values. © 2022 CRL Publishing. All rights reserved.",AIC | BIC | Cross-project defect prediction | Machine learning | Model selection | Software defect prediction,Computer Systems Science and Engineering,2022-01-01,Article,"Albahli, Saleh;Yar, Ghulam Nabi Ahmad Hassan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85118655140,10.32604/cmc.2022.022085,Software defect prediction harnessing on multi 1-dimensional convolutional neural network structure,"Developing successful software with no defects is one of the main goals of software projects. In order to provide a software project with the anticipated software quality, the prediction of software defects plays a vital role. Machine learning, and particularly deep learning, have been advocated for predicting software defects, however both suffer frominadequate accuracy, overfitting, and complicated structure. In this paper, we aim to address such issues in predicting software defects. We propose a novel structure of 1- Dimensional Convolutional Neural Network (1D-CNN), a deep learning architecture to extract useful knowledge, identifying andmodelling the knowledge in the data sequence, reduce overfitting, and finally, predict whether the units of code are defects prone. We design large-scale empirical studies to reveal the proposed model's effectiveness by comparing four established traditional machine learning baseline models and four state-of-the-art baselines in software defect prediction based on the NASA datasets. The experimental results demonstrate that in terms of f-measure, an optimal and modest 1DCNN with a dropout layer outperforms baseline and state-of-the-art models by 66.79% and 23.88%, respectively, in ways that minimize overfitting and improving prediction performance for software defects. According to the results, 1D-CNN seems to be successful in predicting software defects and may be applied and adopted for a practical problem in software engineering. This, in turn, could lead to saving software development resources and producing more reliable software. © 2022 Tech Science Press. All rights reserved.",Convolutional neural network | Deep learning | Defects | Machine learning | Software defect prediction,"Computers, Materials and Continua",2022-01-01,Article,"Zain, Zuhaira Muhammad;Sakri, Sapiah;Ismail, Nurul Halimatul Asmak;Parizi, Reza M.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85123378670,10.1007/978-3-030-82469-3_27,Software Fault Prediction Using Data Mining Techniques on Software Metrics,"Software industries have enormous demand for fault prediction of the faulty module and fault removal techniques. Many researchers have developed different fault prediction models to predict the fault at an early stage of the software development life cycle (SDLC). But the state-of-the-art model still suffers from the performance and generalize validation of the models. However, some researchers refer to data mining techniques, machine learning, and artificial intelligence play crucial roles in developing fault prediction models. A recent study stated that metric selection techniques also help to enhance the performance of models. Hence, to resolve the issue of improving the fault prediction model’s performance and validation, we have used data mining, instance selection, metric selection, and ensemble methods to beat the state-of-the-art results. For the validation, we have collected the 22 software projects from the four different software repositories. We have implemented three machine learning algorithms and three ensemble methods with two metric selection methods on 22 datasets. The statistical evaluation of the implemented model performed using Wilcoxon signed-rank test and the Friedman test followed by the Nemenyi test to find the significant model. As a result, the Random forest algorithm produces the best result with an average median of 95.43% (accuracy) and 0.96 (f-measure) on 22 software projects. Based on the Nemenyi test, Random forest (RF) is performing better with 4.54 (accuracy mean score) and 4.41 (f-measure mean score) shown in the critical diagram. Experimental study shows that data mining techniques with PCA provide better accuracy and f-measure. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",cyberattacks | dynamic analysis | Machine learning | Software vulnerabilities | static analysis,"2021 9th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions), ICRITO 2021",2021-01-01,Conference Paper,"Peerzada, Bareen;Kumar, Deepak",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85116237363,10.32604/iasc.2022.020362,Machine learning empowered software defect prediction system,"Production of high-quality software at lower cost has always been the main concern of developers. However, due to exponential increases in size and complexity, the development of qualitative software with lower costs is almost impossible. This issue can be resolved by identifying defects at the early stages of the development lifecycle. As a significant amount of resources are consumed in testing activities, if only those software modules are shortlisted for testing that is identified as defective, then the overall cost of development can be reduced with the assurance of high quality. An artificial neural network is considered as one of the extensively used machine-learning techniques for predicting defect-prone software modules. In this paper, a cloud-based framework for real-time software-defect prediction is presented. In the proposed framework, empirical analysis is performed to compare the performance of four training algorithms of the back-propagation technique on software-defect prediction: Bayesian regularization (BR), Scaled Conjugate Gradient, Broyden–Fletcher–Goldfarb–Shanno Quasi-Newton, and Levenberg-Marquardt algorithms. The proposed framework also includes a fuzzy layer to identify the best training function based on performance. Publicly available cleaned versions of NASA datasets are used in this study. Various measures are used for performance evaluation including specificity, preci-sion, recall, F-measure, an area under the receiver operating characteristic curve, accuracy, R2, and mean-square error. Two graphical user interface tools are developed in MatLab software to implement the proposed framework. The first tool is developed for comparing training functions as well as for extracting the results; the second tool is developed for the selection of the best training function using fuzzy logic. A BR training algorithm is selected by the fuzzy layer as it outperformed the others in most of the performance measures. The accuracy of the BR training function is also compared with other widely used machine-learn-ing techniques, from which it was found that the BR performed better among all training functions. © 2022, Tech Science Press. All rights reserved.",Artificial neural network | Machine learning | Software defect prediction,Intelligent Automation and Soft Computing,2022-01-01,Article,"Daoud, Mohammad Sh;Aftab, Shabib;Ahmad, Munir;Khan, Muhammad Adnan;Iqbal, Ahmed;Abbas, Sagheer;Iqbal, Muhammad;Ihnaini, Baha",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85115625635,10.1007/978-3-030-85540-6_113,A Machine Learning Model Comparison and Selection Framework for Software Defect Prediction Using VIKOR,"In today’s time, software quality assurance is the most essential and costly set of activities during software development in the information technology (IT) industries. Finding defects in system modules has always been one of the most relevant problems in software engineering, leading to increased costs and reduced confidence in the product, resulting in dissatisfaction with customer requirements. Therefore, to provide and deliver an efficient software product with as few defects as possible on time and of good quality, it is necessary to use machine learning techniques and models, such as supervised learning to accurately classify and predict defects in each of the software development life cycle (SDLC) phases before delivering a software product to the customer. The main objective is to evaluate the performance of different machine learning models in software defect prediction applied to 4 NASA datasets, such as CM1, JM1, KC1, and PC1, then de-terminate and select the best performing model using the MCDM: VIKOR multi-criteria decision-making method. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Machine learning | MCDM | NASA dataset | Software defect prediction | VIKOR method,Lecture Notes in Networks and Systems,2022-01-01,Conference Paper,"Martinez, Miguel Ángel Quiroz;Tayupanda, Byron Alcívar Martínez;Paguay, Sulay Stephanie Camatón;Peñafiel, Luis Andy Briones",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85115268247,10.1007/978-981-16-3728-5_7,Simplify Your Neural Networks: An Empirical Study on Cross-Project Defect Prediction,"Ensuring software quality, when every industry depends on software, is of utmost importance. Software bugs can have grave consequences and thus identifying and fixing them becomes imperative for developers. Software defect prediction (SDP) focuses on identifying defect-prone areas so that testing resources can be allocated judiciously. Sourcing historical data is not easy, especially in the case of new software, and this is where cross-project defect prediction (CPDP) comes in. SDP, and specifically CPDP, have both attracted the attention of the research community. Simultaneously, the versatility of neural networks (NN) has pushed researchers to study the applications of NNs to defect prediction. In most research, the architecture of a NN is arrived at through trial-and-error. This requires effort, which can be reduced if there is a general idea about what kind of architecture works well in a particular field. In this paper, we tackle this problem by testing six different NNs on a dataset of twenty software from the PROMISE repository in a strict CPDP setting. We then compare the best architecture to three proposed methods for CPDP, which cover a wide range of scenarios. During our research, we found that the simplest NN with dropout layers (NN-SD) performed the best and was also statistically significantly better than the CPDP methods it was compared with. We used the area under the curve for receiver operating characteristics (AUC-ROC) as the performance metric, and for testing statistical significance, we use the Friedman chi-squared test and the Wilcoxon signed-rank test. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Cross-project defect prediction | Machine learning | Neural networks | Software defect prediction | Software quality,Lecture Notes on Data Engineering and Communications Technologies,2022-01-01,Book Chapter,"Malhotra, Ruchika;Khan, Abuzar Ahmed;Khera, Amrit",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85115224913,10.1007/978-3-030-85577-2_48,A Novel Feature to Predict Buggy Changes in a Software System,"Researchers have successfully implemented machine learning classifiers to predict bugs in a change file for years. Change classification focuses on determining if a new software change is clean or buggy. In the literature, several bug prediction methods at change level have been proposed to improve software reliability. This paper proposes a model for classification-based bug prediction model. Four supervised machine learning classifiers (Support Vector Machine, Decision Tree, Random Forrest, and Naive Bayes) are applied to predict the bugs in software changes, and performance of these four classifiers are characterized. We considered a public dataset and downloaded the corresponding source code and its metrics. Thereafter, we produced new software metrics by analyzing source code at class level and unified these metrics with the existing set. We obtained new dataset to apply machine learning algorithms and compared the bug prediction accuracy of the newly defined metrics. Results showed that our merged dataset is practical for bug prediction based experiments. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Bug prediction | Classification | Code analysis | Code metrics | Machine learning | Software metrics,Lecture Notes in Networks and Systems,2022-01-01,Conference Paper,"Yılmaz, Rahime;Nalçakan, Yağız;Haktanır, Elif",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85115061753,10.1016/j.scico.2021.102715,Evaluating the impact of feature selection consistency in software prediction,"Many empirical software engineering studies have employed feature selection algorithms to exclude the irrelevant and redundant features from the datasets with the aim to improve prediction accuracy achieved with machine learning-based estimation models as well as their generalizability. However, little has been done to investigate how consistently these feature selection algorithms produce features/metrics across different training samples, which is an important point for the interpretation of the trained models. The interpretation of the models largely depends on the features of the analyzed datasets, so it is recommended to evaluate the potential of various feature selection algorithms in terms of how consistently they extract features from the employed datasets. In this study, we consider eight different feature selection algorithms to evaluate how consistently they select features across different folds of k-fold cross-validation as well as when small changes are made in the training data. To provide a stable and generalized conclusion, we investigate data from two different domains, i.e., six datasets from the domain of Software Development Effort Estimation (SDEE) and six datasets from the Software Fault Prediction (SFP) domain. Our results reveal that a feature selection algorithm could produce 20-100% inconsistent features with an SDEE dataset and 18.8-95.3% inconsistent features in the case of an SFP dataset. The analysis also reveals that it is not necessarily true that the most consistent feature selection algorithm results to be the most accurate one (i.e., leads to better prediction accuracy) in the case of SDEE datasets, while with SFP datasets, the analysis highlights that the most consistent feature selection algorithm also results to be the most accurate in predicting faults. © 2021 Elsevier B.V.",Empirical study | Feature selection consistency | Software efforts estimation | Software faults prediction,Science of Computer Programming,2022-01-01,Article,"Ali, Asad;Gravino, Carmine",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85115627445,10.1007/978-3-030-77916-0_8,Which Static Code Metrics Can Help to Predict Test Case Effectiveness? New Metrics and Their Empirical Evaluation on Projects Assessed for Industrial Relevance,"One of corner stones of software development are test cases, which help in assessment of created production code. As long as they are properly designed, they have a capacity to capture faults. In order to check whether tests are well made, different procedures have been established, like statement coverage or mutation testing, to evaluate their performance. This has an obvious downside of being computationally expensive and as such is not employed on a wide enough scale. Finding solutions to increase efficiency of assessing test cases, could lead to a more widespread adoption and for that reason we investigate one such approach. We tested possibility of predicting test case effectiveness, strictly on a basis of static code metrics of production and test classes. To solve this task we employed three different learning classifiers, to check feasibility of the process and compare their performance. We created our own set of metrics all of which were later assessed for their impact on prediction. Out of seven most impactful predictors, four of them were proposed by us: Number Of test Cases used in Test class (NOCT), Number Of Defined Variables in a class (NODV), Number of New Objects created in a class (NONO), Number Of Assertions used In Test class (NOAIT). Created models yield a promising result, with best of them achieving over 85% for both F-Measure and Precision along with 73% for Matthews Correlation Coefficient. With the fact of well balanced data used in creation of model, it is safe to assume, that they hold some merit. All steps taken to achieve this result are explained in detail. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",machine learning | mutation operators | mutation testing | software testing,Proceedings - International Conference on Software Engineering,2021-05-01,Conference Paper,"Panichella, Annibale;Liem, Cynthia C.S.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85026830618,10.1007/s41870-021-00804-w,Software fault prediction using lion optimization algorithm,"Software fault prediction (SFP) refers to the early prediction of fault-prone modules in software development which are susceptible to faults and incur high development cost. It is highly desirable to ensure the good quality end-product. Machine learning based classifiers are extensively used for SFP. The performance of classifiers to predict fault-prone software modules are threatened by the Curse of dimensionality. This study discusses the metaheuristics available to select optimal feature subset from high-dimensional defect dataset. The study proposes Lion Optimization based Feature Selection (LiOpFS) model and compares the proposed model with the state-of-the-art metaheuristic models statistically. The NASA dataset is used for the experimental work. It is inferred from the experiments that the algorithm LiOpFS performs better than the baseline techniques with the highest value for AUC measure (= 90.1%) and Accuracy measure (= 94.2%). The results bear statistical proofs for validation through the conduction of Friedman Test at the confidence level of 95%. © 2021, Bharati Vidyapeeth's Institute of Computer Applications and Management.",Crowdsourced testing | deep learning | domain adaptation | test report classification,"Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",2017-06-30,Conference Paper,"Wang, Junjie;Cui, Qiang;Wang, Song;Wang, Qing",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85072111655,10.1007/s10489-021-02346-x,Software fault prediction based on the dynamic selection of learning technique: findings from the eclipse project study,"An effective software fault prediction (SFP) model could help developers in the quick and prompt detection of faults and thus help enhance the overall reliability and quality of the software project. Variations in the prediction performance of learning techniques for different software systems make it difficult to select a suitable learning technique for fault prediction modeling. The evaluation of previously presented SFP approaches has shown that single machine learning-based models failed to provide the best accuracy in any context, highlighting the need to use multiple techniques to build the SFP model. To solve this problem, we present and discuss a software fault prediction approach based on selecting the most appropriate learning techniques from a set of competitive and accurate learning techniques for building a fault prediction model. In work, we apply the discussed SFP approach for the five Eclipse project datasets and nine Object-oriented (OO) project datasets and report the findings of the experimental study. We have used different performance measures, i.e., AUC, accuracy, sensitivity, and specificity, to assess the discussed approach’s performance. Further, we have performed a cost-benefit analysis to evaluate the economic viability of the approach. Results showed that the presented approach predicted the software’s faults effectively for the used accuracy, AUC, sensitivity, and specificity measures with the highest achieved values of 0.816, 0.835, 0.98, and 0.903 for AUC, accuracy, sensitivity, and specificity, respectively. The cost-benefit analysis of the approach showed that it could help reduce the overall software testing cost. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Artifical Intelligence | Data | Machine Learning | Process | Software Engineering,"Proceedings - 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice, ICSE-SEIP 2019",2019-05-01,Conference Paper,"Amershi, Saleema;Begel, Andrew;Bird, Christian;DeLine, Robert;Gall, Harald;Kamar, Ece;Nagappan, Nachiappan;Nushi, Besmira;Zimmermann, Thomas",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85121592977,10.3837/TIIS.2021.11.009,Semi-supervised Software Defect Prediction Model Based on Tri-training,"Aiming at the problem of software defect prediction difficulty caused by insufficient software defect marker samples and unbalanced classification, a semi-supervised software defect prediction model based on a tri-training algorithm was proposed by combining feature normalization, over-sampling technology, and a Tri-training algorithm. First, the feature normalization method is used to smooth the feature data to eliminate the influence of too large or too small feature values on the model's classification performance. Secondly, the oversampling method is used to expand and sample the data, which solves the unbalanced classification of labelled samples. Finally, the Tri-training algorithm performs machine learning on the training samples and establishes a defect prediction model. The novelty of this model is that it can effectively combine feature normalization, oversampling techniques, and the Tri-training algorithm to solve both the under-labelled sample and class imbalance problems. Simulation experiments using the NASA software defect prediction dataset show that the proposed method outperforms four existing supervised and semi-supervised learning in terms of Precision, Recall, and F-Measure values. © 2021 KSII.",Feature normalization | Oversampling techniques | Semi-supervised learning | Software defect prediction | Unbalanced classification,KSII Transactions on Internet and Information Systems,2021-11-30,Article,"Meng, Fanqi;Cheng, Wenying;Wang, Jingdong",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85111447656,10.1145/3458817.3476170,G-SEPM: Building an accurate and efficient soft error prediction model for GPGPUs,"As GPUs become ubiquitous in large-scale general purpose HPC systems (GPGPUs), ensuring the reliable execution of such systems in the presence of soft errors is increasingly essential. To provide insights into how resilient GPU programs are toward soft errors, researchers typically rely on random Fault Injection (FI) to evaluate the tolerance of programs. However, it is expensive to obtain a statistically significant resilience profile and not suitable to identify all the error-critical fault sites of GPU programs. To address the above challenges, in this work, we build a GPGPUbased Soft Error Prediction Model (G-SEPM) that can replace FI to estimate the resilience characteristics of individual fault sites accurately and efficiently. We observe that the instruction-Type, bitposition, bit-flip direction, and error propagation information have capabilities to characterize fault site resiliency. Leveraging these heuristic features, G-SEPM drives the machine learning model to reveal the hidden interactions among fault site resiliency and our observed features. Experimental results demonstrate that G-SEPM achieves high accuracy for fault site error estimation and critical fault site identification while introducing negligible overhead. In addition, G-SEPM can provide essential insight for programmers/ architects to design more cost-effective soft error mitigation solutions. © 2021 IEEE Computer Society. All rights reserved.",Deep Learning Testing | Deep Neural Network | Label | Mutation | Test Prioritization,Proceedings - International Conference on Software Engineering,2021-05-01,Conference Paper,"Wang, Zan;You, Hanmo;Chen, Junjie;Zhang, Yingyi;Dong, Xuyuan;Zhang, Wenbin",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85115717137,10.3390/s21227535,An empirical study of training data selection methods for ranking-oriented cross-project defect prediction,"Ranking-oriented cross-project defect prediction (ROCPDP), which ranks software modules of a new target industrial project based on the predicted defect number or density, has been suggested in the literature. A major concern of ROCPDP is the distribution difference between the source project (aka. within-project) data and target project (aka. cross-project) data, which evidently degrades prediction performance. To investigate the impacts of training data selection methods on the performances of ROCPDP models, we examined the practical effects of nine training data selection methods, including a global filter, which does not filter out any cross-project data. Additionally, the prediction performances of ROCPDP models trained on the filtered cross-project data using the training data selection methods were compared with those of ranking-oriented within-project defect prediction (ROWPDP) models trained on sufficient and limited within-project data. Eleven available defect datasets from the industrial projects were considered and evaluated using two ranking performance measures, i.e., FPA and Norm(Popt). The results showed no statistically significant differences among these nine training data selection methods in terms of FPA and Norm(Popt). The performances of ROCPDP models trained on filtered cross-project data were not comparable with those of ROWPDP models trained on sufficient historical within-project data. However, ROCPDP models trained on filtered cross-project data achieved better performance values than ROWPDP models trained on limited historical within-project data. Therefore, we recommended that software quality teams exploit other project datasets to perform ROCPDP when there is no or limited within-project data. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Commit-relevant mutants | Continuous integration | Machine learning | Mutation testing | Regression testing,Proceedings - International Conference on Software Engineering,2021-05-01,Conference Paper,"Ma, Wei;Titcheu Chekam, Thierry;Papadakis, Mike;Harman, Mark",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85109572803,10.1007/s10515-021-00289-8,Discriminating features-based cost-sensitive approach for software defect prediction,"Correlated quality metrics extracted from a source code repository can be utilized to design a model to automatically predict defects in a software system. It is obvious that the extracted metrics will result in a highly unbalanced data, since the number of defects in a good quality software system should be far less than the number of normal instances. It is also a fact that the selection of the best discriminating features significantly improves the robustness and accuracy of a prediction model. Therefore, the contribution of this paper is twofold, first it selects the best discriminating features that help in accurately predicting a defect in a software component. Secondly, a cost-sensitive logistic regression and decision tree ensemble-based prediction models are applied to the best discriminating features for precisely predicting a defect in a software component. The proposed models are compared with the most recent schemes in the literature in terms of accuracy, area under the curve, and recall. The models are evaluated using 11 datasets and it is evident from the results and analysis that the performance of the proposed prediction models outperforms the schemes in the literature. © 2021, The Author(s).",Code clone detection | Code retrieval | Code search | Cross language | Fine tuning | Self supervised | Unlabel data | Unlabelled data,Proceedings - International Conference on Software Engineering,2021-05-01,Conference Paper,"Bui, Nghi D.Q.;Yu, Yijun;Jiang, Lingxiao",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85120614199,10.4018/IJOSSP.287611,A software fault prediction on inter- And intra-release prediction scenarios,"Software quality engineering applied numerous techniques for assuring the quality of software, namely testing, verification, validation, fault tolerance, and fault prediction of the software. The machine learning techniques facilitate the identification of software modules as faulty or non-faulty. In most of the research, these approaches predict the fault-prone module in the same release of the software. The model is found to be more efficient and validated when training and tested data are taken from previous and subsequent releases of the software respectively. The contribution of this paper is to predict the faults in two scenarios (i.e., inter- and intra-release prediction). The comparison of both intra- and inter-release fault prediction by computing various performance matrices using machine learning methods shows that intra-release prediction has better accuracy compared to inter-releases prediction across all the releases. Also, both the scenarios achieve good results in comparison to existing research work. Copyright © 2021, IGI Global.",Inter- and Intra-Release Prediction | Machine Learning | R-Studio | Software Fault-Prediction,International Journal of Open Source Software and Processes,2021-10-01,Article,"Mishra, Ashutosh;Singla, Meenu",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85063633493,10.1109/SmartNets50376.2021.9555416,A hybridized approach for testing neural network based intrusion detection systems,"Enhancing the trust of machine learning-based classifiers with large input spaces is a desirable goal; however, due to high labeling costs and limited resources, this is a challenging task. One solution is to use test input prioritization techniques that aim to identify and label only the most effective test instances. These prioritized test inputs can then be used with some popular testing techniques e.g., Metamorphic testing (MT) to test and uncover implementation bugs in computationally complex machine learning classifiers that suffer from the oracle problem. However, there are certain limitations involved with this approach, (i) using a small number of prioritized test inputs may not be enough to check the program correctness over a large variety of input scenarios, and (ii) traditional MT approaches become infeasible when the programs under test exhibit a non-deterministic behavior during training e.g., Neural Network-based classifiers. Therefore, instead of using MT for testing purposes, we propose a metamorphic relation to solve a data generation/labeling problem; that is, enhancing the test inputs effectiveness by extending the prioritized test set with new tests without incurring additional labeling costs. Further, we leverage the prioritized test inputs (both source and follow-up data sets) and propose a statistical hypothesis testing (for detection) and machine learning-based approach (for prediction) of faulty behavior in two other machine learning classifiers (Neural Network-based Intrusion Detection Systems). In our case, the problem is interesting in the sense that injected bugs represent the high accuracy producing mutated program versions that may be difficult to detect by a software developer. The results indicate that (i) the proposed statistical hypothesis testing is able to identify the induced buggy behavior, and (ii) Random Forest outperforms and achieves the best performance over SVM and k-NN algorithms. © 2021 IEEE.",component | Convolutional Neural Network | fault diagnosis | tread production line,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",2018-07-02,Conference Paper,"Lihao, Wen;Yanni, Deng",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85119212656,10.1109/SEAA53835.2021.00021,Combining CNN with DS3for Detecting Bug-prone Modules in Cross-version Projects,"The paper focuses on Cross-Version Defect Prediction (CVDP) where the classification model is trained on information of the prior version and then tested to predict defects in the components of the last release. To avoid the distribution differences which could negatively impact the performances of machine learning based model, we consider Dissimilarity-based Sparse Subset Selection (DS3) technique for selecting meaningful representatives to be included in the training set. Furthermore, we employ a Convolutional Neural Network (CNN) to generate structural and semantic features to be merged with the traditional software measures to obtain a more comprehensive list of predictors. To evaluate the usefulness of our proposal for the CVDP scenario, we perform an empirical study on a total of 20 cross-version pairs from 10 different software projects. To build prediction models we consider Logistic Regression (LR) and Random Forest (RF) and we adopt 3 evaluation criteria (i.e., F-measure, G-mean, Balance) to assess the prediction accuracy. Our results show that the use of CNN with both LR and RF models has a significant impact, with an improvement of ~20% for each evaluation criteria. Differently, we notice that DS3 does not impact significantly in improving prediction accuracy. © 2021 IEEE.",Classification Models | CNN | Cross-Version Defect Prediction | Semantic and Structural Features,"Proceedings - 2021 47th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2021",2021-09-01,Conference Paper,"Fiore, Andrea;Russo, Alfonso;Gravino, Carmine;Risi, Michele",Include,
10.1016/j.infsof.2022.107128,2-s2.0-78650138468,10.1109/SEAA53835.2021.00056,Predicting Software Defect Severity Level using Sentence Embedding and Ensemble Learning,"Bug tracking is one of the prominent activities during the maintenance phase of software development. The severity of the bug acts as a key indicator of its criticality and impact towards planning evolution and maintenance of various types of software products. This indicator measures how negatively the bug may affect the system functionality. This helps in determining how quickly the development teams need to address the bug for successful execution of the software system. Due to a large number of bugs reported every day, the developers find it really difficult to assign the severity level to bugs accurately. Assigning incorrect severity level results in delaying the bug resolution process. Thus automated systems were developed which will assign a severity level using various machine learning techniques. In this work, five different types of sentence embedding techniques have been applied on bugs description to convert the description comments to an n-dimensional vector. These computed vectors are used as an input of the software defect severity level prediction models and ensemble techniques like Bagging, Random Forest classifier, Extra Trees classifier, AdaBoost and Gradient Boosting have been used to train these models. We have also considered different variants of the Synthetic Minority Oversampling Technique (SMOTE) to handle the class imbalance problem as the considered datasets are not evenly distributed. The experimental results on six projects highlight that the usage of sentence embedding, ensemble techniques, and different variants of SMOTE techniques helps in improving the predictive ability of defect severity level prediction models. © 2021 IEEE.",,"IEEE International Conference on Software Maintenance, ICSM",2010-12-20,Conference Paper,"Bhattacharya, Pamela;Neamtiu, Iulian",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84891717700,10.1016/j.compeleceng.2021.107370,Training data selection for imbalanced cross-project defect prediction,"Machine learning methods have been applied in software engineering to effectively predict software defects. Researchers proposed cross-project defect prediction (CPDP) for cases in which few or no data are available. CPDP uses the labeled data of a source project to construct a prediction model for the target project. However, the prediction performance remains inferior because the training data selection for the source project is ineffective. In this paper, the Jensen-Shannon divergence is first applied to automatically select the source project most similar to the target project. Subsequently, a grouped synthetic minority oversampling technique (SMOTE) is applied to improve the class imbalance of the projects. Finally, relative density estimation is performed to select the data for the source project. The experimental results demonstrate that the proposed method improves the prediction performance and exhibits high adaptability to different classifiers. © 2021",,"IEEE International Conference on Software Maintenance, ICSM",2013-12-01,Conference Paper,"Tian, Yuan;Lo, David;Sun, Chengnian",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85107687766,10.1016/j.jss.2021.110993,Automated defect prioritization based on defects resolved at various project periods,"Defect prioritization is mainly a manual and error-prone task in the current state-of-the-practice. We evaluated the effectiveness of an automated approach that employs supervised machine learning. We used two alternative techniques, namely a Naive Bayes classifier and a Long Short-Term Memory model. We performed an industrial case study with a real project from the consumer electronics domain. We compiled more than 15,000 issues collected over 3 years. We could reach an accuracy level up to 79.36% and we had 3 observations. First, Long Short-Term Memory model has a better accuracy when compared with a Naive Bayes classifier. Second, structured features lead to better accuracy compared to textual descriptions. Third, accuracy is not improved by considering increasingly earlier defects as part of the training data. Increasing the size of the training data even decreases the accuracy compared to the results, when we use data only regarding the recently resolved defects. © 2021 Elsevier Inc.",Defect prioritization | Industrial case study | Machine learning | Process automation | Software maintenance,Journal of Systems and Software,2021-09-01,Article,"Gökçeoğlu, Mustafa;Sözer, Hasan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85116247799,10.1145/3468264.3473931,When life gives you oranges: Detecting and diagnosing intermittent job failures at Mozilla,"Continuous delivery of cloud systems requires constant running of jobs (build processes, tests, etc.). One issue that plagues this continuous integration (CI) process are intermittent failures-non-deterministic, false alarms that do not result from a bug in the software or job specification, but rather from issues in the underlying infrastructure. At Mozilla, such intermittent failures are called oranges as a reference to the color of the build status indicator. As such intermittent failures disrupt CI and lead to failures, they erode the developers' trust in the jobs. We present a novel approach that automatically classifies failing jobs to determine whether job execution failures arise from an actual software bug or were caused by flakiness in the job (e.g., test) or the underlying infrastructure. For this purpose, we train classification models using job telemetry data to diagnose failure patterns involving features such as runtime, cpu load, operating system version, or specific platform with high precision. In an evaluation on a set of Mozilla CI jobs, our approach achieves precision scores of 73%, on average, across all data sets with some test suites achieving precision scores good enough for fully automated classification (i.e., precision scores of up to 100%), and recall scores of 82% on average (up to 94%). © 2021 ACM.",continuous integration | flaky tests | intermittent failures | machine learning | Software testing,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2021-08-20,Conference Paper,"Lampel, Johannes;Just, Sascha;Apel, Sven;Zeller, Andreas",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85040611476,10.1109/ICSESS52187.2021.9522337,The Impact of Feature Selection Techniques on Software Defect Identification Models,"Defect identification is an important task for ensuring the quality of software. Recently, researchers have begun to utilize artificial intelligence techniques to improve the usability of static analysis tools by automatically identifying true defects from the reported SA alarms. Existing methods mainly focus on using the static code features to represent the defective code. However, a challenge that threatens the performance of these machine learning methods is the irrelevant and redundant features. Feature selection techniques can be applied to alleviate this problem. Since many feature selection methods have been proposed, this paper conducts a rigorous experimental evaluation on the impact of feature selection techniques for defect identification and explores whether there is a smallest ratio when using the feature selection techniques for building defect identification models with acceptable performance. Additionally, this paper proposes an effective feature selection approach based on the idea of majority voting, combing the output results of different feature selection techniques. The experimental results for five open-source projects show that there is a best ratio (20%) for feature selection which achieves satisfied performance with far fewer features used for defect identification. This finding can serve as a practical guideline for software defect identification. © 2021 IEEE.",Convolutional neural networks | Deep learning | Duplicate bug detection | Information retrieval | Long short term memory | Natural language processing | Siamese networks | Word embeddings,"Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",2017-11-02,Conference Paper,"Deshmukh, Jayati;Annervaz, K. M.;Podder, Sanjay;Sengupta, Shubhashis;Dubash, Neville",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85040626682,10.1145/3484824.3484911,Does Code Complexity Affect the Quality of Real-Time Projects?: Detection of Code Smell on Software Projects using Machine Learning Algorithms,"Code smell targets to identify bugs that occur due to incorrect analysis of code during software development life cycle. It is the task of analyzing a code design problem. The primary causes of code smell are complexity in structural design, violation of programming paradigm, and lack of unit-level testing by the software programmer. Our research focuses on the identification of code smell using different machine learning classifiers. We have considered 15 software code metrics of the Junit open source project and developed a hybrid model for code smell detection. Our dataset consists of 45 features which is further reduced by 15 using various feature selection techniques. Random sampling is used to handle the imbalance in the dataset. The project's performance is evaluated using 10 machine learning techniques which including regression, ensemble methods, and classification. Based on the statistical analysis, it is analyzed that the Random forest ensemble classifiers give best result with an accuracy of 99.12 % is the most appropriate technique for detecting different types of bad smells like god class, duplicate code, long method, large class, and refused bequest. © 2021 ACM.",,"Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",2017-11-02,Conference Paper,"Romansky, Stephen;Borle, Neil C.;Chowdhury, Shaiful;Hindle, Abram;Greiner, Russ",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85077214670,10.1109/ICESC51422.2021.9533030,Empirical Validation of cross-version and 10-fold cross-validation for Defect Prediction,"Nowadays, the development of a prediction model is one of the most important research area. Prediction models are helpful in providing accurate results for unseen data or future data. The most important phase of the software development life cycle (SDLC) is testing. During testing, we face some uncertain behavior of the program due to the presence of defects. To remove these defects in the early stage of SDLC, we design Software Defect Prediction Model (SDPM). Although many SDPM have been developed using various Machine Learning (ML) and statistical technique, but the generalizability of results have prevailed because the developed models use the same dataset for training and testing. This study aims to develop SDPM using cross-version, which using two different versions of a project for training and testing. The historical labeled defect data of the previous version is used for the updated or upcoming version for defect prediction which is termed as Cross-Version Defect Prediction (CVDP). To complete experimentation, we have used 26 datasets from an open-source repository. The performance of SDPM is analyzed using performance metrics. The SDPM is also developed using 10-cross validation. In the end, the comparison of CVDP and 10-cross validation has been done using a statistical test. The aim of conducting this study is to analyze the applicability of cross-version defect prediction when a sufficient amount of data is not available for training and testing. According to statistical analysis, it has been observed that cross-version can be used if we have to test our prediction model for unseen projects or upcoming projects. © 2021 IEEE.",Bug Triaging | Incremental Learning | Log Analysis | Machine Learning,"Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019",2019-09-01,Conference Paper,"Sarkar, Aindrila;Rigby, Peter C.;Bartalos, Bela",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85111456523,10.1145/3460319.3464840,Continuous test suite failure prediction,"Continuous integration advocates to run the test suite of a project frequently, e.g., for every code change committed to a shared repository. This process imposes a high computational cost and sometimes also a high human cost, e.g., when developers must wait for the test suite to pass before a change appears in the main branch of the shared repository. However, only 4% of all test suite invocations turn a previously passing test suite into a failing test suite. The question arises whether running the test suite for each code change is really necessary. This paper presents continuous test suite failure prediction, which reduces the cost of continuous integration by predicting whether a particular code change should trigger the test suite at all. The core of the approach is a machine learning model based on features of the code change, the test suite, and the development history. We also present a theoretical cost model that describes when continuous test suite failure prediction is worthwhile. Evaluating the idea with 15k test suite runs from 242 open-source projects shows that the approach is effective at predicting whether running the test suite is likely to reveal a test failure. Moreover, we find that our approach improves the AUC over baselines that use features proposed for just-in-time defect prediction and test case failure prediction by 13.9% and 2.9%, respectively. Overall, continuous test suite failure prediction can significantly reduce the cost of continuous integration. © 2021 ACM.",Continuous integration | Continuous test suite failure prediction | Cost model | Machine learning,ISSTA 2021 - Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis,2021-07-11,Conference Paper,"Pan, Cong;Pradel, Michael",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85111442372,10.1145/3460319.3464834,Empirically evaluating readily available information for regression test optimization in continuous integration,"Regression test selection (RTS) and prioritization (RTP) techniques aim to reduce testing efforts and developer feedback time after a change to the code base. Using various information sources, including test traces, build dependencies, version control data, and test histories, they have been shown to be effective. However, not all of these sources are guaranteed to be available and accessible for arbitrary continuous integration (CI) environments. In contrast, metadata from version control systems (VCSs) and CI systems are readily available and inexpensive. Yet, corresponding RTP and RTS techniques are scattered across research and often only evaluated on synthetic faults or in a specific industrial context. It is cumbersome for practitioners to identify insights that apply to their context, let alone to calibrate associated parameters for maximum cost-effectiveness. This paper consolidates existing work on RTP and unsafe RTS into an actionable methodology to build and evaluate such approaches that exclusively rely on CI and VCS metadata. To investigate how these approaches from prior research compare in heterogeneous settings, we apply the methodology in a large-scale empirical study on a set of 23 projects covering 37,000 CI logs and 76,000 VCS commits. We find that these approaches significantly outperform established RTP baselines and, while still triggering 90% of the failures, we show that practitioners can expect to save on average 84% of test execution time for unsafe RTS. We also find that it can be beneficial to limit training data, features from test history work better than change-based features, and, somewhat surprisingly, simple and well-known heuristics often outperform complex machine-learned models. © 2021 ACM.",Machine learning | Regression test optimization | Software testing,ISSTA 2021 - Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis,2021-07-11,Conference Paper,"Elsner, Daniel;Hauer, Florian;Pretschner, Alexander;Reimer, Silke",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85096673171,10.1109/ICDCS51616.2021.00113,Demo: Automatically retrainable self improving model for the automated classification of software incidents into multiple classes,"Developers across most of the organizations face the issue of manually dealing with the classification of the software bug reports. Software bug reports often contain text and other useful information that are common for a particular type of bug. This information can be extracted using the techniques of Natural Language Processing and combined with the manual classification done by the developers until now to create a properly labelled data set for training a supervised learning model for automatically classifying the bug reports into their respective categories. Previous studies have only focused on binary classification of software incident reports as bug and non-bug. Our novel approach achieves an accuracy of 76.94% for a 10-factor classification problem on the bug repository created by Microsoft Dynamics 365 team. In addition, we propose a novel method for automatically retraining the model and updating it with developer feedback in case of misclassification that will significantly reduce the maintenance cost and effort. © 2021 IEEE.",Automatic bug triage | machine learning | text classification,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",2020-09-01,Conference Paper,"Zhang, Wei",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85123361929,10.1109/COMPSAC51774.2021.00205,Predicting number of bugs before launch: An investigation based on machine learning,"Identifying and minimizing the number of bugs before release is a high priority of any team working on software development. This can be achieved by Machine Learning (ML) models By using particular aspects of the code, ML Models can predict the number of bugs that are possible post launch. We use a public dataset consisting of 15 Java projects from GitHub as our training and test dataset. We use five ML models for our investigation: Multilayer Perceptron, K-Nearest Neighbors, Linear Regression, Logistic Regression, and Decision Trees We conduct a preliminary investigation to evaluate how these ML models perform in predicting bugs. The results show that Linear Regression outperforms the other four ML models in finding the number of bugs post release. © 2021 IEEE.",Commit | Ensemble Methods | Issue Report | Link Recovery | Machine Learning | Software Maintenance,"Proceedings - 2021 IEEE International Conference on Software Maintenance and Evolution, ICSME 2021",2021-01-01,Conference Paper,"Mazrae, Pooya Rostami;Izadi, Maliheh;Heydarnoori, Abbas",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85115681149,10.1109/ICSCC51209.2021.9528170,Class Imbalance Issue in Software Defect Prediction Models by various Machine Learning Techniques: An Empirical Study,"Software practitioners are continuing to build advanced software defect prediction (SDP) models to help the tester find fault-prone modules. However, the Class Imbalance (CI) problem consists of uncommonly few defective instances, and more non-defective instances cause inconsistency in the performance. We have conducted 880 experiments to analyze the variation in the performance of 10 SDP models by concerning the class imbalance problem. In our experiments, we have used 22 public datasets consists of 41 software metrics, 10 baseline SDP methods, and 4 sampling techniques. We used Mathews Correlation Coefficient (MCC), which is more useful when a dataset is highly imbalanced. We have also compared the predictive performance of various ML models by applying 4 sampling techniques. To examine the performance of different SDP models, we have used the F-measure. We found the performance of the learning models is unsatisfactory, which needs to mitigate. We have also found a few surprising results, some logical patterns between classifier and sampling technique. It provides a connection between sampling technique, software matrices, and a classifier. © 2021 IEEE.",Class imbalance | Machine learning | Software fault prediction | Software metrics | Statistical methods,"2021 8th International Conference on Smart Computing and Communications: Artificial Intelligence, AI Driven Applications for a Smart World, ICSCC 2021",2021-07-01,Conference Paper,"Pandey, Sushant Kumar;Tripathi, Anil Kumar",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85114271680,10.1109/SEAI52285.2021.9477547,Improving Priority Prediction for Bug Reports with Comment Features,"Deciding the priority of processing a bug report is an important issue for the efficiency of software bug resolution. Generally, this decision process is labor-consuming because the triager needs considerable time to decide the priority. To mitigate this problem, an automatic method for predicting the priority of a bug report is needed. In this paper, we propose a novel prediction scheme to improve the prediction performance with comment features. We have conducted empirical experiments to evaluate the proposed scheme with a previous top-ranked scheme eApp. The experimental results show that the proposed approach can effectively improve the prediction performance. © 2021 IEEE.",bug reports | comment features | machine learning | priority prediction,"2021 IEEE International Conference on Software Engineering and Artificial Intelligence, SEAI 2021",2021-06-11,Conference Paper,"Dao, Anh Hien;Yang, Cheng Zen",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84862335065,10.1007/s10766-021-00707-0,Predicting the Soft Error Vulnerability of Parallel Applications Using Machine Learning,"With the widespread use of the multicore systems having smaller transistor sizes, soft errors become an important issue for parallel program execution. Fault injection is a prevalent method to quantify the soft error rates of the applications. However, it is very time consuming to perform detailed fault injection experiments. Therefore, prediction-based techniques have been proposed to evaluate the soft error vulnerability in a faster way. In this work, we present a soft error vulnerability prediction approach for parallel applications using machine learning algorithms. We define a set of features including thread communication, data sharing, parallel programming, and performance characteristics; and train our models based on three ML algorithms. This study uses the parallel programming features, as well as the combination of all features for the first time in vulnerability prediction of parallel programs. We propose two models for the soft error vulnerability prediction: (1) A regression model with rigorous feature selection analysis that estimates correct execution rates, (2) A novel classification model that predicts the vulnerability level of the target programs. We get maximum prediction accuracy rate of 73.2% for the regression-based model, and achieve 89% F-score for our classification model. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Automatic Fault Localization | Bayesian Networks | Bug Assignment | Large Software Systems | Machine Learning | Naive Bayes | Stacked generalization | Support Vector Machines,"Proceedings - IEEE 5th International Conference on Software Testing, Verification and Validation, ICST 2012",2012-06-21,Conference Paper,"Jonsson, Leif;Broman, David;Sandahl, Kristian;Eldh, Sigrid",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84983364773,10.1007/s12065-020-00434-4,Supervised machine learning approach to predict qualitative software product,"Software development process (SDP) is a framework imposed on software product development and is a multi-stage process wherein a wide range of tasks and activities pan out in each stage. Each stage requires careful observations to improve productivity, quality, etc. to ease the process of development. During each stage, problems surface likes constraint of on-time completion, proper utilization of available resources and appropriate traceability of work progress, etc. and may lead to reiteration due to the defects spotted during testing and then, results into the negative walk-through due to unsatisfactory outcomes. Working on such defects can help to take a step towards the proper steering of activities and thus to improve the expected performance of the software product. Handpicking the proper notable features of SDP and then analyzing their nature towards the outcome can greatly help in getting a reliable software product by meeting the expected objectives. This paper proposed supervised Machine Learning (ML) models for the predictions of better SDP, particularly focusing on cost estimation, defect prediction, and reusability. The experimental studies were conducted on the primary data, and the evaluation reveals the model suitability in terms of efficiency and effectiveness for SDP prediction (accuracy of cost estimation: 65%, defect prediction: 93% and reusability: 82%). © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",,"Proceedings - 2016 IEEE International Conference on Software Testing, Verification and Validation, ICST 2016",2016-07-18,Conference Paper,"Xin, Rui",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85048438492,10.1145/3433210.3453115,Bran: Reduce Vulnerability Search Space in Large Open Source Repositories by Learning Bug Symptoms,"Software is continually increasing in size and complexity, and therefore, vulnerability discovery would benefit from techniques that identify potentially vulnerable regions within large code bases, as this allows for easing vulnerability detection by reducing the search space. Previous work has explored the use of conventional code-quality and complexity metrics in highlighting suspicious sections of (source) code. Recently, researchers also proposed to reduce the vulnerability search space by studying code properties with neural networks. However, previous work generally failed in leveraging the rich metadata that is available for long-running, large code repositories. In this paper, we present an approach, named Bran, to reduce the vulnerability search space by combining conventional code metrics with fine-grained repository metadata. Bran locates code sections that are more likely to contain vulnerabilities in large code bases, potentially improving the efficiency of both manual and automatic code audits. In our experiments on four large code bases, Bran successfully highlights potentially vulnerable functions, outperforming several baselines, including state-of-art vulnerability prediction tools. We also assess Bran's effectiveness in assisting automated testing tools. We use Bran to guide syzkaller, a known kernel fuzzer, in fuzzing a recent version of the Linux kernel. The guided fuzzer identifies 26 bugs (10 are zero-day flaws), including arbitrary writes and reads. © 2021 Owner/Author.",cloud | fault localization,"Proceedings - 2018 IEEE 11th International Conference on Software Testing, Verification and Validation, ICST 2018",2018-05-25,Conference Paper,"Mariani, Leonardo;Monni, Cristina;Pezze, Mauro;Riganelli, Oliviero;Xin, Rui",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85048401472,10.1109/ICSCCC51823.2021.9478165,A Four-Stage dynamic approach for Software Defect Prediction,"Defects are a major constraint in the software development process. Defects can cause a lot of trouble if not found in earlier stages of software development. They can result in cost overruns and can even lead to a complete system failure. Many supervised machine learning algorithms have been used over the past few decades for software defect prediction. However, the huge variety of software projects causes a problem of varying performance. No machine learning algorithm can deliver the best results consistently for all datasets. Also, a given dataset can have a variety of tuples having different similarities and dissimilarities. Therefore, there is a need to address these problems while predicting defects in any project. This paper introduces an approach for defect prediction which takes into consideration the variety of software projects available. This approach is based on dividing the training dataset into multiple subsets and applying multiple supervised learning algorithms on each such subset to determine the best algorithms for each such subset and then using this information to predict software defect in future projects by making use of similarities in datasets. The given dataset is divided into four parts based on whether the results of applying any random machine learning algorithm on that dataset were true positive, true negative, false positive, or false negative. Experimental results on the PROMISE data repository show an improvement in performance over the existing machine learning algorithms. © 2021 IEEE.",automotive software | black box testing | emulation platform | fault injection | GDB | learning based testing | machine learning | Model based testing | QEMU | requirements testing | temporal logic | virtual hardware,"Proceedings - 2018 IEEE 11th International Conference on Software Testing, Verification and Validation, ICST 2018",2018-05-25,Conference Paper,"Khosrowjerdi, Hojat;Meinke, Karl;Rasmusson, Andreas",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85114129518,10.1109/ICSCCC51823.2021.9478153,Extensive Software Fault Prediction: An Ensemble based comparison,"In recent studies, fault prediction is used to envisage the software's quality via conventional machine learning techniques. There are numerous problems that distress the efficiency of the software like redundant data, irrelevance features and missing data. It is observed from the literature that performance of the machine learning methods varies from data-set to data-set. In this paper, authors examines the performance of the model get affected after applying several data processing techniques on different machine learning classifiers. Various performance measures like accuracy, RMSE and ROC (AUC) are considered for predicting the quality of software. © 2021 IEEE.",bagging | boosting | ensemble methods | Software fault prediction,ICSCCC 2021 - International Conference on Secure Cyber Computing and Communications,2021-05-21,Conference Paper,"Sharma, Pooja;Sangal, Amrit Lal",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85067981414,10.1109/ICSCCC51823.2021.9478119,A Study on Modeling Techniques in Software Fault Prediction,"Software fault prediction is a key area in the field of software engineering. Fault can occur in any stage of software development but if it is not correctly identified and removed, it may lead to system failure. The main purpose of software fault prediction is to find faults prior to the testing phase, which will further decrease the time and cost of developing software and upgrade the quality of software. In this paper, we review machine learning and ensemble-based modeling techniques concerning software fault prediction. This paper will help the researchers to choose the best modeling technique with respect to datasets and performance measures. The findings reveal that the presence of various data quality issues related to datasets degrades the accuracy of fault prediction and also find that ensemble-based techniques are more efficient as compared to machine learning techniques. © 2021 IEEE.",Anomaly detection | Cloud reliability | Failure prediction | Machine learning,"Proceedings - 2019 IEEE 12th International Conference on Software Testing, Verification and Validation, ICST 2019",2019-04-01,Conference Paper,"Monni, Cristina;Pezze, Mauro;Prisco, Gaetano",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85067997104,10.1109/MSR52588.2021.00019,Learning off-by-one mistakes: An empirical study,"Mistakes in binary conditions are a source of error in many software systems. They happen when developers use, e.g., '<' or '>' instead of '<=' or '>='. These boundary mistakes are hard to find and impose manual, labor-intensive work for software developers.While previous research has been proposing solutions to identify errors in boundary conditions, the problem remains open. In this paper, we explore the effectiveness of deep learning models in learning and predicting mistakes in boundary conditions. We train different models on approximately 1.6M examples with faults in different boundary conditions. We achieve a precision of 85% and a recall of 84% on a balanced dataset, but lower numbers in an imbalanced dataset. We also perform tests on 41 real-world boundary condition bugs found from GitHub, where the model shows only a modest performance. Finally, we test the model on a large-scale Java code base from Adyen, our industrial partner. The model reported 36 buggy methods, but none of them were confirmed by developers. © 2021 IEEE.",Deep learning | Machine learning | Mutation testing | Software quality | Software testing,"Proceedings - 2019 IEEE 12th International Conference on Software Testing, Verification and Validation, ICST 2019",2019-04-01,Conference Paper,"Mao, Dongyu;Chen, Lingchao;Zhang, Lingming",Include,
10.1016/j.infsof.2022.107128,2-s2.0-80051656116,10.1109/MSR52588.2021.00035,Leveraging models to reduce test cases in software repositories,"Given a failing test case, test case reduction yields a smaller test case that reproduces the failure. This process can be time consuming due to repeated trial and error with smaller test cases. Current techniques speed up reduction by only exploring syntactically valid candidates, but they still spend significant effort on semantically invalid candidates. In this paper, we propose a model-guided approach to speed up test case reduction. The approach trains a model of semantic properties driven by syntactic test case properties. By using this model, we can skip testing even syntactically valid test case candidates that are unlikely to succeed. We evaluate this model-guided reduction on a suite of 14 large fuzzer-generated C test cases from the bug repositories of two well-known C compilers, GCC and Clang. Our results show that with an average precision of 77%, we can decrease the number of removal trials by 14% to 61%. We observe a 30% geomean improvement in reduction time over the state of the art technique while preserving similar reduction power. © 2021 IEEE.",Event based testing | Grammar induction | GUI testing | Machine learning | Software testing | Support vector machines,"Proceedings - 4th IEEE International Conference on Software Testing, Verification, and Validation Workshops, ICSTW 2011",2011-08-18,Conference Paper,"Gove, Robert;Faytong, Jorge",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85050978697,10.1002/spe.2941,Software fault prediction using Whale algorithm with genetics algorithm,"Software fault prediction became an essential research area in the last few years, there are many prediction and optimization techniques that have been developed for fault prediction. In this paper, an approach is developed by integrating genetics algorithm with support vector machine (SVM) classifier and Whale optimization algorithm for software fault prediction. The developed approach is applied to 24 datasets (12-NASA MDP and 12-Java open-source projects), where NASA MDP is considered as a large-scale dataset, and Java open source projects are considered as a small-scale dataset. Results indicate that integrating Genetics algorithm with SVM and Whale algorithm improves the performance of the software fault prediction process when it is applied to large-scale and small-scale datasets and overcome the limitations that appeared in the previous studies. © 2020 John Wiley & Sons, Ltd.",Machine Learning | Software Engineering and Testing | Software Quality,"Proceedings - 2018 IEEE 11th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2018",2018-07-16,Conference Paper,"Masuda, Satoshi;Ono, Kohichi;Yasue, Toshiaki;Hosokawa, Nobuhiro",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85108029707,10.1109/ISCAIE51753.2021.9431842,SAGA: A Hybrid Technique to handle Imbalance Data in Software Defect Prediction,"Software defect prediction has been a concurrent topic in software quality-based research. Predictive models that identify defect prone parts of Software can be evolved from defect data and software metrics. Various studies conducted in the past have explored Machine Learning-based approaches for this purpose but the problem of handling imbalanced defect data without compromising on the model's performance remains at large. In this work, we have proposed, compared, and analyzed a hybrid technique, SAGA(SMOTE + AdaSS + Genetic Algorithm), for solving the imbalance problem faced in software defect prediction. SAGA employs ensemble classification based on feature space partitioning in conjunction with the Synthetic Minority Oversampling technique. Various parameters related to feature space partitioning are optimized using the Genetic Algorithm. The values of ROC-AUC, G-mean, Balance, and Accuracy obtained on open-source datasets confirm the effectiveness of the proposed technique. © 2021 IEEE.",Combinatorial testing | Counterfactual explanation | Debugging DNN models | Deep learning | Explainability | Explainable AI | Image classifiers | Instance-level explanations | Model-agnostic | Software testing,"Proceedings - 2021 IEEE 14th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2021",2021-04-01,Conference Paper,"Chandrasekaran, Jaganmohan;Lei, Yu;Kacker, Raghu;Richard Kuhn, D.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85108023808,10.1109/ICSTW52544.2021.00045,Prioritized test generation guided by software fault prediction,"Writing and running software unit tests is one of the fundamental techniques used to maintain software quality. However, this process is rather costly and time consuming. Thus, much effort has been devoted to generating unit tests automatically. The common objective of test generation algorithms is to maximize code coverage. However, maximizing coverage is not necessarily correlated with identifying faults [1]. In this work, we propose a novel approach for test generation aiming at generating a small set of tests that cover the software components that are likely to contain bugs. To identify which components are more likely to contain bugs, we train a software fault prediction model using machine learning techniques. We implemented this approach in practice in a tool called QUADRANT, and demonstrate its effectiveness on five real-world, open-source projects. Results show the benefit of using QUADRANT, where test generation guided by our fault prediction model can detect more than double the number of bugs compared to a coverage-oriented approach, thereby saving test generation and execution efforts. © 2021 IEEE.",Automated testing generation | Fault prediction | Machine learning | Software testing,"Proceedings - 2021 IEEE 14th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2021",2021-04-01,Conference Paper,"Hershkovich, Eran;Stern, Roni;Abreu, Rui;Elmishali, Amir",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85102249263,10.4018/IJGHPC.2021040105,Performance comparison of various algorithms during software fault prediction,"Producing software of high quality is challenging in view of the large volume, size, and complexity of the developed software. Checking the software for faults in the early phases helps to bring down testing resources. This empirical study explores the performance of different machine learning model, fuzzy logic algorithms against the problem of predicting software fault proneness. The work experiments on the public domain KC1 NASA data set. Performance of different methods of fault prediction is evaluated using parameters such as receiver characteristics (ROC) analysis and RMS (root mean squared), etc. Comparison is made among different algorithms/models using such results which are presented in this paper. © 2021 IGI Global. All rights reserved.",Adaptive Neuro Fuzzy Inference System (ANFIS) | Genetic Algorithm (GA) | k Nearest Neighbor (KNN) | Logistic Regression | Multi-Layer Perceptron (MLP) | Particle Swarm Optimization (PSO) | Support Vector Machine (SVM),International Journal of Grid and High Performance Computing,2021-04-01,Article,"Khanna, Munish;Toofani, Abhishek;Bansal, Siddharth;Asif, Mohammad",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85098621092,10.1016/j.infsof.2020.106508,On the prediction of long-lived bugs: An analysis and comparative study using FLOSS projects,"Context: Software evolution and maintenance activities in today's Free/Libre Open Source Software (FLOSS) rely primarily on information extracted from bug reports registered in bug tracking systems. Many studies point out that most bugs that adversely affect the user's experience across versions of FLOSS projects are long-lived bugs. However, proposed approaches that support bug fixing procedures do not consider the real-world lifecycle of a bug, in which bugs are often fixed very fast. This may lead to useless efforts to automate the bug management process. Objective: This study aims to confirm whether the number of long-lived bugs is significantly high in popular open-source projects and to characterize the population of long-lived bugs by considering the attributes of bug reports. We also aim to conduct a comparative study evaluating the prediction accuracy of five well-known machine learning algorithms and text mining techniques in the task of predicting long-lived bugs. Methods: We collected bug reports from six popular open-source projects repositories (Eclipse, Freedesktop, Gnome, GCC, Mozilla, and WineHQ) and used the following machine learning algorithms to predict long-lived bugs: K-Nearest Neighbor, Naïve Bayes, Neural Networks, Random Forest, and Support Vector Machines. Results: Our results show that long-lived bugs are relatively frequent (varying from 7.2% to 40.7%) and have unique characteristics, confirming the need to study solutions to support bug fixing management. We found that the Neural Network classifier yielded the best results in comparison to the other algorithms evaluated. Conclusion: Research efforts regarding long-lived bugs are needed and our results demonstrate that it is possible to predict long-lived bugs with a high accuracy (around 70.7%) despite the use of simple prediction algorithms and text mining methods. © 2020 Elsevier B.V.",Bug Tracking System | Long-lived bugs | Machine learning | Software maintenance | Text mining,Information and Software Technology,2021-04-01,Article,"Gomes, Luiz Alberto Ferreira;da Silva Torres, Ricardo;Côrtes, Mario Lúcio",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85104966136,10.1145/3412841.3441894,Using regression learners to predict performance problems on software updates: A case study on elevators dispatching algorithms,"Remote software deployment and updating has long been commonplace in many different fields, but now, the increasing expansion of IoT and CPSoS (Cyber-Physcal System of Systems) has highlighted the need for additional mechanisms in these systems, to ensure the correct behaviour of the deployed software version after deployment. In this sense, this paper investigates the use of Machine Learning algorithms to predict acceptable behaviour in system performance of a new software release. By monitoring the real performance, eventual unexpected problems can be identified. Based on previous knowledge and actual run-time information, the proposed approach predicts the response time that can be considered acceptable for the new software release, and this information is used to identify problematic releases. The mechanism has been applied to the post-deployment monitoring of traffic algorithms in elevator systems. To evaluate the approach, we have used performance mutation testing, obtaining good results. This paper makes two contributions. First, it proposes several regression learners that have been trained with different types of traffic profiles to efficiently predict response time of the traffic dispatching algorithm. This prediction is then compared with the actual response time of the new algorithm release, and provides a verdict about its performance. Secondly, a comparison of the different learners is performed. © 2021 ACM.",cyber-physical systems | machine learning | performance bugs,Proceedings of the ACM Symposium on Applied Computing,2021-03-22,Conference Paper,"Gartziandia, Aitor;Arrieta, Aitor;Agirre, Aitor;Sagardui, Goiuria;Arratibel, Maite",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85048484855,10.1109/ICACCS51430.2021.9441809,Empirical Analysis of Artificial Immune System Algorithms for Aging Related Bug Prediction,"The complex nature of human immunology algorithms has motivated the research community to explore their practical applications in various other fields. As a result, Artificial Immune Systems (AISs) is one such class of algorithms that has found its way into software quality predictive modeling. In this paper, we evaluate AIS algorithms for developing Aging-Related Bug (ARB) prediction models. Software Aging, the gradual degradation and resource exhaustion in software systems, is said to be caused by ARBs, which may or may not be identified during software testing. Therefore, predicting ARBs before software release can help software managers in reducing their impact. This paper presents an empirical study that statistically analyzes the effectiveness of AIS classifiers for ARB prediction on five open-source software datasets. In order to account for the imbalanced nature of the investigated datasets, we used resampling and cost-sensitive classifiers. The results of the study indicate the effectiveness of AIS algorithms for developing ARB prediction models. © 2021 IEEE.",Classification | Deep Learning | Fault Analysis | LSTM | Power Grids | PSS/E | SVM,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",2017-07-02,Conference Paper,"Bhattacharya, Biswarup;Sinha, Abhishek",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85107913629,10.1109/INDIACom51348.2021.00143,Development of efficient and optimal models for software maintainability prediction using feature selection techniques,"Software Maintainability is an indispensable characteristic to determine software quality. It can be described as the ease with which necessary changes such as fault correction, performance improvement, addition, or deletion of one or more attributes, etc., can be incorporated. A major purpose of software maintainability is to enable the software to adapt to the changing environment. Machine Learning (ML) algorithms are widely used for Software Maintainability Prediction (SMP). Hence, in the current study, QUES and UIMS, i.e., the two object-oriented datasets are used for SMP. In this study, an attempt has been made to improve the prediction results of five (ML) algorithms, viz., General Regression Neural Network (GRNN), Regularized Greedy Forest (RGF), Gradient Boosting Algorithm (GBA), Multivariate Linear Regression (MLR), and K-Nearest Neighbor (k-NN) on using three different feature selection methods, including the Pearson's Correlation (Filter Method), Backward Elimination (Wrapper Method), and Lasso Regularization (Embedded Method). Feature selection is a procedure to select a set of independent variables that contribute most to the predicted output, hence eliminating the irrelevant features in the data that may reduce the accuracy of an algorithm. The performance of all the models is evaluated using three accuracy measures, i.e., R-Squared, Mean Absolute Error (MAE), and Root Mean Square Error (RMSE). The results portray an improvement in the prediction accuracies after employing feature selection techniques. It is observed that for the QUES dataset, R-Squared value on an average improves by 157.89%. Also, MAE and RMSE values enhance by 19.59% and 24.90%, respectively, depicting an overall decrease in the error. Similarly, for UIMS dataset, R-Squared value on an average increase by 126.08%, representing an improvement in the accuracy. Further, MAE and RMSE values also improve for the UIMS dataset, by 12.44% and 8.16%, respectively. © 2021 Bharati Vidyapeeth, New Delhi. Copy Right in Bulk will be transferred to IEEE by Bharati Vidyapeeth.",Feature selection | General regression neural network | Gradient boosting algorithm | K-nearest neighbors | Machine learning | Multivariate linear regression | Regularized greedy forest | Software maintainability prediction,"Proceedings of the 2021 8th International Conference on Computing for Sustainable Global Development, INDIACom 2021",2021-03-17,Conference Paper,"Lakra, Kirti;Chug, Anuradha",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85106656597,10.1109/SANER50967.2021.00033,Understanding and Facilitating the Co-Evolution of Production and Test Code,"Software products frequently evolve. When the production code undergoes major changes such as feature addition or removal, the corresponding test code typically should co-evolve. Otherwise, the outdated test may be ineffective in revealing faults or cause spurious test failures, which could confuse developers and waste QA resources. Despite its importance, maintaining such co-evolution can be time- and resource-consuming. Existing work has disclosed that, in practice, test code often fails to co-evolve with the production code. To facilitate the co-evolution of production and test code, this work explores how to automatically identify outdated tests. To gain insights into the problem, we conducted an empirical study on 975 open-source Java projects. By manually analyzing and comparing the positive cases, where the test code co-evolves with the production code, and the negative cases, where the co-evolution is not observed, we found that various factors (e.g., the different language constructs modified in the production code) can determine whether the test code should be updated. Guided by the empirical findings, we proposed a machine-learning based approach, SITAR, that holistically considers different factors to predict test changes. We evaluated SITAR on 20 popular Java projects. These results show that SITAR, under the within-project setting, can reach an average precision and recall of 81.4% and 76.1%, respectively, for identifying test code that requires update, which significantly outperforms rule-based baseline methods. SITAR can also achieve promising results under the cross-project setting and multiclass prediction, which predicts the exact change types of test code. © 2021 IEEE.",mining software repositories | Software evolution | test maintenance,"Proceedings - 2021 IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2021",2021-03-01,Conference Paper,"Wang, Sinan;Wen, Ming;Liu, Yepang;Wang, Ying;Wu, Rongxin",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85106642419,10.1109/SANER50967.2021.00052,Evaluating Bug Prediction under Realistic Settings,"Bug prediction is expected to reduce the cost of quality assurance. To build a reliable bug prediction model, we should use realistic settings that satisfy all three of the following conditions. (1) We should build a dataset in a way that allows us to evaluate the prediction performance of the model correctly. (2) We should adopt the optimal granularity of bug prediction to minimize the cost of quality assurance. (3) We should use a dependent variable that correctly represents the presence or absence of bugs in the software modules to be predicted. However, no research has been conducted on bug prediction models built under the above realistic settings. Consequently, we established the following two objectives in this research. (1) We experimentally evaluate the prediction performance of bug prediction models built under realistic settings. (2) We propose techniques to improve the prediction performance of bug prediction models built under realistic settings. The first objective has now been achieved. Our experimental results show that the F-Measure of the bug prediction models built under realistic settings is only 0.19. Thus, there are still some issues to be solved to build a high-performance bug prediction model under realistic settings. © 2021 IEEE.",bug prediction | machine learning | quality assurance,"Proceedings - 2021 IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2021",2021-03-01,Conference Paper,"Ogino, Sho;Higo, Yoshiki;Kusumoto, Shinji",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85114041889,10.1109/GHCI50508.2021.9513994,Implementation and Analysis of Supervised Learning methods for Bugs Classification,"Classification of bugs or logs is a vital aspect in the development of high-quality products. Early detection of errors, frequent patterns showing anomalies and, timely rectification of errors reduces the risk of developing faulty software. The aim of proposed system is to integrate a machine learning based intelligent layer to the existing Automation framework. The focus of this work is to classify the logs by extracting the underlying error messages in logs and hence ease the work of developers to save time invested in analyzing the logs. The data set includes logs collected from the Automation framework that were reported during automation runs in the last few months. This paper aims at finding the optimal algorithm to classify the bugs. In this experiment, we examine Naïve Bayes, Multilayer perceptron, CNN, and a hybrid model with a combination of CNN + Naïve Bayes Algorithm along with feature extraction techniques. It is observed from the experiment, that the Multilayer perceptron network has the highest accuracy of 91 percent. This experiment shows that our proposed system can classify logs effectively into different class of failure-types. © 2021 IEEE.",Classifier algorithms | feature extraction | Intelligent layer | Logs classification | neural networks | supervised learning | text classification,"2021 Grace Hopper Celebration India, GHCI 2021",2021-02-19,Conference Paper,"Katti, Bhagyashree M.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85104605956,10.1109/ICCCIS51004.2021.9397136,A detailed survey on machine intelligence based frameworks for software defect prediction,"Software defect prediction has an important role to play in improving the quality of programming and helps to reduce the time and cost of programming testing. AI focuses on the advancement of computer programs that can be instructed to develop and change at a time when new information is presented. The capacity of a machine to improve its exposure depends on past results. Machine learning improves the productivity of human learning, finds new things or structures that are obscured to people, and discovers important data in the archive. For this reason, distinctive machine learning procedures are used to remove unnecessary, incorrect information from the data set. Software defect prediction is seen as an exceptionally significant capability when a product project is arranged and a much larger effort is expected to address this intricate issue using product measurement and deformity dataset. Metrics are the link between the mathematical value and are subsequently applied to the product for anticipation of deformity. The essential objective of this study paper is to comprehend the existing strategies for foreseeing programming deformity. © 2021 IEEE.",Defect prediction | Machine learning | Software systems,"Proceedings - IEEE 2021 International Conference on Computing, Communication, and Intelligent Systems, ICCCIS 2021",2021-02-19,Conference Paper,"Singh, Raghvendra Omprkash;Thankachan, Blessy",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85062843955,10.1049/sfw2.12012,Correlation feature and instance weights transfer learning for cross project software defect prediction,"Due to the differentiation between training and testing data in the feature space, cross-project defect prediction (CPDP) remains unaddressed within the field of traditional machine learning. Recently, transfer learning has become a research hot-spot for building classifiers in the target domain using the data from the related source domains. To implement better CPDP models, recent studies focus on either feature transferring or instance transferring to weaken the impact of irrelevant cross-project data. Instead, this work proposes a dual weighting mechanism to aid the learning process, considering both feature transferring and instance transferring. In our method, a local data gravitation between source and target domains determines instance weight, while features that are highly correlated with the learning task, uncorrelated with other features and minimizing the difference between the domains are rewarded with a higher feature weight. Experiments on 25 real-world datasets indicate that the proposed approach outperforms the existing CPDP methods in most cases. By assigning weights based on the different contribution of features and instances to the predictor, the proposed approach is able to build a better CPDP model and demonstrates substantial improvements over the state-of-the-art CPDP models. © 2021 The Authors. IET Software published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.",,"2018 9th International Conference on Information, Intelligence, Systems and Applications, IISA 2018",2018-07-02,Conference Paper,"Khondoker, Farib;Rao, Sunil;Spanias, Andreas;Tepedelenlioglu, Cihan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85103858544,10.1109/Confluence51648.2021.9377083,A study on machine learning applied to software bug priority prediction,"Bugs are among the top problems faced by software developers. As the size and complexity of software projects increase so does the number of bugs and their complexity. Bug priority prediction helps software developers focus their efforts on the most critical bugs that affect the core functionality of a software. By automating the process of priority prediction, it is possible to reduce the time spent analyzing new bug reports. In this paper, we extract bug reports from the bug tracking software of six popular open-source projects Hadoop, HBase, HDFS, Mesos, Spark, and MapReduce and apply five machine learning classifiers Multinomial Naive Bayes, Decision Tree, Logistic Regression, Random Forest, AdaBoost to automatically predict bug priority using the title, description, and summary of the bug report. We use tf-idf to extract useful features from the bug reports and employ precision, recall, and F1-score for measuring the performance of the classifiers. A stratified 10-fold cross-validation technique is used for model evaluation and the results are averaged over all 10 folds. We find that machine learning applied to bug priority prediction provides excellent results and can be used to significantly reduce the time involved in the bug prioritization process. From our experiments, we observe that no single classifier consistently performs best on all priority levels and metrics across all datasets. However, trends from results show that Multinomial Naive Bayes gives well balanced performance and is also fast to train and test. Logistic Regression and AdaBoost also performed well and are potential alternatives. © 2021 IEEE",Bug Priority Prediction | Machine Learning | Mining Bug Repository | Software Maintenance | Text Classification,"Proceedings of the Confluence 2021: 11th International Conference on Cloud Computing, Data Science and Engineering",2021-01-28,Conference Paper,"Malhotra, Ruchika;Dabas, Ajay;Hariharasudhan, A. S.;Pant, Manish",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85101382262,10.1109/Confluence51648.2021.9377068,Support vector based oversampling technique for handling class imbalance in software defect prediction,"The importance of software defect prediction has risen significantly over the past decade and is an inseparable part of software quality. Owing to the drawbacks of traditional techniques used for software quality assurance, machine learning algorithms are used to detect the defect in software modules. By the means of this study, we present a support vector based oversampling technique as a part of the software defect procedure and compare it with two other oversampling techniques namely Synthetic minority oversampling technique and Adaptive Synthetic oversampling technique. For the purpose of this study, we chose 5 datasets from the PROMISE and AEEEM repositories. After extracting a subset of attributes from the original dataset through Linear discriminant Analysis, we utilize the improved oversampled dataset to train a support vector machine classifier and a Naïve Bayesian classifier. The proposed Support Vector based oversampling technique along with Linear Discriminant Analysis performs better than the other techniques for the performance evaluation metric of F-measure score and the area under receiver operating characteristic curve and the consistency of result is maintained. © 2021 IEEE",Bug Reports | Continuous Query | Duplicate Detection | Incremental Learning | Information Retrieval | Instance-based Learning | Natural Language Processing | Online Query,"2020 11th International Conference on Information and Knowledge Technology, IKT 2020",2020-12-22,Conference Paper,"Neysiani, Behzad Soleimani;Doostali, Saeed;Babamir, Seyed Morteza;Aminoroaya, Zahra",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85082695988,10.1109/CONISOFT52520.2021.00032,A Novel Software Fault Prediction Approach to Predict Error-type Proneness in the Java Programs Using Stream X-Machine and Machine Learning,"Software fault prediction makes software quality assurance process more efficient and economic. Most of the works related to software fault prediction have mainly focused on classifying software modules as faulty or not, which does not produce sufficient information for developers and testers. In this paper, we explore a novel approach using a streamlined process linking Stream X-Machine and machine learning techniques to predict if software modules are prone to having a particular type of runtime error in Java programs. In particular, Stream X-Machine is used to model and generate test cases for different types of Java runtime errors, which will be employed to extract error-type data from the source codes. This data is subsequently added to the collected software metrics to form new training data sets. We then explore the capabilities of three machine learning techniques (Support Vector Machine, Decision Tree, and Multi-layer Perceptron) for error-type proneness prediction. The experimental results showed that the new data sets could significantly improve the performances of machine learning models in terms of predicting error-type proneness. © 2021 IEEE.",Cross validation | Ensemble machining learning | RMSE | Wafer handling robot arm,"Proceedings of Technical Papers - International Microsystems, Packaging, Assembly, and Circuits Technology Conference, IMPACT",2019-10-01,Conference Paper,"Huang, Ping Wun;Chung, Kuan Jung",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85090567790,10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00232,A software defect detection algorithm based on asymmetric classification evaluation,"This paper proposes a software defect detection algorithm based on asymmetric classification evaluation, and applies the algorithm to a complete system. The system includes software data input interface, controller and detection result output port. The controller is used to detect the received software module, obtain the original software measurement data set, and calculate the number of the original software measurement data set. According to the data preprocessing, the data is divided into training samples and test samples. The training sample data is classified into asymmetric model. Discriminated and structured dictionary is used to evaluate the performance of asymmetric classifier. The detection is transferred to the test samples, the model is used to detect the defects of the software detection module, the evaluation results are fed back to the tester to complete the detection; and then through the detection results, If the detection result is provided to the user. The algorithm proposed in this paper can enhance the representation ability of dictionary and has good discrimination performance. At the same time, it can effectively solve the error caused by the imbalance of data and accurately locate the location of software defects. © 2021 IEEE.",Artificial intelligence | Clustering | Computer vision | Concept learning | Filtering | Machine learning,"2020 International Conference for Emerging Technology, INCET 2020",2020-06-01,Conference Paper,"Dhanalaxmi, B.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85124010066,10.1109/ICECIT54077.2021.9641218,SDP-ML: An Automated Approach of Software Defect Prediction employing Machine Learning Techniques,"Software Defect Prediction (SDP) method plays a vital role to ensure the software quality by predicting bugs in software development phase. In addition, this technique also assists developers to minimize the maintenance costs. In this paper, we proposed a model as SDP-ML that utilizes machine learning classification techniques to predict the faults in the software. In particular, the model uses three gradient boosting classification frameworks LightGBM (LGB), XGBoost (XGB), CatBoost (CB) for the prediction. The classification was performed on five publically available NASA Promise datasets, viz., CM1, JM1, PC1, KC1, KC2, and validated using ten-fold cross-validation techniques. Moreover, the remarkable evaluation techniques Precision, Recall, F1-score, and Accuracy were employed to evaluate the results. The outcomes demonstrated the dominant performance of LightGBM with GridSearchCV library (98%) than other algorithms considering the average F1-score. However, analysts can achieve free comprehension from this examination while choosing an automated research field for their planned application. © 2021 IEEE.",Defect prediction | Gradient boosting frameworks | NASA Promise dataset | Software quality | Supervised learning,"Proceedings of International Conference on Electronics, Communications and Information Technology, ICECIT 2021",2021-01-01,Conference Paper,"Uddin, Md Nasir;Li, Bixin;Mondol, Md Naim;Rahman, Md Mostafizur;Mia, Md Suman;Mondol, Elizabeth Lisa",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85123502553,10.1109/DSA52907.2021.00012,An Empirical Study on Spectral Clustering-based Software Defect Detection,"Software defect detection is essential in software development. Most existing approaches often apply Supervised Machine Learning (SML) techniques for software defect detection. However, SML techniques need to a large number of manual labelling for model training, which is time-consuming and laborious. An alternative solution is to apply UnSupervised Machine Learning (USML) in software defect detection. USML techniques, as an approach without requiring labeled datasets, have been applied for software defect detection. Spectral clustering, as one of approaches in USML, shows the potential performance in software defect detection. The core of spectral clustering is the similarity algorithms, which calculate the similarity between metric values of software entities to detect software defects. Yet, the current studies on spectral clustering-based software defect detection models rarely consider the impact of different similarity algorithms on defect detection results.To address this problem, we construct an empirical study to investigate the impact of similarity algorithms in the spectral clustering-based software defect detection models. We compare the differences of three similarity algorithms, which contains k-nearest neighbours, fully connected, and vector dot product. We conduct experiments on the two real-world data sets of AEEEM and PROMISE, and the experimental results show the fully connected algorithm has better performance than other algorithms in the spectral clustering-based software defect detection. © 2021 IEEE.",similarity algorithms | software defect detection | spectral clustering,"Proceedings - 2021 8th International Conference on Dependable Systems and Their Applications, DSA 2021",2021-01-01,Conference Paper,"Qing, Mingshuang;Ge, Xiuting;Hui, Zhan Wei;Pan, Ya;Fan, Yong;Wang, Xiaojuan;Cao, Xu",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85123492277,10.1109/DSA52907.2021.00055,A Domain Knowledge-Guided Lightweight Approach for Security Bug Reports Prediction,"Security bug report (SBR) prediction has been increasingly investigated for eliminating security attack risks of software products. However, there is still much room for improving the performance of automatic SBR prediction. This work is inspired by the work of two recent studies proposed by Peters et al. and Wu et al., which are focused on SBR prediction and both published on the top tier journal TSE (Transactions on Software Engineering). The goal of this work is to improve the effectiveness of supervised machine learning-based SBR prediction with the help of software security domain knowledge. It first extracts software security domain knowledge from CWE (Common Weakness Enumeration) and CVE (Common Vulnerabilities Exposure), which are authoritative sources of software vulnerability. After that, the matrix of bug reports is generated based on the roots of security domain keywords. Large-scale experiments are conducted on a set of trustworthy datasets cleaned by Wu et al. The results show our domain knowledge-guided approach could improve the effectiveness of SBR prediction by 25% in terms of F1-score on average. © 2021 IEEE.",bug report prediction | domain knowledge | keyBERT | keywords root | software security,"Proceedings - 2021 8th International Conference on Dependable Systems and Their Applications, DSA 2021",2021-01-01,Conference Paper,"Zheng, Wei;Chen, Zheng;Wu, Xiaoxue;Fu, Weiqiang;Sun, Bowen;Cheng, Jingyuan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85123486853,10.1109/DSA52907.2021.00025,Use of Deep Learning Model with Attention Mechanism for Software Fault Prediction,"Software defect prediction is a skill in software engineering that can increase program reliability. In the past, most defect prediction studies have been based on size and complexity metrics. In recent years, machine learning based predictive studies have been conducted. To build an accurate prediction model, choosing effective features remains critical. In this paper, we constructed a deep learning model called Defect Prediction via Self-Attention mechanism (DPSAM) to extract semantic features and predict defects automatically. We transferred programs into abstract syntax trees (ASTs) and encoded them into token vectors. With input features, we trained a self-attention mechanism to extract semantic features of programs and predict defects. We evaluated performance on 7 open source projects. In Within-Project Defect Prediction (WPDP), DPSAM achieved 16.8% and 14.4% performance improvement compared to state-of-the-art deep belief network (DBN)-based method and defect prediction via convolutional neural network (DP-CNN)-based method in F1 score, respectively. Besides, in Cross-Project Defect Prediction (CPDP), DPSAM achieve 23% and 60% performance improvement in F1 score compared to DBN-based method and DP-CNN-based method. © 2021 IEEE.",Attention mechanism | Convolutional Neural Network | Deep learning | Defect prediction | Self-Attention mechanism | Software engineering,"Proceedings - 2021 8th International Conference on Dependable Systems and Their Applications, DSA 2021",2021-01-01,Conference Paper,"Yu, Ting Yan;Huang, Chin Yu;Fang, Neil C.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85123364193,10.1109/ICSME52107.2021.00084,Sine-Cosine Algorithm for Software Fault Prediction,"For developing an efficient and quality Software Fault Prediction (SFP) model, redundant and irrelevant features need to be removed. This task can be achieved, to a significant extent, with Feature Selection (FS) methods. Many empirical studies have been proposed on FS methods (Filter and Wrapper-based) and have shown effective results in reducing the problem of high dimensionality in metrics-based SFP models. This study evaluates the performance of novel wrapper-based Sine Cosine Algorithm (SCA) on five datasets of the AEEEM repository and compares the results with two metaheuristic techniques Genetic Algorithm (GA) and Cuckoo Search algorithm (CSA) on four different Machine Learning (ML) classifiers-Random Forest (RF), Support Vector Machine (SVM), Naïve Bayes (NB), and K-Nearest Neighbor (KNN). We found that the application of FS methods (SCA, GA CSA) has improved the classifier performance. SCA has proved to be more efficient than GA and CSA in terms of lesser convergence time with the smallest subset of selected features and equivalent performance. © 2021 IEEE.",feature selection | metaheuristic techniques | sine cosine algorithm | software fault prediction,"Proceedings - 2021 IEEE International Conference on Software Maintenance and Evolution, ICSME 2021",2021-01-01,Conference Paper,"Sharma, Tamanna;Sangwan, Om Prakash",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85015320263,10.1109/ICSME52107.2021.00053,DeepOrder: Deep Learning for Test Case Prioritization in Continuous Integration Testing,"Continuous integration testing is an important step in the modern software engineering life cycle. Test prioritization is a method that can improve the efficiency of continuous integration testing by selecting test cases that can detect faults in the early stage of each cycle. As continuous integration testing produces voluminous test execution data, test history is a commonly used artifact in test prioritization. However, existing test prioritization techniques for continuous integration either cannot handle large test history or are optimized for using a limited number of historical test cycles. We show that such a limitation can decrease fault detection effectiveness of prioritized test suites. This work introduces DeepOrder, a deep learning-based model that works on the basis of regression machine learning. DeepOrder ranks test cases based on the historical record of test executions from any number of previous test cycles. DeepOrder learns failed test cases based on multiple factors including the duration and execution status of test cases. We experimentally show that deep neural networks, as a simple regression model, can be efficiently used for test case prioritization in continuous integration testing. DeepOrder is evaluated with respect to time-effectiveness and fault detection effectiveness in comparison with an industry practice and the state of the art approaches. The results show that DeepOrder outperforms the industry practice and state-of-the-art test prioritization approaches in terms of these two metrics. © 2021 IEEE.",Operating System | Self-healing | Self-management Component,"2016 6th International Conference on Innovative Computing Technology, INTECH 2016",2017-02-06,Conference Paper,"Ahmad, Maqsood;Samiullah, Muhammad;Pirzada, Muhammad Jawad;Fahad, Muhammad",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85073709663,10.1109/NAS51552.2021.9605431,MLPP: Exploring Transfer Learning and Model Distillation for Predicting Application Performance,"Performance prediction for applications is quintessential towards detecting malicious hardware and software vulnerabilities. Typically application performance is predicted using the profiling data generated from hardware tools such as linux perf. By leveraging the data, prediction models, both machine learning (ML) based and non ML-based have been proposed. However a majority of these models suffer from either loss in prediction accuracy, very large model sizes, and/or lack of general applicability to different hardware types such as wearables, handhelds, desktops etc. To address the aforementioned inefficiencies, in this paper we proposed MLPP, a machine learning based performance prediction model which can accurately predict application performance, and at the same time be easily transferable to a wide both mobile and desktop hardware platforms by leveraging transfer learning technique. Furthermore, MLPP incorporates model distillation techniques to significantly reduce the model size. Through our extensive experimentation and evaluation we show that MLPP can achieve up to 92.5% prediction accuracy while reducing the model size by up to 3.5 ×. © 2021 IEEE.",CART | Fault Injection | k-NN | Linear Least Squares | Machine Learning | Ridge Regression | Single-Event Effects | Support Vector Regression | Transient Faults,"2019 IEEE 25th International Symposium on On-Line Testing and Robust System Design, IOLTS 2019",2019-07-01,Conference Paper,"Lange, Thomas;Balakrishnan, Aneesh;Glorieux, Maximilien;Alexandrescu, Dan;Sterpone, Luca",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85088903267,10.1109/RAMS48097.2021.9605797,Syndrome Diagnostics - Fault Detection and Isolation for Complex Systems using Causation-based AI,"Syndrome Diagnostics (SD) is a real-time Failure Detection and Isolation software tool for complex, safety & mission-critical engineering systems. Utilizing causal-based AI, SD identifies patterns (syndromes) of failures and autonomously associates them with the root cause based on diagnostics (sensor data) and analytics (metadata from external sources).Downtime for complex and capital-intensive equipment from unexpected failures has significant cost safety, and market risks; even more so for design, who must meet stringent safety & certification requirements. In the post COVID world, organizations must also transition to technology that supports distributed operations - increased demand for remote monitoring is an economically-constrained environment.To compensate these impacts, organizations are looking to implement Predictive Maintenance (PdM) to achieve optimal timing of maintenance maximizing availability and components' useful life, with the least chance of downtime. This requires that monitoring happens in real time to 'catch failures before they happen'.PdM nowadays rely heavily on observed physical symptoms using pure data-driven solutions, however this leads to issues such as spurious correlation which presumes incorrect relationships. These errors in a safety critical system are unacceptable and may result in loss of life in addition to those mentioned previously. We present in this paper, Syndrome Diagnostics, which is a fundamentally different approach to FDI - rather than establishing patterns of systems responses based on correlation that is then manually confirmed by a human specialists, a causation-based strategy is used.Causation is derived from a risk model that builds a qualitative representation of the system, trading parametric simulations with straightforward failure associations. The AI aspect uses machine learning methods to detect failures. Unsupervised algorithms trigger more precise supervised techniques to label (state relative to threshold) each sensor reading within a snapshot. The risk model is then leveraged to relate each sensor label to the failure mode that led to those patterns. © 2021 IEEE.",HPC | Node Failures | Online Prediction | Parsing,"Proceedings - 2020 IEEE 34th International Parallel and Distributed Processing Symposium, IPDPS 2020",2020-05-01,Conference Paper,"Das, Anwesha;Mueller, Frank;Rountree, Barry",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85100941169,10.1109/ICON-SONICS53103.2021.9616999,Handling High-Dimensionality on Software Defect Prediction with FLDA,"For years, Software Defect Prediction (SDP) has been used as a way to improve software reliability. It is used as a tool to detect the defects on software module before the testing phase. The steps consist of building machine learning model by training dataset with classifier and predict on defective modules. Many datasets suffer from high dimensionality because of the high number of features and those features are mostly irrelevant to predict defect. This results the data to be unnecessarily bulky and the classification process to be time-consuming. We propose a feature extraction technique called FLDA for handling dimensionality problem of dataset and improving the classification performance. We use a total of four dataset from NASA MDP. For classifiers, we use Support Vector Machine (SVM), Random Forest (RF), Naive Bayes (NF), and Multi Layer Perceptron (MLP). Based on the study results, FLDA can significantly reduced the dimension of datasets by creating new feature that contains the most relevant information. FLDA can also shorten the processing time of the classifiers. When compared to another feature extraction technique such as PCA, FLDA can easily outperform it in terms of Recall and AUC. © 2021 IEEE.",Codelet model | Compiler | Darts | Dataflow model | Exa-scale | Machine learning | Many-core architecture | Program execution model | Software-hardware co-design,"Proceedings of IPDRM 2020: 4th Annual Workshop on Emerging Parallel and Distributed Runtime Systems and Middleware, Held in conjunction with SC 2020: The International Conference for High Performance Computing, Networking, Storage and Analysis",2020-11-01,Conference Paper,"Kabrick, Ryan;Perdomo, Diego A.Roa;Raskar, Siddhisanket;Diaz, Jose M.Monsalve;Fox, Dawson;Gao, Guang R.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85122765350,10.37190/e-Inf210108,Multi-view learning for software defect prediction,"Background: Traditionally, machine learning algorithms have been simply applied for software defect prediction by considering single-view data, meaning the input data contains a single feature vector. Nevertheless, different software engineering data sources may include multiple and partially independent information, which makes the standard single-view approaches ineffective. Objective: In order to overcome the single-view limitation in the current studies, this article proposes the usage of a multi-view learning method for software defect classification problems. Method: The Multi-View k-Nearest Neighbors (MVKNN) method was used in the software engineering field. In this method, first, base classifiers are constructed to learn from each view, and then classifiers are combined to create a robust multi-view model. Results: In the experimental studies, our algorithm (MVKNN) is compared with the standard k-nearest neighbors (KNN) algorithm on 50 datasets obtained from different software bug repositories. The experimental results demonstrate that the MVKNN method outperformed KNN on most of the datasets in terms of accuracy. The average accuracy values of MVKNN are 86.59%, 88.09%, and 83.10% for the NASA MDP, Softlab, and OSSP datasets, respectively. Conclusion: The results show that using multiple views (MVKNN) can usually improve classification accuracy compared to a single-view strategy (KNN) for software defect prediction. © 2021 Wroclaw University of Science and Technology. All rights reserved.",K-nearest neighbors | Machine learning | Multi-view learning | Software defect prediction,E-Informatica Software Engineering Journal,2021-01-01,Article,"Kiyak, Elife Ozturk;Birant, Derya;Birant, Kokten Ulas",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85122596299,10.1007/978-3-030-92635-9_25,An Empirical Study of Model-Agnostic Interpretation Technique for Just-in-Time Software Defect Prediction,"Just-in-time software defect prediction (JIT-SDP) is an effective method of software quality assurance, whose objective is to use machine learning methods to identify defective code changes. However, the existing research only focuses on the predictive power of the JIT-SDP model and ignores the interpretability of the model. The need for the interpretability of the JIT-SDP model mainly comes from two reasons: (1) developers expect to understand the decision-making process of the JIT-SDP model and obtain guidance and insights; (2) the prediction results of the JIT-SDP model will have an impact on the interests of developers. According to privacy protection laws, prediction models need to provide explanations. To this end, we introduced three classifier-agnostic (CA) technologies, LIME, BreakDown, and SHAP for JIT-SDP models, and conducted a large-scale empirical study on six open source projects. The empirical results show that: (1) Different instances have different explanations. On average, the feature ranking difference of two random instances is 3; (2) For a given system, the feature lists and top-1 feature generated by different CA technologies have strong agreement; However, CA technologies have small agreement on the top-3 features in the feature ranking lists. In the actual software development process, we suggest using CA technologies to help developers understand the prediction results of the model. © 2021, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.",Classifier-agnostic interpretation | Just-in-time | Model interpretation | Software defect prediction,"Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST",2021-01-01,Conference Paper,"Yang, Xingguang;Yu, Huiqun;Fan, Guisheng;Huang, Zijie;Yang, Kang;Zhou, Ziyi",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85121231553,10.7717/peerj-cs.739,Software defect prediction using hybrid model (CBIL) of convolutional neural network (CNN) and bidirectional long short-term memory (Bi-LSTM),"In recent years, the software industry has invested substantial effort to improve software quality in organizations. Applying proactive software defect prediction will help developers and white box testers to find the defects earlier, and this will reduce the time and effort. Traditional software defect prediction models concentrate on traditional features of source code including code complexity, lines of code, etc. However, these features fail to extract the semantics of source code. In this research, we propose a hybrid model that is called CBIL. CBIL can predict the defective areas of source code. It extracts Abstract Syntax Tree (AST) tokens as vectors from source code. Mapping and word embedding turn integer vectors into dense vectors. Then, Convolutional Neural Network (CNN) extracts the semantics of AST tokens. After that, Bidirectional Long Short-Term Memory (Bi-LSTM) keeps key features and ignores other features in order to enhance the accuracy of software defect prediction. The proposed model CBIL is evaluated on a sample of seven open-source Java projects of the PROMISE dataset. CBIL is evaluated by applying the following evaluation metrics: F -measure and area under the curve (AUC). The results display that CBIL model improves the average of F -measure by 25% compared to CNN, as CNN accomplishes the top performance among the selected baseline models. In average of AUC, CBIL model improves AUC by 18% compared to Recurrent Neural Network (RNN), as RNN accomplishes the top performance among the selected baseline models used in the experiments. Subjects Artificial Intelligence, Software Engineering © 2021 Farid et al. All Rights Reserved.",Abstract syntax tree | Bidirectional long short-term memory | Convolutional neural network | Deep learning | Defect | Machine learning | Software defect prediction,PeerJ Computer Science,2021-01-01,Article,"Farid, Ahmed Bahaa;Fathy, Enas Mohamed;Eldin, Ahmed Sharaf;Abd-Elmegid, Laila A.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85120531007,10.1007/978-3-030-91265-9_11,Predicting and Monitoring Bug-Proneness at the Feature Level,"Enabling quick feature modification and delivery is important for a project’s success. Obtaining early estimates of software features’ bug-proneness is helpful for effectively allocating resources to the bug-prone features requiring further fixes. Researchers have proposed various studies on bug prediction at different granularity levels, such as class level, package level, method level, etc. However, there exists little work building predictive models at the feature level. In this paper, we investigated how to predict bug-prone features and monitor their evolution. More specifically, we first identified a project’s features and their involved files. Next, we collected a suite of code metrics and selected a relevant set of metrics as attributes to be used for six machine learning algorithms to predict bug-prone features. Through our evaluation, we have presented that using the machine learning algorithms with an appropriate set of code metrics, we can build effective models of bug prediction at the feature level. Furthermore, we build regression models to monitor growth trends of bug-prone features, which shows how these features accumulate bug-proneness over time. © 2021, Springer Nature Switzerland AG.",Code metrics | Feature bug prediction | Machine learning,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021-01-01,Conference Paper,"Wei, Shaozhi;Mo, Ran;Xiong, Pu;Zhang, Siyuan;Zhao, Yang;Li, Zengyang",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84937539294,10.1155/2021/2323100,Deep Learning Software Defect Prediction Methods for Cloud Environments Research,"This paper provides an in-depth study and analysis of software defect prediction methods in a cloud environment and uses a deep learning approach to justify software prediction. A cost penalty term is added to the supervised part of the deep ladder network; that is, the misclassification cost of different classes is added to the model. A cost-sensitive deep ladder network-based software defect prediction model is proposed, which effectively mitigates the negative impact of the class imbalance problem on defect prediction. To address the problem of lack or insufficiency of historical data from the same project, a flow learning-based geodesic cross-project software defect prediction method is proposed. Drawing on data information from other projects, a migration learning approach was used to embed the source and target datasets into a Gaussian manifold. The kernel encapsulates the incremental changes between the differences and commonalities between the two domains. To this point, the subspace is the space of two distributional approximations formed by the source and target data transformations, with traditional in-project software defect classifiers used to predict labels. It is found that real-time defect prediction is more practical because it has a smaller amount of code to review; only individual changes need to be reviewed rather than entire files or packages while making it easier for developers to assign fixes to defects. More importantly, this paper combines deep belief network techniques with real-time defect prediction at a fine-grained level and TCA techniques to deal with data imbalance and proposes an improved deep belief network approach for real-time defect prediction, while trying to change the machine learning classifier underlying DBN for different experimental studies, and the results not only validate the effectiveness of using TCA techniques to solve the data imbalance problem but also show that the defect prediction model learned by the improved method in this paper has better prediction performance. © 2021 Wenjian Liu et al.",Business Intelligence | Geo-Tracking | Pattern Analysis | Text Mining | Web mining,"Proceedings - 2014 2nd International Symposium on Computational and Business Intelligence, ISCBI 2014",2014-01-01,Conference Paper,"Vijaya Kamal, M.;Vasumathi, D.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85119830618,10.1007/978-3-030-90785-3_16,Research on Cross-Project Software Defect Prediction Based on Machine Learning,"In recent years, machine learning technology has developed vigorously. The research on software defect prediction in the field of software engineering is increasingly adopting various algorithms of machine learning. This article has carried out a systematic literature review on the field of defect prediction. First, this article studies the development process of defect prediction, from correlation to prediction model. then this article studies the development process of cross-project defect prediction based on machine learning algorithms (naive Bayes, decision tree, random forest, neural network, etc.). Finally, this paper looks forward to the research difficulties and future directions of software defect prediction, such as imbalance in classification, cost of data labeling, and cross-project data distribution. © 2021, Springer Nature Switzerland AG.",Machine learning | Metric | Software defect prediction model,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021-01-01,Conference Paper,"Wang, Baoping;Wang, Wennan;Zhu, Linkai;Liu, Wenjian",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85119204465,10.1504/IJCSM.2021.118798,Predicting the amount of files required to fix a bug,This paper proposes a classifier that can predict the amount of files required to fix a bug. A newly incoming bug can be classified into one of the three classes (categories): Small; Medium; or Large depending on the amount of files required to fix that bug. For this purpose; 5800 bug reports are studied from three open source projects. The projects are: AspectJ; Tomcat; and SWT. Then; feature sets are extracted for each project separately. The feature sets represent the occurrences of keywords in the summary and description parts of the bug reports. Due to the high dimensionality of the feature vectors; we propose to apply the well-known method; principle component analysis (PCA). The resulting feature vectors are then fed to a number of popular machine learning algorithms. For an enhanced performance; we experiment with multiclass support vector machine quadratic MSVM2. It provides improvements of classification accuracy ranging from 2.3% to 22.3% compared to other classifiers. Copyright © 2021 Inderscience Enterprises Ltd.,Adaboost | Bug reports | Bug tracking systems | Dimensionality reduction | Effort prediction | Machine learning | MSVM 2 | PCA | Principle component analysis | Project management | Software maintenance,International Journal of Computing Science and Mathematics,2021-01-01,Article,"Otoom, Ahmed Fawzi;Hammad, Maen;Al-Jdaeh, Sara;Awwad, Sari;Idwan, Sahar",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84874375128,10.14569/IJACSA.2021.0120884,Comprehensive Study on Machine Learning Techniques for Software Bug Prediction,"Software bugs are defects or faults in computer programs or systems that cause incorrect or unexpected operations. These negatively affect software quality, reliability, and maintenance cost; therefore many researchers have already built and developed several models for software bug prediction. Till now, a few works have been done which used machine learning techniques for software bug prediction. The aim of this paper is to present comprehensive study on machine learning techniques that were successfully used to predict software bug. Paper also presents a software bug prediction model based on supervised machine learning algorithms are Decision Tree (DT), Naïve Bayes (NB), Random Forest (RF) and Logistic Regression (LR) on four datasets. We compared the results of our proposed models with those of the other studies. The results of this study demonstrated that our proposed models performed better than other models that used the same data sets. The evaluation process and the results of the study show that machine learning algorithms can be used effectively for prediction of bugs. © 2021. International Journal of Advanced Computer Science and Applications. All Rights Reserved.",10-fold Cross Validation | Bug priority | Bug repositories | Classifiers | KNN | Naive Bayes | Neural Net | SVM | Triager,"International Conference on Intelligent Systems Design and Applications, ISDA",2012-12-01,Conference Paper,"Sharma, Meera;Bedi, Punam;Chaturvedi, K. K.;Singh, V. B.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85118975718,10.1007/978-3-030-81462-5_29,An Ensemble Learning Approach for Software Defect Prediction in Developing Quality Software Product,"Software Defect Prediction (SDP) is a major research field in the software development life cycle. The accurate SDP would assist software developers and engineers in developing a reliable software product. Several machine learning techniques for SDP have been reported in the literature. Most of these studies suffered in terms of prediction accuracy and other performance metrics. Many of these studies focus only on accuracy and this is not enough in measuring the performance of SDP. In this research, we propose a seven-ensemble machine learning model for SDP. The Cat boost, Light Gradient Boosting Machine (LGBM), Extreme Gradient Boosting (XgBoost), boosted cat boost, bagged logistic regression, boosted LGBM, and boosted XgBoost were used for the experimental analysis. We also used the separate individual base model of logistic regression for the analysis on six datasets. This paper extends the performance metrics from only the accuracy, the Area Under Curve (AUC), precision, recall, F-measure, and Matthew Correlation Coefficient (MCC) were used as performance metrics. The results obtained showed that the proposed ensemble Cat boost model gave an outstanding performance for all the three defects datasets as a result of being able to decrease overfitting and reduce the training time. © 2021, Springer Nature Switzerland AG.",Area under curve | Cat boost | Extreme gradient boosting | F-measure | Light gradient boosting machine | NASA repository | Software defect prediction | Software product,Communications in Computer and Information Science,2021-01-01,Conference Paper,"Saheed, Yakub Kayode;Longe, Olumide;Baba, Usman Ahmad;Rakshit, Sandip;Vajjhala, Narasimha Rao",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85118339820,10.4114/intartif.vol24iss68pp72-88,The effect of the dataset size on the accuracy of software defect prediction models: An empirical study,"The ongoing development of computer systems requires massive software projects. Running the components of these huge projects for testing purposes might be a costly process; therefore, parameter estimation can be used instead. Software defect prediction models are crucial for software quality assurance. This study investigates the impact of dataset size and feature selection algorithms on software defect prediction models. We use two approaches to build software defect prediction models: a statistical approach and a machine learning approach with support vector machines (SVMs). The fault prediction model was built based on four datasets of different sizes. Additionally, four feature selection algorithms were used. We found that applying the SVM defect prediction model on datasets with a reduced number of measures as features may enhance the accuracy of the fault prediction model. Also, it directs the test effort to maintain the most influential set of metrics. We also found that the running time of the SVM fault prediction model is not consistent with dataset size. Therefore, having fewer metrics does not guarantee a shorter execution time. From the experiments, we found that dataset size has a direct influence on the SVM fault prediction model. However, reduced datasets performed the same or slightly lower than the original datasets. © 2021, Asociacion Espanola de Inteligencia Artificial. All rights reserved.",Feature Selection | Software Defect Prediction | Support Vector Machine,Inteligencia Artificial,2021-01-01,Article,"Alshammari, Mashaan A.;Alshayeb, Mohammad",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85116070667,10.1007/978-3-030-87007-2_26,Assessing Ensemble Learning Techniques in Bug Prediction,"The application of ensemble learning techniques is continuously increasing, since they have proven to be superior over traditional machine learning techniques in various domains. These algorithms could be employed for bug prediction purposes as well. Existing studies investigated the performance of ensemble learning techniques only for PROMISE and the NASA MDP public datasets; however, it is important to evaluate the ensemble learning techniques on additional public datasets in order to test the generalizability of the techniques. We investigated the performance of the two most widely-used ensemble learning techniques AdaBoost and Bagging on the Unified Bug Dataset, which encapsulates 3 class level public bug datasets in a uniformed format with a common set of software product metrics used as predictors. Additionally, we investigated the effect of using 3 different resampling techniques on the dataset. Finally, we studied the performance of using Decision Tree and Naïve Bayes as the weak learners in the ensemble learning. We also fine tuned the parameters of the weak learners to have the best possible end results. We experienced that AdaBoost with Decision Tree weak learner outperformed other configurations. We could achieve 54.61% F-measure value (81.96% Accuracy, 50.92% Precision, 58.90% Recall) with the configuration of 300 estimators and 0.05 learning rate. Based on the needs, one can apply RUS resampling to get a recall value up to 75.14% (of course losing precision at the same time). © 2021, Springer Nature Switzerland AG.",AdaBoost | Bug prediction | Resampling | Unified bug dataset,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021-01-01,Conference Paper,"Szamosvölgyi, Zsolt János;Váradi, Endre Tamás;Tóth, Zoltán;Jász, Judit;Ferenc, Rudolf",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85116014291,10.1007/978-3-030-87007-2_28,Deep-Learning Approach with DeepXplore for Software Defect Severity Level Prediction,"Fixing the defects of earlier releases and working on fast and efficient fixing of those software defects is detrimental for the release of further versions. Bug tracking systems like Bugzilla get thousands of software defect reports every day. Manually handling those report to assign severity to the defects is not feasible. Earlier traditional Machine Learning methods have been used to predict the severity level from the defect description. This paper presents different deep learning models to predict defect severity level. Furthermore, the deep neural network was tested using a framework developed similar to that DeepXplore. Different word-embedding techniques, feature-selection techniques, sampling techniques and deep learning models are analyzed and compared for this study. In this paper, we have considered Descriptive statistics, Box-plot, and Significant tests to compare the developed models for defect severity level prediction. The three performance metrics used for testing the models are AUC, Accuracy and Neuron Coverage. This is a preliminary study on DNN testing on this dataset. Thus, the paper focuses on DeepXplore DNN testing technique. However further studies would be undertaken on comparative analysis of different DNN testing techniques on this dataset. © 2021, Springer Nature Switzerland AG.",Deep learning | Feature selection | Imbalance handling | Severity prediction | Word embedding,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021-01-01,Conference Paper,"Kumar, Lov;Dastidar, Triyasha Ghosh;Murthy Neti, Lalitha Bhanu;Satapathy, Shashank Mouli;Misra, Sanjay;Kocher, Vipul;Padmanabhuni, Srinivas",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85115754111,10.1007/s40998-021-00458-1,An Effective Model to Predict the Extension of Code Changes in Bug Fixing Process Using Text Classifiers,"After the issue tracking system (ITS) receives a bug report, this report is analyzed and triaged. To automate bug handling, past researches focused on topics such as automatic detection of duplication, prediction of fixing time, and automatic fault localization, to prioritize tasks, manage resources better and allocate resources more efficiently. However, predicting the amount and type of the changes necessary to fix a reported bug in the code, as soon as the bug report is received and before it is assigned to a programmer, has been neglected in previous researches. It seems that this prediction can be applied in different fields like bug fixing time, levels of integration test, etc. In this work, a model for predicting the amount of software code changes in the bug fixing process is presented. To achieve this, the problem is modeled as a text classification problem and solved using ensemble classifiers. The applicability of the proposed model is justified by statistical analysis. Our study shows that there is a significant relationship between the amount of changes in the software code and bug fix time. An empirical study by analyzing code changes in 15 projects conducted and tested with bagging-based and voting-based classifiers with different basic classifiers. The accuracy of the best model was measured approximately 72% on average. © 2021, Shiraz University.",Bug fixing | Change level | Ensemble | Machine learning,Iranian Journal of Science and Technology - Transactions of Electrical Engineering,2022-03-01,Article,"Sepahvand, Reza;Akbari, Reza;Hashemi, Sattar;Boushehrian, Omid",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85115346610,10.1504/IJCSM.2021.117600,Classifying defective software projects based on machine learning and complexity metrics,"Software defects can lead to software failures or errors at any time. Therefore, software developers and engineers spend a lot of time and effort in order to find possible defects. This paper proposes an automatic approach to predict software defects based on machine learning algorithms. A set of complexity measures values are used to train the classifier. Three public datasets were used to evaluate the ability of mining complexity measures for different software projects to predict possible defects. Experimental results showed that it is possible to min software complexity to build a defect prediction model with a high accuracy rate. © 2021 Inderscience Enterprises Ltd.",Complexity | Decision trees | Defect prediction | Machine learning | Naïve Bayes | Neural networks | Software defects | Software metrics | Support vector machine | SVM,International Journal of Computing Science and Mathematics,2021-01-01,Article,"Hammad, Mustafa",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85114616962,10.1155/2021/5558561,Research on Cross-Company Defect Prediction Method to Improve Software Security,"As the scale and complexity of software increase, software security issues have become the focus of society. Software defect prediction (SDP) is an important means to assist developers in discovering and repairing potential defects that may endanger software security in advance and improving software security and reliability. Currently, cross-project defect prediction (CPDP) and cross-company defect prediction (CCDP) are widely studied to improve the defect prediction performance, but there are still problems such as inconsistent metrics and large differences in data distribution between source and target projects. Therefore, a new CCDP method based on metric matching and sample weight setting is proposed in this study. First, a clustering-based metric matching method is proposed. The multigranularity metric feature vector is extracted to unify the metric dimension while maximally retaining the information contained in the metrics. Then use metric clustering to eliminate metric redundancy and extract representative metrics through principal component analysis (PCA) to support one-to-one metric matching. This strategy not only solves the metric inconsistent and redundancy problem but also transforms the cross-company heterogeneous defect prediction problem into a homogeneous problem. Second, a sample weight setting method is proposed to transform the source data distribution. Wherein the statistical source sample frequency information is set as an impact factor to increase the weight of source samples that are more similar to the target samples, which improves the data distribution similarity between the source and target projects, thereby building a more accurate prediction model. Finally, after the above two-step processing, some classical machine learning methods are applied to build the prediction model, and 12 project datasets in NASA and PROMISE are used for performance comparison. Experimental results prove that the proposed method has superior prediction performance over other mainstream CCDP methods. © 2021 Yanli Shao et al.",,Security and Communication Networks,2021-01-01,Article,"Shao, Yanli;Zhao, Jingru;Wang, Xingqi;Wu, Weiwei;Fang, Jinglong",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85113902811,10.1007/s00500-021-06112-6,Genetic algorithm-based oversampling approach to prune the class imbalance issue in software defect prediction,"Class imbalance is the potential problem that has been existent in machine learning, which hinders the performance of the classification algorithm when applied in real-world applications such as electricity pilferage, fraudulent transactions, anomaly detection, and prediction of rare diseases. Class imbalance refers to the problem where the distribution of the sample is skewed or biased toward one particular class. Due to its intrinsic nature the software fault prediction dataset falls into the same category where the software modules contain fewer defective modules compared to the non-defective modules. The majority of the oversampling techniques that has been proposed is to address the issue by generating synthetic samples of minority class to balance the dataset. But the synthetic samples generated are near duplicates that also results in over-generalization issue. We thus propose a novel oversampling approach to introduce synthetic samples using genetic algorithm (GA). GA is a form of evolutionary algorithm that employs biologically inspired techniques such as inheritance, mutation, selection, and crossover. The proposed algorithm generates synthetic sample of minority class based on the distribution measure and ensures that the samples are diverse within the class and are efficient. The proposed oversampling algorithm has been compared with SMOTE, BSMOTE, ADASYN, random oversampling, MAHAKIL, and no sampling approach with 20 defect prediction datasets from the promise repository and five prediction models. The results indicate that the genetic algorithm oversampling approach improves the fault prediction performance and reduced false alarm rate. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Class imbalance | Evolutionary algorithm | False alarm rate | Generating samples of minority class | Genetic algorithm | Oversampling techniques | Software fault prediction | Synthetic samples,Soft Computing,2022-12-01,Article,"Arun, C.;Lakshmi, C.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85112728681,10.1007/978-3-030-79463-7_35,Continuous Build Outcome Prediction: A Small-N Experiment in Settings of a Real Software Project,"We explain the idea of Continuous Build Outcome Prediction (CBOP) practice that uses classification to label the possible build results (success or failure) based on historical data and metrics (features) derived from the software repository. Additionally, we present a preliminary empirical evaluation of CBOP in a real live software project. In a small-n repeated-measure with two conditions and replicates experiment, we study whether CBOP will reduce the Failed Build Ratio (FBR). Surprisingly, the result of the study indicates a slight increase in FBR while using the CBOP, although the effect size is very small. A plausible explanation of the revealed phenomenon may come from the authority principle, which is rarely discussed in the software engineering context in general, and AI-supported software development practices in particular. © 2021, Springer Nature Switzerland AG.",Agile experimentation | Continuous integration | Machine learning | Software defect prediction,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021-01-01,Conference Paper,"Kawalerowicz, Marcin;Madeyski, Lech",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85112711831,10.1007/978-3-030-79463-7_36,Jaskier: A Supporting Software Tool for Continuous Build Outcome Prediction Practice,"Continuous Defect Prediction (CDP) is an assisting software development practice that combines Software Defect Prediction (SDP) with machine learning aided modelling and continuous developer feedback. Jaskier is a set of software tools developed under the supervision and with the participation of the authors of the article that implements a lightweight version of CDP called Continuous Build Outcome Prediction (CBOP). CBOP uses classification to label the possible build results based on historical data and metrics derived from the software repository. This paper contains a detailed description of the tool that was already started to be used in the production environment of a real software project where the CBOP practice is being evaluated. © 2021, Springer Nature Switzerland AG.",Continuous integration | Software defect prediction,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021-01-01,Conference Paper,"Kawalerowicz, Marcin;Madeyski, Lech",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84964896882,10.3233/KES-210061,Software fault prediction using machine learning techniques with metric thresholds,"BACKGROUND: Fault data is vital to predicting the fault-proneness in large systems. Predicting faulty classes helps in allocating the appropriate testing resources for future releases. However, current fault data face challenges such as unlabeled instances and data imbalance. These challenges degrade the performance of the prediction models. Data imbalance happens because the majority of classes are labeled as not faulty whereas the minority of classes are labeled as faulty. AIM: The research proposes to improve fault prediction using software metrics in combination with threshold values. Statistical techniques are proposed to improve the quality of the datasets and therefore the quality of the fault prediction. METHOD: Threshold values of object-oriented metrics are used to label classes as faulty to improve the fault prediction models The resulting datasets are used to build prediction models using five machine learning techniques. The use of threshold values is validated on ten large object-oriented systems. RESULTS: The models are built for the datasets with and without the use of thresholds. The combination of thresholds with machine learning has improved the fault prediction models significantly for the five classifiers. CONCLUSION: Threshold values can be used to label software classes as fault-prone and can be used to improve machine learners in predicting the fault-prone classes. © 2021 - IOS Press. All rights reserved.",Automated Program Repair | Classification Techniques | Effective Feature Design | Effectiveness Prediction,"2015 IEEE 26th International Symposium on Software Reliability Engineering, ISSRE 2015",2016-01-13,Conference Paper,"Le, Xuan Bach D.;Le, Tien Duy B.;Lo, David",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85111738315,10.5220/0010576102450252,Improved software product reliability predictions using machine learning,"Reliability is one of the key attributes of software product quality. Popular software reliability prediction models are targeted to specific phases of software product development life cycle. After studying, reliability models, authors could conclude that they have limitations in predicting software product reliability. A recent industrial survey performed by the authors identified several factors which practitioners perceived to have influence in predicting reliability. Subsequently authors conducted set of experiments to find out influential factors to reliability. In this paper, authors presented model definition approach using most influential parameters such as review efficiency, skill level of developer/tester and post-delivery defects. Copyright © 2021 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved",Correlation | Empirical Study | Experimentation | Post-delivery Defects | Reliability Prediction | Review Efficiency | Skill Level | Software Product | Software Reliability | SonarQube,"Proceedings of the 16th International Conference on Software Technologies, ICSOFT 2021",2021-01-01,Conference Paper,"Joshi, Sanjay;Badhe, Yogesh",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85040772740,10.1007/978-3-030-79357-9_27,Sofware Quality Prediction: An Investigation Based on Artificial Intelligence Techniques for Object-Oriented Applications,"Exploring the relationship between Object Oriented software metrics and predicting the accuracy rate in defect prediction using the Chidamber & Kemerer (CK) metrics suite are the main purposes of this study. For these purposes, eleven machine learning (ML) techniques were analyzed to predict models and estimate reliability for eight open-source projects. Therefore, evaluating the relation between CK metrics and defect proneness was determined. Moreover, the performances of eleven machine learning techniques were compared to find the best technique for determining defect prone classes in Object Oriented software. The techniques were analyzed using Weka and RapidMiner tools and were validated using a tenfold cross-validation technique with different kernel types and iterations. Also, Bayesian belief’s network forms show which metrics are being the primary estimators for reliability. Due to the imbalanced nature of the dataset, receiver operating characteristic (ROC) analysis was used. The accuracy of the projects was evaluated in terms of precision, recall, accuracy, area under the curve (AUC) and mean absolute error (MAE). Our analyses have shown that Random Forest, Bagging and Nearest Neighbors techniques are good prediction models with AUC values. The least effective model is Support Vector Machines (SVM). © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Detection | Error | Fault | Injection | Machine Learning,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2017-11-14,Conference Paper,"Leeke, Matthew",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85109880315,10.32604/iasc.2021.018405,Feature selection using artificial immune network: An approach for software defect prediction,"Software Defect Prediction (SDP) is a dynamic research field in the software industry. A quality software product results in customer satisfaction. However, the higher the number of user requirements, the more complex will be the software, with a correspondingly higher probability of failure. SDP is a challenging task requiring smart algorithms that can estimate the quality of a software component before it is handed over to the end-user. In this paper, we propose a hybrid approach to address this particular issue. Our approach combines the feature selection capability of the Optimized Artificial Immune Networks (Opt-aiNet) algorithm with benchmark machine-learning classifiers for the better detection of bugs in software modules. Our proposed methodology was tested and validated using 5 open-source National Aeronautics and Space Administration (NASA) data sets from the PROMISE repository: CM1, KC2, JM1, KC1 and PC1. Results were reported in terms of accuracy level and of an AUC with highest accuracy, namely, 94.82%. The results of our experiments indicate that the detection capability of benchmark classifiers can be improved by incorporating Opt-aiNet as a feature selection (FS) method. © 2021, Tech Science Press. All rights reserved.",Feature selection (FS) | Machine learning | Optimized artificial immune networks (Opt-aiNet) | Software defect prediction (SDP) | Software metrics,Intelligent Automation and Soft Computing,2021-01-01,Article,"Mumtaz, Bushra;Kanwal, Summrina;Alamri, Sultan;Khan, Faiza",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85081112062,10.7717/PEERJ-CS.563,Exclusive use and Evaluation of Inheritance Metrics Viability in Software Fault Prediction—an Experimental Study,"Software Fault Prediction (SFP) assists in the identification of faulty classes, and software metrics provide us with a mechanism for this purpose. Besides others, metrics addressing inheritance in Object-Oriented (OO) are important as these measure depth, hierarchy, width, and overriding complexity of the software. In this paper, we evaluated the exclusive use, and viability of inheritance metrics in SFP through experiments. We perform a survey of inheritance metrics whose data sets are publicly available, and collected about 40 data sets having inheritance metrics. We cleaned, and filtered them, and captured nine inheritance metrics. After preprocessing, we divided selected data sets into all possible combinations of inheritance metrics, and then we merged similar metrics. We then formed 67 data sets containing only inheritance metrics that have nominal binary class labels. We performed a model building, and validation for Support Vector Machine(SVM). Results of Cross-Entropy, Accuracy, F-Measure, and AUCadvocate viability of inheritance metrics in software fault prediction. Furthermore, ic, noc, and dit metrics are helpful in reduction of error entropy rate over the rest of the 67 feature sets. Copyright 2021 Aziz et al.",Deep learning | GUI bug detection | GUI testing | Mobile application testing | Text-layout bug detection,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2019-10-01,Conference Paper,"Wang, Yaohui;Xu, Hui;Zhou, Yangfan;Lyu, Michael R.;Wang, Xin",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85097350528,10.1155/2021/6662932,Impact of Parameter Tuning for Optimizing Deep Neural Network Models for Predicting Software Faults,"Deep neural network models built by the appropriate design decisions are crucial to obtain the desired classifier performance. This is especially desired when predicting fault proneness of software modules. When correctly identified, this could help in reducing the testing cost by directing the efforts more towards the modules identified to be fault prone. To be able to build an efficient deep neural network model, it is important that the parameters such as number of hidden layers, number of nodes in each layer, and training details such as learning rate and regularization methods be investigated in detail. The objective of this paper is to show the importance of hyperparameter tuning in developing efficient deep neural network models for predicting fault proneness of software modules and to compare the results with other machine learning algorithms. It is shown that the proposed model outperforms the other algorithms in most cases. © 2021 Mansi Gupta et al.",Dependability | Failure Prediction | Fault Injection | Machine Learning,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2020-10-01,Conference Paper,"Campos, Joao R.;Costa, Ernesto",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85097334928,10.14569/IJACSA.2021.0120587,A Machine Learning based Analytical Approach for Envisaging Bugs,"A software imperfection is a shortcoming, virus, defect, mistake, breakdown or glitch in software that initiates it to establish an unsuitable or unanticipated result. The foremost hazardous components connected with a software imperfection that is not identified at an initial stage of software expansion are time, characteristic, expenditure, determination and wastage of resources. Faults appear in any stage of software expansion. Thriving software businesses emphasize on software excellence, predominantly in the early stage of the software advancement. In succession to disable this setback, investigators have formulated various bug estimation methodologies till now. Though, emerging vigorous bug estimation prototype is a demanding assignment and several practices have been anticipated in the text. This paper exhibits a software fault estimation prototype grounded on Machine Learning (ML) Algorithms. The simulation in the paper directs to envisage the existence or non-existence of a fault, employing machine learning classification models. Five supervised ML algorithms are utilized to envisage upcoming software defects established on historical information. The classifiers are Naïve Bayes (NB), Support Vector Machine (SVM), K- Nearest Neighbors (KNN), Decision Tree (DT) and Random Forest (RF). The assessment procedure indicated that ML algorithms can be manipulated efficiently with high accuracy rate. Moreover, an association measure is employed to evaluate the propositioned extrapolation model with other methods. The accumulated conclusions indicated that the ML methodology has an improved functioning. © 2021. All Rights Reserved.",AUTOSAR | Machine learning | Multi-core | Safety-critical | Software fault tolerance,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2020-10-01,Conference Paper,"Copic, Milan;Leupers, Rainer;Ascheid, Gerd",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85107389489,10.1007/978-981-16-1480-4_3,Software Fault Prediction Using Machine Learning Models and Comparative Analysis,"Software Testing is an important phase of Software Development Life cycle. Effective software testing helps in identifying faulty modules, but this process becomes very time consuming, especially for large /complex software. Moreover early identification of error prone modules can be useful in producing better quality software. Hence software fault prediction has gained significant attention of the researchers in the recent years. Mainly two types of techniques are being used for it: statistical methods and Machine learning models, out of which recent trend is more inclined towards machine Learning based techniques. Identification of faulty modules is a binary classification problem and machine learning models such as Decision Tree and its variants, Random Forest and Support Vector Machine, are best suited for the fault prediction. This paper attempts to predict software faults using four different classification models. Further, the performance evaluation of these models is also carried out in this paper using accuracy, Precision, Recall, F1-Score and execution time of over 12 datasets, extracted from one of the most commonly used PROMISE repository. Based on these performance metrics comparison of applied four models is done and our comparative analysis indicates that in most of the cases Support Vector Machine model is able to predict software faults more efficiently than the rest in terms of accuracy as well as execution time © 2021, Springer Nature Singapore Pte Ltd.",,Communications in Computer and Information Science,2021-01-01,Conference Paper,"Singh, Manpreet;Chhabra, Jitender Kumar",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85097350140,10.1007/978-981-33-4299-6_18,Enhancing Deep Learning Capabilities with Genetic Algorithm for Detecting Software Defects,"Regardless of existing and well-defined processes, some defects are inevitable, resulting in software performance degradation. The use of traditional machine learning techniques can automate the prediction of software defects. This automated approach significantly improves the quality of the finished product and reduces the cost incurred during development and maintenance stages. The accuracy of artificial neural networks for the automatic prediction of software bugs, can be further enhanced with the use of metaheuristics algorithms. We propose a hybrid approach which combines Genetic Algorithm (GA) and Deep Neural Network (DNN) to better classify software defects. GA is used as a pre-learning phase to automatically optimize the input features for the DNN, as irrelevant variables have a substantial negative impact on the prediction accuracy. Results from experiments using the PROMISE dataset, demonstrates that a DNN consuming optimized features yields better results. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Fault Injection | Machine Learning | Resilience,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2020-10-01,Conference Paper,"Chen, Zitao;Narayanan, Niranjhana;Fang, Bo;Li, Guanpeng;Pattabiraman, Karthik;DeBardeleben, Nathan",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85102587077,10.1504/IJCAET.2021.113549,An empirical analysis of the statistical learning models for different categories of cross-project defect prediction,"Currently, the research community is addressing the problem of defect prediction with the availability of project defect data. The availability of different project data leads to extend the research on cross projects. Cross-project defect prediction has now become an accepted area of software project management. In this paper, an empirical study is carried out to investigate the predictive performance of availability within project and cross-project defect prediction models. Furthermore, different categories of cross-project data are taken for training and testing to analyse various statistical models. In this paper data models are analysed and compared using various statistical performance measures. The findings during the empirical analysis of the data models state that gradient boosting predictor outperforms in the cross-project defect prediction scenario. Results also infer that cross-project defect prediction is comparable with project defect prediction and has statistical significance. Copyright © 2021 Inderscience Enterprises Ltd.",Classification | Cross projects | Cross validation | Defect prediction | Homogeneous metrics | Machine learning | Quality assurance | Supervised learning | Training dataset | Within-project,International Journal of Computer Aided Engineering and Technology,2021-01-01,Article,"Goel, Lipika;Sharma, Mayank;Khatri, Sunil Kumar;Damodaran, D.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85102023198,10.14569/IJACSA.2021.0120219,An Hybrid Approach for Cost Effective Prediction of Software Defects,"Identifying software defects during early stages of Software Development life cycle reduces the project effort and cost. Hence there is a lot of research done in finding defective proneness of a software module using machine learning approaches. The main problems with software defect data are cost effective and imbalance. Cost effective problem refers to predicting defective module as non defective induces high penalty compared to predicting non defective module as defective. In our work, we are proposing a hybrid approach to address cost effective problem in Software defect data. To address cost effective problem, we used bagging technique with Artificial Neuro Fuzzy Inference system as base classifier. In addition to that, we also addressed Class Imbalance & High dimensionality problems using Artificial Neuro Fuzzy inference system & principle component analysis respectively. We conducted experiments on software defect datasets, downloaded from NASA dataset repository using our proposed approach and compared with approaches mentioned in literature survey. We observed Area under ROC curve (AuC) for proposed approach was improved approximately 15% compared with highly efficient approach mentioned in literature survey. © 2021. All Rights Reserved.",adaptive neuro fuzzy inference system | area under ROC curve | Cost effective problem | principle component analysis,International Journal of Advanced Computer Science and Applications,2021-01-01,Article,"Maddipati, Satya Srinivas;Srinivas, Malladi",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85040613398,10.4018/IJITSA.2021010104,An empirical study on software fault prediction using product and process metrics,"Product and process metrics are measured from the development and evolution of software. Metrics are indicators of software fault-proneness and advanced models using machine learning can be provided to the development team to select modules for further inspection. Most fault-proneness classifiers were built from product metrics. However, the inclusion of process metrics adds evolution as a factor to software quality. In this work, the authors propose a process metric measured from the evolution of software to predict fault-proneness in software models. The process metrics measures change-proneness of modules (classes and interfaces). Classifiers are trained and tested for five large open-source systems. Classifiers were built using product metrics alone and using a combination of product and the proposed process metric. The classifiers evaluation shows improvements whenever the process metrics were used. Evolution metrics are correlated with quality of software and helps in improving software quality prediction for future releases. Copyright © 2021, IGI Global.",#NAME?,"Proceedings - 2017 IEEE 28th International Symposium on Software Reliability Engineering Workshops, ISSREW 2017",2017-11-14,Conference Paper,"Jagadeesan, Lalita J.;Mendiratta, Veena",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85080930285,10.1007/978-3-030-49795-8_74,Feature Extraction for Software Defect Detection Utilizing Neural Network with Random Forest,"Programming deformity expectation is the most pivotal advancement in enormous programming improvement associations where the multifaceted nature of the production task is expanding at an exponential rate. Allotting the right seriousness level to the imperfections experienced in enormous and complex programming undertakings would help the product professionals to assign their assets and plan for consequent deformity fixing exercises. So as to achieve this, we have built up a model dependent on the neural system method with random forest which includes extraction that will be utilized to dole out the seriousness level to each abscond report dependent on the grouping of existing programming module reports. The proposed model is assessed utilizing PROMISE Software Engineering Repository informational indexes. It is clear from the outcomes that the model has performed very well in anticipating high seriousness programming surrenders than in foreseeing the deformities of other seriousness levels. © 2021, Springer Nature Switzerland AG.",fault injection | machine learning | Testing,"Proceedings - 2019 IEEE 30th International Symposium on Software Reliability Engineering Workshops, ISSREW 2019",2019-10-01,Conference Paper,"Nurminen, Jukka K.;Halvari, Tuomas;Harviainen, Juha;Myllari, Juha;Roysko, Antti;Silvennoinen, Juuso;Mikkonen, Tommi",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85097219715,10.1007/978-981-15-7907-3_37,Prediction of defects in software using machine learning classifiers,"The Software Defect Prediction (SDP) model engages in predicting defects and bugs in the software. This model will detect and predict bugs during early stages of the software development life cycle to improve the overall quality of software and reduce the cost also. In this paper, the author presents a model that will predict the bugs with the help of machine learning classifiers. For this model, the researcher has used the dataset NASA from the known repositories and used two supervised ML classifier algorithms such as linear supervision (LR) and Naive Bayes (NB) for detecting and predicting faults. This study describes how ML algorithms work effectively in SDP models. The results collected showed that the linear regression approach performs better and predicts the faults with accuracy. © 2021, Springer Nature Singapore Pte Ltd.",Linear regression | Machine learning | Naïve Bayes | Software defect prediction,Advances in Intelligent Systems and Computing,2021-01-01,Conference Paper,"Arya, Ashima;Kumar, Sanjay;Singh, Vijendra",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85092744070,10.1007/978-981-15-5566-4_57,Generalized Canonical Correlation Based Bagging Ensembled Relevance Vector Machine Classifier for Software Quality Analysis,"Early detection of defects helps to save cost and efforts. Few research works have been designed in existing works for analyzing the quality of software program using various machine learning techniques. However, the classification performance of existing work was lower which reduces the defect detection accuracy. In order to overcome the above existing issues, the Generalized Canonical Correlation Analysis based Bagging Ensembled Relevance Vector Machine Classification (GCCA-BERVMC) model is proposed. The GCCA-BERVMC model considers the number of source code lines from software program dataset as input. After that, the generalized Canonical Correlation Analysis (gCCA) algorithm designed in GCCA-BERVMC model selects the relevant features (i.e., code metrics) in order to improve the quality of software program. By applying Bagging Ensembled Relevance Vector Machine Classification (BERVMC) algorithm, the classification of program files is performed in GCCA-BERVMC model. The boosting algorithm creates ‘n’ number of weak learners to classify the input source code lines as normal or defected by analyzing the source codes and chosen metrics. After that, the weak learner’s results are combined into strong classifier by using the majority votes. This helps for GCCA-BERVMC model to enhance the accuracy of defect prediction for software quality analysis with a lower amount of time. Experimental evaluation of GCCA-BERVMC model is conducted using metrics such as defect detection accuracy, false positive rate, and time complexity with respect to various software code sizes. The experimental result shows that the GCCA-BERVMC model is able to increase the defect detection accuracy and also minimizes the amount of time required for software quality analysis when compared to state-of-the-art works. © 2021, Springer Nature Singapore Pte Ltd.",Bagging Ensembled Relevance Vector Machine Classification | Generalized canonical correlation | Majority vote | Software defect | Strong classifier and weak learner,Advances in Intelligent Systems and Computing,2021-01-01,Conference Paper,"Ayesha, Noor;Yethiraj, N. G.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85090174819,10.1007/978-3-030-55180-3_33,A Neuro-Fuzzy Model for Software Defects Prediction and Analysis,"Identifying defective software modules is a major factor in creating software in a cost effective and timely manner. A variety of machine learning techniques are available for predicting defective software modules. This paper investigates the effectiveness of using a fuzzy neural network in identifying faulty modules and the relative importance of the various software metrics in predicting defective modules and their role in explaining the presence of software defects. The conclusions of the work is that the model provides good accuracy but low probability of detecting software defects. However, the model does provide useful insight into the predictive and explanatory power of the various metrics affecting defect detection. The work shows that the most influential predictive metrics are those related to the measurement of program length and complexity. However, the degree of their relative influence is different for different datasets. On the explanatory side, metrics relating to program complexity, program length, and number of operators and operands are the most influential in explaining the presence of software defects. Again, their relative importance and the importance of other metrics vary for different datasets. These results are tentative and influenced to some degree by the model architectural parameters and configurations. Future work will concentrate on tuning the model and identifying a group of software characteristic that collectively can provide a good probability of defect detection. © 2021, Springer Nature Switzerland AG.",Fuzzy systems | Neural networks | Neuro-fuzzy inference | Software defects prediction,Advances in Intelligent Systems and Computing,2021-01-01,Conference Paper,"Mehdi, Riyadh A.K.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85099203094,10.1109/Comnetsat50391.2020.9328933,A Widely Analysis Assessment of Software Deformity Prone Datasets Models using LinearNNsearch Classification Model,"In software world, researcher work is always focusing on number of faults present in a software deformity prone datasets model. The main goal of researcher work is that to find and fix present faults in software deformity prone as soon as possible. This has much to do with software deformity prone datasets models and its accuracy growth. Currently world immense software companies, they are used their own accuracy repository system which handling the software deformity prone issues and controlled the accuracy of deformity prone datasets models. This research paper presents the current issues on software deformity prone datasets models and the significant method used on these issues. Our research model method is that to boost the accuracy of forecasts in software deformity prone based on LinearNNsearch Classification method. This method used with K parameters where K=N=1 to 6. The experimental analyses showed that the parameter K=N=3, 4 and 5 are good for linearNNsearch and can enhanced the positive accuracy of software deformity prone with linearNNsearch. The analysis experiments of IBK Filtered neighbor search at K=N=5, 6 can also increase the positive accuracy of software deformity prone. © 2020 IEEE.",BFI to SEM sampling | machine learning | OPC feature vector | PWQ analysis | systematic defects,"2020 4th International Workshop on Advanced Patterning Solutions, IWAPS 2020",2020-11-05,Conference Paper,"Jiang, Jet;Hou, Frank;Li, Gavin;Chen, Summy;Fei, Marfei;Xie, Qian;Cao, Liang;Wan, Qijian;Hu, Xinyi;Du, Chunshan;Wang, David;Huang, Elven;Paninjath, Sankaranarayanan;Madhusudhan, Saikiran;Tian, Leo",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85102660190,10.1109/PuneCon50868.2020.9362471,Handling Imbalanced Dataset Classification in Machine Learning,"Real world dataset consists of normal instances with lesser percentage of interesting or abnormal instances. The cost of misclassifying an abnormal instance as normal instance is very high. The majority class is normal class whereas minority class is the abnormal one. Researchers in data mining and machine learning are looking out numerous strategies to resolve issues associated with dataset that is unbalanced and also the challenges featured in way of life. Irregular distribution in the dataset is the motive behind declining performance of classifier. There are mainly two methods, algorithm based and data level based, the utmost widespread methodology associated to the current is hybrid method. The task of decision making and overall classification accuracy is affected due to bias for majority class. Ensemble technique is an effective technique. The objective of study is providing background related to imbalance class issues, way out to confront the disputes and challenges in studying unbalanced data. In support to experimental result accompanied on one of the dataset, ensemble technique in adjacent to different strategies of data-level offers improved outcomes. The fusion of techniques is going to be advantageous for several applications in real-life like intrusion detection, medical diagnosis, software defect prediction, etc. © 2020 IEEE.",Bias | Classifier | Imbalance class | Machine learning | Minority and majority class | Sampling,"2020 IEEE Pune Section International Conference, PuneCon 2020",2020-12-16,Conference Paper,"Yadav, Seema;Bhole, Girish P.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85105369998,10.1109/TASE49443.2020.00012,Defect Report Severity Prediction Based on Genetic Algorithms and Convolutional Neural Network,"In software maintainence, defect report severity prediction is an important task which can effectively help developer judge the urgency of defects. However, manually classifying the severity of defect reports is very time consuming and tedious. Recently, researchers have proposed many advanced methods to automate the severity prediction of defect reports. But there is still room for improvement in performance. Therefore, in this paper, we propose a distinctive method for automatically identifying the severity of software defect reports using convolutional neural network (CNN). We first preprocess defect reports and select textual features by genetic algorithms (GA). Then, the Word2Vec model is employed to generate word vectors for the selected features in each defect report. Finally we train the classifier based on CNN and leverage the trained classifier to predict the severity of defect reports. To validate the performance of our method, we experiment with defect reports from four open source projects and compare our method with three common machine learning methods. The experimental results show that our method achieves 77.38 % in terms of precision, 62.09 % in terms of recall and 68.76% in terms of Fl-score on average, and outperforms the best baseline method by 11.61 %, 7.23% and 9.32%, respectively. © 2020 IEEE.",Convolutional Neural Network | Defect Report | Feature Selection | Genetic Algorithm | Severity Prediction,"Proceedings - 2020 International Symposium on Theoretical Aspects of Software Engineering, TASE 2020",2020-12-01,Conference Paper,"Guo, Shiming;Chen, Xin;Yu, Dongjin",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85099298253,10.1109/QRS51102.2020.00033,A Lightweight Fault Localization Approach based on XGBoost,"Software fault localization is one of the key activities in software debugging. The program spectrum-based approach is widely used in fault localization. However, lots of program information, for example, the sequence of the execution statement and statement semantics, is missing when such an approach is utilized, which affects the performance. XGBoost is an effective learning algorithm, which can use the characteristics of the training data to build a classification tree during training. In addition, XGBoost can iteratively adjust the information value of the feature, so that the training process retains the importance information of the feature. This paper proposes applying XGBoost into fault localization utilizing information of program execution behaviors. A novel method called XGB-FL is developed, where the program spectrum information is converted into a coverage matrix to train the XGBoost model. We can get the characteristics of the data through the trained model and the importance of the program statement in the classification process. This is also the basis for judging whether the statement is likely to contain a fault. Nine representative data sets have been chosen to evaluate the performance of XGB-FL. The experimental results show that XGB-FL can generally deliver a higher performance in fault localization than those baseline techniques, in terms of precision and efficiency. © 2020 IEEE.",machine learning | software fault localization | software testing | XGBoost,"Proceedings - 2020 IEEE 20th International Conference on Software Quality, Reliability, and Security, QRS 2020",2020-12-01,Conference Paper,"Yang, Bo;He, Yuze;Liu, Huai;Chen, Yixin;Jin, Zhi",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85046710212,10.1145/3416505.3423563,Singling the odd ones out: A novelty detection approach to find defects in infrastructure-as-code,"Infrastructure-as-Code (IaC) is increasingly adopted. However, little is known about how to best maintain and evolve it. Previous studies focused on defining Machine-Learning models to predict defect-prone blueprints using supervised binary classification. This class of techniques uses both defective and non-defective instances in the training phase. Furthermore, the high imbalance between defective and non-defective samples makes the training more difficult and leads to unreliable classifiers. In this work, we tackle the defect-prediction problem from a different perspective using novelty detection and evaluate the performance of three techniques, namely OneClassSVM, LocalOutlierFactor, and IsolationForest, and compare their performance with a baseline RandomForest binary classifier. Such models are trained using only non-defective samples: defective data points are treated as novelty because the number of defective samples is too little compared to defective ones. We conduct an empirical study on an extremely-imbalanced dataset consisting of 85 real-world Ansible projects containing only small amounts of defective instances. We found that novelty detection techniques can recognize defects with a high level of precision and recall, an AUC-PR up to 0.86, and an MCC up to 0.31. We deem our results can influence the current trends in defect detection and put forward a new research path toward dealing with this problem. © 2020 ACM.",Biomedical image processing | computer aided diagnosis | dermatology | feature extraction | microscopy,IEEE Journal of Biomedical and Health Informatics,2019-03-01,Article,"Madooei, Ali;Drew, Mark S.;Hajimirsadeghi, Hossein",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85101317124,10.1109/VTC2020-Fall49728.2020.9348849,Software Failures Prediction in Self-Driving Vehicles,"Nowadays, Advanced Driver Assistance Systems (ADAS) play important roles in improving the safety of driving. ADASs along with new safety standards are now essential to improve the safety of automated driving. These standards differ from the safety standards in other industries because an autonomous vehicle rides in a very intricate stochastic environment where many details should be considered to maintain the safety. From one point of view, autonomous car failures are divided into two categories: hardware and software failures. According to the literature, if the reason of fault belongs to hardware, the troubleshooting will relatively be easier than software failure. In other words, if the fault is the subset of software, that would be more challenging task to detect potential fault and recover from the failure. In this paper, we propose a new approach to detect the software failures, which may be unpredictable using a typical safety protocol. A predictive machine learning method is developed to improve decision making process in order to avert the potential accidents occurred by software defects. The proposed method evaluates detected obstacle in the span of a specified number of frames to discern whether objects are detected correctly or not. Hence, it provides extra time to choose the suitable action before it reaches the danger, which includes lane changes, cut-in maneuvers, stopping the car, accelerating the car, slowing down, and allowing vehicles to pass. Results show that the new approach improve the safety by increasing the precision of failure detections. © 2020 IEEE.",Advanced Driver Assistance Systems | Deep Learning | Faster RCNN | LSTM | Vehicle Detection,IEEE Vehicular Technology Conference,2020-11-01,Conference Paper,"Abedi, Vajiheh;Zadeh, Mehrdad H.;Dargahi, Javad;Fekri, Pedram",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85100563602,10.1109/DSA51864.2020.00020,Aging-Related Bugs Prediction Via Convolutional Neural Network,"Software aging refers to the phenomenon of system performance degradation or system crash in long-term running systems, which is mainly caused by Aging-Related Bugs (ARBs). To predict Aging-Related Bugs, previous studies usually focused on manually designing features, which extracted from the programs, and utilized different machine learning algorithms to detect those buggy codes. However, these traditional features often failed to distinguish programs' semantic differences.To explore deeply programs' semantics and make full use of these information, in this paper, we proposed a method, which based on deep learning method to automatically learn programs' semantic features of source codes. Specifically, we utilized Convolutional Neural Network (CNN) to automatically generate more distinguished features which based on the Abstract Syntax Trees (ASTs) of programs. Meanwhile, we combined these features with conventional aging-related metrics for more accurate ARB prediction. Finally, we evaluated our model on Linux and MySQL datasets, the experiment results showed that our approach was better than the baselines. The improvement can be achieved up to 6.9% on Linux, and 24.1% on MySQL in terms of balance, compared to traditional Naive Bayes method. And compared to Naive Bayes with logarithmic transformation, the improvement is 1% and 4.7% respectively. © 2020 IEEE.",Abstract Syntax Trees | Aging-Related Bugs | Convolutional Neural Network | Defect Prediction | Software Aging,"Proceedings - 2020 7th International Conference on Dependable Systems and Their Applications, DSA 2020",2020-11-01,Conference Paper,"Liu, Qinchen;Xiang, Jianwen;Xu, Bin;Zhao, Dongdong;Hu, Wenhua;Wang, Jian",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85037554389,10.1109/ICDABI51230.2020.9325677,Software Defects Prediction using Machine Learning Algorithms,"Software development and the maintenance life cycle are lengthy processes. However, the possibility of having defects in the software can be high. Software reliability and performance are essential measures of software success, which affects user satisfaction and software cost. Predicting software defects using machine learning (ML) algorithms is one approach in this direction. Implementing this approach in the earlier stages of the software development improves software performance quality and reduces software maintenance cost. Different models and techniques have been implemented in many studies to predict software defects. This investigation implements ML algorithms, such as artificial neural networks (ANNs), random forest (RF), random tree (RT), decision table (DT), linear regression (LR), gaussian processes (GP), SMOreg, and M5P. A new software defect prediction model for software future defect prediction is proposed. The defect prediction is based on historical data. The results showed that a combination of ML algorithms could be used effectively to predict software defects. The SMOreg classifier scored the best performance results, where the ANN classifier scores the worst results. © 2020 IEEE.",Dataset preprocessing | Machine learning | Software defect predict | Software metric | Software quality assurance,IEEE Systems Journal,2018-09-01,Article,"Lin, Zhaowen;Wu, Tin Yu;Sun, Yan;Xu, Jie;Obaidat, Mohammad S.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85049132394,10.1109/ISSSR51244.2020.00021,Empirical evaluation of the active learning strategies on software defects prediction,"Software defect prediction is a popular technical method in software engineering. In order to reduce the cost of a software defects, problems existing in the software are found by testing software products. Software defect prediction often uses machine learning techniques to improve the performance of software testing but requires enough labeled data when training the model. Because the cost of obtaining data is different from the label, the data is easy to obtain, but the label is cumbersome and expensive. In order to demonstrate software defect prediction, after the data obtained active learning algorithm is introduced to query the data, and the most valuable data is selected for expert annotation and then put into the model for training. However, it is not clear which active learning query strategy to choose the most effective in the software defect prediction model. We use different active learning strategy software defect prediction models for comparison. Experiment on the NASA dataset, using Naive Bayes and SVM, Linear Regression as the classifier. Comprehensive research results show that the Density-weighted strategy has a significant effect on the data set. © 2020 IEEE.",Demand-Controlled Ventilation | failure | fault detection and diagnosis | fault injection | heating modeling and simulation | machine learning | MATLAB/Simulink,"2017 IEEE 4th International Conference on Knowledge-Based Engineering and Innovation, KBEI 2017",2017-07-02,Conference Paper,"Behravan, Ali;Obermaisser, Roman;Basavegowda, Deepak Hanike;Mallak, Ahlam;Weber, Christian;Fathi, Madjid",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85102358349,10.1109/SYNASC51798.2020.00032,An analysis of aggregated coupling's suitability for software defect prediction,"Software Defect Prediction is an important problem during the development of a software system, because it helps to focus the testing effort on those parts of the system which have a high probability of being defective. It is also well-researched, there being many papers presenting Machine Learning-based prediction models for this problem. But most of them use the same object-oriented structural software metrics as features. In this paper we investigate the impact of aggregated coupling, which combines structural and conceptual coupling, on software defects proneness. In this regard, we present three software metrics suites derived from both structural and conceptual coupling and analyze how their different combinations influence the performance of software defect prediction models. We analyze the relative performance of the models when using features extracted with LSI versus Doc2Vec in conjunction with Cosine versus Euclidean similarity for computing the conceptual coupling. The results suggest that all these features are complementary and their usage improves the performance of the machine learning models. © 2020 IEEE.",Aggregated coupling | Conceptual coupling | Software coupling | Software defect prediction | Structural coupling,"Proceedings - 2020 22nd International Symposium on Symbolic and Numeric Algorithms for Scientific Computing, SYNASC 2020",2020-09-01,Conference Paper,"Miholca, Diana Lucia;Onet-Marian, Zsuzsanna",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85006974296,10.1109/SCAM51674.2020.00014,Looking for Software Defects? First Find the Nonconformists,"Software defect prediction models play a key role to increase the quality and reliability of software systems. Because, they are used to identify defect prone source code components and assist testing activities during the development life cycle. Prior research used supervised and unsupervised Machine Learning models for software defect prediction. Supervised defect prediction models require labeled data, however it might be time consuming and expensive to obtain labeled data that has the desired quality and volume. The unsupervised defect prediction models usually use clustering techniques to relax the labeled data requirement, however labeling detected clusters as defective is a challenging task. The Pareto principle states that a small number of modules contain most of the defects. Getting inspired from the Pareto principle, this work proposes a novel, unsupervised learning approach that is based on outlier detection. We hypothesize that defect prone software components have different characteristics when compared to others and can be considered as outliers, therefore outlier detection techniques can be used to identify them. The experiment results on 16 software projects from two publicly available datasets (PROMISE and GitHub) indicate that the k-Nearest Neighbor (KNN) outlier detection method can be used to identify the majority of software defects. It could detect 94% of expected defects at best case and more than 63% of the defects in 75% of the projects. We compare our approach with the state-of-The-Art supervised and unsupervised defect prediction approaches. The results of rigorous empirical evaluations indicate that the proposed approach outperforms existing unsupervised models and achieves comparable results with the leading supervised techniques that rely on complex training and tuning algorithms. © 2020 IEEE.",,"Proceedings - 2016 8th International Conference on Knowledge and Systems Engineering, KSE 2016",2016-11-28,Conference Paper,"Phan, Viet Anh;Chau, Ngoc Phuong;Nguyen, Minh Le",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85096690236,10.1109/ICSME46990.2020.00049,"How (Not) to Find Bugs: The Interplay between Merge Conflicts, Co-Changes, and Bugs","Context: In a seminal work, Ball et al. [1] investigate if the information available in version control systems could be used to predict defect density, arguing that practitioners and researchers could better understand errors ""if [our] version control system could talk"". In the meanwhile, several research works have reported that conflict merge resolution is a time consuming and error-prone task, while other contributions diverge about the correlation between co-change dependencies and defect density. Problem: The correlation between conflicting merge scenarios and bugs has not been addressed before, whilst the correlation between co-change dependencies and bug density has been only investigated using a small number of case studies-which can compromise the generalization of the results. Goal: To address this gap in the literature, this paper presents the results of a comprehensive study whose goal is to understand whether or not (a) conflicting merge scenarios and (b) co-change dependencies are good predictors for bug density. Method: We first build a curated dataset comprising the source code history of 29 popular Java Apache projects and leverage the SZZ algorithm to collect the sets of bug-fixing and bug-introducing commits. We then combine the SZZ results with the set of past conflicting merge scenarios and co-change dependencies of the projects. Finally, we use exploratory data analysis and machine learning models to understand the strength of the correlation between conflict resolution and co-change dependencies with defect density. Findings: (a) conflicting merge scenarios are not more prone to introduce bugs than regular commits, (b) there is a negligible to a small correlation between co-change dependencies and defect density-contradicting previous studies in the literature. © 2020 IEEE.",co-change dependencies | merge conflicts | software defects | software integration,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",2020-09-01,Conference Paper,"Amaral, Luis;Oliveira, Marcos C.;Luz, Welder;Fortes, Jose;Bonifacio, Rodrigo;Alencar, Daniel;Monteiro, Eduardo;Pinto, Gustavo;Lo, David",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85093090749,10.1109/ICRITO48877.2020.9198037,Software Defect Categorization based on Maintenance Effort and Change Impact using Multinomial Naïve Bayes Algorithm,"Cost-to-fix software defects increase exponentially as the software moves forward in its life cycle. The fixing of defects found during the maintenance phase is very costly. Software Defect Categorization (SDC) models categorize the software defects into different levels (high, moderate and low) based on various attributes. Such models can aid the software maintenance team to prioritize the defects and plan the resources effectively in fixing them. This study develops SDC models based on three software bug attributes-i) maintenance effort needed to fix a defect ii) change impact on the software for fixing a defect and iii) the combined approach of both- maintenance effort and change impact. In this study, we extracted the important features from the defect reports using text mining and the classification models are developed for each of the attributes using Multinomial Naïve Bayes (NBM) algorithm. The capability of the models is calculated using Area Under the ROC (Receiver Operating Characteristic) curve (AOC). The results of the study show that i) SDC models based on maintenance effort and change impact are capable to categorize the software defects ii) the performance of the SDC models based on the combined approach is better than SDC models based on maintenance effort and change impact. © 2020 IEEE.",Change Impact | Machine Learning | Maintenance Effort | Multinomial Naïve Bayes | Software Defect Categorization | Software Maintenance,"ICRITO 2020 - IEEE 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions)",2020-06-01,Conference Paper,"Malhotra, Ruchika;Cherukuri, Madhukar",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85086760214,10.1093/comjnl/bxz022,Simultaneous Fault Models for the Generation and Location of Efficient Error Detection Mechanisms,"The application of machine learning to software fault injection data has been shown to be an effective approach for the generation of efficient error detection mechanisms (EDMs). However, such approaches to the design of EDMs have invariably adopted a fault model with a single-fault assumption, limiting the relevance of the detectors and their evaluation. Software containing more than a single fault is commonplace, with safety standards recognizing that critical failures are often the result of unlikely or unforeseen combinations of faults. This paper addresses this shortcoming, demonstrating that it is possible to generate efficient EDMs under simultaneous fault models. In particular, it is shown that (i) efficient EDMs can be designed using fault injection data collected under models accounting for the occurrence of simultaneous faults, (ii) exhaustive fault injection under a simultaneous bit flip model can yield improved EDM efficiency, (iii) exhaustive fault injection under a simultaneous bit flip model can be made non-exhaustive and (iv) EDMs can be relocated within a software system using program slicing, reducing the resource costs of experimentation to practicable levels without sacrificing EDM efficiency. © 2019 The British Computer Society 2019. All rights reserved.",detection | error | location | machine learning | model,Computer Journal,2020-05-20,Article,"Leeke, Matthew",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85084218746,,Software bug prediction analysis using various machine learning approaches,"Throughout software development and management systems, software Bug Prediction (SBP) is a major issue concerning overall software quality. Predicting the computer problems of previous stages increases performance, productivity and reduces costs more reliably. In fact, in advance forecasting can force us to take adequate precautionary measures to prevent bugs. Nonetheless, creating a practical framework of bug forecasting is a difficult task and various approaches are suggested in literature. This article presents few models of machine learning prediction software. The assessment process demonstrated that algorithms can be used for high precision. In addition, the proposed prediction model is compared to other approaches by using a comparison measure. The results showed a better performance over other strategy. © 2020 SERSC.",Machine learning; ML Strategy; SBP,International Journal of Advanced Science and Technology,2020,,"Apat S.K., Rao S.V.A., Santosh Kumar Patra P.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85084042023,,Software defect prediction: A survey with machine learning approach,"Software defect is inevitable part of software process, product and people. Developing zero defect software is always challenge to software industry. Software defect can be defined as it is unpredictable flaws in software which increase software product cost in terms of time, resource and budget. Software defect can be detected by developer and tester but it is time consuming which might delay on time delivery of software product to the client. To recover deadline gap software company required to fill unplanned resource in ongoing project. Software defect prediction will help to solve these problems by building machine learning prediction model for software fault. In this paper, we review work done on software defect by researchers in recent years. Many of researchers published their defect prediction work in the leading journals and premier conferences. Main objective of this study is to review different approaches used to predict software defect. Furthermore, few challenges addressed in this paper which might be future research area in finding and fixing software bugs. © 2020 SERSC.",Defect prediction model; Machine learning; Software defect prediction; Software metric; Software quality,International Journal of Advanced Science and Technology,2020,,"Suryawanshi R.S., Kadam A., Anekar D.R.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85085928241,10.1145/3383972.3384012,Safety Critical Software Reliability Model Considering Multiple Influencing Factors,"Safety-critical software accounts for a large proportion of computer applications in high-tech fields such as aerospace, aerospace, and nuclear power. Once a failure occurs, it can seriously endanger human life, cause large-scale environmental damage, or cause huge economic losses. How to accurately evaluate the reliability of safety-critical software has become one of the problems that software reliability engineering needs to solve. Therefore, this paper takes safety-critical software as the research object. Based on the in-depth analysis of the characteristics of safety-critical software, combined with the characteristics of safety-critical software and the common model optimization and improvement ideas, the total number of software failures, the failure detection rate changes, and the troubleshooting rate is less than the correlation between faults and errors is analyzed. The fault detection and elimination process are analyzed. The influencing factors that should be considered in the reliability modeling process of safety critical software are summarized and mathematically described. A multi-influence factor is considered. Safety Critical Software Reliability Model (SCSRM). Based on the safety critical software failure data set, the comparison experiment between SCSRM and the other four software reliability models was completed. The experimental results show that SCSRM can obtain better data fitting and prediction effects in the reliability assessment scenarios of safety-critical software, and proves the validity and stability of the proposed model. © 2020 ACM.",Safety Critical Software | Software Reliability | Software Reliability Model,ACM International Conference Proceeding Series,2020-02-15,Conference Paper,"Cheng, Jing;Zhang, Hao;Qin, Kai",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85083107922,10.1109/IBF50092.2020.9034776,Blve: Should the Current Software Version Be Suitable for Release?,"Recently, agile development has become a popular software development method and many version iterations occur during agile development. It is very important to ensure the quality of each software version. However in actual development, it is difficult to know every stage or version about large-scale software development. That means developers do not know exactly which version the current project corresponds to. Simultaneously, there are many necessary requirements for software release in actual development. When we know exactly the version corresponding to the current project, we can know whether the current software version meets the release requirements. Therefore, we need a good software version division method. This paper presents a novel software version division method Blve by using machine learning method. We construct an accurate division model trained with Support Vector Regression method (SVR) to divide software version by processing the data which is commonly recorded in bug list. Then, we process the results of the regression and use the classification indicators for evaluation. In addition, we propose a slope-based approach to optimize the model, and this optimization can improve the accuracy performance measure to about 95%. © 2020 IEEE.",Bug List | Regression | Software Repository Mining | Software Version,IBF 2020 - Proceedings of the 2020 IEEE 2nd International Workshop on Intelligent Bug Fixing,2020-02-01,Conference Paper,"Zheng, Wei;Chen, Xiaojun;Shi, Zhao;Zhang, Manqing;Chen, Junzheng;Chen, Xiang",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85097869099,,Using rule mining for automatic test oracle generation,"Software testing is essential for checking the quality of software but it is also a costly and time-consuming activity. The mechanism to determine the correct output of the System Under Test (SUT) for a given input space is called test oracle. The test oracle problem is a known bottleneck in situations where tests are generated automatically and no model of the correct behaviour of the SUT exists. To overcome this bottleneck, we developed a method which generates test oracles by comparing information extracted from object state data created during the execution of two subsequent versions of the SUT. In our initial proof-of-concept, we derive the relevant information in the form of rules by using the Association Rule Mining (ARM) technique. As a proof-of-concept, we validate our method on the Stack class from a custom version of the Java Collection classes and discuss the lessons learned from our experiment. The test suite that we use in our experiment to execute the different SUT version is automatically generated using Randoop. Other approaches to generate object state data could be used instead. Our proof-of-concept demonstrates that our method is applicable and that we can detect the presence of failures that are missed by regression testing alone. Automatic analysis of the set of violated association rules provides valuable information for localizing faults in the SUT by directly pointing to the faulty method. This kind of information cannot be found in the execution traces of failing tests. © 2020 Copyright for this paper by its authors.",Association rule mining; Machine learning methods in software testing; Software testing; Test oracle; Test oracle automation,CEUR Workshop Proceedings,2020,,"Duque-Torres A., Shalygina A., Pfahl D., Ramler R.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85093363108,10.1016/j.procs.2020.09.086,Software reliability prediction using package level modularization metrics,"Reliability ensures architectural strength and error free operations of software systems. Software design and architectural measures have been studied as key indicators of faults in software systems. However, association between quantification of reliability prediction using design-level metrics has not been explored. This paper presents a novel approach of developing reliability metrics and it's prediction using package level metrics. In particular, relevant fault severity information is empirically experimented with package level metrics in an effort-aware classification and ranking scenario. Results obtained hint significant view to predict the reliability of software systems using architectural level metrics. Therefore, the empirical analysis can guide development process to be design-focused and avoid accumulation of faults in implementation phase. © 2020 The Authors. Published by Elsevier B.V.",Machine Learning | Reliability | Software Assessment | Software Package Metrics,Procedia Computer Science,2020-01-01,Conference Paper,"Serban, Camelia;Shaikh, Mohsin",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85088525339,10.1007/978-3-030-58802-1_25,Software Defect Prediction on Unlabelled Datasets: A Comparative Study,"Background: Defect prediction on unlabelled datasets is a challenging and widespread problem in software engineering. Machine learning is of great value in this context because it provides techniques - called unsupervised - that are applicable to unlabelled datasets. Objective: This study aims at comparing various approaches employed over the years on unlabelled datasets to predict the defective modules, i.e. the ones which need more attention in the testing phase. Our comparison is based on the measurement of performance metrics and on the real defective information derived from software archives. Our work leverages a new dataset that has been obtained by extracting and preprocessing its metrics from a C++ software. Method: Our empirical study has taken advantage of CLAMI with its improvement CLAMI+ that we have applied on high energy physics software datasets. Furthermore, we have used clustering techniques such as the K-means algorithm to find potentially critical modules. Results: Our experimental analysis have been carried out on 1 open source project with 34 software releases. We have applied 17 ML techniques to the labelled datasets obtained by following the CLAMI and CLAMI+ approaches. The two approaches have been evaluated by using different performance metrics, our results show that CLAMI+ performs better than CLAMI. The predictive average accuracy metric is around 95% for 4 ML techniques (4 out of 17) that show a Kappa statistic greater than 0.80. We applied K-means on the same dataset and obtained 2 clusters labelled according to the output of CLAMI and CLAMI+. Conclusion: Based on the results of the different statistical tests, we conclude that no significant performance differences have been found in the selected classification techniques. © 2020, Springer Nature Switzerland AG.",Deep Neural Networks | Functional Failure Rate (FFR) | Gate-level Circuit Abstraction | GraphSAGE (Graph Based Neural Network) | Single Event Transient (SET) and Soft Errors | Single Event Upset (SEU),"2020 9th Mediterranean Conference on Embedded Computing, MECO 2020",2020-06-01,Conference Paper,"Balakrishnan, Aneesh;Lange, Thomas;Glorieux, Maximilien;Alexandrescu, Dan;Jenihhin, Maksim",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85091395185,,Time-series approaches to change-prone class prediction problem,"During the development and maintenance of a large software project, changes can occur due to bug fix, code refactoring, or new features. In this scenario, the prediction of change-prone classes can be very useful in guiding the development team since it can focus its efforts on these pieces of software to improve their quality and make them more flexible for future changes. A considerable number of related works uses machine learning techniques to predict change-prone classes based on different kinds of metrics. However, the related works use a standard data structure, in which each instance contains the metric values for a particular class in a specific release as independent variables. Thus, these works are ignoring the temporal dependencies between the instances. In this context, we propose two novel approaches, called Concatenated and Recurrent, using time-series in order to keep the temporal dependence between the instances to improve the performance of the predictive models. The Recurrent Approach works for imbalanced datasets without the need for resampling. Our results show that the Area Under the Curve (AUC) of both proposed approaches has improved in all evaluated datasets, and they can be up to 23.6% more effective than the standard approach in state-of-art. Copyright © 2020 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved",Change-prone Class; Deep Learning; Machine Learning; Recurrent Algorithm; Time-series,ICEIS 2020 - Proceedings of the 22nd International Conference on Enterprise Information Systems,2020,,"Melo C.S., da Cruz M.M.L., Martins A.D.F., da Silva Monteiro Filho J.M., de Castro Machado J.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85075542870,10.1007/978-3-030-52856-0_20,Prevalence of machine learning techniques in software defect prediction,"Software Defect Prediction (SDP) is a popular research area which plays an important role for software quality. It works as an indicator of whether a software module is defect-free or defective. In this study, a review has been conducted from January 2015 to August 2019 and 165 articles are selected in the area of SDP to know the prevalence of Machine Learning (ML) techniques. These articles are collected by searching in Google Scholar, and they are published in various platforms (e.g., IEEE, Springer, Elsevier). Firstly the information has been extracted from the collected particles, and then the information has been pre-processed, categorized, visualized, and finally, the results have been reported. The result shows the most frequently used data sets, classifiers, performance metrics, and techniques in SDP. This investigation will help to find the prevalence of ML techniques in SDP and give a quick view to understand the trends of ML techniques in defect prediction research. © ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2020.",data mining | enterprise resilience | machine learning | predictive data-analytics,IEEE Intelligent Systems,2019-05-01,Article,"Xu, Donna;Tsang, Ivor W.;Chew, Eng K.;Siclari, Cosimo;Kaul, Varun",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85050998097,10.1002/smr.2290,IMNRFixer: A hybrid approach to alleviate class-imbalance problem for predicting the fixability of Non-Reproducible bugs,"Software maintenance is an important phase in the software development life cycle. Software projects maintain bug repositories to gather, organize, and keep track of bug reports. These bug reports are resolved by numerous software developers. Whenever the reported bug does not get resolved by the assigned developer, he marks the resolution of bug report as Non-Reproducible (NR). When NR bugs are reconsidered, few of them get resolved, and their resolution changes from NR to fix (NRF). The main aim of this paper is to predict these fixable NRF bug reports. A major challenge in predicting NRF bugs from NR bugs is that only a small portion of NR bugs get fixed, i.e., class-imbalance problem. For example, NRF bugs account for only 8.64%, 4.73 %, 4.56%, and 1.06% in NetBeans, Eclipse, Open Office, and Mozilla Firefox projects respectively. In this paper, we work on improving the classification performance on these imbalanced datasets. We propose IMNRFixer, a novel and hybrid NRF prediction tool. IMNRFixer uses three different techniques to combat class-imbalance problem: undersampling, oversampling, and ensemble models. We evaluate the performance of IMNRFixer models on four large and open-source projects of Bugzilla repository. Our results show that IMNRFixer outperforms conventional machine learning techniques. IMNRFixer achieves performance up to 71.7%, 93.1%, 91.7%, and 96.5% while predicting the minority class (NRF) for NetBeans, Eclipse, Open Office, and Mozilla Firefox projects, respectively. © 2020 John Wiley & Sons, Ltd.",class imbalance | ensemble | Fault priority | fault slippage | inspections reviews | machine learning | part of speech | sampling,"Proceedings - 2017 International Conference on Machine Learning and Data Science, MLDS 2017",2017-07-02,Conference Paper,"Singh, Maninder;Walia, Gursimran S.;Goswami, Anurag",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85051007543,10.5373/JARDCS/V12SP3/20201399,Classifiers recommendation system for overlapped software defect prediction using multi-label framework,"The recent research issue is the quality of software defect prediction (SDP) data set which is available publicly. The major quality issues in software defect prediction data set are class imbalance and class overlap. The class overlap increases the difficulty for the classifiers to learn and predict the defectiveness of source files accurately. In this paper our primary goal is to recommend the classifier for overlapped Software Defect Data sets (SDP). The recommendation of classifiers is based Multi label framework on which we use the meta learning extracted from defect data sets. The results obtained shows that the our proposed framework provides the recommendation of classifiers suitable for new instances in SDP data set that are highly overlapped. © 2020, Institute of Advanced Scientific Research, Inc.. All rights reserved.",class imbalance | faults | feature sets | inspection reviews | machine learning | part of speech | sampling,"Proceedings - 2017 International Conference on Machine Learning and Data Science, MLDS 2017",2017-07-02,Conference Paper,"Singh, Maninder;Walia, Gursimran S.;Goswami, Anurag",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84991769772,10.3233/MGS-200321,Software defect prediction based on weighted extreme learning machine,"The uncertainty of developers' activity can lead to engineering problems such as increased software defects during software development. Therefore, advanced approaches to discovering software defects are needed to improve software systems by software practitioners. This paper describes a novel framework named Weighted Supervised-And-Unsupervised Extreme Learning Machine (WSAU-ELM) including the construction of supervised weighted extreme learning machine for software defect prediction (WELM-SDP) and unsupervised weighted extreme learning machine with spectral clustering for software defect prediction (WELMSC-SDP) that can perform significantly better than the previous software prediction methods. The key advantages of this proposed work are: (i) both the two algorithms can reveal the better learning capability and computational efficiency; (ii) the supervised prediction algorithm is more precisely and faster to handle data sets than the common models, and save more time and resources for software companies; (iii) the unsupervised prediction algorithm can increase accuracy compared to the current method; (iv) the paper also discusses the software defect priority for the defective data, and provides the detailed priority levels that is not discussed before. Experimental results on the benchmark data sets show that the proposed framework is not only more effectively than the existing works, but also can extend the study by the priority analysis of software defects. © 2020-IOS Press and the authors. All rights reserved.",extreme learning machine | pattern recognition | power quality events | smart grid | wavelet transform,"2016 21st International Conference on Methods and Models in Automation and Robotics, MMAR 2016",2016-09-22,Conference Paper,"Ucar, Ferhat;Alcin, Omer Faruk;Dandil, Besir;Ata, Fikret",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85112588113,10.1007/978-3-030-43192-1_90,Test Case Minimization for Object Oriented Testing Using Random Forest Algorithm,"Software maintenance is one of the most costly and crucial phases in the life cycle of software. It consumes almost 70% of the resources and cost of the software. Software testing aims to execute or examine the software with the intention of detecting the faults in it. Reducing the cost of the testing process is one of the major concerns of the testers. With the growing complexities in Object Oriented (OO) software, the number of faults present in the software module is increased. In this paper, a technique has been presented for minimizing the test cases for the OO systems. A case study of Xerces 1.4 open source software is carried for the evaluation of proposed technique. The mathematical model used in the proposed methodology was generated using the open source software WEKA. The approach is based on selecting significant Object Oriented metrics. Highly Efficient, Less efficient or inefficient Object Oriented metrics were identified by the techniques based on feature selection. Test case generation and minimization is achieved on the basis of coverage of highly fault prone classes. To minimize the test cases, proposed methodology used only significant OO metrics for assigning weights to the test paths. The proposed work promisingly reduced the cost and time taken during test suite minimization. © Springer Nature Switzerland AG 2020.",,IEEE Network,2021-05-01,Article,"Manias, DImitrios Michael;Shami, Abdallah",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85083439338,10.1007/978-3-030-37309-2_24,Adversarial Samples for Improving Performance of Software Defect Prediction Models,"Software defect prediction (SDP) is a valuable tool since it can help to software quality assurance team through predicting defective code locations in the software testing phase for improving software reliability and saving budget. This leads to growth in the usage of machine learning techniques to SDP. However, the imbalanced class distribution within SDP datasets is a severe problem for conventional machine learning classifiers, since result in the models with poor performance. Over-sampling the minority class is one of the good solutions to overcome the class imbalance issue. In this paper, we propose a novel over-sampling method, which trained a generative adversarial nets (GANs) to generate synthesized data aimed for output mimicked minority class samples, which were then combined with training data into an increased training dataset. In the tests, we investigated ten freely accessible defect datasets from the PROMISE repository. We assessed the performance of our offered method by comparing it with standard over-sampling techniques including SMOTE, Random Over-sampling, ADASYN, and Borderline-SMOTE. Based on the test results, the proposed method provides better mean performance of SDP models among all tested techniques. © 2020, Springer Nature Switzerland AG.",Class imbalance | Generative Adversarial Nets | Over-sampling | Software defect prediction,Lecture Notes on Data Engineering and Communications Technologies,2020-01-01,Book Chapter,"Eivazpour, Z.;Keyvanpour, Mohammad Reza",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85067804872,10.1007/978-981-15-0751-9_75,A Support Vector Machine Based Approach for Effective Fault Localization,"Software maintenance is one of the most costly activities in software life cycle. It costs almost 70% of the total cost of the software. Testing aims to reveal the faults from the software under test (SUT). The fault localization is tiresome, dull, costly but crucial for program debugging. As size and complexity of software increase, manual locating faults becomes very tedious and hence necessitates automatic fault localization. If the fault proneness of the software components can be predicted, then such components may be given more focus. Such approach would not only save time but also enhance the quality of the software. Support vector machine (SVM) is a prominent machine learning algorithm. Regularization of parameters, convex optimization and kernel tricks are the prevailing features of SVM. This work reports a SVM-based framework for fault localization on the basis of code smells. Paper presents a performance analysis against four popular algorithms, namely ZeroR, OneR, Naive Bayes and Decision Stump. The proposed model is empirically evaluated in the reference of Json project. The results of the experimentation show that the proposed model can effectively classify the instances in the classes of their respective categories of code smells. Also, the kernel used in the proposed model gives better performance than counterpart kernels and the proposed model itself performs better than the other compared algorithms in terms of accuracy, precision, recall and F-measure. © 2020, Springer Nature Singapore Pte Ltd.",Affective Computing | cost-sensitive learning | data mining | defect escalation | Machine learning | machine learning | Modeling and prediction | Natural language | Sentiment analysis | software defect escalation prediction,IEEE Software,2019-09-01,Article,"Werner, Colin;Li, Ze Shi;Damian, Daniela",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85111024778,10.1007/978-3-030-39237-6_12,A Conceptual Framework for Software Fault Prediction Using Neural Networks,"Software testing is a very expensive and critical activity in the software systems’ life-cycle. Finding software faults or bugs is also time-consuming, requiring good planning and a lot of resources. Therefore, predicting software faults is an important step in the testing process to significantly increase efficiency of time, effort and cost usage. In this study we investigate the problem of Software Faults Prediction (SFP) based on Neural Network. The main contribution is to empirically establish the combination of Chidamber and Kemer software metrics that offer the best accuracy for faults prediction with numeric estimations by using feature selection. We also proposed a conceptual framework that integrates the model for fault prediction. © 2020, Springer Nature Switzerland AG.",,IEEE Software,2022-01-01,Article,"Tuzun, Eray;Erdogmus, Hakan;Baldassarre, Maria Teresa;Felderer, Michael;Feldt, Robert;Turhan, Burak",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85079889209,10.1145/3374549.3374553,Classification based software defect prediction model for finance software system - An industry study,"Automated software defect prediction is an important and fundamental activity in the domain of software development. Successful software defect prediction can save testing effort thus reduce the time and cost for software development. However, software systems for finance company are inherently large and complex with numerous interfaces with other systems. Thus, identifying and selecting a good model and a set of features is important but challenging problem. In our paper, we first define the problem we want to solve. Then we propose a prediction model based on binary classification and a set of novel features, which is more specific for finance software systems. We collected 15 months real production data and labelled it as our dataset. The experiment shows our model and features can give a better prediction accuracy for finance systems. In addition, we demonstrate how our prediction model helps improve our production quality further. Unlike other research papers, our proposal focuses to solve problem in real finance industry. © 2019 Association for Computing Machinery.",Faulty change | Finance system | Machine learning | Software defect prediction,ACM International Conference Proceeding Series,2019-12-09,Conference Paper,"Zong, Liang",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84957103031,10.1109/FIT47737.2019.00073,On fault localization using machine learning techniques,"Early prediction of faulty modules provides a way to support software quality assurance activities through improved scheduling and effectiveness of process control. Different fault prediction models have been proposed by researchers for early prediction of faults. However, despite predicting faulty classes, these techniques don't provide information about identifying the location of occurrence of faults. In this paper, we aim to propose a methodology not only to predict faults but also for fault localization as well. We first make use of various datasets to build a software fault prediction model based. We make use of random forest machine learning technique to train our model. We also extract CK-metrics from different modules and we then make use of these metrics for fault localization. In order to do validation, we make use of case studies where we do fault prediction, fault localization and then run test cases to see if our prediction was correct. © 2019 IEEE.",Computer bugs | Context | Data mining | History | Mathematical model | Software | XML,IEEE International Working Conference on Mining Software Repositories,2015-08-04,Conference Paper,"Zanjani, Motahareh Bahrami;Kagdi, Huzefa;Bird, Christian",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85026548340,10.1145/3373477.3373486,Software code analysis using ensemble learning techniques,"Ensuing the advent of advancements in software systems, the probability of them containing high severity defects is exponentially on the rise. With each technological addition, the complexity of software is increasing. Reproduction and rectification of a defect requires time and effort. Current state of the art analysis tools cater to the investigation of static aspects of a production level code. However, it is imperative to assess the dynamic development process of a system so as to be able to timely detect erroneous components early on in the development life cycle of a software. A novel automated defect prediction feature enhancement is proposed that analyses the static structure of the current code and state of the software in past releases to extract relevant static and dynamic feature sets. Data generated is modelled for defect trends in the future release of the software by four ensemble classifiers. Results demonstrate the superiority of Voting algorithm for the problem of defect prediction. © 2019 Association for Computing Machinery.",Completion Time Prediction | Machine Learning | Mining Software Repositories | Release Planning,IEEE International Working Conference on Mining Software Repositories,2017-06-29,Conference Paper,"Dehghan, Ali;Neal, Adam;Blincoe, Kelly;Linaker, Johan;Damian, Daniela",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85075512918,,An evolutionary approach for software defect prediction on high dimensional data using subspace clustering and machine learning,"Since last decade, due to increasing demand, huge amount of software is being developed, whereas the data intensive applications have also increased the complexity in these types of systems. Also, during the development process, software bugs may severely impact the growth of industries. Hence, the development of bug free software application is highly recommended in the real-time systems. Several approaches have been developed recently that are based on the manual inspection but those techniques are not recommended for huge software development scenario due to maximum chances of error during manual inspection. Thus, machine learning based data mining techniques has gained huge attraction from researchers due to their analyzing and efficiently detect the defect by learning the different attributes. In this work, we present machine learning based approach for software defect prediction. However, software defect datasets suffer from the high dimensionality issues, thus we present a novel subspace clustering approach using evolutionary computation based optimal solution identification for dimension reduction. Later, Support Vector Machine Classification scheme is implemented to obtain the defect prediction performance. Proposed approach is implemented using MATLAB simulation tool by considering NASA software defect dataset. A comparative study is presented which shows that proposed approach achieves better performance when compared with the existing techniques. © 2005 – ongoing JATIT & LLS.",Evolutionary Computation; High Dimensional Data; Machine Learning; Software Defect Prediction; Subspace Clustering,Journal of Theoretical and Applied Information Technology,2019,,"Patil S., Nagaraja Rao A., Shoba Bindu C.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85085502552,10.1109/ISPCC48220.2019.8988366,Learning from Software defect datasets,"Development of high-quality software is very much essential now-a-days. Prediction of good quality software in the early phase during the development reduces the cost of the testing resources. Various data mining and machine learning algorithms are developed to predict the quality of the software. In the real-life scenarios while dealing with software modules many of the time the underline datasets are imbalanced. In order to increase the efficiency of the prediction or classification algorithms balancing algorithms are implemented as a preprocessing stage before the prediction phase. In this paper we conduct extensive experiments to explore the effect of imbalance learning including undersampling, oversampling and hybrid methods and its interaction with different types of classifiers on various projects. We evaluate six imbalance learning methods with six classifiers on 12 data sets. The imbalance learning methods used are Random Oversampling (ROS), Random under-sampling, SMOTE, TOMEK, SMOTE+TOMEK, and SMOTE+ENN. This study reveals that the appropriate combination of imbalanced method and classifier can improve the accuracy of software fault prediction models. © 2019 IEEE.",classification | imbalancing | Software Fault Prediction,"Proceedings of IEEE International Conference on Signal Processing,Computing and Control",2019-10-01,Conference Paper,"Singh, Pradeep",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85072306720,10.1109/ISSREW.2019.00099,An empirical comparison of two different strategies to automated fault detection: Machine learning versus dynamic analysis,"Software testing is an established method to ensure software quality and reliability, but it is an expensive process. In recent years, the automation of test case generation has received significant attention as a way to reduce costs. However, the oracle problem (a mechanism for determine the (in) correctness of an executed test case) is still major problem which has been largely ignored. Recent work has shown that building a test oracle using the principles of anomaly detection techniques (mainly semisupervised/ unsupervised learning models based on dynamic execution data consisting of an amalgamation of input/output pairs and execution traces) is able to demonstrate a reasonable level of success in automatically detect passing and failing execution [1], [2]. In this paper, we present a comparison study between our machine-learning based approaches and an existing techniques from the specification mining domain (the data invariant detector Daikon [3]). The two approaches are evaluated on a range of midsized systems and compared in terms of their fault detection ability. The results show that in most cases semi-supervised learning techniques perform far better as an automated test classifier than Daikon. However, there is one system for which our strategy struggles and Daikon performed far better. Furthermore, unsupervised learning techniques performed on a par when compared with Daikon in several cases. © 2019 IEEE.",Machine learning | Stackoverflow | Topic modeling,IEEE International Working Conference on Mining Software Repositories,2019-05-01,Conference Paper,"Bangash, Abdul Ali;Sahar, Hareem;Chowdhury, Shaiful;Wong, Alexander William;Hindle, Abram;Ali, Karim",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85072331325,10.1109/CRC.2019.00028,Empirical and comparative study of various classifiers with forecast deformity prone software models,"The main principle thought of this research is to give a general outline about Deformity Prone Software Datasets Models utilizing machine learning classifiers. Deformity Prone Software Datasets Models are also classification problems so it is needed to used Classifiers and analysis the defected datasets models. The evaluation measure unit is used to evaluate the performance of defect prone model datasets. TP-Rate, F-Measure, ROC and CCI these we have used as evaluation measure unit. We have used NASA PROMISE repository Models as Forecast Deformity Prone Software Models. We have selected 17 NASA PROMISE repositories. These datasets files all are with class interests which are Defective and Non-Defective Class. In this research paper, our class interest is already fixed because we are interested in Defective Class so Defective Class is assign. A Comparative study of these classifiers utilized inside the Deformity Prone Software Models are also covered in this research. Experimental Analysis results showed that stacking is worst classifiers and cannot enhanced the efficiency and accuracy of Deformity Prone Software Datasets Models but LMT, Multiclass, Navie Bayes Updateable and Multilayer Perceptron have increased the positive accuracy of defected models and enhanced the efficiency in correctly classified instances. © 2019 IEEE.",Assisted code review | Code style | Decision tree forest | Interpretable machine learning,IEEE International Working Conference on Mining Software Repositories,2019-05-01,Conference Paper,"Markovtsev, Vadim;Long, Waren;Mougard, Hugo;Slavnov, Konstantin;Bulychev, Egor",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85073719181,10.35940/ijitee.K1233.09811S19,Genetic evolutionary learning fitness function (GELFF) for feature diagnosis to software fault prediction,"Nowadays, proper feature selection f+orFault prediction is very perplexing task. Improper feature selection may lead to bad result. To avoid this, there is a need to find the aridity of software fault. This is achieved by finding the fitness of the evolutionaryAlgorithmic function. In this paper, we finalize the Genetic evolutionarynature of our Feature set with the help of Fitness Function. Feature Selection is the objective of the prediction model tocreate the underlying process of generalized data. The wide range of data like fault dataset, need the better objective function is obtained by feature selection, ranking, elimination and construction. In this paper, we focus on finding the fitness of the machine learning function which is used in the diagnostics of fault in the software for the better classification. © BEIESP.",Chromosomes | Crossover | Fault diagnostics | Feature extraction | Feature Selection | Fitness function | Genetic evolutionary Learning Fitness Function (GELFF) | Genetic evolutionary Programming (GEP) | Machine Learning (ML) | Mutations | Parents,International Journal of Innovative Technology and Exploring Engineering,2019-09-01,Article,"Patchaiammal, P.;Thirumalaiselvi, R.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85098224175,10.18576/amis/13s121,Investigation Analysis for Software Fault Prediction Using Error Probabilities and Integral Methods,"Abstract: In this paper, in-depth analysis of faults in the code phase is detected through integral methods that identify the error in software. The repositories of the data set are collected during the software product development life cycle model, which is then integrated with a machine learning algorithm namely Bayesian decision theory to detect the error probabilities and to predict unbound error during the prediction of the software faults. In prior, the faults are predicted in repository for a given data set using error probability and error integral method that identify the probability of error and correction, which is then applied with Gaussian method to find the levels of the error probability with minimum and maximum integral of acceptable faults in the repository.",error probability | integral method | repository mining | Software fault prediction,Applied Mathematics and Information Sciences,2019-08-01,Article,"Karuppusamy, S.;Giraldo, Eduardo",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85069945166,,Prediction of cost and defects in software development using bayesian and subspace algorithms,"Software Development is the discipline of initiating, organizing, executing, controlling and completing the project work of a group to accomplish target and meet progress. Prediction of software defects plays an important role while building high quality software. Machine learning algorithms are utilized in software development for better Performance. Machine learning algorithms have proven to be of great practical value in a variety of application domains. They are particularly useful for poorly understood problem domains where little learning exists to develop powerful algorithms and for the domains where there are expensive databases containing valuable implicit regularities to be discovered. Machine learning is a kind of Artificial Intelligence (AI) that enables programming applications to end up more exact in expectation results. The main objective of this paper is to predict the cost and defects of a project or an application in an efficient manner by applying machine learning algorithms. The Bayesian and subspace algorithms are implemented to predict the defects and to make decisions whether the project can be continued or not. Two algorithms are compared and the results are exhibited by applying on software defect data set. © BEIESP.",Bayesian algorithm; Machine learning; Software development; Subspace algorithm,International Journal of Engineering and Advanced Technology,2019,,"Kotha S.K., Sodagudi S., Anuradha T.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85065607053,10.1109/AICAI.2019.8701345,Defect Prediction based on Machine Learning using System Test Parameters,"Since its inception, software testing has evolved over the years. In this era of Artificial Intelligence Machine learning, there are many recent advancements in Software Engineering. Various software engineering metrics are analyzed; deciphered and the required predictions are made. Defect prediction is one such activity which is of great significance in improving the software quality which helps software developers and testers to focus on the modules which are more likely to defect prone. In the literature survey, we found several techniques to predict defects based on features which were mostly captured from the development perspective. In this paper, we propose our machine learning based approach with the main objective of finding potential areas of the defects by considering parameters from System Testing matrices a unique 'parameter called Component Dependency score'. This method also helps in controlling the software quality of a dynamically evolving software. © 2019 IEEE.",,"Proceedings - 2019 Amity International Conference on Artificial Intelligence, AICAI 2019",2019-04-26,Conference Paper,"Sutar, Shantanu;Kumar, Rajesh;Pai, Sriram;Shwetha, B. R.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85080874348,10.1007/978-3-030-37352-8_13,Software defect prediction model based on GA-BP algorithm,"The novel software defect prediction model based on GA-BP algorithm was proposed in the paper considering the disadvantage of traditional BP (abbreviated for Back Propagation) neural network, which has the problem of easy to fall into local optimization when constructing software defect prediction model, and finally affects the prediction accuracy. Firstly, the optimization ability of GA (abbreviated for Genetic Algorithms) is introduced to optimize the weights and thresholds of Back Propagation neural network. Then the prediction model was constructed based on the GA-BP. Meanwhile the public dataset MDP from NASA was selected and the tool WEKA was used to clean the data and format conversion and as the result, four datasets is available. In the end, experimental results show that the proposed method in the paper is effective for software defect prediction. © 2019, Springer Nature Switzerland AG.",Arcing Fault Detection | Distribution Networks | Power System Protection | Renewable Energy,"51st North American Power Symposium, NAPS 2019",2019-10-01,Conference Paper,"Hashmy, Yousaf;Cui, Qiushi;Ma, Zhihao;Weng, Yang",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85071478421,10.18293/DMSVIVA2019-022,An empirical analysis on effectiveness of source code metrics for aging related bug prediction,"—Reliability of any software plays a very important role to assess the quality of the software systems. Reliability analysis and evaluation during early phases of software development life cycle (SDLC) significantly help developer and analysts in the proper allocation of limited resources during testing and maintenance process. The goal of this work is to develop one model using the measurement of the internal structure of the software system i.e., source code metrics for predicting the reason of failure in software during continuously running for a certain time i.e., software aging. Aging-Related Bugs are related with failure during the execution of the software, that leads to degradation in quality, system crashing, misuse of resources, etc.. In this paper, seven different sets of software metrics, seven model training algorithms, one data sampling technique have been empirically investigated and evaluated for predicting aging-related bugs classes. The trained aging-related bugs prediction models are validated using 5-fold cross-validation techniques. The final observation of this experiment work is assessed over seven open source application software systems. The high-value of performance parameters confirm the predicting capability of data sampling, sets of metrics, and training algorithms to predict aging-related bugs classes. ©DMSVIVA 2019: 25th International DMS Conference on Visualization and Visual Languages.All right reserved.",Feature selection | Index Terms—Aging | Machine Learning | Software Engineering | Source Code Metrics,Proceedings - DMSVIVA 2019: 25th International DMS Conference on Visualization and Visual Languages,2019-01-01,Conference Paper,"Hota, Chinmay;Kumar, Lov;Murthy Neti, Lalita Bhanu",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85070598505,10.1109/CONFLUENCE.2019.8776946,Analyzing the effectiveness of machine learning algorithms for determining faulty classes: A comparative analysis,"The quality of the software can be improved by determining its faulty portions in the initial phases of the lifecycle of a software product. There are various machine learning algorithms proposed in literature studies that can be used to predict faulty classes. The machine learning algorithms determine faulty classes by using object oriented metrics as predictors. These models will allow the developers to predict faulty classes and concentrate the constraint resources in testing these weaker portions of the software. This study evaluates and compares the predictive capability of six machine learning algorithms amongst themselves and with logistic regression, a statistical algorithm for determining faulty portions of a software. The results are validated using seven open source software. © 2019 IEEE.",Fault Prediction | Machine Learning Algorithms | Object-Oriented Metrics,"Proceedings of the 9th International Conference On Cloud Computing, Data Science and Engineering, Confluence 2019",2019-01-01,Conference Paper,"Singh, Prabhpahul;Malhotra, Ruchika;Bansal, Siddartha",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85123493289,10.1007/978-981-13-1747-7_53,A phase-wise fault prediction using soft computing,"Software testing is performed to increase quality of the software and also increase the customer satisfaction. In today’s scenario, most of the work is dependent upon software and need of quality software in minimum time. If the fault is not found in the early development phase is having more chance of failure rate in a later phase. Since the failure data or test data are not available in requirement, design, coding and testing phase. For this goal, the phase-wise approach is proposed using soft computing to find the fault in software development life cycle (SDLC) phase. In proposed approach, top reliability-based software metrics/quality attributes for fault prediction is selected from the phases of SDLC. In the proposed approach, four fuzzy inference systems are designed for SDLC phases. The given result from the proposed approach is validated with ten real software datasets from promise datasets. The predicted fault results are near to actual dataset fault. The result from proposed approach is useful for a software developer team, design team, and testing team to plan and take decision for their respective phases of development. © Springer Nature Singapore Pte Ltd. 2019.",Logistic Regression | Machine Learning | Neural Networks | Simulation-based Fault Injection,"2021 IEEE Nordic Circuits and Systems Conference, NORCAS 2021 - Proceedings",2021-01-01,Conference Paper,"Lu, Li;Chen, Junchao;Breitenreiter, Anselm;Schrape, Oliver;Ulbricht, Markus;Krstic, Milos",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85069470069,10.1109/PDGC.2018.8745804,Deep learning for software defect prediction in time,"Various machine learning techniques for defect prediction are characterize by their performance in within project and cross project environment by taking into consideration F-measure, Recall and Precision. This paper will include the comparative study of various machine learning techniques for software defect prediction in time. Data collected from different research papers, websites and books shows the practical applicability of various machine learning techniques for software defect prediction in time. © 2018 IEEE.",Comparison | Deep Learning | Defect Prediction | Machine Learning,"PDGC 2018 - 2018 5th International Conference on Parallel, Distributed and Grid Computing",2018-12-01,Conference Paper,"Yadav, Monika;Singh, Vijendra;Rastogi, Priyanka",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85015690290,10.23919/SOFTCOM.2018.8555818,Improving Software Fault Prediction with Threshold Values,"Software fault prediction has been studied by many researchers to assess the quality of software and to predict where faults may appear in future. However, the performance of fault prediction degrades because of many reasons including unlabeled instances or data imbalance, i.e., modules that contain faults are minority. The data imbalance is common in fault data where the majority of software modules are marked as non-faulty. However, part of these modules are still fault-prone but faults are uncovered yet. Threshold values are used to identify the modules that are complex and more fault prone. The fault prediction models are combined with the use of threshold values to improve the prediction performance. Fault prediction models are built in two phases. First, threshold values are used to spot the most fault prone modules. The modules that have metrics larger than thresholds and were fault free are classified as medium, while modules with faults are classified as high. Second, the new data are used to build prediction models using five machine learning models. Five classifiers were built for ten software systems. We have found improvements in the classification performance of all classifiers when compared with traditional classification. © 2018 University of Split, FESB.",LabVIEW | PHM | Reconfigurable | System development | Systematic approach | Watchdog aegnt toolkit,"Proceedings of 2016 Prognostics and System Health Management Conference, PHM-Chengdu 2016",2017-01-16,Conference Paper,"Shi, Zhe;Lee, Jay;Cui, Peng",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85055755951,,An empirical study to evaluate the relationship of object-oriented metrics and change proneness,"Software maintenance deals with changes or modifications which software goes through. Change prediction models help in identification of classes/modules which are prone to change in future releases of a software product. As change prone classes are probable sources of defects and modifications, they represent the weak areas of a product. Thus, change prediction models would aid software developers in delivering an effective quality software product by allocating more resources to change prone classes/modules as they need greater attention and resources for verification and meticulous testing. This would reduce the probability of defects in future releases and would yield a better quality product and satisfied customers. This study deals with the identification of change prone classes in an Object-Oriented (OO) software in order to evaluate whether a relationship exists between OO metrics and change proneness attribute of a class. The study also compares the effectiveness of two sets of methods for change prediction tasks i.e. the traditional statistical methods (logistic regression) and the recently widely used machine learning methods like Bagging, Multi-layer perceptron etc. © 2018, Zarka Private University. All rights reserved.",Change proneness; Empirical validation; Machine learning; Object-oriented and software quality,International Arab Journal of Information Technology,2018,,"Malhotra R., Khanna M.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85078493174,10.1109/ICCSEC.2017.8446897,A defect detection technology based on software behavior decision tree,"With the increase of software system's size and complexity, a high requirement of the reliability, stability and security of software quality is put forward. At present, Machine learning technology is adopted in defect detection to realize code scanning and semantic analysis on software defects. The traditional machine learning technology for software defect detection is generally based on algorithms such as BP neural network model, naive Bayes model, and fingerprint identification model, etc. Regarding the features of software defect detection, this paper proposes a layered detection technology based on software behavior decision tree model. Furthermore, a corresponding test environment is established to make contrast test of previously tested software. The results of the experiment shows that, with the comprehensive consideration of building time cost and false alarm rate and other factors, the defect detection technology based on software behavior decision tree model is superior to other technologies. © 2017 IEEE.",Diversity | Image classification | Machine learning | Reliability | Software fault-tolerance,"Proceedings of IEEE Pacific Rim International Symposium on Dependable Computing, PRDC",2019-12-01,Conference Paper,"Machida, Fumio",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85060994348,10.1145/3236454.3236455,Buffer overflow detection for c programs is hard to learn,"Machine learning has been used to detect bugs such as buffer overflow [6, 8]. Models get trained to report bugs at the function or file level, and reviewers of the results have to eyeball the code to determine whether there is a bug in that function or file, or not. Contrast this to static code analysers which report bugs at the statement level along with traces [3, 7], easing the effort required to review the reports. Based on our experience with implementing scalable and precise bug finders in the Parfait tool [3], we experiment with machine learning to understand how close the techniques can get to a precise static code analyser. In this paperwe summarise our finding in using ML techniques to find buffer overflow in programs written in C language.We treat bug detection as a classification problem.We use feature extraction and train a model to determine whether a buffer overflow has occurred or not at the function level. Training is done over labelled data used for regression testing of the Parfait tool.We evaluate the performance of different classifiers using the 10-fold cross-validation and the leave-one-out strategy. To understand the generalisability of the trained model, we use it on a collection of unlabelled real-world programs and manually check the reported warnings. Our experiments show that, even though the models give good results over training data, they do not perform that well when faced with larger, unlabelled data. We conclude with open questions that need addressing before machine learning techniques can be used for buffer overflow detection. Copyright © 2018 by the Association for Computing Machinery, Inc.",,Companion Proceedings for the ISSTA/ECOOP 2018 Workshops,2018-07-16,Conference Paper,"Zhao, Yang;Du, Xingzhong;Krishnan, Paddy;Cifuentes, Cristina",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85015293873,10.1145/3239576.3239622,Automated parameter tuning of artificial neural networks for software defect prediction,"Defect prediction can help predict defect-prone software modules and improve the efficiency and accuracy of defect location and repair, which plays an extremely important role in software quality assurance. Artificial Neural Networks (ANNs), a family of powerful machine learning regression or classification models, have been widely applied for defect prediction. However, the performance of these models will be degraded if they use suboptimal default parameter settings (e.g., the number of units in the hidden layer). This paper utilizes an automated parameter tuning technique-Caret to optimize parameter settings. In our study, 30 datasets are downloaded from the Tera-PROMISE Repository. According to the characteristics of the datasets, we select key features (metrics) as predictors to train defect prediction models. The experiment applies feed-forward, single hidden layer artificial neural network as classifier to build different defect prediction models respectively with optimized parameter settings and with default parameter settings. Confusion matrix and ROC curve are used for evaluating the quality of the models above. The results show that the models trained with optimized parameter settings outperform the models trained with default parameter settings. Hence, we suggest that researchers should pay attention to tuning parameter settings by Caret for ANNs instead of using suboptimal default settings if they select ANNs for training models in the future defect prediction studies. © 2018 Association for Computing Machinery.",High-level parallel programming frameworks | Iterative algorithms | MapReduce,"Proceedings of PyHPC 2016: 6th Workshop on Python for High-Performance and Scientific Computing - Held in conjunction with SC16: The International Conference for High Performance Computing, Networking, Storage and Analysis",2017-01-30,Conference Paper,"Lund, Jeffrey;Ashcraft, Chace;McNabb, Andrew;Seppi, Kevin",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85044588917,10.1142/S0218194018500110,Predicting Code Hotspots in Open-Source Software from Object-Oriented Metrics Using Machine Learning,"Software engineers are able to measure the quality of their code using a variety of metrics that can be derived directly from analyzing the source code. These internal quality metrics are valuable to engineers, but the organizations funding the software development effort find external quality metrics such as defect rates and time to develop features more valuable. Unfortunately, external quality metrics can only be calculated after costly software has been developed and deployed for end-users to utilize. Here, we present a method for mining data from freely available open source codebases written in Java to train a Random Forest classifier to predict which files are likely to be external quality hotspots based on their internal quality metrics with over 75% accuracy. We also used the trained model to predict hotspots for a Java project whose data was not used to train the classifier and achieved over 75% accuracy again, demonstrating the method's general applicability to different projects. © 2018 World Scientific Publishing Company.",classification | data mining | hotspot prediction | java | machine learning | object-oriented metrics | open source | quality metrics | random forest | Software engineering | software quality,International Journal of Software Engineering and Knowledge Engineering,2018-03-01,Article,"Hilton, Rod;Gethner, Ellen",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84991824134,10.1109/ICIIECS.2017.8276059,A novel probabilistic-ABC based boosting model for software defect detection,"As the development platforms in software technology grow rapidly; the development of software systems has become more and more complex and versatile. It is quite necessary for the software industries to develop software products with good qualities. As the size of the product and development technologies increasing, there is need for identification of software defects at the early stage of development phase. Detection of defects appropriately and accurately measured in the early stage to increase the quality of the product. Software metric can be defined as building blocks of prediction models. The evaluation of software metric is carried out in the design and coding phase prior to actual implementation of the product. Defects are described as a type of conditions that does not satisfy minimum requirement and this can lead to malfunction or unexpected outcomes. The numbers of defects are inversely proportional with the quality which means, on decreasing numbers of defects will result better software quality. Detection of these defects at the early stage of development life cycle will reduce the cost and effort to a great extent. Since decades many researchers have proposed different models for detection of defects in software modules. In this paper we emphasized on defect detection approaches using machine learning scheme to improve true positive rate. Prediction metrics are basically split into two categories, those are:-code metrics and process metrics. In this paper we have designed and implemented a novel defect prediction model using probabilistic ABC based classification model. We compared our model with various defect detection approaches in terms of statistical measures. Experimental results proved that proposed model has high computational detection rate compared to traditional models. © 2017 IEEE.",Coverage | Machine learning | Mutation Score | Redundancy | Software testing,"Proceedings - 2016 IEEE International Conference on Software Quality, Reliability and Security-Companion, QRS-C 2016",2016-09-21,Conference Paper,"Felbinger, Hermann;Wotawa, Franz;Nica, Mihai",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85052536544,10.1007/978-981-10-8633-5_34,An approach for improving classification accuracy using discretized software defect data,"Predicting software defects in software systems at early stages of its development has always been a very crucial and desirable aspect of software development industry. Today the machine learning algorithms are playing a massive role in classifying and predicting the possible bugs during the design phase. In this research work, the authors have proposed a discretization method based on metrics threshold values in order to gain better classification accuracy on a given data set. For the experimentation purpose, the authors have chosen the defect data sets from NASA repositories. In this Jedit, Lucene, Tomcat, Velocity, Xalan, Xerces software systems have been considered for experimentation using WEKA. The authors have also considered object-oriented CK metrics specifically for the study. Two very common and popular classifiers namely Naive Bayes and voted perceptron for the classification purpose. In the proposed work, various performance measures like ROC, RMSE values have been considered and analyzed. The results show that classification accuracy improvements can be made while using proposed discretization method with both classifiers. © Springer Nature Singapore Pte Ltd. 2018",Complex network | Defect prediction | Machine learning | Software metrics,"Proceedings - 2018 IEEE 18th International Conference on Software Quality, Reliability, and Security Companion, QRS-C 2018",2018-08-09,Conference Paper,"Yang, Yiwen;Ai, Jun;Wang, Fei",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85045854437,10.1007/978-3-319-77028-4_68,Increasing the Prediction Quality of Software Defective Modules with Automatic Feature Engineering,"This paper reviews the main concepts related to software testing, its difficulties and the impossibility of a complete software test. Then, it proposes an approach to predict which module is defective, aiming to assure the usually limited software test resources will be wisely distributed to maximize the coverage of the modules most prone to defects. The used approach employs the recently proposed Kaizen Programming (KP) to automatically discover high-quality nonlinear combinations of the original features of a database to be used by the classification technique, replacing a human in the feature engineering process. Using a NASA open dataset with Software metrics of over 9500 modules, the experimental analysis shows that the new features can significantly boost the detection of detective modules, allowing testers to find 216% more defects than with a random module selection; this is also an improvement of 1% when compared to the original features. © 2018, Springer International Publishing AG, part of Springer Nature.",Feature engineering | Machine learning | Software defect prediction | Software test | Test effort,Advances in Intelligent Systems and Computing,2018-01-01,Conference Paper,"Nascimento, Alexandre Moreira;de Melo, Vinícius Veloso;Dias, Luiz Alberto Vieira;da Cunha, Adilson Marques",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85099358626,10.1007/978-981-10-5699-4_1,Designing ANFIS model to predict the reliability of component-based system,"Predicting reliability of any product is always a desire of quality-oriented industry. The fault-free working of products depends on large number of parameters, and designing a machine learning model that can predict the reliability considering these as input parameters will help to plan testing and maintenance of the product. In this paper, ANFIS approach is adopted to train a model that can predict reliability of component-based software system. The parameters considered for designing the model are standard design metrics which are evaluated for quality benchmarking during software development process. © 2018, Springer Nature Singapore Pte Ltd.",machine learning | online prediction | server crash | software reliability,"Proceedings - Companion of the 2020 IEEE 20th International Conference on Software Quality, Reliability, and Security, QRS-C 2020",2020-12-01,Conference Paper,"Zou, Zhuoliang;Ai, Jun",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85099374821,10.1109/ICRAS.2017.8071941,Impact of defect velocity at class level,"Predicting software defect at the class level is important in defect prediction studies. In addition, class level defect prediction enables software team to have an idea on the possible number of defects in an upcoming product release prior to testing. Considerable effort is needed to investigate factors which influence the number of defects at the class level of a software product. In this paper, we investigate few factors, namely, time and defect velocity in relation to defect density of a class using a defect density correlation technique. An experiment was conducted on 69 open source Java projects containing 131,034 classes. Results show that the defect density achieved a correlation coefficient of 61.41%, defect introduction time achieved a correlation coefficient of-11.413%, and the defect velocity achieved 93.56%, respectively. Defect velocity is positively correlated with the number of defects at the class level. © 2017 IEEE.",Adversarial Machine Learning | Attack Libraries | Model Inference and Perturbation and Evasion Attacks | Requirements Elicitation Using Threat Modeling | Security Requirements Engineering | STRIDE,"Proceedings - Companion of the 2020 IEEE 20th International Conference on Software Quality, Reliability, and Security, QRS-C 2020",2020-12-01,Conference Paper,"Wilhjelm, Carl;Younis, Awad A.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85027448730,10.1109/IICIP.2016.7975323,Stepping towards dynamic measurement for object oriented software,"Software metrics are very helpful in measuring the different aspects of software like cohesion, coupling, polymorphism, inheritance etc. The objective of measuring software metrics are quality assurance, defect prediction, maintainability prediction, cost estimation, debugging, etc. Many authors proposed the use of static metrics for the software maintainability prediction (SMP) and were successful, but static metrics don't take into account the run-time behavior of software. Hence, to capture this dynamic behavior, dynamic metrics are necessary to be evaluated. This paper presents the empirical investigation of dynamic metrics for SMP and also compares them with static metrics. Six machine learning algorithms are used to build the prediction models for both the static and dynamic metrics. The performance of all models is compared using prevalent accuracy measures. Results show that dynamic metrics perform better than static metrics, and can be used as a sound alternative for SMP. © 2016 IEEE.",machine learning algorithms | prediction | software maintainability | software maintenance | software metrics,"India International Conference on Information Processing, IICIP 2016 - Proceedings",2017-07-11,Conference Paper,"Jain, Ashu;Chug, Anuradha",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85106777134,10.1007/978-981-10-3935-5_16,Exploration of Machine Learning Techniques for Defect Classification,"To develop good quality software product, there is a need of continuous defect identification and classification in each module before delivering a software product to the customer. Developing software needs proper managing of the available software resources. To deliver a software product on time, developing quality software products, Information Technology (IT) industries normally use software tools for defect detection. Based on severity, defects are detected and classified. This can be automated to reduce the development time and cost. Nowadays, machine learning algorithms have been applied by many researchers to accurately classify the defects. In this paper, a novel software defect detection and classification method is proposed and neural network models such as Probabilistic Neural Network model (PNN) and Generalized Regression Neural Network (GRNN) are integrated to identify, classify the defects from large software repository. Defects are classified into three layers based on the severity in the proposed method abstraction layer, core layer, and application layer. The performance accuracy of the proposed model is compared with MLP and J48 classifiers. © Springer Nature Singapore Pte Ltd. 2017.",Defect classification | Defect tracking | Neural network models | Software defect | Software quality,Lecture Notes in Networks and Systems,2017-01-01,Book Chapter,"Ajay Prakash, B. V.;Ashoka, D. V.;Manjunath Aradya, V. N.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84962129143,10.4018/978-1-5225-3923-0.ch007,Important issues in software fault prediction: A road map,"Quality assurance tasks such as testing, verification and validation, fault tolerance, and fault prediction play a major role in software engineering activities. Fault prediction approaches are used when a software company needs to deliver a finished product while it has limited time and budget for testing it. In such cases, identifying and testing parts of the system that are more defect prone is reasonable. In fact, prediction models are mainly used for improving software quality and exploiting available resources. Software fault prediction is studied in this chapter based on different criteria that matters in this research field. Usually, there are certain issues that need to be taken care of such as different machine-learning techniques, artificial intelligence classifiers, variety of software metrics, distinctive performance evaluation metrics, and some statistical analysis. In this chapter, the authors present a roadmap for those researchers who are interested in working in this area. They illustrate problems along with objectives related to each mentioned criterion, which could assist researchers to build the finest software fault prediction model. © 2018 by IGI Global. All rights reserved.",Anomaly Detection Systems | Boolean Combination | Intrusion Detection Systems | Multiple-Detector Systems | Pruning Techniques,"Proceedings - 2015 IEEE International Conference on Software Quality, Reliability and Security, QRS 2015",2015-09-21,Conference Paper,"Soudi, Amirreza;Khreich, Wael;Hamou-Lhadj, Abdelwahab",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84961328724,,Data-augmented software diagnosis,"The task of software diagnosis algorithms is to identify which software components are faulty, based on the observed behavior of the system. Software diagnosis algorithms have been studied in the Artificial Intelligence community, using a model-based and spectrum-based approaches. In this work we show how software fault prediction algorithms, which have been studied in the software engineering literature, can be used to improve software diagnosis. Software fault prediction algorithms predict which software components is likely to contain faults using machine learning techniques. The resulting dataaugmented diagnosis algorithm we propose is able to overcome of key problems in software diagnosis algorithms: ranking diagnoses and distinguishing between diagnoses with high probability and low probability. This allows to significantly reduce the outputted list of diagnoses. We demonstrate the efficiency of the proposed approach empirically on both synthetic bugs and bugs extracted from the Eclipse open source project. Results show that the accuracy of the found diagnoses is substantially improved when using the proposed combination of software fault prediction and software diagnosis algorithms.",,CEUR Workshop Proceedings,2015,,"Elmishali A., Stern R., Kalech M.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84927765241,10.1007/978-81-322-2196-8_5,Machine learning and its application in software fault prediction with similarity measures,"Nowadays, the challenge is how to exactly understand and apply various techniques to discover fault from the software module. Machine learning is the process of automatically discovering useful information in knowledgebase. It also provides capabilities to predict the outcome of future solutions. Case-based reasoning is a tool or method to predict error level with respect to LOC and development time in software module. This paper presents some new ideas about process and product metrics to improve software quality prediction. At the outset, it deals with the possibilities of using lines of code and development time from any language may be compared and be used as a uniform metric. The system predicts the error level with respect to LOC and development time, and both are responsible for checking the developer’s ability or efficiency of the developer. Prediction is based on the analogy. We have used different similarity measures to find the best method that increases the correctness. The present work is also credited through introduction of some new terms such as coefficient of efficiency, i.e., developer’s ability and normalizer. In order to obtain the result, we have used indigenous tool. © Springer India 2015.",Development time | LOC | Normalizer | Similarity function | Software fault prediction,Advances in Intelligent Systems and Computing,2015-01-01,Conference Paper,"Rashid, Ekbal;Patnaik, Srikanta;Usmani, Arshad",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84896872767,10.14257/ijseia.2014.8.2.09,Search-based information retrieval and fault prediction with distance functions,"This research paper presents a study towards search-based information retrieval and fault prediction with distance functions. The objective of this research is to minimize software costs. Predict the error in software correctly and use the results in future estimation. Through this prediction technique, we have taken different software metrics as input and give output either the software is fault prone or not fault prone or shows any problem in software module in terms of number of errors. This paper presents a work in which we have extended our previous work [12]. In this paper, we discuss an application of Machine learning to error prediction. We have used five different similarity measures namely Euclidean method, Canberra method, Clark method, Exponential method and a Manhattan method to find the best method that increases accuracy. It is observed that the CBR method using the Exponential distance weighted function yielded the best error prediction. In this paper we have used the terms errors and faults, and no explicit distinction made between errors and faults. This software is compiled using Turbo C++ 3.0 and hence it is very compact and standalone, it can be readily deployed on any lower configuration system and it would not impact its performance, as it does not rely on external runtimes and DLL's like the. NET programs rely on. The software is a console based application and thus does not use the GUI functions of the Operating System, which makes it very fast in execution. In order to obtain a result we have used indigenous tool. © 2014 SERSC.",CBR | Distance functions | Fault prediction,International Journal of Software Engineering and its Applications,2014-03-17,Article,"Rashid, Ekbal;Patnaik, Srikanta;Bhattacherjee, Vandana",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84888421534,10.1007/978-3-319-03095-1_44,Study on Agile Process Methodology and Emergence of Unsupervised Learning to Identify Patterns from Object Oriented System,"Data mining is knowledge extraction for secure software engineering, improves the quality and productivity, poses several challenges, requiring various algorithms to effectively mine text, graph from such database. Fact that building models in the context of the framework one of the task data miners, almost important though all other tasks associated with data mining. Data mining techniques are tackling the right business problem, must understand the data this is available and turn noisy data into data from which we can build robust models. It is important to be aware data mining is really what we might call an agile model. The concept of agility comes from the agile software engineering principles includes increment development of the business requirements and need to check whether the requirement satisfies with the client inputs our testing and rebuilding models improves the performance. For software engineering code execution, code changes list of bugs and requirement engineering our system uses mining techniques to explore valuable data patterns in order to meet better projects inputs and higher quality software systems that delivered on time. Our research uses frequent mining, pattern matching and machine learning applied to agile software architecture model in gathering and also extracting security requirements best effort business rules for novel research. © Springer International Publishing Switzerland 2014.",Agile Model | Architecture & Design Pattern | Data Mining | Software Engineering,Advances in Intelligent Systems and Computing,2014-01-01,Conference Paper,"Narendhar, Mulugu;Anuradha, K.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84937701467,,Bug prediction for fine-grained source code changes,"Software is constructed by a series of changes and each change has a risk of introducing bugs. Building bug prediction models for software changes can help developers know the existence of bugs immediately upon the completion of the change, which allows them to allocate more resources of testing and inspecting on the current risky changes, and to find and fix the introduced bugs timely. In this paper, we present a bug prediction model for fine-grained source code changes based on machine learning method, which takes a fine-grained source code change as a learning instance and a series of properties of the fine-grained change as features. This model has two desirable qualities: 1) Compared with previous research work that building bug prediction models for software changes at the file level or commit level (including one or more files), this model can predict bugs for changes at the statement level, which increases the granularity of prediction and thus reduces manual inspection efforts for developers. 2) This model can help developers or managers gain better knowledge on key factors of bug injection and provide guidance for software change of high quality. From the experiments on 8 famous open source projects, we observe that when using Random Forest as the classifier, the model proposed in this paper achieves the best performance, which can predict bugs for fine-grained source code changes with 78% precision, 71% recall, and 75% F-measure on average. Furthermore, among all the four feature groups (i.e. where, what, who, and when) defined in this paper, where is most influential, which has the strongest discriminative power in predicting bugs. Copyright © 2013 by Knowledge Systems Institute Graduate School.",Bug prediction; Code metrics; Fine-grained source code changes,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",2013,,"Yuan Z., Yu L., Liu C.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84898855478,,Fault prediction for apache open source framework using Chidamber and kemerer metrics suite,"Open source tools are coded in different languages and developed in difierent project management styles. Hence, the quality varies from version to version of any software. Quality can be best measured from the number of faults in the software. This paper presents an approach for fault prediction in case of Apache open source integration framework using Chidamber and Kemerer (CK) Metrics suite. Statistical evaluation (such as liner and logistic regression) and machine learning method (neural network) are used for fault prediction. The six CK Metrics were used in analyzing the framework. This paper emphasizes on performance of CK Metrics values for different versions of the Apache integration framework (AIP). The results achieved highlight the inuence of CK Metrics in fault prediction. © Computer Society of India, 2013.",Camel integration framework; CK metric; Faults prediction; Regression analysis,CONSEG 2013 - Proceedings of the 7th CSI International Conference on Software Engineering,2013,,"Kumar L., Suresh Y., Rath S.Ku.",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85073795911,10.2316/P.2012.780-021,Anticipating software fault proneness using classifier ensemble: An optimize approach,"Prediction of software faults earlier in software development life cycle makes testing process more effective. This enables software testers to concentrate on the modules that are being predicted as defective ones. By doing so, testing effort can be greatly reduced and renders the development of quality product. Different approaches have been proposed so far to predict software module as faulty or non-faulty (defective or non-defective). No doubt existing approaches perform well however there is still a great room for improvement. In this paper, we have proposed a novel approach for predicting software modules, based on combination of different machine learning algorithms and optimizing results by using Genetic algorithm. Performance measures used for evaluation purpose are accuracy of particular classifier in predicting faulty/non faulty module, probability of detection (Pd) and probability of false alarm (Pf). We have made a comparative analysis of our proposed approach with other existing defect prediction approaches.",Deep Learning | TensorFlow program | Testing | Training issues,"Proceedings - 19th IEEE International Conference on Software Quality, Reliability and Security, QRS 2019",2019-07-01,Conference Paper,"Braiek, Houssem Ben;Khomh, Foutse",Include,
10.1016/j.infsof.2022.107128,2-s2.0-84886938932,,Machine learning and value-based software engineering: A research agenda,"Software engineering research and practice thus far are primarily conducted in a value-neutral setting where each artifact in software development such as requirement, use case, test case, and defect, is treated as equally important during a software system development process. There are a number of shortcomings of such value-neutral software engineering. Value-based software engineering is to integrate value considerations into the full range of existing and emerging software engineering principles and practices. Machine learning has been playing an increasingly important role in helping develop and maintain large and complex software systems. However, machine learning applications to software engineering have been largely confined to the value-neutral software engineering setting. In this paper, the general message to be conveyed is to apply machine learning methods and algorithms to value-based software engineering. The training data or the background knowledge or domain theory or heuristics or bias used by machine learning methods in generating target models or functions should be aligned with stakeholders' value propositions. An initial research agenda is proposed for machine learning in value-based software engineering.",Machine learning; Pareto modules; Stakeholder value propositions; Value-based software engineering,"20th International Conference on Software Engineering and Knowledge Engineering, SEKE 2008",2008,,Zhang D.,Include,
10.1016/j.infsof.2022.107128,2-s2.0-85099293301,10.1142/S0218194007003161,Detecting fault modules using bioinformatics techniques,"Many software reliability studies attempt to develop a model for predicting the faults of a software module because the application of good prediction models provides important information on significant metrics that should be observed in the early stages of implementation during software development. In this article we propose a new method inspired by a multi-agent based system that was initially used for classification and attribute selection in microarray analysis. Best classifying gene subset selection is a common problem in the field of bioinformatics. If we regard the software metrics measurement values of a software module as a genome of that module, and the real world dynamic characteristic of that module as its phenotype (i.e. failures as disease symptoms) we can borrow the established bioinformatics methods in the manner first to predict the module behavior and second to data mine the relations between metrics and failures. © World Scientific Publishing Company.",machine learning models | prioritization | Pull requests | pull-based development,"Proceedings - 2020 IEEE 20th International Conference on Software Quality, Reliability, and Security, QRS 2020",2020-12-01,Conference Paper,"Azeem, Muhammad Ilyas;Peng, Qiang;Wang, Qing",Include,
10.1016/j.infsof.2022.107128,2-s2.0-85073921700,10.1002/cae.22166,UTCPredictor: An uncertainty-aware novel teaching cases predictor,"Teaching cases are crucial for computer science (e.g., software testing) teaching. With the fast development of computer science, old and outdated teaching cases cannot meet the requirements of teaching. Therefore, teachers need to update teaching case repository continually and timely. However, teaching case development is an extremely time-consuming work. Given today's complex and fast-moving environment of computer science, teachers often feel blind about about what types of cases should be added for teaching. This paper presents UTCPredictor—an automated approach of predicting novel teaching cases from real production by identifying the most uncertain data, which are always with new features that reflect the latest developments and trends in the field. The implementation of UTCPredictor is based on the idea of interactive machine learning as well as several text mining techniques. To evaluate the effectiveness of UTCPredictor, we take bug report case building in software testing teaching as an example, using UTCPredictor to perform 10-fold cross-validation on an existing teaching case set. The performance in terms of indicators; Recall, Precision, and F1-score, achieved three very competitive values—0.91, 0.94, and 0.85, respectively. We further evaluate the effectiveness of UTCPredictor through a user study and a questionnaire. The results are very positive; the user study indicates that educators can build a teaching case set from latest bug report repository by spending only 8.16%–18.11% time costs compared with traditional manual approach; the responses from 2,000 students for the questionnaire show that the teaching cases built with UTCPredictor are very popular among students. © 2019 Wiley Periodicals, Inc.",interactive machine learning | novelty cases prediction | teaching case building | uncertainty sampling,Computer Applications in Engineering Education,2019-11-01,Article,"Wu, Xiaoxue;Zheng, Wei;Mu, Dejun;Li, Ning",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85016594110,10.1002/smr.1859,Using discriminative feature in software entities for relevance identification of code changes,"Developers often bundle unrelated changes (eg, bug fix and feature addition) in a single commit and then submit a “poor cohesive” commit to version control system. Such a commit consists of multiple independent code changes and makes review of code changes harder. If the code changes before commit can be identified as related and unrelated ones, the “cohesiveness” of a commit can be guaranteed. Inspired by the effectiveness of machine learning techniques in classification field, we model the relevance identification of code changes as a binary classification problem (ie, related and unrelated changes) and propose discriminative feature in software entities to characterize the relevance of code changes. In particular, to quantify the discriminative feature, 21 coupling rules and 4 cochanged type relationships are elaborately extracted from software entities to construct related changes vector (RCV). Twenty-one coupling rules at granularities of class, attribute, and method can capture the relevance of code changes from structural coupling dimension, and 4 cochanged type relationships are defined to capture the change type combinations of software entities that may cause related changes. Based on RCV, machine learning algorithms are applied to identify the relevance of code changes. The experiment results show that probabilistic neural network and general regression neural network provide statistically significant improvements in accuracy of relevance identification of code changes over the other 4 machine learning algorithms. Related changes vector with 72 dimensions (RCV72) outperforms other 2RCVs with less dimensions. Copyright © 2017 John Wiley & Sons, Ltd.",cochanged types | coupling rules | discriminative feature | relevance of code changes,Journal of Software: Evolution and Process,2017-07-01,Article,"Huang, Yuan;Chen, Xiangping;Liu, Zhiyong;Luo, Xiaonan;Zheng, Zibin",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85071235277,10.1002/smr.2181,Watch out for this commit! A study of influential software changes,"One single code change can significantly influence a wide range of software systems and their users. For example, (a) adding a new feature can spread defects in several modules, while (b) changing an API method can improve the performance of all client programs. Unfortunately, developers often may not clearly know whether code changes are influential at commit time. This paper investigates influential software changes and proposes an approach to identify them immediately when they are applied. Our goals are to (a) identify existing influential changes (ICs) in software projects, (b) understand their characteristics, and (c) build a classification model of ICs to help developers find and address them early. We first conduct a post-mortem analysis to discover existing influential changes by using intuitions (eg, changes referred by other changes). Then, we re-categorize all identified changes through an open-card sorting process. Subsequently, we conduct a survey with about 100 developers to finalize a taxonomy. Finally, from our ground truth, we extract features, including metrics such as the complexity of changes and file centrality in co-change graphs to build machine learning classifiers. The experiment results show that our classification model with random samples achieves 86.8% precision, 74% recall, and 80.4% F-measure, respectively. © 2019 John Wiley & Sons, Ltd.",change prediction | change risk | influential change | software changes | software evolution,Journal of Software: Evolution and Process,2019-12-01,Article,"Li, Daoyuan;Li, Li;Kim, Dongsun;Bissyandé, Tegawendé F.;Lo, David;Le Traon, Yves",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85076089201,10.1002/smr.2238,A machine learning approach for classification of equivalent mutants,"Mutation testing is a fault-based technique to test the quality of test suites by inducing artificial syntactic faults or mutants in a source program. However, some mutants have the same semantics as original program and cannot be detected by any test suite input known as equivalent mutants. Equivalent mutant problem (EMP) is undecidable as it requires manual human effort to identify a mutant as equivalent or killable. The constraint-based testing (CBT) theory suggests the use of mathematical constraints which can help reveal some equivalent mutants using mutant features. In this paper, we consider three metrics of CBT theory, ie, reachability, necessity, and sufficiency to extract feature constraints from mutant programs. Constraints are extracted using program dependency graphs. Other features such as degree of significance, semantic distance, and information entropy of mutants are also extracted to build a binary classification model. Machine learning algorithms such as Random Forest, GBT, and SVM are applied under two application scenarios (split-project and cross-project) on ten Java programs to predict equivalent mutants. The analysis of the study demonstrates that that the proposed techniques not only improves the efficiency of the equivalent mutant detection but also reduces the effort required to perform it with small accuracy loss. © 2019 John Wiley & Sons, Ltd.",equivalent mutants | machine learning | mutation testing | program semantics | static analysis,Journal of Software: Evolution and Process,2020-05-01,Article,"Naeem, Muhammad Rashid;Lin, Tao;Naeem, Hamad;Liu, Hailu",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84955649457,10.1002/stvr.1575,Assessing and generating test sets in terms of behavioural adequacy,"Identifying a finite test set that adequately captures the essential behaviour of a program such that all faults are identified is a well-established problem. This is traditionally addressed with syntactic adequacy metrics (e.g. branch coverage), but these can be impractical and may be misleading even if they are satisfied. One intuitive notion of adequacy, which has been discussed in theoretical terms over the past three decades, is the idea of behavioural coverage: If it is possible to infer an accurate model of a system from its test executions, then the test set can be deemed to be adequate. Despite its intuitive basis, it has remained almost entirely in the theoretical domain because inferred models have been expected to be exact (generally an infeasible task) and have not allowed for any pragmatic interim measures of adequacy to guide test set generation. This paper presents a practical approach to incorporate behavioural coverage. Our BESTEST approach (1) enables the use of machine learning algorithms to augment standard syntactic testing approaches and (2) shows how search-based testing techniques can be applied to generate test sets with respect to this criterion. An empirical study on a selection of Java units demonstrates that test sets with higher behavioural coverage significantly outperform current baseline test criteria in terms of detected faults. © 2015 The Authors. Software Testing, Verification and Reliability published by John Wiley & Sons, Ltd.",search-based software testing | test adequacy | test generation,Software Testing Verification and Reliability,2015-12-01,Article,"Fraser, Gordon;Walkinshaw, Neil",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85055679867,10.1007/978-3-030-00374-6_13,Measuring the quality of machine learning and optimization frameworks,"Software frameworks are daily and extensively used in research, both for fundamental studies and applications. Researchers usually trust in the quality of these frameworks without any evidence that they are correctly build, indeed they could contain some defects that potentially could affect to thousands of already published and future papers. Considering the important role of these frameworks in the current state-of-the-art in research, their quality should be quantified to show the weaknesses and strengths of each software package. In this paper we study the main static quality properties, defined in the product quality model proposed by the ISO 25010 standard, of ten well-known frameworks. We provide a quality rating for each characteristic depending on the severity of the issues detected in the analysis. In addition, we propose an overall quality rating of 12 levels (ranging from A+ to D−) considering the ratings of all characteristics. As a result, we have data evidence to claim that the analysed frameworks are not in a good shape, because the best overall rating is just a C+ for Mahout framework, i.e., all packages need to go for a revision in the analysed features. Focusing on the characteristics individually, maintainability is by far the one which needs the biggest effort to fix the found defects. On the other hand, performance obtains the best average rating, a result which conforms to our expectations because frameworks’ authors used to take care about how fast their software runs. © Springer Nature Switzerland AG 2018.",Maintainability | Performance | Quality | Reliability | Security,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,Conference Paper,"Villalobos, Ignacio;Ferrer, Javier;Alba, Enrique",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85058619119,10.1007/978-3-030-05063-4_42,Anomaly detection and diagnosis for container-based microservices with performance monitoring,"With emerging container technologies, such as Docker, microservices-based applications can be developed and deployed in cloud environment much agiler. The dependability of these microservices becomes a major concern of application providers. Anomalous behaviors which may lead to unexpected failures can be detected with anomaly detection techniques. In this paper, an anomaly detection system (ADS) is designed to detect and diagnose the anomalies in microservices by monitoring and analyzing real-time performance data of them. The proposed ADS consists of a monitoring module that collects the performance data of containers, a data processing module based on machine learning models and a fault injection module integrated for training these models. The fault injection module is also used to assess the anomaly detection and diagnosis performance of our ADS. Clearwater, an open source virtual IP Multimedia Subsystem, is used for the validation of our ADS and experimental results show that the proposed ADS works well. © Springer Nature Switzerland AG 2018.",Anomaly detection | Machine learning | Microservices | Performance monitoring,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,Conference Paper,"Du, Qingfeng;Xie, Tiandi;He, Yu",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85064900689,10.1007/978-3-030-16145-3_25,Deepreview: Automatic code review using deep multi-instance learning,"Code review, an inspection of code changes in order to identify and fix defects before integration, is essential in Software Quality Assurance (SQA). Code review is a time-consuming task since the reviewers need to understand, analysis and provide comments manually. To alleviate the burden of reviewers, automatic code review is needed. However, this task has not been well studied before. To bridge this research gap, in this paper, we formalize automatic code review as a multi-instance learning task that each change consisting of multiple hunks is regarded as a bag, and each hunk is described as an instance. We propose a novel deep learning model named DeepReview based on Convolutional Neural Network (CNN), which is an end-to-end model that learns feature representation to predict whether one change is approved or rejected. Experimental results on open source projects show that DeepReview is effective in automatic code review tasks. In terms of F1 score and AUC, DeepReview outperforms the performance of traditional single-instance based model TFIDF-SVM and the state-of-the-art deep feature based model Deeper. © Springer Nature Switzerland AG 2019.",Automatic code review | Machine learning | Multi-instance learning | Software mining,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Li, Heng Yi;Shi, Shu Ting;Thung, Ferdian;Huo, Xuan;Xu, Bowen;Li, Ming;Lo, David",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85067802043,10.1007/978-3-030-22744-9_20,Recognizing Faults in Software Related Difficult Data,"In this paper we have investigated the use of numerous machine learning algorithms, with emphasis on multilayer artificial neural networks in the domain of software source code fault prediction. The main contribution lies in enhancing the data pre-processing step as the partial solution for handling software related difficult data. Before we put the data into an Artificial Neural Network, we are implementing PCA (Principal Component Analysis) and k-means clustering. The data-clustering step improves the quality of the whole dataset. Using the presented approach we were able to obtain 10% increase of accuracy of the fault detection. In order to ensure the most reliable results, we implement 10-fold cross-validation methodology during experiments. We have also evaluated a wide range of hyperparameter setups for the network, and compared the results to the state of the art, cost-sensitive approaches - Random Forest, AdaBoost, RepTrees and GBT. © 2019, Springer Nature Switzerland AG.",ANN | Data clustering | Faults detection | Pattern recognition,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Choraś, Michał;Pawlicki, Marek;Kozik, Rafał",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85072849560,10.1007/978-3-030-27455-9_1,Search-Based Predictive Modelling for Software Engineering: How Far Have We Gone?,"In this keynote I introduce the use of Predictive Analytics for Software Engineering (SE) and then focus on the use of search-based heuristics to tackle long-standing SE prediction problems including (but not limited to) software development effort estimation and software defect prediction. I review recent research in Search-Based Predictive Modelling for SE in order to assess the maturity of the field and point out promising research directions. I conclude my keynote by discussing best practices for a rigorous and realistic empirical evaluation of search-based predictive models, a condicio sine qua non to facilitate the adoption of prediction models in software industry practices. © Springer Nature Switzerland AG 2019.",Machine learning | Predictive analytics | Predictive modelling | Search-based software engineering | Software analytics,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Sarro, Federica",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85075565228,10.1007/978-3-030-30923-7_13,A Study of Learning Data Structure Invariants Using Off-the-shelf Tools,"Data structure invariants play a key role in checking correctness of code, e.g., a model checker can use an invariant, e.g., acyclicity of a binary tree, that is written in the form of an assertion to search for program executions that violate it, e.g., erroneously introduce a cycle in the structure. Traditionally, the properties are written manually by the users. However, writing them manually can itself be error-prone, which can lead to false alarms or missed bugs. This paper presents a controlled experiment on applying a suite of off-the-shelf machine learning (ML) tools to learn properties of dynamically allocated data structures that reside on the program heap. Specifically, we use 10 data structure subjects, and systematically create training and test data for 6 ML methods, which include decision trees, support vector machines, and neural networks, for binary classification, e.g., to classify input structures as valid binary search trees. The study reveals two key findings. One, most of the ML methods studied – with off-the-shelf parameter settings and without fine tuning – achieve at least 90% accuracy on all of the subjects. Two, high accuracy is achieved even when the size of the training data is significantly smaller than the size of the test data. We believe future work can utilize the learnt invariants to automate dynamic and static analyses, thereby enabling advances in machine learning to further enhance software testing and verification techniques. © Springer Nature Switzerland AG 2019.",Data structure invariants | Korat | Machine learning,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Usman, Muhammad;Wang, Wenxi;Wang, Kaiyuan;Yelen, Cagdas;Dini, Nima;Khurshid, Sarfraz",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85075603666,10.1007/978-3-030-31129-2_76,Using Resampling Techniques with Heterogeneous Stacking Ensemble for Mobile App Stores Reviews Analytics,"Over the past few years, a boom in the popularity of mobile devices and mobile apps has appeared. More than 205 billion apps were downloaded in 2018. Developers directly distribute mobile apps to end users via a centralized platform like the “App Store” for iOS or the “Play Store” for Android. The Mobile app developers get continuous feedback from users’ reviews added to these stores. Tools like CLAP or AR-MINER were used to crawl reviews from the stores and try to classify them into lots of classifications like (Bug, required feature, usability issue, performance issue,) to facilitate the categorization of issues or features addition to the developer. Some machine learning techniques is used to get the most accurate data classification to help the developer to classify the reported reviews on the stores. This paper presents a machine learning model that uses the Resampling techniques for handling imbalanced classes in addition to ensemble learning and stacking. The model outperforms those tools and enhances the results applied on Mobile App Stores Reviews Analytics. In addition to that the paper provides experiments applied on different kinds of datasets and showed improvements in accuracy from 85% (previous model) to 90% (our model) and ROC from 96% to 98% especially on the Reviews dataset. © Springer Nature Switzerland AG 2020.",Classification | Ensemble | Heterogeneous | Machine learning | Mobile app reviews | SMOTE | Software engineering | Stacking,Advances in Intelligent Systems and Computing,2020-01-01,Conference Paper,"Gomaa, Ahmed;El-Shorbagy, Sara;El-Gammal, Wael;Magdy, Mohamed;Abdelmoez, Walid",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85076635963,10.1007/978-3-030-33607-3_12,The prevalence of errors in machine learning experiments,"Context: Conducting experiments is central to research machine learning research to benchmark, evaluate and compare learning algorithms. Consequently it is important we conduct reliable, trustworthy experiments. Objective: We investigate the incidence of errors in a sample of machine learning experiments in the domain of software defect prediction. Our focus is simple arithmetical and statistical errors. Method: We analyse 49 papers describing 2456 individual experimental results from a previously undertaken systematic review comparing supervised and unsupervised defect prediction classifiers. We extract the confusion matrices and test for relevant constraints, e.g., the marginal probabilities must sum to one. We also check for multiple statistical significance testing errors. Results: We find that a total of 22 out of 49 papers contain demonstrable errors. Of these 7 were statistical and 16 related to confusion matrix inconsistency (one paper contained both classes of error). Conclusions: Whilst some errors may be of a relatively trivial nature, e.g., transcription errors their presence does not engender confidence. We strongly urge researchers to follow open science principles so errors can be more easily be detected and corrected, thus as a community reduce this worryingly high error rate with our computational experiments. © 2019, Springer Nature Switzerland AG.",Classifier | Computational experiment | Error | Reliability,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Shepperd, Martin;Guo, Yuchen;Li, Ning;Arzoky, Mahir;Capiluppi, Andrea;Counsell, Steve;Destefanis, Giuseppe;Swift, Stephen;Tucker, Allan;Yousefi, Leila",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85083433363,10.1007/978-3-030-34706-2_5,Software defect prediction using bad code smells: A systematic literature review,"The challenge of effective refactoring in the software development cycle brought forward the need to develop automated defect prediction models. Among many existing indicators of bad code, code smells have attracted particular interest of both the research community and practitioners in recent years. In this paper, we describe the current state-of-the-art in the field of bug prediction with the use of code smells and attempt to identify areas requiring further research. To achieve this goal, we conducted a systematic literature review of 27 research papers published between 2006 and 2019. For each paper, we (i) analysed the reported relationship between smelliness and bugginess, as well as (ii) evaluated the performance of code smell data used as a defect predictor in models developed using machine learning techniques. Our investigation confirms that code smells are both positively correlated with software defects and can positively influence the performance of fault detection models. However, not all types of smells and smell-related metrics are equally useful. God Class, God Method, Message Chains smells and Smell intensity metric stand out as particularly effective. Smells such as Inappropriate Intimacy, Variable Re-assign, Clones, Middle Man or Speculative Generality require further research to confirm their contribution. Metrics describing the introduction and evolution of anti-patterns in code present a promising opportunity for experimentation. © Springer Nature Switzerland AG 2020.",,Lecture Notes on Data Engineering and Communications Technologies,2020-01-01,Book Chapter,"Piotrowski, Paweł;Madeyski, Lech",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85076980668,10.1007/978-3-030-34995-0_44,Deep-Learning-Based Computer Vision System for Surface-Defect Detection,"Automating optical-inspection systems using machine learning has become an interesting and promising area of research. In particular, the deep-learning approaches have shown a very high and direct impact on the application domain of visual inspection. This paper presents a complete inspection system for automated quality control of a specific industrial product. Both hardware and software part of the system are described, with machine vision used for image acquisition and pre-processing followed by a segmentation-based deep-learning model used for surface-defect detection. The deep-learning model is compared with the state-of-the-art commercial software, showing that the proposed approach outperforms the related method on the specific domain of surface-crack detection. Experiments are performed on a real-world quality-control case and demonstrate that the deep-learning model can be successfully used even when only 33 defective training samples are available. This makes the deep-learning method practical for use in industry where the number of available defective samples is limited. © 2019, Springer Nature Switzerland AG.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2019-01-01,Conference Paper,"Tabernik, Domen;Šela, Samo;Skvarč, Jure;Skočaj, Danijel",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85078479877,10.1007/978-3-030-35510-4_6,Semantic Similarities in Natural Language Requirements,"Semantic similarity information supports requirements tracing and helps to reveal important requirements quality defects such as redundancies and inconsistencies. Previous work has applied semantic similarity algorithms to requirements, however, we do not know enough about the performance of machine learning and deep learning models in that context. Therefore, in this work we create the largest dataset for analyzing the similarity of requirements so far through the use of Amazon Mechanical Turk, a crowd-sourcing marketplace for micro-tasks. Based on this dataset, we investigate and compare different types of algorithms for estimating semantic similarities of requirements, covering both relatively simple bag-of-words and machine learning models. In our experiments, a model which relies on averaging trained word and character embeddings as well as an approach based on character sequence occurrences and overlaps achieve the best performances on our requirements dataset. © Springer Nature Switzerland AG 2020.",Machine learning | Requirements engineering | Similarity detection,Lecture Notes in Business Information Processing,2020-01-01,Conference Paper,"Femmer, Henning;Müller, Axel;Eder, Sebastian",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85081616752,10.1007/978-3-030-41418-4_3,Distortion and faults in machine learning software,"Machine learning software, deep neural networks (DNN) software in particular, discerns valuable information from a large dataset, a set of data, so as to synthesize approximate input-output relations. The outcomes of such DNN programs are dependent on the quality of both learning programs and datasets. However, the quality assurance of DNN software is difficult. The trained machine learning models, defining the functional behavior of the approximate relations, are unknown prior to its development, and the validation is conducted indirectly in terms of the prediction performance. This paper introduces a hypothesis that faults in DNN programs manifest themselves as distortions in trained machine learning models. Relative distortion degrees measured with appropriate observer functions may indicate that the programs have some hidden faults. The proposal is demonstrated with the cases of the MNIST dataset. © Springer Nature Switzerland AG 2020.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020-01-01,Conference Paper,"Nakajima, Shin",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85085213137,10.1007/978-3-030-47358-7_32,Evaluation of a Failure Prediction Model for Large Scale Cloud Applications,"Modern cloud-based applications, including smart homes and cities require high levels of reliability and availability. All cloud services, including hardware and software experience failures because of their large scale and heterogeneity nature. In this paper, the main objective is to develop a failure prediction model that can early detect failed jobs. The advantage of the proposed model is to enhance resource utilization and to increase the efficiency of cloud applications. The proposed model is evaluated based on three public available traces, which are the Google cluster, Mustang, and Trinity. Moreover, four different machine learning algorithms have been applied to the traces in order to select the best accurate model. Furthermore, we have improved the prediction accuracy using different feature selection techniques. The evaluation results show that the proposed model has achieved a high rate of precision, recall, and f1-score. © Springer Nature Switzerland AG 2020.",Failure prediction | Fault tolerance | Google cluster trace | Mustang trace | Trinity trace,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020-01-01,Conference Paper,"Jassas, Mohammad S.;Mahmoud, Qusay H.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85088741109,10.1007/978-3-030-51156-2_132,Machine Learning Approach for Automatic Categorization of Service Support Requests on University Information Management System,"The bug reports and service support requests provided by users to software developers serve as an important source of information for software maintenance. For large software projects, the number of users involved may be high. As a result, large number of service support requests may be generated. In this study, we present a machine learning approach for automatic categorization of service support requests on university information management system of Izmir Katip Celebi University. We have collected a text corpus, which contains 17,831 bug reports and service support requests. On the corpus, preprocessing stages, such as, tokenization, stop word filtering and stemming have been employed. To represent text documents, TF-IDF term weighting scheme has been utilized in conjunction with 1-gram model. In the empirical analysis, five conventional classification algorithms (i.e., Naïve Bayes, k-nearest neighbor algorithm, C4.5 decision tree algorithm, random forest algorithm and support vector machines) have been taken into consideration. The experimental results indicate that the presented machine learning scheme can yield promising results on assigning service support requests to one of the related modules, as student information system, personnel information system, electronic document management system and scientific research projects system. We achieved a classification accuracy of 92.26% with support vector machines. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG.",Bug reports | Machine learning | Software requirements | Text mining,Advances in Intelligent Systems and Computing,2021-01-01,Conference Paper,"Onan, Aytuğ;Atik, Erdem;Yalçın, Adnan",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85092608318,10.1007/978-3-030-58817-5_66,Entropy Based Machine Learning Models for Software Bug Severity Assessment in Cross Project Context,"There can be noise and uncertainty in the bug reports data as the bugs are reported by a heterogeneous group of users working across different countries. Bug description is an essential attribute that helps to predict other bug attributes, such as severity, priority, and time fixes. We need to consider the noise and confusion present in the text of the bug report, as it can impact the output of different machine learning techniques. Shannon entropy has been used in this paper to calculate summary uncertainty about the bug. Bug severity attribute tells about the type of impact the bug has on the functionality of the software. Correct bug severity estimation allows scheduling and repair bugs and hence help in resource and effort utilization. To predict the severity of the bug we need software project historical data to train the classifier. These training data are not always available in particular for new software projects. The solution which is called cross project prediction is to use the training data from other projects. Using bug priority, summary weight and summary entropy, we have proposed cross project bug severity assessment models. Results for proposed summary entropy based approach for bug severity prediction in cross project context show improved performance of the Accuracy and F-measure up to 70.23% and 93.72% respectively across all the machine learning techniques over existing work. © 2020, Springer Nature Switzerland AG.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2020-01-01,Conference Paper,"Kumari, Madhu;Singh, Ujjawal Kumar;Sharma, Meera",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85097427859,10.1007/978-3-030-62144-5_1,Quality Management of Machine Learning Systems,"In the past decade, Artificial Intelligence (AI) has become a part of our daily lives due to major advances in Machine Learning (ML) techniques. In spite of an explosive growth in the raw AI technology and in consumer facing applications on the internet, its adoption in business applications has conspicuously lagged behind. For business/mission-critical systems, serious concerns about reliability and maintainability of AI applications remain. Due to the statistical nature of the output, software ‘defects’ are not well defined. Consequently, many traditional quality management techniques such as program debugging, static code analysis, functional testing, etc. have to be reevaluated. Beyond the correctness of an AI model, many other new quality attributes, such as fairness, robustness, explainability, transparency, etc. become important in delivering an AI system. The purpose of this paper is to present a view of a holistic quality management framework for ML applications based on the current advances and identify new areas of software engineering research to achieve a more trustworthy AI. © 2020, Springer Nature Switzerland AG.",AI Engineering | Artificial Intelligence | Machine learning | Quality management,Communications in Computer and Information Science,2020-01-01,Conference Paper,"Santhanam, P.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85101832822,10.1007/978-3-030-68851-6_29,Software Developer Recommendation in Terms of Reducing Bug Tossing Length,"In close software development, it is easy for the project manager to recommend the right developer to resolve a bug that is reported by an end-user. However, in the case of open-source software developments, where most developers are engaged on different project either on the same or different repositories. Due to their agile involvement on repositories, bug triaging might be slow and increases the Bug Tossing Length (BTL) which is encounter as the time between reporting and resolving bugs. In open-source software repositories like GitHub, numerous developers are involved with well-known projects to resolve the issue reported by end-users. The assignment of the reported bug to an appropriate developer may lead to a reduced BTL time. Though, several metrics based and Machine Learning (ML) based approaches have been introduced to recommend the appropriate developer on the bases of several parameters. However, few studies are related to the recommendation of developers on the bases of their historical information regarding their attempts to reduce the BTL. To address this issue, we have proposed a new approach to recommend a developer for bug triaging on the bases of their involvement in reducing the BTL. In the proposed study, the model is trained once and new bug reports are automatically assigned to relevant developers. In this regard, we exploit the proposed methodology through using the XGBoost, Support Vector Machine, Random Forest, Decision Tree, KNearest Neighbor, and Naïve Bayes for the recommendation of the developer for a reported bug. We used widely-known two datasets namely Eclipse, and Mozilla. The experimental result indicate the effectiveness of proposed methodology in terms of developer recommendation for a new reported bug. © 2021, Springer Nature Switzerland AG.",Bugs report | Github | Machine learning | Open source software | Recommendation,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2021-01-01,Conference Paper,"Baloch, Muhammad Zubair;Hussain, Shahid;Afzal, Humaira;Mufti, Muhammad Rafiq;Ahmad, Bashir",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85104706606,10.1007/978-3-030-71472-7_3,Machine Learning and Value Generation in Software Development: A Survey,"Machine Learning (ML) has become a ubiquitous tool for predicting and classifying data and has found application in several problem domains, including Software Development (SD). This paper reviews the literature between 2000 and 2019 on the use the learning models that have been employed for programming effort estimation, predicting risks and identifying and detecting defects. This work is meant to serve as a starting point for practitioners willing to add ML to their software development toolbox. It categorises recent literature and identifies trends and limitations. The survey shows as some authors have agreed that industrial applications of ML for SD have not been as popular as the reported results would suggest. The conducted investigation shows that, despite having promising findings for a variety of SD tasks, most of the studies yield vague results, in part due to the lack of comprehensive datasets in this problem domain. The paper ends with concluding remarks and suggestions for future research. © 2021, Springer Nature Switzerland AG.",Literature review | Machine learning | Software engineering,Communications in Computer and Information Science,2021-01-01,Conference Paper,"Akinsanya, Barakat J.;Araújo, Luiz J.P.;Charikova, Mariia;Gimaeva, Susanna;Grichshenko, Alexandr;Khan, Adil;Mazzara, Manuel;Ozioma Okonicha, N.;Shilintsev, Daniil",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85106422731,10.1007/978-3-030-75078-7_31,A Survey on Data Science Techniques for Predicting Software Defects,"In recent years, data science has been used extensively to solve several problems and its application has been extended to several domains. This paper summarises the literature on the synergistic use of Software Engineering and Data Science techniques (e.g. descriptive statistics, inferential statistics, machine learning, and deep learning models) for predicting defects in software. It shows that there is a variation in the use of data science techniques and limited reasoning behind the choice of certain machine learning models but also, in the evaluation of the obtained results. The contribution of this paper has to be intended as a categorization of the literature according to the most used data science concepts and techniques from the perspectives of descriptive and inferential statistics, machine learning, and deep learning. Furthermore, challenges in software defect prediction and comments on future research are discussed and forwarded. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.",,Lecture Notes in Networks and Systems,2021-01-01,Conference Paper,"Atif, Farah;Rodriguez, Manuel;Araújo, Luiz J.P.;Amartiwi, Utih;Akinsanya, Barakat J.;Mazzara, Manuel",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85111355456,10.1007/978-3-030-77246-8_34,OntoReq: An Ontology Focused Collective Knowledge Approach for Requirement Traceability Modelling,"There is need for software-based applications in every domain and software developers offer high customizability to their client; it is very much important to deliver a defect free product that matches all the specification of the client and must pass all the testcases. So, to do this systematically, a requirement traceability matrix is used. Requirement traceability matrix is set of documents that consist of all the requirements and the test cases and the track of relation between the customer’s requirements for the system. Manually creating a Requirement traceability matrix is a very time-consuming job and are always prone to errors. There is a lot of methods that are proposed in order to create a efficient traceability matrix using Language processing and machine learning techniques but these approaches require lot of data and does not take care of the real world knowledge and lead to errors. This paper proposes a knowledge engineering-based architecture to this problem with the use of ontology, machine learning and optimization algorithm which produces a dependability and steadiness of 97% and 95% respectively and the performance of the proposed model is compared with baseline approaches. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Ontology | Requirement traceability matrix | SRS (Software Requirement Specification) document | Sunflower optimization | Support vector classifier,Lecture Notes in Networks and Systems,2021-01-01,Conference Paper,"Adithya, V.;Deepak, Gerard",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85115877287,10.1007/978-3-030-82529-4_50,Data Collection: Use and Transformation in Predictive Maintenance Models,"Operational Reliability of the equipment for the operation of industrial plants is an indicator of great importance in industrial processes, which must be measured as continuously as possible with a minimum of interruptions. To avoid the occurrence of failures, the appropriate time to intervene the equipment must be established. The prediction of this indicator is based on data collected from the equipment condition and its proper analysis. However, failure data are not always available. The key issue is how to obtain and process good quality fault data to obtain high precision results. This document studies the use of Industry 4.0 and big data principles in the identification of potential faults that leadinglead to a functional failure in a multistage injection pump, which is a critical equipment for the disposal of wastewater in the hydrocarbon industry. The values of torque, amperage and rpm of the equipment, as well as temperature, and pressure and flow of the water were generated using the specialized simulation software Simscape™ from the MATLAB tool kit. Various fault types were defined for the pump and some models were built using machine learning with the simulation data to detect the presence of faults and identify their type. The model with the highest precision was selected and corresponds to a tree classification algorithm with an accuracy of 95%. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Data generation | Fault detection | Machine learning,Lecture Notes in Networks and Systems,2022-01-01,Conference Paper,"Vélez Sánchez, Hernando;Hurtado Cortés, Luini Leonardo",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85123288786,10.1007/978-3-030-93135-3_3,AI Empowered DevSecOps Security for Next Generation Development,"In DevSecOps, development phase advancement goes through various effective solutions, but efficient bug detection, reliability, accurate reports, and user-friendly solution are still lacking. The existing tools raising a false alarm and somewhere no alarm at all at potential threats are no rare sight. Still, there has been no advancement towards a practical solution that could solve the issue mentioned above. In this paper, we have developed a state-of-the-art approach to the problem by leveraging artificial intelligence, enabling us to facilitate the analysis detection and generate more advanced reporting. In particular, we have integrated Machine Learning with DevSecOps to minimize false error rates. The proposed approach determines debugging errors in less time. Moreover, it provides beginner-friendly analysis for developers to accomplish by our precisely tailored machine learning models trained on the data-set derived from SEI CERT Standard. © 2021, Springer Nature Switzerland AG.",DevSecOps | Machine learning | Security | Software development,Communications in Computer and Information Science,2021-01-01,Conference Paper,"Yadav, Bhawna;Choudhary, Gaurav;Shandilya, Shishir Kumar;Dragoni, Nicola",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84921354062,10.1007/978-3-319-13332-4_12,Utilizing customers’ purchase and contract renewal details to predict defection in the cloud software industry,"This study aims to predict customer defection in the growing market of the cloud software industry. Using the original unstructured data of a company, we propose a procedure to identify the actual defection condition (i.e., whether the customer is defecting from the company or merely stopped using a current product to up/downgrade it) and to produce a measure of customer loyalty by compiling the number of customers’ purchases and renewals. Based on the results, we investigated important variables for classifying defecting customers using a random forest and built a prediction model using a decision tree. The final results indicate that defecting customers are mainly characterized by their loyalty and their number of total payments. © Springer International Publishing Switzerland 2014.",Cloud software industry | Customer defection | Decision tree | Machine learning | Random forest,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2014-01-01,Article,"Martono, Niken Prasasti;Kanamori, Katsutoshi;Ohwada, Hayato",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85026529437,10.1007/978-3-319-31861-5_8,An analytics-driven approach to identify duplicate bug records in large data repositories,"Typically, the identification and analysis of duplicate bug records of a software application are mundane activities, carried out by software maintenance engineers. As the bug repository grows in size for a large software application, this manual process becomes erroneous and a time-consuming activity. Automatic detection of these duplicate bug records will reduce the manual effort spent by the maintenance engineers. It also results in the reduction of costs of software maintenance. There are two types of duplicate bug records: (1) the records that describe the same problem using similar vocabulary, and (2) the records that describe different problems using dissimilar vocabulary but share the same underlying root cause. Each of these types of records needs a different set of techniques to identify the duplicate bug records. In this chapter, we explain the various machine learning techniques that are used to detect both types of duplicate bug records. Some of these duplicate bug records reappear, that is, they show up continuously over a long period of time. Here, we present a framework that can be used to automate the entire process of detection of both types of duplicates and recurring bug records. Using the framework, we conducted empirical studies on the opensource Chrome bug data records that are accessible online and the results are reported. © Springer International Publishing Switzerland 2016. All rights reserved.",Bug fix | Clustering techniques | Co-occurrence models | Duplicate data records | Mining software repositories | Software maintenance | Vector space model,Data Science and Big Data Computing: Frameworks and Methodologies,2016-01-01,Book Chapter,"Pasala, Anjaneyulu;Guha, Sarbendu;Agnihotram, Gopichand;Prateek B, Satya;Padmanabhuni, Srinivas",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84992451628,10.1007/978-3-319-44257-0_8,Analytics for network security: A survey and taxonomy,"IT operations produce data such as log files, events, packets, configuration data, etc. Security attacks, for example, an intrusion, can be detected and mitigated by analyzing and finding abnormal patterns from collected data. Intelligent and effective algorithms are needed for analyzing the massive amount of unstructured data created within computing networks. This has motivated research on and development of information analytics like tools, solutions, and services for network security. Processing of the vast amount of monitoring data from agents and sensors in an intrusion detection system (IDS), updating a database with this data, combining this data with this rapidly growing database, and applying a decision support database for real-time responses require the use of technologies for processing unstructured big data in agents, sensors, and management servers. The best defense against intrusion is prevention. Forensic information about intrusion attack sources can be used for blacklisting corresponding network addresses and for reconfiguring network firewalls to prevent communication from these sources. An IDS should prevent malicious communication to a network. Anomaly-based intrusion detection identifies deviations from normal and typical user activity, communication patterns, application behavior, etc. Different users can have different profiles of normal activity. Normal behavior in a network must be learned from a training dataset or from monitoring communication and activity in a network. Anomaly identification can therefore detect also previously unknown intrusion attempt types. Forensic investigations after intrusion/intrusion attempts manage at least the same dataset that was processed before an intrusion response. By capturing, recording, and analyzing network events, the attack source can be found by using big data tools and real-time analysis techniques. An advanced persistent threat (APT) is a targeted attack with a low profile during a long time. The purpose is to keep a target network unaware of the ongoing intrusion. APT attackers often use stolen user credentials and/or zero-day exploits to avoid triggering IDS responses. Big data analytics tools are particularly suitable for APT detection. To detect APT attacks, collection and correlation of large quantities of diverse data including internal data sources and external shared intelligence data is a necessity. Long-term historical correlation to incorporate a posteriori attack information in the history of a network must be performed. Network security analytics uses big data software technologies like Hadoop and Apache Mahout which extend the MapReduce programming model. MapReduce decomposes data into smaller pieces, which are processed on their own network hosts instead of being moved to other network nodes for processing. Hadoop can process stored big data but cannot process data streams. The open-source, distributed, scalable, and fault-tolerant real-time processing system Storm and some commercial platforms can process big data streams. Apache Mahout, implemented on the top of Hadoop, is a machine learning software library. Taxonomies have been proposed for information analytics and for intrusion detection/response systems. Our proposed security analytics taxonomy includes descriptive analytics for identification of network security threats, diagnostic analytics for forensics, predictive analytics for proactive intrusion prevention, prescriptive analytics for protection against malware, firewalls, protected network communication, recovery after security incidents, and network security visualization. © Springer International Publishing Switzerland 2016.",,Studies in Computational Intelligence,2017-01-01,Book Chapter,"Grahn, Kaj;Westerlund, Magnus;Pulkkis, GÖran",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84989313936,10.1007/978-3-319-46295-0_9,Monitoring-based task scheduling in large-scale SaaS cloud,"With the increasing scale of SaaS and the continuous growth in server failures, task scheduling problems become more intricate, and both scheduling quality and scheduling speed raise further concerns. In this paper, we first propose a virtualized and monitoring SaaS model with predictive maintenance to minimize the costs of fault tolerance. Then with the monitored and predicted available states of servers, we focus on dynamic real-time task scheduling in large-scale heterogeneous SaaS, targeting at jointly optimizing the long-term performance benefits and energy costs in order to improve scheduling quality. We formulate a dynamic programming problem, where both the state and action spaces are too large to be solved by simple iterations. To address these issues, we take advantage of Machine Learning theory, and put forward an approximate dynamic programming algorithm. We utilize value function approximation and candidate-heuristic method to separately solve state and action explosions. Thus, computation complexity is significantly reduced and scheduling speed is greatly enhanced. Finally, we conduct experiments with both random simulation data and Google cloud trace-logs. Qos evaluations and comparisons demonstrate that our approach is effective and efficient under bursty requests and high throughputs. © Springer International Publishing Switzerland 2016.",Approximate dynamic programming | Data center | Multi-objective optimization | SaaS cloud | Task scheduling,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2016-01-01,Conference Paper,"Zhang, Puheng;Lin, Chuang;Ma, Xiao;Ren, Fengyuan;Li, Wenzhuo",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85020846211,10.1007/978-3-319-59060-8_4,Spark-based cluster implementation of a bug report assignment recommender system,"The use of recommenders for bug report triage decisions is especially important in the context of large software development projects, where both the frequency of reported problems and a large number of active developers can pose problems in selecting the most appropriate developer to work on a certain issue. From a machine learning perspective, the triage problem of bug report assignment in software projects may be regarded as a classification problem which can be solved by a recommender system. We describe a highly scalable SVM-based bug report assignment recommender that is able to run on massive datasets. Unlike previous desktop-based implementations of bug report triage assignment recommenders, our recommender is implemented on a cloud platform. The system uses a novel sequence of machine learning processing steps and compares favorably with other SVM-based bug report assignment recommender systems with respect to prediction performance. We validate our approach on real-world datasets from the Netbeans, Eclipse and Mozilla projects. © Springer International Publishing AG 2017.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017-01-01,Conference Paper,"Florea, Adrian Cătălin;Anvik, John;Andonie, Răzvan",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85029722379,10.1007/978-3-319-67389-9_21,Scalable and fault tolerant platform for distributed learning on private medical data,"Medical image data is naturally distributed among clinical institutions. This partitioning, combined with security and privacy restrictions on medical data, imposes limitations on machine learning algorithms in clinical applications, especially for small and newly established institutions. We present InsuLearn: an intuitive and robust open-source (open-source code available at: https://github.com/DistributedML/InsuLearn) platform designed to facilitate distributed learning (classification and regression) on medical image data, while preserving data security and privacy. InsuLearn is built on ensemble learning, in which statistical models are developed at each institution independently and combined at secure coordinator nodes. InsuLearn protocols are designed such that the liveness of the system is guaranteed as institutions join and leave the network. Coordination is implemented as a cluster of replicated state machines, making it tolerant to individual node failures. We demonstrate that InsuLearn successfully integrates accurate models for horizontally partitioned data while preserving privacy. © 2017, Springer International Publishing AG.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2017-01-01,Conference Paper,"Amir-Khalili, Alborz;Kianzad, Soheil;Abugharbieh, Rafeef;Beschastnikh, Ivan",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85041823642,10.1007/978-3-319-70578-1_8,An initial investigation of concurrency bugs in open source systems,"In the last 10 years CPUs have evolved focusing on performance improvements based on the introduction of multi-core architectures forcing developers to build software in a completely different way. Concurrent programming is now the main approach to improve performances in any software product. Unfortunately, this paradigm is prone to bugs which are particularly hard to fix, since their occurrence depends on specific thread interleaving. The paper investigates bugs related to concurrency analyzing their characteristics with machine learning methods to automatically distinguish them from other kinds of bugs based on the data available in the issue tracking systems and in the code repositories. The best model we developed for Apache HTTP Server has a precision of 0.97 and a recall of 0.843 when considering linked bugs (bug reports information in bug repository and the corresponding fix in the version control system). © Springer International Publishing AG 2018.",,Advances in Intelligent Systems and Computing,2018-01-01,Conference Paper,"Ciancarini, Paolo;Poggi, Francesco;Rossi, Davide;Sillitti, Alberto",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85047659967,10.1007/978-3-319-92807-4_3,Hierarchical and clustering WSN models: Their requirements for complex applications,"Generally, WSN consists of thousands of inexpensive devices, called sensor nodes, capable of computation, communication and sensing events in a specific environment [1–3]. WSNs have attracted intensive interest from both academia and industry due to their wide application in civil and military scenarios [4–6]. Enormous advances that are emerging in WSNs act as a revolution in all aspects of our life. WSNs have unique specifications describe it and different from other networks. Sensor nodes have energy and computational challenges. Moreover, WSNs may be prone to software failure, unreliable wireless connections, malicious attacks, and hardware faults; that make the network performance may degrade significantly over time. Recently, there is a great interest related to routing process in WSNs using intelligent and machine learning algorithms such as Genetic Algorithms [7–9]. Security aspects in routing protocols have not been given enough attention, since most of the routing protocols in WSNs have not been designed with security requirements in mind [10–14]. In this chapter, the main models of WSN with their advantages and limitations are discussed, specially the clustering model. In addition, it provides a literature of the existing clustering methods of WSN that aims to increase the network lifetime. After that, the security aspects are explained in details. Finally, the existing secure clustering methods are discussed and evaluated based on a set of criteria. © 2019, Springer International Publishing AG, part of Springer Nature.",,"Studies in Systems, Decision and Control",2019-01-01,Book Chapter,"Elhoseny, Mohamed;Hassanien, Aboul Ella",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85049787816,10.1007/978-3-319-93638-3_35,Automatically identifying security bug reports via multitype features analysis,"Bug-tracking systems are widely used by software developers to manage bug reports. Since it is time-consuming and costly to fix all the bugs, developers usually pay more attention to the bugs with higher impact, such as security bugs (i.e., vulnerabilities) which can be exploited by malicious users to launch attacks and cause great damages. However, manually identifying security bug reports from millions of reports in bug-tracking systems is difficult and error-prone. Furthermore, existing automated identification approaches to security bug reports often incur many false negatives, causing a hidden danger to the computer system. To address this important problem, we present an automatic security bug reports identification model via multitype features analysis, dubbed Security Bug Report Identifier (SBRer). Specifically, we make use of multiple kinds of information contained in a bug report, including meta features and textual features, to automatically identify the security bug reports via natural language processing and machine learning techniques. The experimental results show that SBRer with imbalanced data processing can successfully identify the security bug reports with a much higher precision of 99.4% and recall of 79.9% compared to existing work. © Springer International Publishing AG, part of Springer Nature 2018.",Bug report | Machine learning | Natural language processing | Security bug identification,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2018-01-01,Conference Paper,"Zou, Deqing;Deng, Zhijun;Li, Zhen;Jin, Hai",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-35448959626,10.1007/978-3-540-69912-5_37,1996-2006 Autonomous robots,"The ""fifth decade of Artificial Intelligence"", the period from 1996 to 2006, has been a decade of success for AI. Computers beat the residing world chess champion. AI companies like Google and Yahoo! have been taking over the world. And AI research changed forever the nature of other scientific fields, like biology and cognitive science. So how could this happen? In my humble opinion, the fifth decade of AI has been the decade of data and statistics. Data has been available for many years, but somehow the amount of available data exploded in the past decade. The advent of the World Wide Web made huge numbers of documents, images, and videos available online. Hundreds of AI researchers refocused their energy on the Web. AI systems were developed for learning people's browsing patterns, for raking online music, for finding and parsing job advertisements, and for making search more effective. Out of this grew a billion-dollar industry, with Google being the most significant example. Data has also exploded in biology. Early in fifth decade of AI, the human genome was decoded, and now the race is up to actually understand it. AI techniques were developed to find genes in the DNA string, to model gene expression in cells, to link DNA information to diseases, and to find commonalities among the DNA in different species. Today, we have a field of biocomputation that has profoundly changed the field of biology. And AI has played a major role in all this. Data has also become abundant in robotics and in physical spaces. In the beginning of this decade, there were less than one million robots operating around the world. iRobot alone, an AI company, has sold over two million robots in the past five years. Thanks to much improved sensor technology, it has become easy to equip cars with sensors for driver and environment perception. Five of these cars just drove autonomously across a 131-miles long desert course, fueled by some very advanced AI. And the field of sensor networks, which is still in its infancy, is beginning to create some interesting data sets worthy of AI research. But data along would not have been sufficient. The other ingredient is statistics. AI has embraced the field of statistics in great many ways. Early examples include statistical approaches to machine translation, which outperformed then-popular hand-crafted grammars in a stunning series of experiments. Since then, many other fields have gone statistic: computer vision, machine learning, inference, information retrieval, robotics, speech recognition, to name a few. The advent of statistics in AI has led to great new ways to extract information from data. To do all this, we needed faster computers. Somehow in the mid-1990s, we reached the critical speed that makes it possible to run complex statistical techniques on large amount of data in reasonable time. In my talk, I intend to focus on some of these advances. I will in large parts focus on robotics, in which statistics, data, and fast computers have had a profound effect in the recent decade. Robots are now programmed probabilistically, and they use extensive numerical simulations of statistical equations to make robust decisions. In doing so, they are now much more robust to noise in the sensor data, and to the uncertainly that naturally exists in physical spaces. One of my own robots, named Stanley, just won the DARPA Grand Challenge, the 131-miles long autonomous robot desert race. In developing this robot, AI played a significant role, as did data and statistics. Stanley uses probabilistic techniques to reason about sensor data. We, the developers, extensively relied on machine learning to endow the robot with advanced perception and control abilities ahead of the race. During the race, the robot used statistical machine learning to continuously adapt its perceptual routines to the terrain ahead-which gave it a key advantage. For a field like robotics, which has traditionally been dominated by non-AI research (despite the perception of many AI researchers!), this is a treat. So what's next? It is time for AI to become mainstram in computer science. Why don't we have programming languages that make it possible to handle uncertain data? Why can't software diagnose itself and fix its bug based on data? What don't we routinly train our computers with examples, and why can't our databases adapt to the type unstructured information available in places like the Web? And what have we learned about human-level AI lately? How can we understand brain activities recoded by fMRI, a technology that has been rapidly improving in the past few years? And where are those universal robots that can do many different task, not just one, and act as if endowed with common sense? These are all challenging topics. But with all the success in the fifth decade of AI, and all those new technical insights, it's time to try again to solve the big problems in AI. © Springer-Verlag Berlin Heidelberg 2007.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2007-01-01,Conference Paper,"Thrun, Sebastian",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84879478680,10.1007/978-3-642-23391-3_17,Automatic Assignment of Work Items,"Many software development projects use work items such as tasks or bug reports to describe the work to be done. Some projects allow end-users or clients to enter new work items. New work items have to be triaged. The most important step is to assign new work items to a responsible developer. There are existing approaches to automatically assign bug reports based on the experience of certain developers based on machine learning. We propose a novel modelbased approach, which considers relations from work items to the system specification for the assignment. We compare this new approach to existing techniques mining textual content as well as structural information. All techniques are applied to different types of work items, including bug reports and tasks. For our evaluation, we mine the model repository of three different projects. We also included history data to determine how well they work in different states. © Springer-Verlag Berlin Heidelberg 2011.",Bug report | Machine learning | Task assignment | UJP | UNICASE | Unified model,Communications in Computer and Information Science,2011-12-01,Conference Paper,"Helming, Jonas;Arndt, Holger;Hodaie, Zardosht;Koegel, Maximilian;Narayan, Nitesh",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-82255164591,10.1007/978-3-642-23626-6_3,Regularized tensor factorization for multi-modality medical image classification,"This paper presents a general discriminative dimensionality reduction framework for multi-modal image-based classification in medical imaging datasets. The major goal is to use all modalities simultaneously to transform very high dimensional image to a lower dimensional representation in a discriminative way. In addition to being discriminative, the proposed approach has the advantage of being clinically interpretable. We propose a framework based on regularized tensor decomposition. We show that different variants of tensor factorization imply various hypothesis about data. Inspired by the idea of multi-view dimensionality reduction in machine learning community, two different kinds of tensor decomposition and their implications are presented. We have validated our method on a multi-modal longitudinal brain imaging study. We compared this method with a publically available classification software based on SVM that has shown state-of-the-art classification rate in number of publications. © 2011 Springer-Verlag.",Basis Learning | Classification | Multi-Modality | Multi-view Learning | Optimization | Tensor factorization,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2011-10-11,Conference Paper,"Batmanghelich, Nematollah;Dong, Aoyan;Taskar, Ben;Davatzikos, Christos",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-80052400545,10.1007/978-3-642-23808-6_34,Differentiating code from data in x86 binaries,"Robust, static disassembly is an important part of achieving high coverage for many binary code analyses, such as reverse engineering, malware analysis, reference monitor in-lining, and software fault isolation. However, one of the major difficulties current disassemblers face is differentiating code from data when they are interleaved. This paper presents a machine learning-based disassembly algorithm that segments an x86 binary into subsequences of bytes and then classifies each subsequence as code or data. The algorithm builds a language model from a set of pre-tagged binaries using a statistical data compression technique. It sequentially scans a new binary executable and sets a breaking point at each potential code-to-code and code-to-data/data-to-code transition. The classification of each segment as code or data is based on the minimum cross-entropy. Experimental results are presented to demonstrate the effectiveness of the algorithm. © 2011 Springer-Verlag.",classification | segmentation | statistical data compression | x86 binary disassembly,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2011-09-09,Conference Paper,"Wartell, Richard;Zhou, Yan;Hamlen, Kevin W.;Kantarcioglu, Murat;Thuraisingham, Bhavani",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84862197943,10.1007/978-3-642-30561-0_20,Learning to classify bug reports into components,"Bug reports in widely used defect tracking systems contains standard and mandatory fields like product name, component name, version number and operating system. Such fields provide important information required by developers during bug fixing. Previous research shows that bug reporters often assign incorrect values for such fields which cause problems and delays in bug fixing. We conduct an empirical study on the issue of incorrect component assignments or component reassignments in bug reports. We perform a case study on open-source Eclipse and Mozilla projects and report results on various aspects such as the percentage of reassignments, distribution across number of assignments until closure of a bug and time difference between creation and reassignment event. We perform a series of experiments using a machine learning framework for two prediction tasks: categorizing a given bug report into a pre-defined list of components and predicting whether a given bug report will be reassigned. Experimental results demonstrate correlation between terms present in bug reports (textual documents) and components which can be used as linguistic indicators for the task of component prediction. We study component reassignment graphs and reassignment probabilities and investigate their usefulness for the task of component reassignment prediction. © 2012 Springer-Verlag.",Automated Software Engineering (ASE) | Empirical Software Engineering and Measurements (ESEM) | Mining Software Repositories (MSR),Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2012-06-18,Conference Paper,"Sureka, Ashish",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84884951159,10.1007/978-3-642-39742-4_2,Some recent work on multi-objective approaches to search-based software engineering,"Multi-objective algorithms have been used to solve difficult software engineering problems for a long time. This article summarises some selected recent work of applying latest meta-heuristic optimisation algorithms and machine learning algorithms to software engineering problems, including software module clustering, testing resource allocation in modular software system, protocol tuning, Java container testing, software project scheduling, software project effort estimation, and software defect prediction. References will be given, from which the details of such application of computational intelligence techniques to software engineering problems can be found. © 2013 Springer-Verlag.",,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),2013-10-08,Conference Paper,"Yao, Xin",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85027095691,10.1007/978-981-10-3174-8_53,Pragmatic investigation on performance of instance-based learning algorithms,"In the recent past, there has been increasing usage of machine learning algorithms for classifying the data for various real-world applications such as weather prediction, medical diagnosis, network intrusion detection, software fault detection, etc. The instance-based learning algorithms play a major role in the classification process since they do not learn the data until the need of the developing the classification model. Therefore, these learning algorithms are called as lazy learning algorithms and implemented in various applications. However, there is a pressing need among the researchers to analyze the performance of various types of the instance-based classifier. Therefore, this chapter presents a pragmatic investigation on performance of the instance-based learning algorithms. In order to conduct this analysis, different instance-based classifier namely instance-based one (IB1), instance-based K (IBK), Kstar, lazy learning of Bayesian rules (LBR), locally weighted learning (LWL) are adopted. The performance of these algorithms is evaluated in terms of sensitivity, specificity, and accuracy on various real-world datasets. © Springer Nature Singapore Pte Ltd. 2017.",IB1 | IBK | Instance-based learning | Kstar | LBR | LWL,Advances in Intelligent Systems and Computing,2017-01-01,Conference Paper,"Venkatesh, Bharathan;Singh, Danasingh Asir Antony Gnana;Leavline, Epiphany Jebamalar",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85049055831,10.1007/978-981-10-8660-1_34,Spark based ANFIS approach for anomaly detection using big data,"Business intelligence is one of the applications that can benefit from various techniques and methodologies to patronize the unlablled big data anomalies. To address this issue, in this paper, we present a model to identify anomalies in spark environment using related big data. To optimize this instance, we use an open source software framework named Spark for analyzing the big data. Spark contains powerful APIS for machine learning and soft computing algorithms. To handle and detect the anomaly instances in the perspective of big data, Apache spark is installed on the top of the Hadoop and Adaptive Neuro Fuzzy Interface System (ANFIS) is implemented in spark. The variant of Hadoop HDFS is used as a data source through resilient distributed data sets (RDDs) data which is fetched in the spark. Experimental results show that the proposed method outperforms in a fault tolerant manner and also records accurate instances in the distributed environment. © Springer Nature Singapore Pte Ltd. 2018.",ANFIS | Anomaly detection | Apache spark | Big data | Hadoop,Communications in Computer and Information Science,2018-01-01,Conference Paper,"Santosh, Thakur;Ramesh, Dharavath",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85049750006,10.1007/978-981-10-8715-8_4,Software fault dataset,"Machine learning and statistical techniques are used in software fault prediction to predict the presence or the absence of faults in the given software modules. In order to make the predictions, a software fault prediction learns upon the software fault data having the information about the software system (software metrics) augmented with the fault value. An implicit requirement to perform effective software fault prediction is the availability of reasonable quality fault data. However, obtaining quality software fault data is difficult as in general software development companies are not keen to share their software development information or they are not having any software repository in first place. © 2018, The Author(s).",,SpringerBriefs in Computer Science,2018-01-01,Book Chapter,"Kumar, Sandeep;Rathore, Santosh Singh",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85055711957,10.1007/978-981-13-2922-7_23,Identification of high priority bug reports via integration method,"Many software projects use bug tracking systems to collect and allocate the bug reports, but the priority assignment tasks become difficult to be completed because of the increasing bug reports. In order to assist developers to reduce the pressure on assigning the priority for each bug report, we propose an integration method to predict priority levels based on machine learning. Our approach considers the textual description in bug reports as features and feeds these features to three different classifiers. We utilize these classifiers to predict the bug reports with unknown type and obtain three different results. Simultaneously, we set weights to balance the abilities of identifying different categories based on the characteristics of different projects for each classifier. Finally, we utilize the weights to adjust prediction results and produce a unique priority for assigning to each bug reports. We perform experiments on datasets from 4 products in Mozilla and the experimental results show that our approach has a better performance in terms of identifying the priority of bug reports than previous general methods and ensemble methods. © Springer Nature Singapore Pte Ltd. 2018.",Bug report | Classification prediction | Integration method,Communications in Computer and Information Science,2018-01-01,Conference Paper,"Gao, Guofeng;Li, Hui;Chen, Rong;Ge, Xin;Guo, Shikai",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85084171526,10.1007/978-981-15-4451-4_19,Comparative Analysis of Different Machine Learning Techniques,"Artificial learning (AI) is one of the areas of computer science which develops the system in a way that our system starts learning and gives the reaction as a human brain does. This paper discussed one of the branches of AI i.e. Machine learning as the name suggests it focuses on the development of the computers especially their programs that access the data that can be used for self-development. The main objective of the paper is to compare the best machine learning model by using the performance parameter R squared and Mean square Error (MSE). The data set was taken from the promise repository for the analysis. Feature selection technique Boruta was applied to find the important/confirmed variables from the dataset which are having information regarding the JAVA projects. This paper finds out that among different algorithms which one outperforms when the comparative analysis was done to find the best model. © 2020, Springer Nature Singapore Pte Ltd.",Feature selection | Machine learning model | Random forest | Software bug detection,Communications in Computer and Information Science,2020-01-01,Conference Paper,"Srivastava, Nidhi;Lamba, Tripti;Agarwal, Manisha",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85089231398,10.1007/978-981-15-6634-9_23,Measuring the Effectiveness of Software Code Review Comments,"Code reviewing becomes a more popular technique to find out early defects in source code. Nowadays practitioners are going for peer reviewing their codes by their co-developers to make the source code clean. Working on a distributed or dispersed team, code review is mandatory to check the patches to merge. Code reviewing can also be a form of validating functional and non-functional requirements. Sometimes reviewers do not put structured comments, which becomes a bottle neck to developers for solving the findings or suggestions commented by the reviewers. For making the code review participation more effective, structured and efficient review comments is mandatory. Mining the repositories of five commercialized projects, we have extracted 15223 review comments and labelled them. We have used 8 different machine learning and deep learning classifiers to train our model. Among those Stochastic Gradient Descent (SGD) technique achieves higher accuracy of 80.32%. This study will help the practitioners to build up structured and effective code review culture among global software developers. © 2020, Springer Nature Singapore Pte Ltd.",Empirical software engineering | Machine learning | Mining software repositories | Modern code review | Sentiment Analysis,Communications in Computer and Information Science,2020-01-01,Conference Paper,"Hossain, Syeda Sumbul;Arafat, Yeasir;Hossain, Md Ekram;Arman, Md Shohel;Islam, Anik",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85107070457,10.1007/978-981-16-0081-4_16,Malware Detection System Using Ensemble Learning: Tested Using Synthetic Data,"Malwares refer to the malicious programs that are used to exploit the target system’s vulnerabilities, such as a bug or a legitimate software. Malware infiltration can have disastrous consequences on any corporation which includes stealing confidential data, damaging network devices, and crippling of network systems. So, there is a need to filter the malware out from the network and this is achieved with the help of intrusion detection systems; the malware detection model sits at the core of those systems. This paper aims to design a malware detection model using machine learning techniques and training the model on synthetically generated data. In this paper, we first harness or generate synthetic dataset using a tool named CICFlowMeter. CICFlowMeter is a network traffic flow generator tool that captures the network traffic to produce a featured dataset of the network. We first capture the data using the tool, and then, this data would be used to produce synthetic dataset of the network. After that we use various machine learning techniques to build our malware detection model, which is trained and tested using synthetically produced dataset. © 2021, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Decision tree | Ensemble learning | Intrusion detection system | K-nearest neighbor | Malware detection | Synthetic data,Lecture Notes on Data Engineering and Communications Technologies,2021-01-01,Book Chapter,"Kaushik, Raghav;Dave, Mayank",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85119001557,10.1007/978-981-16-2102-4_18,Categorizing Code Review Comments Using Machine Learning,"Code review turns into a progressively mainstream method to detect early defects in the codebase. These days experts are rushing towards peer-investigating the codebases written by any co-located team members or other authors from distributed or dispersed teams. Chipping away at a circulated or scattered team, reviewing a codebase is required to inspect the patches before consolidating. Code looking into can likewise be a structure of approving practical and non-useful necessities. In certain circumstances, analysts do not invest enough time to comment in an organized manner, which turns into a bottleneck to other developers for tackling the discoveries or recommendations remarked by the peer-reviewers. To make the review process more progressively successful and well-organized, productive remarks are compulsory. We have extricated 2185 human code review comments of five marketed projects by mining respective projects’ repositories. Six machine learning classifiers have been utilized to train our model. Stochastic Gradient Descent (SGD) vector machine accomplishes a higher accuracy of 63.89% among the others. This work will assist the specialists with building up organized and viable code review culture among worldwide programmers or software engineers by categorizing code review comments. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Classification | Global software development | Machine learning | Mining software repositories | Modern code review,Lecture Notes in Networks and Systems,2022-01-01,Conference Paper,"Arafat, Yeasir;Sumbul, Syeda;Shamma, Hossain",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85103352776,10.1007/s00138-021-01192-8,TsanKit: artificial intelligence for solder ball head-in-pillow defect inspection,"In this paper, we propose an AI (Artificial Intelligence) solution for solder ball HIP (Head-In-Pillow) defect inspection. The HIP defect will affect the conductivity of the solder balls leading to intermittent failures. Due to the variable location and shape of the HIP defect, traditional machine vision algorithms cannot solve the problem completely. In recent years, Convolutional Neural Network (CNN) has an outstanding performance in image recognition and classification, but it is easy to cause overfitting problems due to insufficient data. Therefore, we combine CNN and the machine learning algorithm Support Vector Machine (SVM) to design our inspection process. Referring to the advantages of several state-of-the-art models, we propose our 3D CNN model and adopt focal loss as well as triplet loss to solve the data imbalance problem caused by rare defective data. Our inspection method has the best performance and fast testing speed compared with several classic CNN models and the deep learning inspection software SuaKIT. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Artificial intelligence | Convolutional neural networks | Defect inspection | Head-In-Pillow | Solder ball,Machine Vision and Applications,2021-05-01,Article,"Tsan, Ting Chen;Shih, Teng Fu;Fuh, Chiou Shann",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85103359065,10.1007/s00500-021-05740-2,Machine seismic: an automatic approach for the identification of subsurface structural models,"Subsurface structure identification and modeling in three-dimensional dataset is a very complex problem for the geoscientist. Manual dealing of seismic data involves different complexities and requires more time and effort in order to address the errors and mistakes associated with the quality of seismic data. It is sometimes not straightforward to classify the features of interest especially when the data are of poor quality or associated with complex micro-tectonic regime. This brings the need of automatic working engine which can handle such issues and can find the target-oriented features with the more accuracy. Therefore, advance machine learning approach has been considered as a powerful tool which extensively used in different fields of data science to enhance the quality of data and fast processing. In this study, a new technology has been introduced in the seismic processing and development to make processes in automated routine. This method is based on deep neural network (DNN) that directly translates the raw seismic data into final fault models. A test example of deep semi-supervised learning method was performed for seismic faults and reflector identification without prior information and without going into complete dataset for labeling. This example explains the challenges to the issue and gives the solution to the problem related to the structure and tectonic modeling to explain the performance gain. In this approach of semi-supervised learning, only one of any reflectors and one or two of the faults has been introduced in the learning phase to identify such features in the complete 3D cube. This is an example of deep learning neural network for machine seismic in real case study and allows exploration geoscientists to easier mark the hot spots of their interest before mining or drilling. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Advance machine learning | Deep neural network | Micro-tectonic structures | Seismic method,Soft Computing,2021-07-01,Article,"Ahmed, Khawar Ashfaq;Khan, Sarfraz;Nisar, Umair Bin;Mughal, Muhammad Rizwan;Sultan, Mahmood",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84984844840,10.1007/s00521-018-3912-4,Efficient parallel implementation of reservoir computing systems,"Reservoir computing (RC) is a powerful machine learning methodology well suited for time-series processing. The hardware implementation of RC systems (HRC) may extend the utility of this neural approach to solve real-life problems for which software solutions are not satisfactory. Nevertheless, the implementation of massive parallel-connected reservoir networks is costly in terms of circuit area and power, mainly due to the requirement of implementing synapse multipliers that increase gate count to prohibitive values. Most HRC systems present in the literature solve this area problem by sequencializing the processes, thus loosing the expected fault-tolerance and low latency of fully parallel-connected HRCs. Therefore, the development of new methodologies to implement fully parallel HRC systems is of high interest to many computational intelligence applications requiring quick responses. In this article, we propose a compact hardware implementation for Echo-State Networks (an specific type of reservoir) that reduces the area cost by simplifying the synapses and using linear piece-wise activation functions for neurons. The proposed design is synthesized in a Field-Programmable Gate Array and evaluated for different time-series prediction tasks. Without compromising the overall accuracy, the proposed approach achieves a significant saving in terms of power and hardware when compared with recently published implementations. This technique pave the way for the low-power implementation of fully parallel reservoir networks containing thousands of neurons in a single integrated circuit. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Artificial neural networks; Echo sate networks; Field-programmable gate array; Hardware neural network; Recurrent neural networks; Reservoir computing; Time-series prediction,Neural Computing and Applications,2020,,"Alomar M.L., Skibinsky-Gitlin E.S., Frasser C.F., Canals V., Isern E., Roca M., Rosselló J.L.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85104647118,10.1007/s00521-021-06028-0,Fault diagnosis and prognosis of steer-by-wire system based on finite state machine and extreme learning machine,"In this paper, an integrated condition monitoring method combining model-based fault diagnosis and data-driven prognosis is proposed for steer-by-wire (SBW) system. First, the SBW system is modeled by bond graph (BG) technique and a two-degree-of-freedom (2-DOF) state-space model of the vehicle is built. Based on the 2-DOF model, the estimated self-aligning torque is used for the control of feedback motor. The fault detection is carried out by evaluating the analytical redundancy relations derived from the BG model. Since the fault isolation performance is essential to subsequent fault estimation process, a new fault isolation method based on finite state machine is developed to improve the isolation ability by combining the dependent and independent analytical redundancy relations, where the number of potential faults could be decreased. In order to refine the possible fault set to determine the true fault, a cuckoo search (CS)–particle filter is developed for fault estimation. Based on the estimated true fault, prognosis can be implemented which is important to achieve failure prevention and prolong system lifespan. To this end, an optimized extreme learning machine (OELM) is proposed where the input weights and hidden layer biases are optimized by CS. Based on data representing fault values obtained from the fault identification, the OELM model is trained for remaining useful life prediction of failing component. Finally, the proposed methodologies are validated by simulations. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Extreme learning machine | Finite state machine | Integrated condition monitoring | Steer-by-wire system,Neural Computing and Applications,2022-04-01,Article,"Lan, Dun;Yu, Ming;Huang, Yunzhi;Ping, Zhaowu;Zhang, Jie",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85086127192,10.1007/s10009-020-00577-w,A study of learning likely data structure properties using machine learning models,"Data structure properties are important for many testing and analysis tasks. For example, model checkers use these properties to find program faults. These properties are often written manually which can be error prone and lead to false alarms. This paper presents the results of controlled experiments performed using existing machine learning (ML) models on various data structures. These data structures are dynamic and reside on the program heap. We use ten data structure subjects and ten ML models to evaluate the learnability of data structure properties. The study reveals five key findings. One, most of the ML models perform well in learning data structure properties, but some of the ML models such as quadratic discriminant analysis and Gaussian naive Bayes are not suitable for learning data structure properties. Two, most of the ML models have high performance even when trained on just 1% of data samples. Three, certain data structure properties such as binary heap and red black tree are more learnable than others. Four, there are no significant differences between the learnability of varied-size (i.e., up to a certain size) and fixed-size data structures. Five, there can be significant differences in performance based on the encoding used. These findings show that using machine learning models to learn data structure properties is very promising. We believe that these properties, once learned, can be used to provide a run-time check to see whether a program state at a particular point satisfies the learned property. Learned properties can also be employed in the future to automate static and dynamic analysis, which would enhance software testing and verification techniques. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",Data structure invariants | Korat | Learnability | Machine learning,International Journal on Software Tools for Technology Transfer,2020-10-01,Article,"Usman, Muhammad;Wang, Wenxi;Wang, Kaiyuan;Yelen, Cagdas;Dini, Nima;Khurshid, Sarfraz",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85113811937,10.1007/s10270-021-00920-y,Enhancing software model encoding for feature location approaches based on machine learning techniques,"Feature location is one of the main activities performed during software evolution. In our previous works, we proposed an approach for feature location in models based on machine learning, providing evidence that machine learning techniques can obtain better results than other retrieval techniques for feature location in models. However, to apply machine learning techniques optimally, the design of an encoding is essential to be able to identify the best realization of a feature. In this work, we present more thorough research about software model encoding for feature location approaches based on machine learning. As part of this study, we have provided two new software model encodings and compared them with the source encoding. The first proposed encoding is an extension of the source encoding to take advantage of not only the main concepts and relations of a domain but also the properties of these concepts and relations. The second proposed encoding is inspired by the characteristics used in benchmark datasets for research on Learning to Rank. Afterward, the new encodings are used to compare three different machine learning techniques (RankBoost, Feedforward Neural Network, and Recurrent Neural Network). The study also considers whether a domain-independent encoding such as the ones proposed in this work can outperform an encoding that is specifically designed to exploit human experience and domain knowledge. Furthermore, the results of the best encoding and the best machine learning technique were compared to two traditional approaches that have been widely applied for feature location as well as for traceability link recovery and bug localization. The evaluation is based on two real-world case studies, one in the railway domain and the other in the induction hob domain. An approach for feature location in models evaluates these case studies with the different encodings and machine learning techniques. The results show that when using the second proposed encoding and RankBoost, the approach outperforms the results of the other encodings and machine learning techniques and the results of the traditional approaches. Specifically, the approach achieved the best results for all the performance indicators, providing a mean precision value of 90.11%, a recall value of 86.20%, a F-measure value of 87.22%, and a MCC value of 0.87. The statistical analysis of the results shows that this approach significantly improves the results and increases the magnitude of the improvement. The promising results of this work can serve as a starting point toward the use of machine learning techniques in other engineering tasks with software models, such as traceability or bug location. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Encoding | Feature location | Learning to Rank | Machine learning | Neural networks | Software models,Software and Systems Modeling,2022-02-01,Article,"Marcén, Ana C.;Pérez, Francisca;Pastor, Óscar;Cetina, Carlos",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84898870457,10.1007/s10515-013-0129-8,DConfusion: A technique to allow cross study performance evaluation of fault prediction studies,"There are many hundreds of fault prediction models published in the literature. The predictive performance of these models is often reported using a variety of different measures. Most performance measures are not directly comparable. This lack of comparability means that it is often difficult to evaluate the performance of one model against another. Our aim is to present an approach that allows other researchers and practitioners to transform many performance measures back into a confusion matrix. Once performance is expressed in a confusion matrix alternative preferred performance measures can then be derived. Our approach has enabled us to compare the performance of 600 models published in 42 studies. We demonstrate the application of our approach on 8 case studies, and discuss the advantages and implications of doing this. © 2013 Springer Science+Business Media New York.",Confusion matrix | Fault | Machine learning,Automated Software Engineering,2014-04-01,Article,"Bowes, David;Hall, Tracy;Gray, David",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84982298222,10.1007/s10515-016-0203-0,An exploratory study for software change prediction in object-oriented systems using hybridized techniques,"Variation in software requirements, technological upgrade and occurrence of defects necessitate change in software for its effective use. Early detection of those classes of a software which are prone to change is critical for software developers and project managers as it can aid in efficient resource allocation of limited resources. Moreover, change prone classes should be efficiently restructured and designed to prevent introduction of defects. Recently, use of search based techniques and their hybridized counter-parts have been advocated in the field of software engineering predictive modeling as these techniques help in identification of optimal solutions for a specific problem by testing the goodness of a number of possible solutions. In this paper, we propose a novel approach for change prediction using search-based techniques and hybridized techniques. Further, we address the following issues: (i) low repeatability of empirical studies, (ii) less use of statistical tests for comparing the effectiveness of models, and (iii) non-assessment of trade-off between runtime and predictive performance of various techniques. This paper presents an empirical validation of search-based techniques and their hybridized versions, which yields unbiased, accurate and repeatable results. The study analyzes and compares the predictive performance of five search-based, five hybridized techniques and four widely used machine learning techniques and a statistical technique for predicting change prone classes in six application packages of a popular operating system for mobile—Android. The results of the study advocate the use of hybridized techniques for developing models to identify change prone classes. © 2016, Springer Science+Business Media New York.",Change proneness | Empirical validation | Hybridized techniques | Object-oriented metrics | Predictive modeling | Search-based techniques,Automated Software Engineering,2017-09-01,Article,"Malhotra, Ruchika;Khanna, Megha",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85070984462,10.1007/s10515-019-00264-4,Automatic B-model repair using model checking and machine learning,"The B-method, which provides automated verification for the design of software systems, still requires users to manually repair faulty models. This paper proposes B-repair, an approach that supports automated repair of faulty models written in the B formal specification language. After discovering a fault in a model using the B-method, B-repair is able to suggest possible repairs for the fault, estimate the quality of suggested repairs and use a suitable repair to revise the model. The suggestion of repairs is produced using the Isolation method, which suggests changing the pre-conditions of operations, and the Revision method, which suggests changing the post-conditions of operations. The estimation of repair quality makes use of machine learning techniques that can learn the features of state transitions. After estimating the quality of suggested repairs, the repairs are ranked, and a best repair is selected according to the result of ranking and is used to revise the model. This approach has been evaluated using a set of finite state machines seeded with faults and a case study. The evaluation has revealed that B-repair is able to repair a large number of faults, including invariant violations, assertion violations and deadlock states, and gain high accuracies of repair. Using the combination of model checking and machine learning-guided techniques, B-repair saves development time by finding and repairing faults automatically during design. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",B-method | Formal verification | Machine learning | Model checking | Model repair,Automated Software Engineering,2019-09-15,Article,"Cai, Cheng Hao;Sun, Jing;Dobbie, Gillian",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85119969463,10.1007/s10515-021-00301-1,Opinion mining for app reviews: an analysis of textual representation and predictive models,"Popular mobile applications receive millions of user reviews. These reviews contain relevant information for software maintenance, such as bug reports and improvement suggestions. The review’s information is a valuable knowledge source for software requirements engineering since the apps review analysis helps make strategic decisions to improve the app quality. However, due to the large volume of texts, the manual extraction of the relevant information is an impracticable task. Opinion mining is the field of study for analyzing people’s sentiments and emotions through opinions expressed on the web, such as social networks, forums, and community platforms for products and services recommendation. In this paper, we investigate opinion mining for app reviews. In particular, we compare textual representation techniques for classification, sentiment analysis, and utility prediction from app reviews. We discuss and evaluate different techniques for the textual representation of reviews, from traditional Bag-of-Words (BoW) to the most recent state-of-the-art Neural Language models (NLM). Our findings show that the traditional Bag-of-Words model, combined with a careful analysis of text pre-processing techniques, is still competitive. It obtains results close to the NLM in the classification, sentiment analysis and utility prediction tasks. However, NLM proved to be more advantageous since they achieved very competitive performance in all the predictive tasks covered in this work, provide significant dimensionality reduction, and deals more adequately with semantic proximity between the reviews’ texts. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Machine learning | Mobile applications | Opinion mining | Requirements engineering,Automated Software Engineering,2022-05-01,Article,"Araujo, Adailton F.;Gôlo, Marcos P.S.;Marcacini, Ricardo M.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85122086312,10.1007/s10515-021-00314-w,On the documentation of refactoring types,"Commit messages are the atomic level of software documentation. They provide a natural language description of the code change and its purpose. Messages are critical for software maintenance and program comprehension. Unlike documenting feature updates and bug fixes, little is known about how developers document their refactoring activities. Specifically, developers can perform multiple refactoring operations, including moving methods, extracting classes, renaming attributes, for various reasons, such as improving software quality, managing technical debt, and removing defects. Yet, there is no systematic study that analyzes the extent to which the documentation of refactoring accurately describes the refactoring operations performed at the source code level. Therefore, this paper challenges the ability of refactoring documentation, written in commit messages, to adequately predict the refactoring types, performed at the commit level. Our analysis relies on the text mining of commit messages to extract the corresponding features (i.e., keywords) that better represent each class (i.e., refactoring type). The extraction of text patterns, specific to each refactoring type (e.g., rename, extract, move, inline, etc.) allows the design of a model that verifies the consistency of these patterns with their corresponding refactoring. Such verification process can be achieved via automatically predicting, for a given commit, the method-level type of refactoring being applied, namely Extract Method, Inline Method, Move Method, Pull-up Method, Push-down Method, and Rename Method. We compared various classifiers, and a baseline keyword-based approach, in terms of their prediction performance, using a dataset of 5004 commits. Our main findings show that the complexity of refactoring type prediction varies from one type to another. Rename Method and Extract Method were found to be the best documented refactoring activities, while Pull-up Method, and Push-down Method were the hardest to be identified via textual descriptions. Such findings bring the attention of developers to the necessity of paying more attention to the documentation of these types. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Machine learning | Refactoring | Software engineering | Software quality,Automated Software Engineering,2022-05-01,Article,"AlOmar, Eman Abdullah;Liu, Jiaqian;Addo, Kenneth;Mkaouer, Mohamed Wiem;Newman, Christian;Ouni, Ali;Yu, Zhe",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84979519042,10.1007/s10586-016-0606-5,Nonlinear approach for estimating WCET during programming phase,"For the safety of real-time systems, it is very important that the execution time of programs must meet all time constraints, even under the worst case. To expose timeliness defects which may cause an execution timeout as early as possible, we have studied a novel nonlinear approach for estimating worst case execution time (WCET) during programming phase, called NL-WCET. In this paper, we propose a program features model, based on which NL-WCET employs least square support vector machine (LSSVM) to learn the program features, and then estimates WCET. To improve the accuracy of NL-WCET, we develop an algorithm for training samples optimization. The experimental results show that both the model and the algorithm have distinct effects on the accuracy of NL-WCET. When static similarity is ≥ 80 %, cosine similarity is ≥ 99.5 % and max quotient between corresponding items is ≤ 50, the average error of NL-WCET is merely 0.82 %, quite lower than conventional WCET measurement. Meanwhile it also has higher efficiency than conventional WCET analysis. Thus NL-WCET is suitable for being used during programming phase, and can help programmers to discover timeliness defects as early as possible. © 2016, Springer Science+Business Media New York.",Least square support vector machine | Machine learning | Real-time system | Software safety | Worst case execution time,Cluster Computing,2016-09-01,Article,"Meng, Fanqi;Su, Xiaohong;Qu, Zhaoyang",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85117892702,10.1007/s10586-021-03449-3,Intelligent transmission control layer for efficient node management in SDN,"The Software Defined Networking (SDN) promises exciting new networking functionality. However, there is always remains a chance of programming errors that result in unreliable data communication. The centralized programming model helps decrease bugs' probability where a single controller manages the whole network. Yet, many real-time events occur at switches and end hosts, which often affect and add a delay in the communication process. One of those events includes unannounced destination host migration after installing flow rules during receiving of data packets. Such destination host movement results in the loss of packets because controller is not aware of this recent event. Therefore, we need an efficient approach to transmit packets without any packet loss despite destination host migration. This paper proposed a design to achieve the objective as mentioned above by defining a layer named Intelligent Transmission Control Layer (ITCL). It monitors all the end hosts' connections at their specific locations and performs necessary actions whenever the connection state changes for one or multiple hosts. The controller collects information of end nodes and state change through ITCL using A star search algorithm. After that, it updates flow tables accordingly to accommodate a location-change scenario with a route-change policy. ICTL is developed on prototype-based implementation using a popular POX controller platform. By comparing ITCL with the existing solution, we conclude that our proposed approach exhibits efficient performance in terms of Packet loss, Bandwidth usage, and Network Throughput. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",A star search algorithm | Intelligent transmission control layer (ITCL) | Machine learning | Routing | Software defined networking (SDN) | Traffic engineering,Cluster Computing,2022-04-01,Article,"Aldabbas, Hamza;Khatatneh, Khalaf",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-61849088918,10.1007/s10664-008-9064-x,On modeling software defect repair time,"The ability to predict the time required to repair software defects is important for both software quality management and maintenance. Estimated repair times can be used to improve the reliability and time-to-market of software under development. This paper presents an empirical approach to predicting defect repair times by constructing models that use well-established machine learning algorithms and defect data from past software defect reports. We describe, as a case study, the analysis of defect reports collected during the development of a large medical software system. Our predictive models give accuracies as high as 93.44%, despite the limitations of the available data. We present the proposed methodology along with detailed experimental results, which include comparisons with other analytical modeling approaches. © 2008 Springer Science+Business Media, LLC.",Data mining | Defect report analysis | Quality assurance | Software testing | Testing management,Empirical Software Engineering,2009-04-01,Article,"Hewett, Rattikorn;Kijsanayothin, Phongphun",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84940725650,10.1007/s10664-014-9331-y,Automated prediction of bug report priority using multi-factor analysis,"Bugs are prevalent. To improve software quality, developers often allow users to report bugs that they found using a bug tracking system such as Bugzilla. Users would specify among other things, a description of the bug, the component that is affected by the bug, and the severity of the bug. Based on this information, bug triagers would then assign a priority level to the reported bug. As resources are limited, bug reports would be investigated based on their priority levels. This priority assignment process however is a manual one. Could we do better? In this paper, we propose an automated approach based on machine learning that would recommend a priority level based on information available in bug reports. Our approach considers multiple factors, temporal, textual, author, related-report, severity, and product, that potentially affect the priority level of a bug report. These factors are extracted as features which are then used to train a discriminative model via a new classification algorithm that handles ordinal class labels and imbalanced data. Experiments on more than a hundred thousands bug reports from Eclipse show that we can outperform baseline approaches in terms of average F-measure by a relative improvement of up to 209 %. © 2014, Springer Science+Business Media New York.",Bug report management | Multi-factor analysis | Priority prediction,Empirical Software Engineering,2015-10-04,Article,"Tian, Yuan;Lo, David;Xia, Xin;Sun, Chengnian",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84928629842,10.1007/s10664-015-9377-5,An empirical study of software release notes,"Release notes are an important source of information about a new software release. Such notes contain information regarding what is new, changed, and/or got fixed in a release. Despite the importance of release notes, they are rarely explored in the research literature. Little is known about the contained information, e.g., contents and structure, in release notes. To better understand the types of contained information in release notes, we manually analyzed 85 release notes across 15 different software systems. In our manual analysis, we identify six different types of information (e.g., caveats and addressed issues) that are contained in release notes. Addressed issues refer to new features, bugs, and improvements that were integrated in that particular release. We observe that most release notes list only a selected number of addressed issues (i.e., 6-26 % of all addressed issues in a release). We investigated nine different factors (e.g., issue priority and type) to better understand the likelihood of an issue being listed in release notes. The investigation is conducted on eight release notes of three software systems using four machine learning techniques. Results show that certain factors, e.g., issue type, have higher influence on the likelihood of an issue to be listed in release notes. We use machine learning techniques to automatically suggest the issues to be listed in release notes. Our results show that issues listed in all release notes can be automatically determined with an average precision of 84 % and an average recall of 90 %. To train and build the classification models, we also explored three scenarios: (a) having the user label some issues for a release and automatically suggest the remaining issues for that particular release, (b) using the previous release notes for the same software system, and (c) using prior releases for the current software system and the rest of the studied software systems. Our results show that the content of release notes vary between software systems and across the versions of the same software system. Nevertheless, automated techniques can provide reasonable support to the writers of such notes with little training data. Our study provides developers with empirically-supported advice about release notes instead of simply relying on adhoc advice from on-line inquiries. © 2015, Springer Science+Business Media New York.",Empirical study | Machine learning techniques | Software release notes,Empirical Software Engineering,2016-06-01,Article,"Abebe, Surafel Lemma;Ali, Nasir;Hassan, Ahmed E.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84941356343,10.1007/s10664-015-9401-9,Automated bug assignment: Ensemble-based machine learning in large scale industrial contexts,"Bug report assignment is an important part of software maintenance. In particular, incorrect assignments of bug reports to development teams can be very expensive in large software development projects. Several studies propose automating bug assignment techniques using machine learning in open source software contexts, but no study exists for large-scale proprietary projects in industry. The goal of this study is to evaluate automated bug assignment techniques that are based on machine learning classification. In particular, we study the state-of-the-art ensemble learner Stacked Generalization (SG) that combines several classifiers. We collect more than 50,000 bug reports from five development projects from two companies in different domains. We implement automated bug assignment and evaluate the performance in a set of controlled experiments. We show that SG scales to large scale industrial application and that it outperforms the use of individual classifiers for bug assignment, reaching prediction accuracies from 50 % to 89 % when large training sets are used. In addition, we show how old training data can decrease the prediction accuracy of bug assignment. We advice industry to use SG for bug assignment in proprietary contexts, using at least 2,000 bug reports for training. Finally, we highlight the importance of not solely relying on results from cross-validation when evaluating automated bug assignment. © 2015, Springer Science+Business Media New York.",Bug assignment | Bug reports | Classification | Ensemble learning | Industrial scale; Large scale | Machine learning,Empirical Software Engineering,2016-08-01,Article,"Jonsson, Leif;Borg, Markus;Broman, David;Sandahl, Kristian;Eldh, Sigrid;Runeson, Per",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85077025531,10.1007/s10664-019-09778-7,Selecting fault revealing mutants,"Mutant selection refers to the problem of choosing, among a large number of mutants, the (few) ones that should be used by the testers. In view of this, we investigate the problem of selecting the fault revealing mutants, i.e., the mutants that are killable and lead to test cases that uncover unknown program faults. We formulate two variants of this problem: the fault revealing mutant selection and the fault revealing mutant prioritization. We argue and show that these problems can be tackled through a set of ‘static’ program features and propose a machine learning approach, named FaRM, that learns to select and rank killable and fault revealing mutants. Experimental results involving 1,692 real faults show the practical benefits of our approach in both examined problems. Our results show that FaRM achieves a good trade-off between application cost and effectiveness (measured in terms of faults revealed). We also show that FaRM outperforms all the existing mutant selection methods, i.e., the random mutant sampling, the selective mutation and defect prediction (mutating the code areas pointed by defect prediction). In particular, our results show that with respect to mutant selection, our approach reveals 23% to 34% more faults than any of the baseline methods, while, with respect to mutant prioritization, it achieves higher average percentage of revealed faults with a median difference between 4% and 9% (from the random mutant orderings). © 2019, The Author(s).",Machine learning | Mutant prioritization | Mutant selection | Mutation testing,Empirical Software Engineering,2020-01-01,Article,"Titcheu Chekam, Thierry;Papadakis, Mike;Bissyandé, Tegawendé F.;Le Traon, Yves;Sen, Koushik",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85013500003,10.1007/s10845-019-01476-x,Segmentation-based deep-learning approach for surface-defect detection,"Automated surface-anomaly detection using machine learning has become an interesting and promising area of research, with a very high and direct impact on the application domain of visual inspection. Deep-learning methods have become the most suitable approaches for this task. They allow the inspection system to learn to detect the surface anomaly by simply showing it a number of exemplar images. This paper presents a segmentation-based deep-learning architecture that is designed for the detection and segmentation of surface anomalies and is demonstrated on a specific domain of surface-crack detection. The design of the architecture enables the model to be trained using a small number of samples, which is an important requirement for practical applications. The proposed model is compared with the related deep-learning methods, including the state-of-the-art commercial software, showing that the proposed approach outperforms the related methods on the specific domain of surface-crack detection. The large number of experiments also shed light on the required precision of the annotation, the number of required training samples and on the required computational cost. Experiments are performed on a newly created dataset based on a real-world quality control case and demonstrates that the proposed approach is able to learn on a small number of defected surfaces, using only approximately 25–30 defective training samples, instead of hundreds or thousands, which is usually the case in deep-learning applications. This makes the deep-learning method practical for use in industry where the number of available defective samples is limited. The dataset is also made publicly available to encourage the development and evaluation of new methods for surface-defect detection. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Computer vision; Deep learning; Industry 4.0; Quality control; Segmentation networks; Surface-defect detection; Visual inspection,Journal of Intelligent Manufacturing,2020,,"Tabernik D., Šela S., Skvarč J., Skočaj D.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85122659920,10.1007/s11042-021-11441-7,EkmEx - an extended framework for labeling an unlabeled fault dataset,"Software fault prediction (SFP) is a quality assurance process that identifies if certain modules are fault-prone (FP) or not-fault-prone (NFP). Hence, it minimizes the testing efforts incurred in terms of cost and time. Supervised machine learning techniques have capacity to spot-out the FP modules. However, such techniques require fault information from previous versions of software product. Such information, accumulated over the life-cycle of software, may neither be readily available nor reliable. Currently, clustering with experts’ opinions is a prudent choice for labeling the modules without any fault information. However, the asserted technique may not fully comprehend important aspects such as selection of experts, conflict in expert opinions, catering the diverse expertise of domain experts etc. In this paper, we propose a comprehensive framework named EkmEx that extends the conventional fault prediction approaches while providing mathematical foundation through aspects not addressed so far. The EkmEx guides in selection of experts, furnishes an objective solution for resolve of verdict-conflicts and manages the problem of diversity in expertise of domain experts. We performed expert-assisted module labeling through EkmEx and conventional clustering on seven public datasets of NASA. The empirical outcomes of research exhibit significant potential of the proposed framework in identifying FP modules across all seven datasets. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Expert opinion | Labeling datasets | Software fault proneness | Software metrics,Multimedia Tools and Applications,2022-04-01,Article,"Rizwan, Muhammad;Nadeem, Aamer;Sarwar, Sohail;Iqbal, Muddesar;Safyan, Muhammad;Qayyum, Zia Ul",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-79952760269,10.1007/s11047-010-9219-8,Priming: Making the reaction to intrusion or fault predictable,"We propose and evaluate an immuno-inspired approach for misbehavior detection in ad hoc wireless networks. Misbehavior is the result of an intrusion, or a software or hardware failure. Our misbehavior detection approach is inspired by the role of co-stimulation and priming in the biological immune system (BIS). We translate priming into a computational paradigm that can increase robustness as well as stimulate energy efficiency of misbehavior detection. We provide a detailed energy consumption analysis with respect to the IEEE 802.11 and IEEE 802.15.4 protocols. We analyze the efficiency of misbehavior detection with co-stimulation and priming. This analysis is complemented with experimental results. We show that co-stimulation and priming introduce new options such as the ability to choose a trade-off between detection performance and energy efficiency. We provide a summary of the challenges related to the design of co-stimulation and priming based architectures. We argue that co-stimulation and priming are rather general paradigms with possible applications in other areas than misbehavior detection. © 2010 Springer Science+Business Media B.V.",Ad hoc wireless network | Artificial immune system | Co-stimulation | Energy efficient design | Misbehavior detection | Sensor network,Natural Computing,2011-03-01,Conference Paper,"Drozda, Martin;Schaust, Sven;Schildt, Sebastian;Szczerbicka, Helena",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85106471496,10.1007/s11219-020-09546-7,An improved text classification modelling approach to identify security messages in heterogeneous projects,"Security remains under-addressed in many organisations, illustrated by the number of large-scale software security breaches. Preventing breaches can begin during software development if attention is paid to security during the software’s design and implementation. One approach to security assurance during software development is to examine communications between developers as a means of studying the security concerns of the project. Prior research has investigated models for classifying project communication messages (e.g., issues or commits) as security related or not. A known problem is that these models are project-specific, limiting their use by other projects or organisations. We investigate whether we can build a generic classification model that can generalise across projects. We define a set of security keywords by extracting them from relevant security sources, dividing them into four categories: asset, attack/threat, control/mitigation, and implicit. Using different combinations of these categories and including them in the training dataset, we built a classification model and evaluated it on industrial, open-source, and research-based datasets containing over 45 different products. Our model based on harvested security keywords as a feature set shows average recall from 55 to 86%, minimum recall from 43 to 71% and maximum recall from 60 to 100%. An average f-score between 3.4 and 88%, an average g-measure of at least 66% across all the dataset, and an average AUC of ROC from 69 to 89%. In addition, models that use externally sourced features outperformed models that use project-specific features on average by a margin of 26–44% in recall, 22–50% in g-measure, 0.4–28% in f-score, and 15–19% in AUC of ROC. Further, our results outperform a state-of-the-art prediction model for security bug reports in all cases. We find using sound statistical and effect size tests that (1) using harvested security keywords as features to train a text classification model improve classification models and generalise to other projects significantly. (2) Including features in the training dataset before model construction improve classification models significantly. (3) Different security categories represent predictors for different projects. Finally, we introduce new and promising approaches to construct models that can generalise across different independent projects. © 2021, The Author(s).",Classification model | Machine learning | Security | Software repository | Text classification,Software Quality Journal,2021-06-01,Article,"Oyetoyan, Tosin Daniel;Morrison, Patrick",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85037378589,10.1007/s11390-017-1783-2,What Are They Talking About? Analyzing Code Reviews in Pull-Based Development Model,"Code reviews in pull-based model are open to community users on GitHub. Various participants are taking part in the review discussions and the review topics are not only about the improvement of code contributions but also about project evolution and social interaction. A comprehensive understanding of the review topics in pull-based model would be useful to better organize the code review process and optimize review tasks such as reviewer recommendation and pull-request prioritization. In this paper, we first conduct a qualitative study on three popular open-source software projects hosted on GitHub and construct a fine-grained two-level taxonomy covering four level-1 categories (code correctness, pull-request decision-making, project management, and social interaction) and 11 level-2 subcategories (e.g., defect detecting, reviewer assigning, contribution encouraging). Second, we conduct preliminary quantitative analysis on a large set of review comments that were labeled by TSHC (a two-stage hybrid classification algorithm), which is able to automatically classify review comments by combining rule-based and machine-learning techniques. Through the quantitative study, we explore the typical review patterns. We find that the three projects present similar comments distribution on each subcategory. Pull-requests submitted by inexperienced contributors tend to contain potential issues even though they have passed the tests. Furthermore, external contributors are more likely to break project conventions in their early contributions. © 2017, Springer Science+Business Media, LLC & Science Press, China.",code review | pull-request | review comment,Journal of Computer Science and Technology,2017-11-01,Article,"Li, Zhi Xing;Yu, Yue;Yin, Gang;Wang, Tao;Wang, Huai Min",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85073253487,10.1007/s11390-019-1953-5,Bug Triaging Based on Tossing Sequence Modeling,"Bug triaging, which routes the bug reports to potential fixers, is an integral step in software development and maintenance. To make bug triaging more efficient, many researchers propose to adopt machine learning and information retrieval techniques to identify some suitable fixers for a given bug report. However, none of the existing proposals simultaneously take into account the following three aspects that matter for the efficiency of bug triaging: 1) the textual content in the bug reports, 2) the metadata in the bug reports, and 3) the tossing sequence of the bug reports. To simultaneously make use of the above three aspects, we propose iTriage which first adopts a sequence-to-sequence model to jointly learn the features of textual content and tossing sequence, and then uses a classification model to integrate the features from textual content, metadata, and tossing sequence. Evaluation results on three different open-source projects show that the proposed approach has significantly improved the accuracy of bug triaging compared with the state-of-the-art approaches. © 2019, Springer Science+Business Media, LLC & Science Press, China.",bug triaging | software repository minings | tossing sequence,Journal of Computer Science and Technology,2019-09-01,Article,"Xi, Sheng Qu;Yao, Yuan;Xiao, Xu Sheng;Xu, Feng;Lv, Jian",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84919754115,10.1016/j.asoc.2014.11.023,A systematic review of machine learning techniques for software fault prediction,"Background: Software fault prediction is the process of developing models that can be used by the software practitioners in the early phases of software development life cycle for detecting faulty constructs such as modules or classes. There are various machine learning techniques used in the past for predicting faults. Method: In this study we perform a systematic review of studies from January 1991 to October 2013 in the literature that use the machine learning techniques for software fault prediction. We assess the performance capability of the machine learning techniques in existing research for software fault prediction. We also compare the performance of the machine learning techniques with the statistical techniques and other machine learning techniques. Further the strengths and weaknesses of machine learning techniques are summarized. Results: In this paper we have identified 64 primary studies and seven categories of the machine learning techniques. The results prove the prediction capability of the machine learning techniques for classifying module/class as fault prone or not fault prone. The models using the machine learning techniques for estimating software fault proneness outperform the traditional statistical models. Conclusion: Based on the results obtained from the systematic review, we conclude that the machine learning techniques have the ability for predicting software fault proneness and can be used by software practitioners and researchers. However, the application of the machine learning techniques in software fault prediction is still limited and more number of studies should be carried out in order to obtain well formed and generalizable results. We provide future guidelines to practitioners and researchers based on the results obtained in this work. © 2014 Elsevier B.V. All rights reserved.",Machine learning | Software fault proneness | Systematic literature review,Applied Soft Computing Journal,2015-01-01,Article,"Malhotra, Ruchika",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85034604087,10.1016/j.asoc.2017.10.047,Learning to classify software defects from crowds: A novel approach,"In software engineering, associating each reported defect with a category allows, among many other things, for the appropriate allocation of resources. Although this classification task can be automated using standard machine learning techniques, the categorization of defects for model training requires expert knowledge, which is not always available. To circumvent this dependency, we propose to apply the learning from crowds paradigm, where training categories are obtained from multiple non-expert annotators (and so may be incomplete, noisy or erroneous) and, dealing with this subjective class information, classifiers are efficiently learnt. To illustrate our proposal, we present two real applications of the IBM's orthogonal defect classification working on the issue tracking systems from two different real domains. Bayesian network classifiers learnt using two state-of-the-art methodologies from data labeled by a crowd of annotators are used to predict the category (impact) of reported software defects. The considered methodologies show enhanced performance regarding the straightforward solution (majority voting) according to different metrics. This shows the possibilities of using non-expert knowledge aggregation techniques when expert knowledge is unavailable. © 2017 Elsevier B.V.",Bayesian network classifiers | Learning from crowds | Missing ground truth | Orthogonal defect classification,Applied Soft Computing Journal,2018-01-01,Article,"Hernández-González, Jerónimo;Rodriguez, Daniel;Inza, Iñaki;Harrison, Rachel;Lozano, Jose A.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85095565376,10.1016/j.asoc.2020.106839,Toward an adaptive protection scheme in active distribution networks: Intelligent approach fault detector,"Conventional protection schemes have proven insufficient for the protection of Active Distribution Networks (ADN). Novel protection schemes with an adaptive approach should be developed to guarantee the protection of ADN under all their operating conditions. This paper proposes an ADN adaptive protection methodology, which is based on an intelligent approach fault detector over locally available measurements. This approach uses Machine Learning (ML) based techniques to reduce the strong dependence of the adaptive protection schemes on the availability of communication systems and to determine if, over a fault condition, an Intelligent Electronic Device (IED) should operate considering the changes in operational conditions of an ADN. Additionally, the methodology takes into account different and remarkable recommendations for the use of ML techniques. The proposed methodology is validated on the modified IEEE 34-nodes test feeder. Additionally, it takes into consideration typical features of ADN and micro-grids like the load imbalance, reconfiguration, changes in impedance upstream from the micro-grid, and off-grid/on-grid operation modes. The results demonstrate the flexibility and simplicity of the methodology to determine the best accuracy performance among several ML models. Besides, they show the methodology's versatility to find the suitable ML model for IEDs located on different zones of an ADN. The ease of design's implementation, formulation of parameters, and promising test results indicate the potential for real-life applications. © 2020 Elsevier B.V.",Active distribution networks | Adaptive protection | Fault detector | Machine learning | Micro-grid,Applied Soft Computing,2021-01-01,Article,"Marín-Quintero, J.;Orozco-Henao, C.;Percybrooks, W. S.;Vélez, Juan C.;Montoya, Oscar Danilo;Gil-González, W.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85048823521,10.1016/j.bspc.2018.05.008,A novel heart-mobile interface for detection and classification of heart sounds,"Diagnosis of heart disease requires that a medical practitioner investigate heart auscultations for irregular sounds, followed by echocardiography and electrocardiography tests. These expensive tests also require specialized technicians to operate. We present a low-cost, patient-centered device for the initial screening of the heart sounds that can be potentially used by the users on themselves. They can later share these readings with their healthcare providers. We have created an innovative mobile-health service platform for analyzing and classifying heart sounds. The presented system enables remote patient-monitoring by integrating advanced wireless communications with a customized low-cost stethoscope. This system also permits remote management of a patient's cardiac status while maximizing patient mobility. The smartphone application facilitates recording, processing, visualizing, listening to, and classification of heart sounds. We build our classification model using the Mel-Frequency Cepstral Coefficient and Hidden Markov Model. This application is tested in a hospital environment to collect live recordings from patients with positive results. The smartphone application correctly detected 92.68% of abnormal heart conditions in clinical trials at UT Southwestern Hospital. © 2018 Elsevier Ltd",Heart sound classification | Hidden Markov Model | Mel-Frequency Cepstral Coefficient | Signal processing | Smartphone application | Split detection,Biomedical Signal Processing and Control,2018-08-01,Article,"Thiyagaraja, Shanti R.;Dantu, Ram;Shrestha, Pradhumna L.;Chitnis, Anurag;Thompson, Mark A.;Anumandla, Pruthvi T.;Sarma, Tom;Dantu, Siva",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85079317663,10.1016/j.bspc.2020.101886,Spark and Rule-KNN based scalable machine learning framework for EEG deceit identification,"Brain computer interface (BCI) provides communication between the computer and the brain. It is the combination of hardware and software which provides non-muscular channel to send the various messages to control the computer. BCI is useful in various medical applications such as patients with neuromuscular injuries, locked-in syndrome (LiS) etc. BCI is not only useful in medical applications, but also useful in lie detection, entertainment, etc. In this paper, spark and rule-KNN based scalable framework has been presented using BCI with the EEG data collected on 20 subjects in which 10 are acted as innocent and 10 are acted as guilty. Using BCI P300, Deceit identification Test (DIT) is performed. To perform DIT, we classify the P300 signals which have a positive peak of 300 ms–1000 ms in one stimulus start. Data processing is performed with band pass filter to cut the frequency ranges and features are extracted using non-parametric weighted feature extraction followed by rule based discriminant classification. For training and testing, the data ratio selected as 80:20 and achieved the accuracy 92.46 %. Proposed framework provides better results in comparison with existing models presented in literature. Hence this model is accurate, scalable and fault tolerant. © 2020 Elsevier Ltd",Apache spark | Brain computer interface | EEG data analysis | Non-parametric | Weighted feature extraction,Biomedical Signal Processing and Control,2020-04-01,Article,"Thakur, Santosh;Dharavath, Ramesh;Edla, Damodar Reddy",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85087483181,10.1016/j.cmpb.2020.105623,A novel U-Net approach to segment the cardiac chamber in magnetic resonance images with ghost artifacts,"Objective: We propose a robust technique for segmenting magnetic resonance images of post-atrial septal occlusion intervention in the cardiac chamber. Methods: A variant of the U-Net architecture is used to perform atrial segmentation via a deep convolutional neural network, and we compare performance with the Kass snake model. It can be used to determine the surgical success of atrial septal occlusion (ASO) pre- and post- the implantation of the septal occluder, which is based on the volume restoration of the right atria (RA) and left atria (LA). Results: The method was evaluated on a test dataset containing 550 two-dimensional image slices, outperforming conventional active contouring regarding the Dice similarity coefficient, Jaccard index, and Hausdorff distance, and achieving segmentation in the presence of ghost artifacts that occlude the atrium outline. This problem has been unsolvable using traditional machine learning algorithm pertaining to active contouring via the Kass snake algorithm. Moreover, the proposed technique is closer to manual segmentation than the snakes active contour model in mean of atrial area (M-AA), mean of atrial maximum diameter (M-AMXD), mean atrial minimum diameter (M-AMID), and mean angle of the atrial long axis (M-AALA). Conclusion: After segmentation, we compute the volume ratio of right to left atria, obtaining a smaller ratio that indicates better restoration. Hence, the proposed technique allows to evaluate the surgical success of atrial septal occlusion and may support diagnosis regarding the accurate evaluation of atrial septal defects before and after occlusion procedures. © 2020",Active contour | Atrial septal defect | Deep learning | Kass snake algorithm | MRI Diagnostics | U-Net,Computer Methods and Programs in Biomedicine,2020-11-01,Article,"Zhao, Ming;Wei, Yang;Lu, Yu;Wong, Kelvin K.L.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84928826771,10.1016/j.comcom.2015.01.006,Fault diagnosis in DSL networks using support vector machines,"Abstract The adequate operation for a number of service distribution networks relies on the effective maintenance and fault management of their underlay DSL infrastructure. Thus, new tools are required in order to adequately monitor and further diagnose anomalies that other segments of the DSL network cannot identify due to the pragmatic issues raised by hardware or software misconfigurations. In this work we present a fundamentally new approach for classifying known DSL-level anomalies by exploiting the properties of novelty detection via the employment of one-class Support Vector Machines (SVMs). By virtue of the imbalance residing in the training samples that consequently lead to problematic prediction outcomes when used within two-class formulations, we adopt the properties of one-class classification and construct models for independently identifying and classifying a single type of a DSL-level anomaly. Given the fact that the greater number of the installed Digital Subscriber Line Access Multiplexers (DSLAMs) within the DSL network of a large European ISP were misconfigured, thus unable to accurately flag anomalous events, we utilize as inference solutions the models derived by the one-class SVM formulations built by the known labels as flagged by the much smaller number of correctly configured DSLAMs in the same network in order to aid the classification aspect against the monitored unlabeled events. By reaching an average over 95% on a number of classification accuracy metrics such as precision, recall and F-score we show that one-class SVM classifiers overcome the biased classification outcomes achieved by the traditional two-class formulations and that they may constitute as viable and promising components within the design of future network fault management strategies. In addition, we demonstrate their superiority over commonly used two-class machine learning approaches such as Decision Trees and Bayesian Networks that has been used in the same context within past solutions. © 2015 Elsevier B.V. All rights reserved.",DSL anomalies | Network management | One-class classifiers | Supervised learning | Support vector machines,Computer Communications,2015-05-15,Article,"Marnerides, A. K.;Malinowski, S.;Morla, R.;Kim, H. S.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85075379402,10.1016/j.commatsci.2019.109364,Classification of clusters in collision cascades,"The structure of defect clusters formed in a displacement cascade plays a significant role in the micro-structural evolution during irradiation. We present a novel method to pattern match and classify defect clusters from Molecular Dynamics simulations of collision cascades. The methods are applied on a database of collision cascades in Fe and W at energies ranging from 10 keV to 200 keV. The results show families and classes of cluster shapes providing new insights and parameters that can be used in simulations at higher scales. We discuss each step, starting from efficient identification of defects from simulation output to reduction of physics problems to machine learning stages viz. feature engineering, dimensionality reduction and unsupervised classification. The open-source software implementation of the exploratory data analysis and visualizations of the cluster patterns provides new approach to the study of defect clusters in cascades. We find the geometrical histograms of angles and distances between neighboring defects in a cluster to characterize different qualitative cluster shapes. We show that the histograms can be effectively used to find clusters of specific shapes in a database of cascades irrespective of differences in superfluous details. We further use these histogram representations to classify the clusters with density based unsupervised classification. We find many already known categories of clusters such as crowdions, planar crowdion pairs, rings or C15 cluster, etc. as different classes. The results also show new cluster shape classes. We analyse quantitative properties of the classes such as their sizes, dimensionality and preferences to elements and energies. The cluster shape preferences for Fe and W agree with previous studies. The distribution of cluster shapes along with their properties like diffusivity, stability, etc. can be used as input to higher scale models in a multi-scale radiation damage study. © 2019 Elsevier B.V.",Classification | Cluster shapes | Collision cascades | Machine learning | Molecular dynamics | Pattern matching | Radiation damage,Computational Materials Science,2020-02-01,Article,"Bhardwaj, Utkarsh;Sand, Andrea E.;Warrier, Manoj",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85058367657,10.1016/j.cpc.2018.11.016,XTALOPT Version r12: An open-source evolutionary algorithm for crystal structure prediction,"Version 12 of XTALOPT, an evolutionary algorithm for crystal structure prediction, is now available for download from the CPC program library or the XTALOPT website, http://xtalopt.github.io. The new version includes: a method for calculating hardness using a machine learning algorithm within AFLOW-ML (Automatic FLOW for Materials Discovery — Machine Learning), the ability to predict hard materials, a generic optimizer (which allows the user to employ many optimizers that were previously not supported), and the ability to generate simulated XRD (X-ray diffraction) patterns. New version program summary: Program Title: XTALOPT Program Files doi: http://dx.doi.org/10.17632/jt5pvnnm39.3 Licensing provisions: 3-Clause BSD [1] Programming language: C++ External routines/libraries: QT [2], QWT [3], AVOGADRO2 [4,5] (optional), LIBSSH [6], OPEN BABEL [7,8] (separate executable), OBJCRYST++ [9,10] (separate executable), AFLOW-ML [11,12] (through network), and an external program for optimizing the geometries of extended systems. Subprograms used: PUGIXML [13], SPGLIB [14], XTALCOMP [15], RANDSPG [16]. Nature of problem: Computationally predicting stable and/or hard crystal structures given only their stoichiometry. Solution method: Evolutionary algorithms (EAs), which use ideas from biological evolution, are optimization algorithms whose goal is to find the optimal solution for a problem that has many degrees of freedom. For a priori crystal structure prediction (CSP), EAs search to find the lattice parameters and atomic coordinates that, for example, minimize the energy/enthalpy or maximize the hardness. The XTALOPT EA for crystal structure prediction is published under the 3-Clause BSD License, which is an open source license that is officially recognized by the Open Source Initiative [17]. More information is available in the following publications: XTALOPT's original implementation [18], previous version announcements [19–22], manuscripts detailing the subprograms XTALOPT employs: XTALCOMP [23] and RANDSPG [24], and the XtalOpt website [25]. Reasons for new version: Since the release of XTALOPT version r11 in January 2018, the following changes have been made: • Added a hardness calculation via AFLOW-ML (Automatic FLOW for Materials Discovery — Machine Learning). • Added a hardness fitness function, which allows for the prediction of hard structures. • Added a generic optimizer, which allows the user to employ many previously unsupported optimizers for minimizing the geometry of an extended system. • Added the ability to generate a simulated XRD (X-ray Diffraction) pattern. • Added the ability to use different optimizers and queuing interfaces for each optimization step. • Implemented various bug fixes. Summary of revisions: The theoretical hardness of a crystal can now be automatically calculated during an XTALOPT run. The hardness is calculated through a linear relationship with the shear modulus (originally discovered by Teter [26]) as reported by Chen [27]. The shear modulus is obtained via AFLOW-ML [11,12], which employs a machine learning model trained with the AFLOW Automatic Elasticity Library (AEL) [28,29]. As a result, the EA can employ a new fitness function, which attempts to minimize the enthalpy and maximize the hardness of the predicted structures. This facilitates the search for crystals that are both stable and hard. Additionally, a new generic optimizer was added that allows the user to employ optimizers that were previously not supported (ADF BAND [30] and ADF DFTB [31] are examples that we have thoroughly tested). The only caveat is that the rules for the generic optimizer, which are provided in the online tutorial, must be followed. OPEN BABEL [7,8] is used to read the output of the generic optimizer. Because of the addition of an executable that uses OBJCRYST++ [9,10], a simulated XRD pattern of a crystal can now also be generated during a structure search. Finally, different optimizers and different queuing interfaces can now be used for each optimization step. © 2018 Elsevier B.V.",Crystal structures | Evolutionary algorithm | Genetic algorithm | Materials discovery | Structure prediction | Superhard materials | X-ray diffraction pattern,Computer Physics Communications,2019-04-01,Article,"Avery, Patrick;Toher, Cormac;Curtarolo, Stefano;Zurek, Eva",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85040456672,10.1016/j.engappai.2017.12.011,An Artificial Intelligence paradigm for troubleshooting software bugs,"Software bugs are prevalent and fixing them is time consuming, and therefore troubleshooting is an important part of software engineering. This paper presents a novel paradigm for incorporating Artificial Intelligence (AI) in the modern software troubleshooting process that can drastically reduce troubleshooting costs. In this paradigm, which we call Learn, Diagnose, and Plan (LDP), we integrate three AI technologies: (1) machine learning: learning from source-code structure, revisions history and past failures, which software components are more likely to contain bugs, (2) automated diagnosis: identifying the software components that need to be modified in order to fix an observed bug, and (3) automated planning: planning additional tests when such are needed to improve diagnostic accuracy. Importantly, these AI technologies are integrated in LDP in a synergistic manner: the diagnosis algorithm is modified to consider the learned fault predictions and the planner is modified to consider the possible diagnoses outputted by the diagnosis algorithm. The overall solution is demonstrated on real faults observed in four open source software projects. © 2017 Elsevier Ltd",Artificial Intelligence | Automated diagnosis | Automated troubleshooting | Software engineering | Software fault prediction,Engineering Applications of Artificial Intelligence,2018-03-01,Article,"Elmishali, Amir;Stern, Roni;Kalech, Meir",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84951567957,10.1016/j.envsoft.2015.10.029,Identifying the controls on coastal cliff landslides using machine-learning approaches,"Transformations are underway in our ability to collect and interrogate remotely sensed data. Here we explore the utility of three machine-learning methods for identifying the controls on coastal cliff landsliding using a dataset from Auckland, New Zealand. Models were built using all available data with a resampling approach used to evaluate uncertainties. All methods identify two dominant landslide predictors (unfailed cliff slope angle and fault proximity). This information could support a range of management approaches, from the development of 'rules-of-thumb' to detailed models that incorporate all predictor information. In our study all statistical approaches correctly predict a high proportion (>85%) of cases. Similar 'success' has been shown in other studies, but important questions should be asked about possible error sources, particularly in regard to absence data. In coastal landslide studies sign decay is a vexing issue, because sites prone to landsliding may also be sites of rapid evidence removal. © 2015 Elsevier Ltd..",Cliffs | Erosion | Landsliding | Machine learning | Maxent | Regression trees,Environmental Modelling and Software,2016-02-01,Article,"Dickson, Mark E.;Perry, George L.W.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-78650716308,10.1016/j.eswa.2010.08.130,Thresholds based outlier detection approach for mining class outliers: An empirical case study on software measurement datasets,"Predicting the fault-proneness labels of software program modules is an emerging software quality assurance activity and the quality of datasets collected from previous software version affects the performance of fault prediction models. In this paper, we propose an outlier detection approach using metrics thresholds and class labels to identify class outliers. We evaluate our approach on public NASA datasets from PROMISE repository. Experiments reveal that this novel outlier detection method improves the performance of robust software fault prediction models based on Naive Bayes and Random Forests machine learning algorithms. © 2010 Elsevier Ltd. All rights reserved.",Empirical software engineering | Outlier detection | Software fault prediction | Software metrics thresholds,Expert Systems with Applications,2011-04-01,Article,"Alan, Oral;Catal, Cagatay",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84977070618,10.1016/j.eswa.2016.06.005,A multiobjective weighted voting ensemble classifier based on differential evolution algorithm for text sentiment classification,"Typically performed by supervised machine learning algorithms, sentiment analysis is highly useful for extracting subjective information from text documents online. Most approaches that use ensemble learning paradigms toward sentiment analysis involve feature engineering in order to enhance the predictive performance. In response, we sought to develop a paradigm of a multiobjective, optimization-based weighted voting scheme to assign appropriate weight values to classifiers and each output class based on the predictive performance of classification algorithms, all to enhance the predictive performance of sentiment classification. The proposed ensemble method is based on static classifier selection involving majority voting error and forward search, as well as a multiobjective differential evolution algorithm. Based on the static classifier selection scheme, our proposed ensemble method incorporates Bayesian logistic regression, naïve Bayes, linear discriminant analysis, logistic regression, and support vector machines as base learners, whose performance in terms of precision and recall values determines weight adjustment. Our experimental analysis of classification tasks, including sentiment analysis, software defect prediction, credit risk modeling, spam filtering, and semantic mapping, suggests that the proposed classification scheme can predict better than conventional ensemble learning methods such as AdaBoost, bagging, random subspace, and majority voting. Of all datasets examined, the laptop dataset showed the best classification accuracy (98.86%). © 2016 Elsevier Ltd",Ensemble learning | Multiobjective optimization | Sentiment analysis | Weighted majority voting,Expert Systems with Applications,2016-11-15,Article,"Onan, Aytuğ;Korukoğlu, Serdar;Bulut, Hasan",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85111071972,10.1016/j.eswa.2021.115364,Multiple Similarity-based Features Blending for Detecting Code Clones using Consensus-Driven Classification,"Code clone detection helps to reduce the costs associated with software maintenance and bug prevention. Machine learning methods have previously suggested many ways by which to detect code clones. The majority of clone detectors are traditional in their approach, they can detect syntactic clones but are poor at detecting semantic clones. Researchers use machine learning to detect semantic clones and automatically scan the data to learn latent semantic features. In this study, we have introduced a new formal model of similarity which combines similarity measures so that method blocks can measure both the syntactic and semantic distances between method block pairs. The uniqueness of our study is in the use of different similarity measures, and similarity scores as features in machine learning, to detect code clones. We use a number of similarity measure computations to extract similarity score features, these features are then represented as vectors. Using ensemble classification models, we perform extensive comparisons and evaluations of the effectiveness of our proposed idea. The results indicate that our approach is significantly better at detecting clone types compared to contemporary code clone detectors. We achieved a 99% success rate in detecting cloned codes based on F-score, recall, and precision. Our approach achieves 98–100% accuracy in the majority of cases. © 2021 Elsevier Ltd",Classification | Code clones | Features | Machine learning | Semantic clones | Similarity measures | Software engineering | Syntactic clones,Expert Systems with Applications,2021-11-30,Article,"Sheneamer, Abdullah M.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85120887507,10.1016/j.eswa.2021.116290,Dilated convolutional neural network based model for bearing faults and broken rotor bar detection in squirrel cage induction motors,"Deep learning can play a pivotal role in early fault detection in squirrel cage induction motors (SCIMs) and achieving Industry 4.0. SCIM finds application in industries like mining, textile, manufacturing, and many more. Early fault detection in SCIM can significantly reduce downtime and optimize productivity. This paper proposes a novel fault detection technique for bearing faults and broken rotor bar detection in SCIM using the dilated convolutional neural network-based model. A simple 1-D signal to image conversion technique is also proposed for transforming the 1-D vibration signal acquired from multiple accelerometers to images. The proposed method provides an end-to-end learning solution for fault detection. The propounded approach has accomplished an average accuracy of more than 99.50%. A comparison has also been made between different convolutional neural network (CNN) models and conventional machine learning models to show the proposed method's efficiency. The complete experimental work has been carried out on a 5 kW, 3-phase, 415 V, 50 Hz SCIM. The dilated CNN model development has been done using python software, and the packages used are Keras and TensorFlow. © 2021 Elsevier Ltd",Bearing fault | Broken rotor bar | Dilated convolutional neural network (DCNN) | Squirrel cage induction motor (SCIM),Expert Systems with Applications,2022-04-01,Article,"Kumar, Prashant;Hati, Ananda Shankar",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84917709364,10.1016/j.future.2014.11.015,On-line failure prediction in safety-critical systems,"In safety-critical systems such as Air Traffic Control system, SCADA systems, Railways Control Systems, there has been a rapid transition from monolithic systems to highly modular ones, using off-the-shelf hardware and software applications possibly developed by different manufactures. This shift increased the probability that a fault occurring in an application propagates to others with the risk of a failure of the entire safety-critical system. This calls for new tools for the on-line detection of anomalous behaviors of the system, predicting thus a system failure before it happens, allowing the deployment of appropriate mitigation policies. The paper proposes a novel architecture, namely CASPER, for online failure prediction that has the distinctive features to be (i) black-box: no knowledge of applications internals and logic of the system is required (ii) non-intrusive: no status information of the components is used such as CPU or memory usage; The architecture has been implemented to predict failures in a real Air Traffic Control System. CASPER exhibits high degree of accuracy in predicting failures with low false positive rate. The experimental validation shows how operators are provided with predictions issued a few hundred of seconds before the occurrence of the failure. © 2014 Elsevier B.V.",Complex distributed systems | Complex event processing | Critical infrastructures | Failure prediction | Machine learning,Future Generation Computer Systems,2015-01-01,Article,"Baldoni, Roberto;Montanari, Luca;Rizzuto, Marco",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84899897203,10.1016/j.ijar.2014.02.008,Bayesian network approach to multinomial parameter learning using data and expert judgments,"One of the hardest challenges in building a realistic Bayesian Network (BN) model is to construct the node probability tables (NPTs). Even with a fixed predefined model structure and very large amounts of relevant data, machine learning methods do not consistently achieve great accuracy compared to the ground truth when learning the NPT entries (parameters). Hence, it is widely believed that incorporating expert judgments can improve the learning process. We present a multinomial parameter learning method, which can easily incorporate both expert judgments and data during the parameter learning process. This method uses an auxiliary BN model to learn the parameters of a given BN. The auxiliary BN contains continuous variables and the parameter estimation amounts to updating these variables using an iterative discretization technique. The expert judgments are provided in the form of constraints on parameters divided into two categories: linear inequality constraints and approximate equality constraints. The method is evaluated with experiments based on a number of well-known sample BN models (such as Asia, Alarm and Hailfinder) as well as a real-world software defects prediction BN model. Empirically, the new method achieves much greater learning accuracy (compared to both state-of-the-art machine learning techniques and directly competing methods) with much less data. For example, in the software defects BN for a sample size of 20 (which would be considered difficult to collect in practice) when a small number of real expert constraints are provided, our method achieves a level of accuracy in parameter estimation that can only be matched by other methods with much larger sample sizes (320 samples required for the standard machine learning method, and 105 for the directly competing method with constraints). © 2014 Elsevier B.V. All rights reserved.",Bayesian networks | Expert judgments | Multinomial parameter learning,International Journal of Approximate Reasoning,2014-01-01,Article,"Zhou, Yun;Fenton, Norman;Neil, Martin",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85011277924,10.1016/j.infsof.2017.01.002,Analyzing and predicting effort associated with finding and fixing software faults,"Context: Software developers spend a significant amount of time fixing faults. However, not many papers have addressed the actual effort needed to fix software faults. Objective: The objective of this paper is twofold: (1) analysis of the effort needed to fix software faults and how it was affected by several factors and (2) prediction of the level of fix implementation effort based on the information provided in software change requests. Method: The work is based on data related to 1200 failures, extracted from the change tracking system of a large NASA mission. The analysis includes descriptive and inferential statistics. Predictions are made using three supervised machine learning algorithms and three sampling techniques aimed at addressing the imbalanced data problem. Results: Our results show that (1) 83% of the total fix implementation effort was associated with only 20% of failures. (2) Both post-release failures and safety-critical failures required more effort to fix than pre-release and non-critical counterparts, respectively; median values were two or more times higher. (3) Failures with fixes spread across multiple components or across multiple types of software artifacts required more effort. The spread across artifacts was more costly than spread across components. (4) Surprisingly, some types of faults associated with later life-cycle activities did not require significant effort. (5) The level of fix implementation effort was predicted with 73% overall accuracy using the original, imbalanced data. Oversampling techniques improved the overall accuracy up to 77% and, more importantly, significantly improved the prediction of the high level effort, from 31% to 85%. Conclusions: This paper shows the importance of tying software failures to changes made to fix all associated faults, in one or more software components and/or in one or more software artifacts, and the benefit of studying how the spread of faults and other factors affect the fix implementation effort. © 2017 Elsevier B.V.",Analysis | Case study | Prediction | Software faults and failures | Software fix implementation effort,Information and Software Technology,2017-07-01,Article,"Hamill, Maggie;Goseva-Popstojanova, Katerina",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85063908968,10.1016/j.infsof.2019.03.014,Ranking of software developers based on expertise score for bug triaging,"Context: Existing bug triage approaches for developer recommendation systems are mainly based on machine learning (ML) techniques. These approaches have shown low prediction accuracy and high bug tossing length (BTL). Objective: The objective of this paper is to develop a robust algorithm for reducing BTL based on the concept of developer expertise score (DES). Method: None of the existing approaches to the best of our knowledge have utilized metrics to build developer expertise score. The novel strategy of DES is consisted of two stages: Stage-I consisted of an offline process for detecting the developers based on DES which computes the score using priority, versatility and average fix-time for his individual contributions. The online system process consisted of finding the capable developers using three kinds of similarity measures (feature-based, cosine-similarity and Jaccard). Stage-II of the online process consisted of simply ranking the developers. Hit-ratio and reassignment accuracy were used for performance evaluation. We compared our system against the ML-based bug triaging approaches using three types of classifiers: Navies Bayes, Support Vector Machine and C4.5 paradigms. Results: By adapting the five open source databases, namely: Mozilla, Eclipse, Netbeans, Firefox, and Freedesktop, covering 41,622 bug reports, our novel DES system yielded a mean accuracy, precision, recall rate and F-score of 89.49%, 89.53%, 89.42% and 89.49%, respectively, reduced BTLs of up to 88.55%. This demonstrates an improvement of up to 20% over existing strategies. Conclusion: This work presented a novel developer recommendation algorithm to rank the developers based on a metric-based integrated score for bug triaging. This integrated score was based on the developer's expertise with an objective to improve (i) bug assignment and (ii) reduce the bug tossing length. Such architecture has an application in software bug triaging frameworks. © 2019",Bug assignment | Bug reports | Bug repository | Bug tossing | Bug triaging | Developer contribution assessment | Developer expertise | Open source software (OSS) | Software metrics | Software process,Information and Software Technology,2019-08-01,Article,"Yadav, Asmita;Singh, Sandeep Kumar;Suri, Jasjit S.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85065891466,10.1016/j.infsof.2019.05.007,An HMM-based approach for automatic detection and classification of duplicate bug reports,"Context: Software projects rely on their issue tracking systems to guide maintenance activities of software developers. Bug reports submitted to the issue tracking systems carry crucial information about the nature of the crash (such as texts from users or developers and execution information about the running functions before the occurrence of a crash). Typically, big software projects receive thousands of reports every day. Objective: The aim is to reduce the time and effort required to fix bugs while improving software quality overall. Previous studies have shown that a large amount of bug reports are duplicates of previously reported ones. For example, as many as 30% of all reports in for Firefox are duplicates. Method: While there exist a wide variety of approaches to automatically detect duplicate bug reports by natural language processing, only a few approaches have considered execution information (the so-called stack traces) inside bug reports. In this paper, we propose a novel approach that automatically detects duplicate bug reports using stack traces and Hidden Markov Models. Results: When applying our approach to Firefox and GNOME datasets, we show that, for Firefox, the average recall for Rank k = 1 is 59%, for Rank k = 2 is 75.55%. We start reaching the 90% recall from k = 10. The Mean Average Precision (MAP) value is up to 76.5%. For GNOME, The recall at k = 1 is around 63%, while this value increases by about 10% for k = 2. The recall increases to 97% for k = 11. A MAP value of up to 73% is achieved. Conclusion: We show that HMM and stack traces are a powerful combination for detecting and classifying duplicate bug reports in large bug repositories. © 2019",Duplicate bug reports | Hidden Markov models | Machine learning | Mining software repositories | Stack traces,Information and Software Technology,2019-09-01,Article,"Ebrahimi, Neda;Trabelsi, Abdelaziz;Islam, Md Shariful;Hamou-Lhadj, Abdelwahab;Khanmohammadi, Kobra",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-51949084076,10.1016/j.infsof.2019.106205,Automatic prediction of the severity of bugs using stack traces and categorical features,"Context: The severity of a bug is often used as an indicator of how a bug negatively affects system functionality. It is used by developers to prioritize bugs which need to be fixed. The problem is that, for various reasons, bug submitters often enter the incorrect severity level, delaying the bug resolution process. Techniques that can automatically predict the severity of a bug can significantly reduce the bug triaging overhead. In our previous work, we showed that the accuracy of description-based severity prediction techniques could be significantly improved by using stack traces as a source of information. Objective: In this study, we expand our previous work by exploring the effect of using categorical features, in addition to stack traces, to predict the severity of bugs. These categorical features include faulty product, faulty component, and operating system. We experimented with other features and observed that they do not improve the severity prediction accuracy. A Software system is composed of many products; each has a set of components. Components interact with each to provide the functionality of the product. The operating system field refers to the operating system on which the software was running on during the crash. Method: The proposed approach uses a linear combination of stack trace and categorical features similarity to predict the severity. We adopted a cost sensitive K Nearest Neighbor approach to overcome the unbalance label distribution problem and improve the classifier accuracy. Results: Our experiments on bug reports of Eclipse submitted between 2001 and 2015 and Gnome submitted between 1999 and 2015 show that the accuracy of our severity prediction approach can be improved from 5% to 20% by considering categorical features, in addition to stack traces. Conclusion: The accuracy of predicting the severity of bugs is higher when combining stack traces and three categorical features, product, component, and operating system. © 2019",Bug severity; Machine learning mining software repositories; Software maintenance; Stack traces,Information and Software Technology,2020,,"Sabor K.K., Hamdaqa M., Hamou-Lhadj A.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85083464754,10.1016/j.infsof.2020.106314,LTRWES: A new framework for security bug report detection,"Context: Security bug reports (SBRs) usually contain security-related vulnerabilities in software products, which could be exploited by malicious attackers. Hence, it is important to identify SBRs quickly and accurately among bug reports (BRs) that have been disclosed in bug tracking systems. Although a few methods have been already proposed for the detection of SBRs, challenging issues still remain due to noisy samples, class imbalance and data scarcity. Object: This motivates us to reveal the potential challenges faced by the state-of-the-art SBRs prediction methods from the viewpoint of data filtering and representation. Furthermore, the purpose of this paper is also to provide a general framework and new solutions to solve these problems. Method: In this study, we propose a novel approach LTRWES that incorporates learning to rank and word embedding into the identification of SBRs. Unlike previous keyword-based approaches, LTRWES is a content-based data filtering and representation framework that has several desirable properties not shared in other methods. Firstly, it exploits ranking model to efficiently filter non-security bug reports (NSBRs) that have higher content similarity with respect to SBRs. Secondly, it applies word embedding technology to transform the rest of NSBRs, together with SBRs, into low-dimensional real-value vectors. Result: Experiment results on benchmark and large real-world datasets show that our proposed method outperforms the state-of-the-art method. Conclusion: Overall, the LTRWES is valid with high performance. It will help security engineers to identify SBRs from thousands of NSBRs more accurately than existing algorithms. Therefore, this will positively encourage the research and development of the content-based methods for security bug report detection. © 2020 Elsevier B.V.",Content-based filtering | Machine learning | Security bug report | Word embedding,Information and Software Technology,2020-08-01,Article,"Jiang, Yuan;Lu, Pengcheng;Su, Xiaohong;Wang, Tiantian",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85086502960,10.1016/j.infsof.2020.106344,Efficient feature extraction model for validation performance improvement of duplicate bug report detection in software bug triage systems,"Context: There are many duplicate bug reports in the semi-structured software repository of various software bug triage systems. The duplicate bug report detection (DBRD) process is a significant problem in software triage systems. Objective: The DBRD problem has many issues, such as efficient feature extraction to calculate similarities between bug reports accurately, building a high-performance duplicate detector model, and handling continuous real-time queries. Feature extraction is a technique that converts unstructured data to structured data. The main objective of this study is to improve the validation performance of DBRD using a feature extraction model. Method: This research focuses on feature extraction to build a new general model containing all types of features. Moreover, it introduces a new feature extractor method to describe a new viewpoint of similarity between texts. The proposed method introduces new textual features based on the aggregation of term frequency and inverse document frequency of text fields of bug reports in uni-gram and bi-gram forms. Further, a new hybrid measurement metric is proposed for detecting efficient features, whereby it is used to evaluate the efficiency of all features, including the proposed ones. Results: The validation performance of DBRD was compared for the proposed features and state-of-the-art features. To show the effectiveness of our model, we applied it and other related studies to DBRD of the Android, Eclipse, Mozilla, and Open Office datasets and compared the results. The comparisons showed that our proposed model achieved (i) approximately 2% improvement for accuracy and precision and more than 4.5% and 5.9% improvement for recall and F1-measure, respectively, by applying the linear regression (LR) and decision tree (DT) classifiers and (ii) a performance of 91%−99% (average ~97%) for the four metrics, by applying the DT classifier as the best classifier. Conclusion: Our proposed features improved the validation performance of DBRD concerning runtime performance. The pre-processing methods (primarily stemming) could improve the validation performance of DBRD slightly (up to 0.3%), but rule-based machine learning algorithms are more useful for the DBRD problem. The results showed that our proposed model is more effective both for the datasets for which state-of-the-art approaches were effective (i.e., Mozilla Firefox) and those for which state-of-the-art approaches were less effective (i.e., Android). The results also showed that the combination of all types of features could improve the validation performance of DBRD even for the LR classifier with less validation performance, which can be implemented easily for software bug triage systems. Without using the longest common subsequence (LCS) feature, which is effective but time-consuming, our proposed features could cover the effectiveness of LCS with lower time-complexity and runtime overhead. In addition, a statistical analysis shows that the results are reliable and can be generalized to other datasets or similar classifiers. © 2020",Bug reports | Dimension reduction | Duplicate detection | Feature extraction | Feature selection | Information retrieval | Natural language processing | Textual similarity metric,Information and Software Technology,2020-10-01,Article,"Soleimani Neysiani, Behzad;Babamir, Seyed Morteza;Aritsugi, Masayoshi",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85090970251,10.1016/j.infsof.2020.106408,Large-scale intent analysis for identifying large-review-effort code changes,"Context: Code changes to software occur due to various reasons such as bug fixing, new feature addition, and code refactoring. Change intents have been studied for years to help developers understand the rationale behind code commits. However, in most existing studies, the intent of the change is rarely leveraged to provide more specific, context aware analysis. Objective: In this paper, we present the first study to leverage change intent to characterize and identify Large-Review-Effort (LRE) changes—changes with large review effort. Method: Specifically, we first propose a feedback-driven and heuristics-based approach to identify change intents of code changes. We then characterize the changes regarding review effort by using various features extracted from change metadata and the change intents. We further explore the feasibility of automatically classifying LRE changes. We conduct our study on four large-scale projects, one from Microsoft and three are open source projects, i.e., Qt, Android, and OpenStack. Results: Our results show that, (i) code changes with some intents (i.e., Feature and Refactor) are more likely to be LRE changes, (ii) machine learning based prediction models are applicable for identifying LRE changes, and (iii) prediction models built for code changes with some intents achieve better performance than prediction models without considering the change intent, the improvement in AUC can be up to 19 percentage points and is 7.4 percentage points on average. Conclusion: The change intent analysis and its application on LRE identification proposed in this study has already been used in Microsoft to provide the review effort and intent information of changes for reviewers to accelerate the review process. To show how to deploy our approaches in real-world practice, we report a case study of developing and deploying the intent analysis system in Microsoft. Moreover, we also evaluate the usefulness of our approaches by using a questionnaire survey. The feedback from developers demonstrate its practical value. © 2020 Elsevier B.V.",Change intent analysis | Machine learning | Review effort,Information and Software Technology,2021-02-01,Article,"Wang, Song;Bansal, Chetan;Nagappan, Nachiappan",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85103314692,10.1016/j.infsof.2021.106573,An empirical study on clone consistency prediction based on machine learning,"Context: Code Clones have been accepted as a common phenomenon in software, thanks to the increasing demand for rapid production of software. The existence of code clones is recognized by developers in the form of clone group, which includes several pieces of clone fragments that are similar to one another. A change in one of these clone fragments may indicate necessary “consistent changes” are required for the rest of the clones within the same group, which can increase extra maintenance costs. A failure in making such consistent change when it is necessary is commonly known as a “clone consistency-defect”, which can adversely impact software maintainability. Objective: Predicting the need for “clone consistent changes” after successful clone-creating or clone-changing operations can help developers maintain clone changes effectively, avoid consistency-defects and reduce maintenance cost. Method: In this work, we use several sets of attributes in two scenarios of clone operations (clone-creating and clone-changing), and conduct an empirical study on five different machine-learning methods to assess each of their clone consistency predictability — whether any one of the clone operations will require or be free of clone consistency maintenance in future. Results: We perform our experiments on eight open-source projects. Our study shows that such predictions can be reasonably effective both for clone-creating and changing operating instances. We also investigate the use of five different machine-learning methods for predictions and show that our selected features are effective in predicting the needs of consistency-maintenance across all selected machine-learning methods. Conclusion: The empirical study conducted here demonstrates that the models developed by different machine-learning methods with the specified sets of attributes have the ability to perform clone-consistency prediction. © 2021 Elsevier B.V.",Clone consistency prediction | Clone consistent change | Code clones | Machine learning | Software maintenance,Information and Software Technology,2021-08-01,Article,"Zhang, Fanlong;Khoo, Siau cheng",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85108368818,10.1016/j.infsof.2021.106665,“Won't We Fix this Issue?” Qualitative characterization and automated identification of wontfix issues on GitHub,"Context:: Addressing user requests in the form of bug reports and Github issues represents a crucial task of any successful software project. However, user-submitted issue reports tend to widely differ in their quality, and developers spend a considerable amount of time handling them. Objective:: By collecting a dataset of around 6,000 issues of 279 GitHub projects, we observe that developers take significant time (i.e., about five months, on average) before labeling an issue as a wontfix. For this reason, in this paper, we empirically investigate the nature of wontfix issues and methods to facilitate issue management process. Method:: We first manually analyze a sample of 667 wontfix issues, extracted from heterogeneous projects, investigating the common reasons behind a “wontfix decision”, the main characteristics of wontfix issues and the potential factors that could be connected with the time to close them. Furthermore, we experiment with approaches enabling the prediction of wontfix issues by analyzing the titles and descriptions of reported issues when submitted. Results and conclusion:: Our investigation sheds some light on the wontfix issues’ characteristics, as well as the potential factors that may affect the time required to make a “wontfix decision”. Our results also demonstrate that it is possible to perform prediction of wontfix issues with high average values of precision, recall, and F-measure (90%–93%). © 2021 The Authors",Empirical study | Issue management | Issue tracking | Machine learning,Information and Software Technology,2021-11-01,Article,"Panichella, Sebastiano;Canfora, Gerardo;Di Sorbo, Andrea",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85117324409,10.1016/j.infsof.2021.106743,Classifying issue reports according to feature descriptions in a user manual based on a deep learning model,"Context: Issue reports are documents with which users report problems and state their opinions on a software system. Issue reports are useful for software maintenance, but managing them requires developers’ considerable manual effort. To reduce such effort, previous studies have mostly suggested methods for automatically classifying issue reports. However, most of those studies classify issue reports according to issue types, based only on whether the report is relevant to a bug, whether the report is duplicated, or whether the issue is functional or nonfunctional. Objective: In this paper, we intend to link issue reports and a user manual and so propose a deep learning model-based method that classifies issue reports according to software features that are described in the user manual in order to help developers relate issue reports to features to make changes to a software system. Method: In order to classify issue reports according to the feature descriptions in a user manual, our method uses a deep learning technique with a word embedding technique. The key insight in our method is that the sections of a user manual that describe software features contain the words and sentences similar to those in issue reports. Based on the insight, we construct a classification model that learns the feature descriptions (i.e. sections) in a user manual and classifies issue reports according to the feature descriptions. Results: We evaluate the proposed method by comparing its classification performance with that of the state-of-the-art method, TicketTagger. The experimental results show that the proposed method yields 10% ∼ 24% higher classification f1-score than that of TicketTagger. We also experiment with two deep learning models and four word embedding techniques and find out that the Convolution Neural Network model with FastText (or GloVe) yields the best performance. Conclusion: Our study shows the feasibility of classifying issue reports according to software features, which can be the basis for successive studies to classify issue reports into software features. © 2021",Classification | Convolution neural network | Data-based software engineering | Deep learning | Issue reports | Machine learning | Recurrent neural network | Software features | User manual,Information and Software Technology,2022-02-01,Article,"Cho, Heetae;Lee, Seonah;Kang, Sungwon",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85117959779,10.1016/j.infsof.2021.106756,Early prediction for merged vs abandoned code changes in modern code reviews,"Context: The modern code review process is an integral part of the current software development practice. Considerable effort is given here to inspect code changes, find defects, suggest an improvement, and address the suggestions of the reviewers. In a code review process, several iterations usually take place where an author submits code changes and a reviewer gives feedback until is happy to accept the change. In around 12% cases, the changes are abandoned, eventually wasting all the efforts. Objective: In this research, our objective is to design a tool that can predict whether a code change would be merged or abandoned at an early stage to reduce the waste of efforts of all stakeholders (e.g., program author, reviewer, project management, etc.) involved. The real-world demand for such a tool was formally identified by a study by Fan et al. (2018). Method: We have mined 146,612 code changes from the code reviews of three large and popular open-source software and trained and tested a suite of supervised machine learning classifiers, both shallow and deep learning-based. We consider a total of 25 features in each code change during the training and testing of the models. The features are divided into five dimensions: reviewer, author, project, text, and code. Results: The best performing model named PredCR (Predicting Code Review), a LightGBM-based classifier achieves around 85% AUC score on average and relatively improves the state-of-the-art (Fan et al., 2018) by 14%–23%. In our extensive empirical study involving PredCR on the 146,612 code changes from the three software projects, we find that (1) The new features like reviewer dimensions that are introduced in PredCR are the most informative. (2) Compared to the baseline, PredCR is more effective towards reducing bias against new developers. (3) PredCR uses historical data in the code review repository and as such the performance of PredCR improves as a software system evolves with new and more data. Conclusion: PredCR can help save time and effort by helping developers/code reviewers to prioritize the code changes that they are asked to review. Project management can use PredCR to determine how code changes can be assigned to the code reviewers (e.g., select code changes that are more likely to be merged for review before the changes that might be abandoned). © 2021 Elsevier B.V.",Abandoned | Code review | Early prediction | Merged | Patch,Information and Software Technology,2022-02-01,Article,"Islam, Khairul;Ahmed, Toufique;Shahriyar, Rifat;Iqbal, Anindya;Uddin, Gias",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85047421055,10.1016/j.ins.2018.05.035,Novel algorithms for cost-sensitive classification and knowledge discovery in class imbalanced datasets with an application to NASA software defects,"Software defect prediction (SDP) involves using machine learning to locate bugs in source code. Datasets used for SDP are typically affected by an issue called class imbalance. Traditional learning algorithms do not perform well on class imbalanced datasets. Cost-sensitive learning has been used in SDP to minimise the monetary costs incurred by predictions. We propose a framework which produces cost-sensitive predictions and also mitigates class imbalance. Since our algorithm builds a decision forest classifier, knowledge can be extracted by manual inspection of the individual decision trees. To enhance this knowledge discovery process, we propose an algorithm for extracting the most interesting patterns from a decision forest. Our algorithm calculates interestingness as the potential financial gain of knowing the pattern. We then present a process which combines the above-mentioned techniques into an end-to-end cost-sensitive knowledge discovery process. This process is demonstrated by extracting knowledge from four software projects undertaken by the National Aeronautics and Space Administration (NASA). © 2018",Class imbalance | Cost-sensitive | Decision forest | Knowledge discovery | Software defect prediction,Information Sciences,2018-08-01,Article,"Siers, Michael J.;Islam, Md Zahidul",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85076026855,10.1016/j.isatra.2019.11.020,Evolutionary extreme learning machine with sparse cost matrix for imbalanced learning,"Extreme learning machine is a popular machine learning technique for single hidden layer feed-forward neural network. However, due to the assumption of equal misclassification cost, the conventional extreme learning machine fails to properly learn the characteristics of the data with skewed category distribution. In this paper, to enhance the representation of few-shot cases, we break down that assumption by assigning penalty factors to different classes, and minimizing the cumulative classification cost. To this end, a case-weighting extreme learning machine is developed on a sparse cost matrix with a diagonal form. To be more actionable, we formulate a multi-objective optimization with respect to penalty factors, and optimize this problem using an evolutionary algorithm combined with an error bound model. By doing so, this proposed method is developed into an adaptive cost-sensitive learning, which is guided by the relation between the generalization ability and the case-weighting factors. In a broad experimental study, our method achieves competitive results on benchmark and real-world datasets for software bug reports identification. © 2019 ISA",Cost matrix | Error bound model | Evolutionary algorithm | Extreme learning machine | Imbalanced learning,ISA Transactions,2020-05-01,Article,"Li, Hui;Yang, Xi;Li, Yang;Hao, Li Ying;Zhang, Tian Lun",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85123893673,10.1016/j.isatra.2022.01.014,"Intelligent framework for automated failure prediction, detection, and classification of mission critical autonomous flights","Autonomous flights are the major industry contributors towards next-generation developments in pervasive and ubiquitous computing. Modern aerial vehicles are designed to receive actuator commands from the primary autopilot software as input to regulate their servos for adjusting control surfaces. Due to real-time interaction with the actual physical environment, there exists a high risk of control surface failures for engine, rudder, elevators, and ailerons etc. If not anticipated and then timely controlled, failures occurring during the flight can have severe and cataclysmic consequences, which may result in mid-air collision or ultimate crash. Humongous amount of sensory data being generated throughout mission-critical flights, makes it an ideal candidate for applying advanced data-driven machine learning techniques to identify intelligent insights related to failures for instant recovery from emergencies. In this paper, we present a novel framework based on machine learning techniques for failure prediction, detection, and classification for autonomous aerial vehicles. The proposed framework utilizes long short-term memory recurrent neural network architecture to analyze time series data and has been applied at the AirLab Failure and Anomaly flight dataset, which is a comprehensive publicly available dataset of various fault types in fixed-wing autonomous aerial vehicles’ control surfaces. The proposed framework is able to predict failure with an average accuracy of 93% and the average time-to-predict a failure is 19 s before the actual occurrence of the failure, which is 10 s better than current state-of-the-art. Failure detection accuracy is 100% and average detection time is 0.74 s after happening of failure, which is 1.28 s better than current state-of-the-art. Failure classification accuracy of proposed framework is 100%. The performance analysis shows the strength of the proposed methodology to be used as a real-time failure prediction and a pseudo-real-time failure detection along with a failure classification framework for eventual deployment with actual mission-critical autonomous flights. © 2022 ISA",Autonomous aerial vehicles | Autonomous flights | Failure classification | Failure detection | Failure prediction | Long short-term memory,ISA Transactions,2022-10-01,Article,"Ahmad, Muhammad Waqas;Akram, Muhammad Usman;Ahmad, Rashid;Hameed, Khurram;Hassan, Ali",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84960864852,10.1016/j.jcp.2017.05.021,A resilient and efficient CFD framework: Statistical learning tools for multi-fidelity and heterogeneous information fusion,"Exascale-level simulations require fault-resilient algorithms that are robust against repeated and expected software and/or hardware failures during computations, which may render the simulation results unsatisfactory. If each processor can share some global information about the simulation from a coarse, limited accuracy but relatively costless auxiliary simulator we can effectively fill-in the missing spatial data at the required times by a statistical learning technique – multi-level Gaussian process regression, on the fly; this has been demonstrated in previous work [1]. Based on the previous work, we also employ another (nonlinear) statistical learning technique, Diffusion Maps, that detects computational redundancy in time and hence accelerate the simulation by projective time integration, giving the overall computation a “patch dynamics” flavor. Furthermore, we are now able to perform information fusion with multi-fidelity and heterogeneous data (including stochastic data). Finally, we set the foundations of a new framework in CFD, called patch simulation, that combines information fusion techniques from, in principle, multiple fidelity and resolution simulations (and even experiments) with a new adaptive timestep refinement technique. We present two benchmark problems (the heat equation and the Navier–Stokes equations) to demonstrate the new capability that statistical learning tools can bring to traditional scientific computing algorithms. For each problem, we rely on heterogeneous and multi-fidelity data, either from a coarse simulation of the same equation or from a stochastic, particle-based, more “microscopic” simulation. We consider, as such “auxiliary” models, a Monte Carlo random walk for the heat equation and a dissipative particle dynamics (DPD) model for the Navier–Stokes equations. More broadly, in this paper we demonstrate the symbiotic and synergistic combination of statistical learning, domain decomposition, and scientific computing in exascale simulations. © 2017 Elsevier Inc.",Diffusion maps; Domain decomposition; Exascale simulation; Gappy data; Gaussian process regression; Machine learning; Patch dynamics,Journal of Computational Physics,2017,,"Lee S., Kevrekidis I.G., Karniadakis G.E.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85116803611,10.1016/j.jksuci.2021.09.017,Artificial intelligence-based fault prediction framework for WBAN,"Wireless Body Area Networks (WBAN) can provide continuous monitoring of patients’ health. Such monitoring can be a decisive factor in health and death situations. Fault management in WBANs is a key reliability component to make it socially acceptable and to overcome pertained challenges such as unpredicted faults, massive data streaming, and detection accuracy. Failures in fault detection due to hardware, software, and network issues may put human lives at risk. This paper focuses on detecting and predicting faults in sensors in the context of a WBAN. A framework is proposed to manage AI-based prediction models and fault detection using thresholds where four Machine learning techniques: Artificial Neural Networks (ANN), Deep Neural Networks (DNN), Support Vector Machines (SVM), and Decision Trees (DT), are used. The framework also provides alarm notifications, prediction model deployment, version control, and sensing node profiling. As a proof of concept, a fault management prototype is implemented and validated. The prototype classifies faults, manages automation of sensing node profiling, training, and validation of new models. The obtained experimental results show an accuracy greater than 96% for detecting faults with an inferior false alarm rate. © 2021 The Authors",Fault management | Fault prediction | Machine learning | Sensor health packets | WBAN,Journal of King Saud University - Computer and Information Sciences,2022-10-01,Article,"Awad, Mamoun;Sallabi, Farag;Shuaib, Khaled;Naeem, Faisal",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85085297137,10.1016/j.jmsy.2020.05.010,Digital Twin-enabled Collaborative Data Management for Metal Additive Manufacturing Systems,"Metal Additive Manufacturing (AM) has been attracting a continuously increasing attention due to its great advantages compared to traditional subtractive manufacturing in terms of higher design flexibility, shorter development time, lower tooling cost, and fewer production wastes. However, the lack of process robustness, stability and repeatability caused by the unsolved complex relationships between material properties, product design, process parameters, process signatures, post AM processes and product quality has significantly impeded its broad acceptance in the industry. To facilitate efficient implementation of advanced data analytics in metal AM, which would support the development of intelligent process monitoring, control and optimisation, this paper proposes a novel Digital Twin (DT)-enabled collaborative data management framework for metal AM systems, where a Cloud DT communicates with distributed Edge DTs in different product lifecycle stages. A metal AM product data model that contains a comprehensive list of specific product lifecycle data is developed to support the collaborative data management. The feasibility and advantages of the proposed framework are validated through the practical implementation in a distributed metal AM system developed in the project MANUELA. A representative application scenario of cloud-based and deep learning-enabled metal AM layer defect analysis is also presented. The proposed DT-enabled collaborative data management has shown great potential in enhancing fundamental understanding of metal AM processes, developing simulation and prediction models, reducing development times and costs, and improving product quality and production efficiency. © 2020 The Society of Manufacturing Engineers",data management | data model | Digital Twin | machine learning | Metal Additive Manufacturing | product lifecycle management,Journal of Manufacturing Systems,2022-01-01,Article,"Liu, Chao;Le Roux, Léopold;Körner, Carolin;Tabaste, Olivier;Lacan, Franck;Bigot, Samuel",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84955275883,10.1016/j.jnca.2015.09.009,Sociopsychological trust model for Wireless Sensor Networks,"Trust plays a crucial role in establishing and retaining relationships. Sociopsychological analysis identifies three major constructs, such as ability, benevolence and integrity, upon which trust is being built up. On a similar note, in a Wireless Sensor Network (WSN), it is indispensable to have trust among nodes since nodes collectively sense physical parameters and send them to the base station. The nodes, however, can behave fraudulently and send bad information, mostly due to hardware and software faults. Taking inspiration from the sociopsychological account, the present paper introduces a novel model for computing trust of sensor nodes. Additionally, the immune inspired model is suggested for removing fraudulent nodes whose trust ratings fall below the threshold. Roles of the three factors, viz. ability, benevolence and integrity, are examined in WSN domain. The proposed model proves itself to be more advantageous than other methods that adopt machine learning and neural network models in performance metrics such as detection time, reliability, scalability, efficiency and complexity. Proposed work has been implemented on LabVIEW platform and the results substantiate the reliability of the proposed mathematical model. © 2015 Elsevier Ltd. All rights reserved.",Ability | Benevolence | Integrity | Security | Sociopsychological trust model | Wireless Sensor Network,Journal of Network and Computer Applications,2016-02-01,Article,"Rathore, Heena;Badarla, Venkataramana;J, George K.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84863612007,10.1016/j.jss.2012.04.053,"Automated, highly-accurate, bug assignment using machine learning and tossing graphs","Empirical studies indicate that automating the bug assignment process has the potential to significantly reduce software evolution effort and costs. Prior work has used machine learning techniques to automate bug assignment but has employed a narrow band of tools which can be ineffective in large, long-lived software projects. To redress this situation, in this paper we employ a comprehensive set of machine learning tools and a probabilistic graph-based model (bug tossing graphs) that lead to highly-accurate predictions, and lay the foundation for the next generation of machine learning-based bug assignment. Our work is the first to examine the impact of multiple machine learning dimensions (classifiers, attributes, and training history) along with bug tossing graphs on prediction accuracy in bug assignment. We validate our approach on Mozilla and Eclipse, covering 856,259 bug reports and 21 cumulative years of development. We demonstrate that our techniques can achieve up to 86.09 prediction accuracy in bug assignment and significantly reduce tossing path lengths. We show that for our data sets the Naïve Bayes classifier coupled with product-component features, tossing graphs and incremental learning performs best. Next, we perform an ablative analysis by unilaterally varying classifiers, features, and learning model to show their relative importance of on bug assignment accuracy. Finally, we propose optimization techniques that achieve high prediction accuracy while reducing training and prediction time. © 2011 Elsevier Inc. All rights reserved.",Bug assignment | Bug tossing | Empirical studies | Machine learning,Journal of Systems and Software,2012-10-01,Article,"Bhattacharya, Pamela;Neamtiu, Iulian;Shelton, Christian R.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84967329239,10.1016/j.jss.2016.05.002,A machine learning based software process model recommendation method,"Among many factors that influence the success of a software project, the software process model employed is an essential one. An improper process model will be time consuming, error-prone and cost expensive, and further lower the quality of software. Therefore, how to choose an appropriate software process model is a very important problem for software development. Current works focus on the selection criteria and often lead to subjective results. In this paper, we propose a software process model recommendation method, to help project managers choose the most appropriate software process model for a new project at an early stage of development process according to historical software engineering data. The proposed method casts the process model recommendation into a classification problem. It first evaluates the different combinations of the alternative classification and attribute selection algorithms, and the best one is used to build the recommendation model with historical software engineering data; then, the constructed recommendation model is used to predict process models for a new software project with only a few data. We also analyze the mutual impacts between process models and different types of project factors, to further help managers locate the most suitable process model. We found process models are also responsible for defect count, defect severity and software change. Experiments on the data sets from 37 different development teams of different countries show that the average recommendation accuracy of our method reaches up to 82.5%, which makes it potentially useful in practice. © 2016 Elsevier Inc. All rights reserved.",Impact analysis | Machine learning | Model recommendation | Software process model | Software project management,Journal of Systems and Software,2016-08-01,Article,"Song, Qinbao;Zhu, Xiaoyan;Wang, Guangtao;Sun, Heli;Jiang, He;Xue, Chenhao;Xu, Baowen;Song, Wei",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85042747168,10.1016/j.jss.2018.01.039,Anomaly detection and diagnosis for cloud services: Practical experiments and lessons learned,"The dependability of cloud computing services is a major concern of cloud providers. In particular, anomaly detection techniques are crucial to detect anomalous service behaviors that may lead to the violation of service level agreements (SLAs) drawn with users. This paper describes an anomaly detection system (ADS) designed to detect errors related to the erroneous behavior of the service, and SLA violations in cloud services. One major objective is to help providers to diagnose the anomalous virtual machines (VMs) on which a service is deployed as well as the type of error associated to the anomaly. Our ADS includes a system monitoring entity that collects software counters characterizing the cloud service, as well as a detection entity based on machine learning models. Additionally, a fault injection entity is integrated into the ADS for the training the machine learning models. This entity is also used to validate the ADS and to assess its anomaly detection and diagnosis performance. We validated our ADS with two case studies deployments: a NoSQL database, and a virtual IP Multimedia Subsystem developed implementing a virtual network function. Experimental results show that our ADS can achieve a high detection and diagnosis performance. © 2018 Elsevier Inc.",Anomaly detection | Diagnosis | Fault injection | Machine learning | SLA | System monitoring | Virtualization,Journal of Systems and Software,2018-05-01,Article,"Sauvanaud, Carla;Kaâniche, Mohamed;Kanoun, Karama;Lazri, Kahina;Da Silva Silvestre, Guthemberg",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85074980613,10.1016/j.jss.2019.110456,CVE-assisted large-scale security bug report dataset construction method,"Identifying SBRs (security bug reports) is crucial for eliminating security issues during software development. Machine learning are promising ways for SBR prediction. However, the effectiveness of the state-of-the-art machine learning models depend on high-quality datasets, while gathering large-scale datasets are expensive and tedious. To solve this issue, we propose an automated data labeling approach based on iterative voting classification. It starts with a small group of ground-truth traing samples, which can be labeled with the help of authoritative vulnerability records hosted in CVE (Common Vulnerabilities and Exposures). The accuracy of the prediction model is improved with an iterative voting strategy. By using this approach, we label over 80k bug reports from OpenStack and 40k bug reports from Chromium. The correctness of these labels are then manually reviewed by three experienced security testing members. Finally, we construct a large-scale SBR dataset with 191 SBRs and 88,472 NSBRs (non-security bug reports) from OpenStack; and improve the quality of existing SBR dataset Chromium by identifying 64 new SBRs from previously labeled NSBRs and filtering out 173 noise bug reports from this dataset. These share datasets as well as the proposed dataset construction method help to promote research progress in SBR prediction research domain. © 2019 Elsevier Inc.",Common vulnerabilities and exposures | Dataset construction | Security bug report prediction | Voting classification,Journal of Systems and Software,2020-02-01,Article,"Wu, Xiaoxue;Zheng, Wei;Chen, Xiang;Wang, Fang;Mu, Dejun",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85076597363,10.1016/j.jss.2019.110464,Predicting failures in multi-tier distributed systems,"Many applications are implemented as multi-tier software systems, and are executed on distributed infrastructures, like cloud infrastructures, to benefit from the cost reduction that derives from dynamically allocating resources on-demand. In these systems, failures are becoming the norm rather than the exception, and predicting their occurrence, as well as locating the responsible faults, are essential enablers of preventive and corrective actions that can mitigate the impact of failures, and significantly improve the dependability of the systems. Current failure prediction approaches suffer either from false positives or limited accuracy, and do not produce enough information to effectively locate the responsible faults. In this paper, we present PreMiSE, a lightweight and precise approach to predict failures and locate the corresponding faults in multi-tier distributed systems. PreMiSE blends anomaly-based and signature-based techniques to identify multi-tier failures that impact on performance indicators, with high precision and low false positive rate. The experimental results that we obtained on a Cloud-based IP Multimedia Subsystem indicate that PreMiSE can indeed predict and locate possible failure occurrences with high precision and low overhead. © 2019 Elsevier Inc.",Cloud computing | Data analytics | Failure prediction | Machine learning | Multi-tier distributed systems | Self-healing systems,Journal of Systems and Software,2020-03-01,Article,"Mariani, Leonardo;Pezzè, Mauro;Riganelli, Oliviero;Xin, Rui",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85081030171,10.1016/j.jss.2020.110542,On testing machine learning programs,"Nowadays, we are witnessing a wide adoption of Machine learning (ML) models in many software systems. They are even being tested in safety-critical systems, thanks to recent breakthroughs in deep learning and reinforcement learning. Many people are now interacting with systems based on ML every day, e.g., voice recognition systems used by virtual personal assistants like Amazon Alexa or Google Home. As the field of ML continues to grow, we are likely to witness transformative advances in a wide range of areas, from finance, energy, to health and transportation. Given this growing importance of ML-based systems in our daily life, it is becoming utterly important to ensure their reliability. Recently, software researchers have started adapting concepts from the software testing domain (e.g., code coverage, mutation testing, or property-based testing) to help ML engineers detect and correct faults in ML programs. This paper reviews current existing testing practices for ML programs. First, we identify and explain challenges that should be addressed when testing ML programs. Next, we report existing solutions found in the literature for testing ML programs. Finally, we identify gaps in the literature related to the testing of ML programs and make recommendations of future research directions for the scientific community. We hope that this comprehensive review of software testing practices will help ML engineers identify the right approach to improve the reliability of their ML-based systems. We also hope that the research community will act on our proposed research directions to advance the state of the art of testing for ML programs. © 2020",Data cleaning | Feature engineering testing | Implementation testing | Machine learning | Model testing,Journal of Systems and Software,2020-06-01,Article,"Braiek, Houssem Ben;Khomh, Foutse",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85116351574,10.1016/j.jss.2021.111096,On misbehaviour and fault tolerance in machine learning systems,"Machine learning (ML) provides us with numerous opportunities, allowing ML systems to adapt to new situations and contexts. At the same time, this adaptability raises uncertainties concerning the run-time product quality or dependability, such as reliability and security, of these systems. Systems can be tested and monitored, but this does not provide protection against faults and failures in adapted ML systems themselves. We studied software designs that aim at introducing fault tolerance in ML systems so that possible problems in ML components of the systems can be avoided. The research was conducted as a case study, and its data was collected through five semi-structured interviews with experienced software architects. We present a conceptualisation of the misbehaviour of ML systems, the perceived role of fault tolerance, and the designs used. Common patterns to incorporating ML components in design in a fault tolerant fashion have started to emerge. ML models are, for example, guarded by monitoring the inputs and their distribution, and enforcing business rules on acceptable outputs. Multiple, specialised ML models are used to adapt to the variations and changes in the surrounding world, and simpler fall-over techniques like default outputs are put in place to have systems up and running in the face of problems. However, the general role of these patterns is not widely acknowledged. This is mainly due to the relative immaturity of using ML as part of a complete software system: the field still lacks established frameworks and practices beyond training to implement, operate, and maintain the software that utilises ML. ML software engineering needs further analysis and development on all fronts. © 2021 The Author(s)",Case study | Fault tolerance | Machine learning | Software architecture | Software engineering,Journal of Systems and Software,2022-01-01,Article,"Myllyaho, Lalli;Raatikainen, Mikko;Männistö, Tomi;Nurminen, Jukka K.;Mikkonen, Tommi",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84902359961,10.1016/j.knosys.2014.04.033,Cost-sensitive learning for defect escalation,"While most software defects (i.e., bugs) are corrected and tested as part of the prolonged software development cycle, enterprise software venders often have to release software products before all reported defects are corrected, due to deadlines and limited resources. A small number of these reported defects will be escalated by customers whose businesses are seriously impacted. Escalated defects must be resolved immediately and individually by the software vendors at a very high cost. The total costs can be even greater, including loss of reputation, satisfaction, loyalty, and repeat revenue. In this paper, we develop a Software defecT Escalation Prediction (STEP) system to mine historical defect report data and predict the escalation risk of current defect reports for maximum net profit. More specifically, we first describe a simple and general framework to convert the maximum net profit problem to cost-sensitive learning. We then apply and compare four well-known cost-sensitive learning approaches for STEP. Our experiments suggest that cost-sensitive decision trees (CSTree) is the best methods for producing the highest positive net profit. © 2014 Elsevier B.V. All rights reserved.",Cost-sensitive learning | Data mining | Defect escalation | Machine learning | Software defect escalation prediction,Knowledge-Based Systems,2014-01-01,Article,"Sheng, Victor S.;Gu, Bin;Fang, Wei;Wu, Jian",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85018955129,10.1016/j.knosys.2017.04.014,Code smell severity classification using machine learning techniques,"Several code smells detection tools have been developed providing different results, because smells can be subjectively interpreted and hence detected in different ways. Machine learning techniques have been used for different topics in software engineering, e.g., design pattern detection, code smell detection, bug prediction, recommending systems. In this paper, we focus our attention on the classification of code smell severity through the use of machine learning techniques in different experiments. The severity of code smells is an important factor to take into consideration when reporting code smell detection results, since it allows the prioritization of refactoring efforts. In fact, code smells with high severity can be particularly large and complex, and create larger issues to the maintainability of software a system. In our experiments, we apply several machine learning models, spanning from multinomial classification to regression, plus a method to apply binary classifiers for ordinal classification. In fact, we model code smell severity as an ordinal variable. We take the baseline models from previous work, where we applied binary classification models for code smell detection with good results. We report and compare the performance of the models according to their accuracy and four different performance measures used for the evaluation of ordinal classification techniques. From our results, while the accuracy of the classification of severity is not high as in the binary classification of absence or presence of code smells, the ranking correlation of the actual and predicted severity for the best models reaches 0.88–0.96, measured through Spearman's ρ. © 2017 Elsevier B.V.",Code smell severity | Code smells detection | Machine learning | Ordinal classification | Refactoring prioritization,Knowledge-Based Systems,2017-07-15,Article,"Arcelli Fontana, Francesca;Zanoni, Marco",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85124466041,10.1016/j.knosys.2022.108308,A spatial–temporal graph neural network framework for automated software bug triaging,"The bug triaging process, an essential process of assigning bug reports to the most appropriate developers, is related closely to the quality and costs of software development. Since manual bug assignment is a labor-intensive task, especially for large-scale software projects, many machine learning-based approaches have been proposed to triage bug reports automatically. Although developer collaboration networks (DCNs) are dynamic and evolving in the real world, most automated bug triaging approaches focus on static tossing graphs at a single time slice. Also, none of the previous studies consider periodic interactions among developers. To address the problems mentioned above, in this article, we propose a novel spatial–temporal dynamic graph neural network (ST-DGNN) framework, including a joint random walk (JRWalk) mechanism and a graph recurrent convolutional neural network (GRCNN) model. In particular, JRWalk aims to sample topological structures in a developer collaboration network with two sampling strategies by considering both developer reputation and interaction preference. GRCNN has three components with the same structure, i.e., hourly-periodic, daily-periodic, and weekly-periodic components, to learn the spatial–temporal features of nodes on dynamic DCNs. We evaluated our approach's effectiveness by comparing it with several state-of-the-art graph representation learning methods in three domain-specific tasks (i.e., the bug fixer prediction task and two downstream tasks of graph representation learning: node classification and link prediction). In the three tasks, experiments on two real-world, large-scale developer collaboration networks collected from the Eclipse and Mozilla projects indicate that the proposed approach outperforms all the baseline methods on three different time scales (i.e., long-term, medium-term, and short-term predictions) in terms of F1−score. © 2022 Elsevier B.V.",Attention | Bug triage | Graph neural network | Random walk | Representation learning,Knowledge-Based Systems,2022-04-06,Article,"Wu, Hongrun;Ma, Yutao;Xiang, Zhenglong;Yang, Chen;He, Keqing",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85119193292,10.1016/j.media.2021.102293,Ms RED: A novel multi-scale residual encoding and decoding network for skin lesion segmentation,"Computer-Aided Diagnosis (CAD) for dermatological diseases offers one of the most notable showcases where deep learning technologies display their impressive performance in acquiring and surpassing human experts. In such the CAD process, a critical step is concerned with segmenting skin lesions from dermoscopic images. Despite remarkable successes attained by recent deep learning efforts, much improvement is still anticipated to tackle challenging cases, e.g., segmenting lesions that are irregularly shaped, bearing low contrast, or possessing blurry boundaries. To address such inadequacies, this study proposes a novel Multi-scale Residual Encoding and Decoding network (Ms RED) for skin lesion segmentation, which is able to accurately and reliably segment a variety of lesions with efficiency. Specifically, a multi-scale residual encoding fusion module (MsR-EFM) is employed in an encoder, and a multi-scale residual decoding fusion module (MsR-DFM) is applied in a decoder to fuse multi-scale features adaptively. In addition, to enhance the representation learning capability of the newly proposed pipeline, we propose a novel multi-resolution, multi-channel feature fusion module (M2F2), which replaces conventional convolutional layers in encoder and decoder networks. Furthermore, we introduce a novel pooling module (Soft-pool) to medical image segmentation for the first time, retaining more helpful information when down-sampling and getting better segmentation performance. To validate the effectiveness and advantages of the proposed network, we compare it with several state-of-the-art methods on ISIC 2016, 2017, 2018, and PH2. Experimental results consistently demonstrate that the proposed Ms RED attains significantly superior segmentation performance across five popularly used evaluation criteria. Last but not least, the new model utilizes much fewer model parameters than its peer approaches, leading to a greatly reduced number of labeled samples required for model training, which in turn produces a substantially faster converging training process than its peers. The source code is available at https://github.com/duweidai/Ms-RED. © 2021 Elsevier B.V.",Feature fusion | Multi-scale | Residual encoding/decoding | Skin lesion segmentation | Soft-pool,Medical Image Analysis,2022-01-01,Article,"Dai, Duwei;Dong, Caixia;Xu, Songhua;Yan, Qingsen;Li, Zongfang;Zhang, Chunyan;Luo, Nana",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85092535990,10.1016/j.micpro.2020.103285,Flow-based anomaly intrusion detection using machine learning model with software defined networking for OpenFlow network,"Moving towards recent technologies, Software Defined Networking (SDN) produces a promising network framework to combine the overall network management system with network programming. It gives a more effective tracking system towards the data center. By centralized system and symmetric controller, it prevents security cracks from creating new threats during OpenFlow packet transmission with vulnerabilities. It creates more interest to the researchers to work towards Flow-based SDN for the priority-driven algorithm in anomaly intruder detection. In this paper, we made a study towards a priority-based model using SDN to control the flow of data packets over the network, gives assurance to the bandwidth enforcement, and reallocation is made through virtual circuits. The network behavior of the system is continuously monitored through the machine learning model for normal and abnormal traffic data transmission to detect anomaly intruders. Flow-based machine learning (ML) model with SDN act as an intelligent system to limits the throughput virtually through the flow of reserved bandwidth and make use of extra bandwidth, which presents more than the utilization bandwidth for priority-based applications with minimal cost while compared with the traditional methods. The proposed work also compared with the schemes available at the network to produce outcomes with fast routing and the fault tolerance of existing networks to overcome the gap open at the security of the SDN architecture to detect and identify vulnerabilities. © 2020",Anomaly intrusion detection | Machine learning model | Multi-layer classification | Network traffic | OpenFlow | Packet data flow | QoS | Software defined networking,Microprocessors and Microsystems,2020-11-01,Article,"Satheesh, N.;Rathnamma, M. V.;Rajeshkumar, G.;Sagar, P. Vidya;Dadheech, Pankaj;Dogiwal, S. R.;Velayutham, Priya;Sengan, Sudhakar",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85044327325,10.1016/j.neucom.2018.03.014,ACDIN: Bridging the gap between artificial and real bearing damages for bearing fault diagnosis,"Data-driven algorithms for bearing fault diagnosis have achieved much success. However, it is difficult and even impossible to collect enough data containing real bearing damages to train the classifiers, which hinders the application of these methods in industrial environments. One feasible way to address the problem is training the classifiers with data generated from artificial bearing damages instead of real ones. In this way, the problem changes to how to extract common features shared by both kinds of data because the differences between the artificial one and the natural one always baffle the learning machine. In this paper, a novel model, deep inception net with atrous convolution (ACDIN), is proposed to cope with the problem. The contribution of this paper is threefold. First and foremost, ACDIN improves the accuracy from 75% (best results of conventional data-driven methods) to 95% on diagnosing the real bearing faults when trained with only the data generated from artificial bearing damages. Second, ACDIN takes raw temporal signals as inputs, which means that it is pre-processing free. Last, feature visualization is used to analyze the mechanism behind the high performance of the proposed model. © 2018 Elsevier B.V.",Artificial damages | Convolutional neural network | End-to-end | Intelligent fault diagnosis | Real damages,Neurocomputing,2018-06-14,Article,"Chen, Yuanhang;Peng, Gaoliang;Xie, Chaohao;Zhang, Wei;Li, Chuanhao;Liu, Shaohui",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85055479097,10.1016/j.neunet.2018.09.001,DGCNN: A convolutional neural network over large-scale labeled graphs,"Exploiting graph-structured data has many real applications in domains including natural language semantics, programming language processing, and malware analysis. A variety of methods has been developed to deal with such data. However, learning graphs of large-scale, varying shapes and sizes is a big challenge for any method. In this paper, we propose a multi-view multi-layer convolutional neural network on labeled directed graphs (DGCNN), in which convolutional filters are designed flexibly to adapt to dynamic structures of local regions inside graphs. The advantages of DGCNN are that we do not need to align vertices between graphs, and that DGCNN can process large-scale dynamic graphs with hundred thousands of nodes. To verify the effectiveness of DGCNN, we conducted experiments on two tasks: malware analysis and software defect prediction. The results show that DGCNN outperforms the baselines, including several deep neural networks. © 2018 Elsevier Ltd",abstract syntax trees (ASTs) | Control flow graphs (CFGs) | Convolutional neural networks (CNNs) | Labeled directed graphs,Neural Networks,2018-12-01,Article,"Phan, Anh Viet;Nguyen, Minh Le;Nguyen, Yen Lam Hoang;Bui, Lam Thu",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84962772563,10.1016/j.procs.2015.10.059,A Novel Way of Assessing Software Bug Severity Using Dictionary of Critical Terms,"Due to increase in demands of software and decreased delivery span of software, assuring the quality of software is becoming a challenge. However, no software can claim to be error free due to the complexity of software and inadequate testing. There is a well-known principle of testing, which states that exhaustive testing is impossible. Hence, maintenance activities are required to ensure smooth functioning of the software. Many open source software provides bug tracking systems to aid corrective maintenance task. These bug tracking systems allow users to report the bugs that are encountered while operating the software. However, in software maintenance, severity prediction has gained much attention recently. Bugs having higher severity should be fixed prior to the bugs having lesser severity. Triager analyzes the bug reports and assesses the severity based upon his/her knowledge and experience. But due to the presence of a large number of bug reports, it becomes a tedious job to manually assign severity. Thus, there is growing need for making the whole process of severity prediction automatic. The paper presents an approach of creating a dictionary of critical terms specifying severity using two different feature selection methods, namely- info gain and Chi square and classification of bug reports are performed using Naïve Bayes Multinomial (NBM) and K-nearest neighbor (KNN) algorithms. © 2015 The Authors.",Bug | Bug Tracking System | Bug Triaging System | Feature selection methods | Machine Learning Algorithms,Procedia Computer Science,2015-01-01,Conference Paper,"Sharma, Gitika;Sharma, Sumit;Gujral, Shruti",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85028649486,10.1016/j.procs.2017.06.137,Automated Anomaly Detection in Virtualized Services Using Deep Packet Inspection,"Virtualization technologies have proven to be important drivers for the fast and cost-efficient development and deployment of services. While the benefits are tremendous, there are many challenges to be faced when developing or porting services to virtualized infrastructure. Especially critical applications like Virtualized Network Functions must meet high requirements in terms of reliability and resilience. An important tool when meeting such requirements is detecting anomalous system components and recovering the anomaly before it turns into a fault and subsequently into a failure visible to the client. Anomaly detection for virtualized services relies on collecting system metrics that represent the normal operation state of every component and allow the usage of machine learning algorithms to automatically build models representing such state. This paper presents an approach for collecting service-layer metrics while treating services as black-boxes. This allows service providers to implement anomaly detection on the application layer without the need to modify third-party software. Deep Packet Inspection is used to analyse the traffic of virtual machines on the hypervisor layer, producing both generic and protocol-specific communication metrics. An evaluation shows that the resulting metrics represent the normal operation state of an example Virtualized Network Function and are therefore a valuable contribution to automatic anomaly detection in virtualized services. © 2017 The Authors. Published by Elsevier B.V.",Anomaly Detection | Cloud | Deep Packet Inspection | Network | Network Function Virtualization,Procedia Computer Science,2017-01-01,Conference Paper,"Wallschläger, Marcel;Gulenko, Anton;Schmidt, Florian;Kao, Odej;Liu, Feng",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85049103501,10.1016/j.procs.2018.05.115,Taxonomy of machine learning algorithms in software fault prediction using object oriented metrics,"Prediction of Fault proneness of a software component is the compelling field of investigations in software testing arena. Software coupling plays a vital role in assessing the software quality through fault prediction and complexity measures. Various fault prediction models, have used the object oriented metrics for the predicting and localizing the faults. Many of these metrics have direct influence on the quality of software. More over prior knowledge of the fault proneness of a component may significantly reduce the testing effort and time. The measures of object oriented features like inheritance, polymorphism and encapsulation etc may be used to estimate fault proneness. Many researchers have investigated the usage of object oriented metrics in the software fault prediction. In this study we present taxonomy of usage these metrics in the fault prediction. We also present the analysis of machine learning techniques in fault prediction. © 2018 The Authors. Published by Elsevier Ltd.",machine learning | Object Oriented Coupling | Object Oriented Testing | Software fault prediction | software faults prediction,Procedia Computer Science,2018-01-01,Conference Paper,"Singh, Ajmer;Bhatia, Rajesh;Sighrova, Anita",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85049101609,10.1016/j.procs.2018.05.194,A Supervised Bug Report Classification with Incorporate and Textual field Knowledge,"Performance of the bug prediction model is directly depends on the misclassification of bug reports. Misclassification issue surely scarifies the accuracy of the system. To resolve this issue the manual examination of bug reports are required, but it is very time consuming and tedious job for a developer and tester. In this paper the hybrid approach of merging text mining, natural language processing and machine learning techniques is used to identify bug report as bug or non-bug. The four incorporates fields with textual fields are added to bug reports to improve the performance of classifier. TF-IDF and Bigram feature extraction methods are used with feature selection and K-nearest neighbor (K-NN) classifier. The performance of the proposed system is evaluated by using Precision, Recall and F-measure by using five datasets. It is observed that the performance of K-NN classifier is changed according to the dataset and addition of bigram method improve the performance of classifier. © 2018 The Authors. Published by Elsevier Ltd.",Bug Triaging System | development | Incorporate | K-nearest neighbor | Natural Language Processing | Software maintenance | textual fields based prediction,Procedia Computer Science,2018-01-01,Conference Paper,"Kukkar, Ashima;Mohana, Rajni",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85091800167,10.1016/j.procs.2020.04.299,Data Traffic Classification in Software Defined Networks (SDN) using supervised-learning,"Traffic classification with accuracy is of prime importance in network activities such as security monitoring, traffic engineering, fault detection, accounting of network usage, billing and for providing differentiation in Quality of Service (QoS) parameters of the various network services. Network Traffic Classification is significant in recent days due to rapid growth in the number of internet consumers. The different primitive techniques of network traffic classification have failed to provide reliable accuracy because of 1000 fold scaling in the amount of devices as well as flows. To overcome this drawback, the integration of Software Defined Network (SDN) architecture and machine learning technology is proposed in this paper. Three different supervised learning models, namely Support Vector Machine (SVM), nearest centroid and Naïve Bayes (NB), are applied to classify the data traffic based on the applications in a software-defined network platform. The network traffic traces are captured and flows features are generated, which is sent to the classifier for prediction. The accuracy obtained for SVM is 92.3%, NB is 96.79% and the nearest centroid is 91.02%. The challenges faced are in the live network data traffic capture and classification of the applications in the SDN platform. © 2020 The Authors. Published by Elsevier B.V.",flow statistics; packet length; protocol; supervised learning,Procedia Computer Science,2020,,"Raikar M.M., Meena S.M., Mulla M.M., Shetti N.S., Karanandi M.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85121770917,10.1016/j.procs.2021.10.065,Fault Detection based on Deep Learning for Digital VLSI Circuits,"As growing complexity of digital VLSI circuits, fault detection and correction processes have been the most crucial phases during IC design. Many CAD tools and formal approaches have been used for debugging and localizing different kinds of design bugs. However, the search space explosion problem remains the main problem for IC designers. Recently, Artificial intelligence and machine learning models have been expanded in feature extraction and reduction models. In this paper, we introduce a new fault detection model based on deep learning for extracting features and detecting faults from large-sized digital circuits. The main goal of the proposed model is to avoid the search space using stacked sparse autoencoder, a specific type of artificial neural network. The model consists of three phases: test pattern generation using ATALANTA software, feature reduction using SSAE and classification for fault detection. Test vectors are utilized in SSAE as a training data for unsupervised learning phase. The performance of feature extraction is tested by changing the architecture of SSAE network and sparsity constraint. The proposed algorithm has been implemented using eight combinational digital circuits from ISCAS'85. From experimental results, the maximum fault coverage using ATALANTA tool delivers around 99.2% using ISCAS'85. In addition, the maximum validation accuracy of proposed SSAE model delivers around 99.7% in feature reduction phase. © 2021 The Authors. Published by Elsevier B.V.",Deep Learning | Design Debugging | ML | Sparse Autoencoder,Procedia Computer Science,2021-01-01,Conference Paper,"Gaber, Lamya;Hussein, Aziza I.;Moness, Mohammed",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85100265697,10.1016/j.scico.2020.102598,Predicting issue types on GitHub,"Software maintenance and evolution involves critical activities for the success of software projects. To support such activities and keep code up-to-date and error-free, software communities make use of issue trackers, i.e., tools for signaling, handling, and addressing the issues occurring in software systems. However, in popular projects, tens or hundreds of issue reports are daily submitted. In this context, identifying the type of each submitted report (e.g., bug report, feature request, etc.) would facilitate the management and the prioritization of the issues to address. To support issue handling activities, in this paper, we propose TICKET TAGGER, a GitHub app analyzing the issue title and description through machine learning techniques to automatically recognize the types of reports submitted on GitHub and assign labels to each issue accordingly. We empirically evaluated the tool's prediction performance on about 30,000 GitHub issues. Our results show that the Ticket Tagger can identify the correct labels to assign to GitHub issues with reasonably high effectiveness. Considering these results and the fact that the tool is designed to be easily integrated in the GitHub issue management process, Ticket Tagger consists in a useful solution for developers. © 2020 Elsevier B.V.",Issue reports management | Labeling unstructured data | Software maintenance and evolution,Science of Computer Programming,2021-05-01,Article,"Kallis, Rafael;Di Sorbo, Andrea;Canfora, Gerardo;Panichella, Sebastiano",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85086427001,10.1016/j.softx.2020.100551,Deep-water framework: The Swiss army knife of humans working with machine learning models,"Working with machine learning models has become an everyday task not only for software engineers, but for a much wider spectrum of researchers and professionals. Training such models involves finding the best learning methods and their best hyper-parameters for a specific task, keeping track of the achieved performance measures, comparing the results visually, etc. If we add feature extraction methods – that precede the learning phase and depend on many hyper-parameters themselves – into the mixture, like source code embedding that is quite common in the field of software analysis, the task cries out for supporting tools. We propose a framework called Deep-Water that works similarly to a configuration management tool in the area of software engineering. It supports defining arbitrary feature extraction and learning methods for an input dataset and helps in executing all the training tasks with different hyper-parameters in a distributed manner. The framework stores all circumstances, parameters and results of training, which can be filtered and visualized later. We successfully used the tool in several software analysis based prediction tasks, like vulnerability or bug prediction, but it is general enough to be applicable in other areas as well, e.g. NLP, image processing, or even other non-IT fields. © 2020 The Authors",Data visualization | Deep-learning | Machine learning | Model management | Software analysis,SoftwareX,2020-07-01,Article,"Ferenc, Rudolf;Viszkok, Tamás;Aladics, Tamás;Jász, Judit;Hegedűs, Péter",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85002466321,10.1016/j.swevo.2016.10.002,On the application of search-based techniques for software engineering predictive modeling: A systematic review and future directions,"Software engineering predictive modeling involves construction of models, with the help of software metrics, for estimating quality attributes. Recently, the use of search-based techniques have gained importance as they help the developers and project-managers in the identification of optimal solutions for developing effective prediction models. In this paper, we perform a systematic review of 78 primary studies from January 1992 to December 2015 which analyze the predictive capability of search-based techniques for ascertaining four predominant software quality attributes, i.e., effort, defect proneness, maintainability and change proneness. The review analyses the effective use and application of search-based techniques by evaluating appropriate specifications of fitness functions, parameter settings, validation methods, accounting for their stochastic natures and the evaluation of developmental models with the use of well-known statistical tests. Furthermore, we compare the effectiveness of different models, developed using the various search-based techniques amongst themselves, and also with the prevalent machine learning techniques used in literature. Although there are very few studies which use search-based techniques for predicting maintainability and change proneness, we found that the results of the application of search-based techniques for effort estimation and defect prediction are encouraging. Hence, this comprehensive study and the associated results will provide guidelines to practitioners and researchers and will enable them to make proper choices for applying the search-based techniques to their specific situations. © 2016 Elsevier B.V.",Change prediction | Defect prediction | Effort estimation | Maintainability prediction | Search-based techniques | Software quality,Swarm and Evolutionary Computation,2017-02-01,Article,"Malhotra, Ruchika;Khanna, Megha;Raje, Rajeev R.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85110057833,10.1016/j.vrih.2020.07.002,Virtual simulation experiment of the design and manufacture of a beer bottle-defect detection system.,"Background: Machine learning-based beer bottle-defect detection is a complex technology that runs automatically; however, it consumes considerable memory, is expensive, and poses a certain danger when training novice operators. Moreover, some topics are difficult to learn from experimental lectures, such as digital image processing and computer vision. However, virtual simulation experiments have been widely used to good effect within education. A virtual simulation of the design and manufacture of a beer bottle-defect detection system will not only help the students to increase their image-processing knowledge, but also improve their ability to solve complex engineering problems and design complex systems. Methods: The hardware models for the experiment (camera, light source, conveyor belt, power supply, manipulator, and computer) were built using the 3DS MAX modeling and animation software. The Unreal Engine 4 (UE4) game engine was utilized to build a virtual design room, design the interactive operations, and simulate the system operation. Results: The results showed that the virtual-simulation system received much better experimental feedback, which facilitated the design and manufacture of a beer bottle-defect detection system. The specialized functions of the functional modules in the detection system, including a basic experimental operation menu, power switch, image shooting, image processing, and manipulator grasping, allowed students (or virtual designers) to easily build a detection system by retrieving basic models from the model library, and creating the beer-bottle transportation, image shooting, image processing, defect detection, and defective-product removal. The virtual simulation experiment was completed with image processing as the main body. Conclusions: By mainly focusing on bottle mouthdefect detection, the detection system dedicates more attention to the user and the task. With more detailed tasks available, the virtual system will eventually yield much better results as a training tool for imageprocessing education. In addition, a novel visual perception-thinking pedagogical framework enables better comprehension than the traditional lecture-tutorial style. © 2019 Beijing Zhongke Journal Publishing Co. Ltd",Beer bottle defect detection | Image processing | Training tool | Virtual simulation experiment,Virtual Reality and Intelligent Hardware,2020-08-01,Article,"Zhao, Yuxiang;An, Xiaowei;Sun, Nongliang",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85031778357,10.1080/17517575.2017.1390167,Narrowing the scope of failure prediction using targeted fault load injection,"As society becomes more dependent upon computer systems to perform increasingly critical tasks, ensuring that those systems do not fail becomes increasingly important. Many organizations depend heavily on desktop computers for day-to-day operations. Unfortunately, the software that runs on these computers is written by humans and, as such, is still subject to human error and consequent failure. A natural solution is to use statistical machine learning to predict failure. However, since failure is still a relatively rare event, obtaining labelled training data to train these models is not a trivial task. This work presents new simulated fault-inducing loads that extend the focus of traditional fault injection techniques to predict failure in the Microsoft enterprise authentication service and Apache web server. These new fault loads were successful in creating failure conditions that were identifiable using statistical learning methods, with fewer irrelevant faults being created. © 2017 Informa UK Limited, trading as Taylor & Francis Group.",enterprise architecture | fault injection | machine learning | Online failure prediction,Enterprise Information Systems,2018-05-28,Article,"Jordan, Paul L.;Peterson, Gilbert L.;Lin, Alan C.;Mendenhall, Michael J.;Sellers, Andrew J.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85070709311,10.1089/cmb.2018.0102,Prediction of drug target sensitivity in cancer cell lines using apache spark,"Predicting the drug targets in the base of cancer cell line is one of the hottest issues in cancer treatment. Drug sensitivity describes which drug is perfect for cell line in certain condition or disease. This condition exists due to change in human metabolism. Different techniques are used for cancer treatment such as radiotherapy, hormone therapy, chemotherapy, and surgery. Many statistical methods and machine learning algorithms such as support vector machine, principal component analysis (PCA), logistic regression, simple linear regression, naive Bayes classifier, generalized linear regression, and random forest have been used for drug target prediction. However, these predictors take more time for computation using different tools such as MATLAB and R tool. In this study, different machine learning techniques are applied using Apache Spark to predict drug targets. Apache Spark uses the resilient distributed dataset (RDD) technique for in-memory fast computation and fault tolerance. The obtained results indicate that Spark provides better accuracy in short time when compared with existing tools. © Copyright 2019, Mary Ann Liebert, Inc., publishers 2019.",cancer | drug design | drug sensitivity | drug target prediction,Journal of Computational Biology,2019-08-01,Conference Paper,"Hussain, Shahid;Ferzund, Javed;Ul-Haq, Raza",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85100270069,10.1109/3ICT51146.2020.9311954,Word2Vec Duplicate Bug Records Identification Prediction Using Tensorflow,"Bug duplication reporting is one of the most widespread software problems that cause inconvenience for the internal software stakeholders. It is useful for developers to eliminate redundant bug records where the fewer bugs duplicated records in bug reports documentation the more efficiently allocated resources are set to fix and enhance the software features. In this paper, the word embedding (Word2Vec) approach is used on four different software components from the Mozilla Core dataset with different sentence types through the duplicated bug category records to compare whether two given bug record descriptions are categorized as related bugs records. Besides, this paper proposes three different similarity measures and explores the accuracy of each measure. The study results show that the approach's accuracy is proportional to the existence of similar words within any of the two given two bug records descriptions. Additionally, we found that percentage of similarity accuracy is improved by finding the closest word using the Euclidean distance method than traversing for more index adjacent values within the trained word vector array. © 2020 IEEE.",Bug reports | duplicated bug | duplicated records word embedding | machine-learning | natural language processing | prediction,"2020 International Conference on Innovation and Intelligence for Informatics, Computing and Technologies, 3ICT 2020",2020-12-20,Conference Paper,"Mahfoodh, Hussain;Hammad, Mustafa",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85049343831,10.1109/ACCESS.2018.2850910,Emotion based automated priority prediction for bug reports,"Issue tracking systems allow users to report bugs. Bug reports often contain product name, product component, description, and severity. Based on such information, triagers often manually prioritize the bug reports for investigation. However, manual prioritization is time consuming and cumbersome. DRONE is an automated state-of-the-art approach that recommends the priority level information of the bug reports. However, its performance for all levels of priorities is not uniform and may be improved. To this end, in this paper, we propose an emotion-based automatic approach to predict the priority for a report. First, we exploit natural language processing techniques to preprocess the bug report. Second, we identify the emotion-words that are involved in the description of the bug report and assign it an emotion value. Third, we create a feature vector for the bug report and predict its priority with a machine learning classifier that is trained with history data collected from the Internet. We evaluate the proposed approach on Eclipse open-source projects and the results of the cross-project evaluation suggest that the proposed approach outperforms the state-of-the-art. On average, it improves the F1 score by more than 6%. © 2013 IEEE.",Bug reports | classification | machine learning | priority prediction | software maintenance,IEEE Access,2018-06-29,Article,"Umer, Qasim;Liu, Hui;Sultan, Yasir",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85074701182,10.1109/ACCESS.2019.2949631,The Analysis of Node Planning and Control Logic Optimization of 5G Wireless Networks under Deep Mapping Learning Algorithms,"In order to meet the needs of blowout growth of data flow and 10-100 times increase of user experience rate, the next generation mobile communication(5G) heterogeneous network deployment will use ultra-dense network. Accurate fault location is the basis of slice management. In virtualized network model, the function of network elements is software, the underlying layer is highly abstract, the structure changes dynamically, and common cause faults occur frequently, which brings difficulties to fault location. Deep Learning is a machine Learning method that USES Deep Neural Network (DNN) with multiple hidden layers to complete Learning tasks. The essence is that by building a neural network model with multiple hidden layers and using a large amount of training data to learn more useful features, the accuracy of model prediction or classification can be improved. Based on the theory of deep mapping learning algorithm, a scheduling algorithm based on event-driven access point, improved time-driven access point and idle network is designed for centralized wireless network control system. Aiming at the centralized wireless network control system, this paper designs a scheduling algorithm based on event-driven access point, improved time-driven access point and network idleness. The designed sensor network node platform uses the programmable logic controller as the main control chip, contains the data interface module that realizes the communication function between the node and the upper computer, and the acquisition and conditioning module responsible for the collection and conditioning and conversion of the analog signal. At the same time, the TCP and UDP are compared and studied. The accuracy of the model is verified, and the interference distribution of the model and the influence of the D2D inter-pair distance on the system performance are simulated. The distance between the D2D pairs under the given simulation parameters has a certain influence on the system performance. As the distance increases, the system performance will deteriorate, but the impact is not a big conclusion. © 2013 IEEE.",5G | D2D | network node planning,IEEE Access,2019-01-01,Article,"Han, Zidong;Liang, Junyu",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85089501673,10.1109/ACCESS.2020.3007499,Parallel Machine Learning Algorithm Using Fine-Grained-Mode Spark on a Mesos Big Data Cloud Computing Software Framework for Mobile Robotic Intelligent Fault Recognition,"An accurate and efficient intelligent fault diagnosis of mobile robotic roller bearings can significantly enhance the reliability and safety of mechanical systems. To improve the efficiency of intelligent fault classification of mobile robotic roller bearings, this paper proposes a parallel machine learning algorithm using fine-grained-mode Spark on a Mesos big data cloud computing software framework. Through the segmentation of datasets and the support of a parallel framework, the parallel processing technology Spark is combined with a support vector machine (SVM), and a parallel single-SVM algorithm is realized using Scala language. In this approach, empirical mode decomposition (EMD) is applied to extract the energy of the acceleration vibration signal in different frequency bands as features. The parallel EMD-SVM approach is applied to detect faults in mobile robotic roller bearings from fault vibration signals. The experimental results show that it can accurately and effectively identify the faults, and it outperforms existing methods based on parallel deep belief network (DBN) and parallel radial basis function neural network under different training set sizes. Fault classification tests are conducted on outer-race and inner-race faults: in both cases, the proposed parallel EMD-SVM outperforms the serial EMD-SVM in terms of the classification accuracy and classification time under different test sizes. For a small number of nodes, the processing time of the proposed Spark model is less than that of Hadoop but close to that of Storm. For 17 slave nodes, the average precision, average recall, and average F1 score of Spark on Mesos in the fine-grained mode reach 97.3, 97.8, and 97.9%, respectively. The parallel EMD-SVM algorithm in the big data Spark cloud computing framework can improve the accuracy of intelligent fault classification, albeit by a small margin, with higher processing speed and learning convergence rate. © 2013 IEEE.",big data Spark | cloud computing software framework | empirical mode decomposition | intelligent fault recognition | mesos cluster manager | mobile robotic roller bearing | parallel deep belief network | Parallel machine learning algorithm | parallel support vector machine,IEEE Access,2020-01-01,Article,"Xian, Guangming",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85102749792,10.1109/ACCESS.2020.3024251,Brake fault identification and fault-tolerant directional stability control of heavy road vehicles,"Accurate fault diagnosis in air brake is crucial to reduce frequent brake inspection and maintenance in heavy commercial road vehicles. Existing model-based fault diagnostic schemes work well under limited vehicle operating conditions, which is insufficient for developing an on-board monitoring device. In this context, a learning-based fault identification scheme using the Random Forest technique, which accommodates the vehicle's wide operating conditions, is proposed. This scheme identifies the brake's fault levels with a better classification accuracy of 92% compared to techniques such as Naïve Bayes, k-Nearest Neighbors, Support Vector Machine, and Decision Tree. Further, a fault-tolerant controller is proposed to overcome the vehicle's directional instability arising due to the brake fault. Two sliding mode controllers, namely differential brake control and steering angle control, were developed to control the yaw angle. These have been implemented in a Hardware in Loop experimental platform with the vehicle dynamic simulation software TruckMaker R. © This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",Classification algorithms | Fault diagnosis | Fault-tolerant controller | Heavy road vehicle | Machine learning | Random forests | Sliding mode controller | Supervised learning,IEEE Access,2020-01-01,Article,"Raveendran, Radhika;Devika, K. B.;Subramanian, Shankar C.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85096232168,10.1109/ACCESS.2020.3033045,Duplicate Bug Report Detection and Classification System Based on Deep Learning Technique,"Duplicate bug report detection is a process of finding a duplicate bug report in the bug tracking system. This process is essential to avoid unnecessary work and rediscovery. In typical bug tracking systems, more than thousands of duplicate bug reports are reported every day. In turn, human cost, effort and time are increased. This makes it an important problem in the software management process. The solution is to automate the duplicate bug report detection system for reducing the manual effort, thus the productivity of triager's and developer's is increased. It also speeds up the process of software management as a result software maintenance cost is also reduced. However, existing systems are not quite accurate yet, in spite of these systems used various machine learning approaches. In this work, an automatic bug report detection and classification model is proposed using deep learning technique. The proposed system has three modules i.e. Preprocessing, Deep Learning Model and Duplicate Bug report Detection and Classification. Further, the proposed model used Convolutional Neural Network based deep learning model to extract relevant feature. These relevant features are used to determine the similar features of bug reports. Hence, the bug reports similarity is computers through these similar features. The performance of the proposed system is evaluated on six publicly available datasets using six performance metrics. It is noticed that the proposed system outperforms the existing systems by achieving an accuracy rate in the range of 85% to 99 % and recall@k rate in between 79%-94%. Moreover, the effectiveness of the proposed system is also measured on the cross training datasets of same and different domain. The proposed system achieves a good high accuracy rate for same domain data sets and low accuracy rate for different domain datasets. © 2013 IEEE.",bug tracking system | convolutional neural network | deep learning | Duplicate bug report detection | natural language processing | Siamese networks | software development | software engineering | software maintenance,IEEE Access,2020-01-01,Article,"Kukkar, Ashima;Mohana, Rajni;Kumar, Yugal;Nayyar, Anand;Bilal, Muhammad;Kwak, Kyung Sup",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85097199941,10.1109/ACCESS.2020.3040065,Applying Convolutional Neural Networks with Different Word Representation Techniques to Recommend Bug Fixers,"Bug triage processes are intended to assign bug reports to appropriate developers effectively, but they typically become bottlenecks in the development process-especially for large-scale software projects. Recently, several machine learning approaches, including deep learning-based approaches, have been proposed to recommend an appropriate developer automatically by learning past assignment patterns. In this paper, we propose a deep learning-based bug triage technique using a convolutional neural network (CNN) with three different word representation techniques: Word to Vector (Word2Vec), Global Vector (GloVe), and Embeddings from Language Models (ELMo). Experiments were performed on datasets from well-known large-scale open-source projects, such as Eclipse and Mozilla, and top-k accuracy was measured as an evaluation metric. The experimental results suggest that the ELMo-based CNN approach performs best for the bug triage problem. GloVe-based CNN slightly outperforms Word2Vec-based CNN in many cases. Word2Vec-based CNN outperforms GloVe-based CNN when the number of samples per class in the dataset is high enough. © 2013 IEEE.",bug fixing | bug report | Bug triage | CNN | deep learning | ELMo | GloVe | recommending bug fixer | word embedding | word representation | Word2Vec,IEEE Access,2020-01-01,Article,"Zaidi, Syed Farhan Alam;Awan, Faraz Malik;Lee, Minsoo;Woo, Honguk;Lee, Chan Gun",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85092396833,10.1109/AECT47998.2020.9194187,Attribute rule performance in data mining for software deformity prophecy datasets models,"In recently, all the developers, programmer and software engineers, they are working specially on software component and software testing to compete the software technology in the world. For this competition, they are using different kind of sources to analysis the software reliability and importance. Nowadays Data mining is one of source, which is used in software for overcome the problem of software fault which occur during the software test and its analysis. This kind of problem leads software deformity prophecy in software. In this research paper, we are also trying to overcome the software deformity prophecy problem with the help of our proposed solution called ONER rule attribute. We have used REPOSITORY datasets models, these datasets models are defected and non-defected datasets models. Our analysis class of interest is defected models. In our research, we have analyzed the efficiency of our proposed solution methods. The experiments results showed that using of ONER with discretize, have improved the efficiency of correctly classified instances in all. Using percentage split and training datasets with ONER discretize rule attribute have improved correctly classified in all datasets models. The analysis of positive accuracy f-measure is also increased in percentage split during the use of ONER with discretize but in some datasets models, the training data and cross validation is better with use of ONER rule attribute. The area under curve (ROC) in both scenarios using ONER rule attribute and discretize with ONER rule attribute is almost same or equal with each other. © 2020 IEEE.",Attribute Rule | Data Mining | Datasets Model | Machine Learning | ONER | Software Deformity Prophecy,"2019 International Conference on Advances in the Emerging Computing Technologies, AECT 2019",2020-02-01,Conference Paper,"Shaikh, Salahuddin;Changan, Liu;Malik, Maaz Rasheed",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85088402861,10.1109/AFRICON46755.2019.9133784,Development of a hybrid fault diagnostic method for power distribution network,"Fault detection in power systems with high reliability has been a subject of concern for protection engineers over the years. In this paper, we model a 132kV distribution system in Power World software package. Various types of fault cases are obtained through the study of the model. Stationery wavelet transform (SWT) is used to decompose the signal into its coefficients and extract statistical features. Subsequent to the extraction of features, support vector machine (SVM) and artificial neural network (ANN) schemes are used for fault detection and classification. Support vector regression (SVR) scheme is used for fault location. A hybrid technique for fault diagnostic comprising of SWT-(SVM, ANN) and SVR is proposed. The method showed good accuracy of classification and minimum error for fault estimation. The proposed method is tested on a machine learning platform WEKA. © 2019 IEEE.",Classification | Fault diagnostic | Power system,IEEE AFRICON Conference,2019-09-01,Conference Paper,"Moloi, K.;Jordaan, J. A.;Abe, B. T.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85084755652,10.1109/AIAM48774.2019.00079,Cross-Project Code Clone Consistency Prediction,"There are large numbers of code clones in software, which are introduced by developers to speed software development. Due to the similarity of code clones, consistent change usually happens to code clones, which can lead to extra maintenance cost and/or consistent-defect. To address this problem, researchers have proposed approaches to predict the requirement of consistent change. Although their works have good predictive effectiveness within one project, there is not enough data to make such prediction at the beginning phase of development. Therefore, in this paper, we first try to make use of the data from cross-projects and propose an approach to predict cross-project clone consistency-requirement. We construct the experiments on four open-source projects, and the results show that the cross-project prediction can be employed for clone consistency prediction. What's more, we suggest that the scale of data have not a significant impact on the effectiveness prediction. © 2019 IEEE.",clone consistency prediction | Code clones | cross-project | machine learning,"Proceedings - 2019 International Conference on Artificial Intelligence and Advanced Manufacturing, AIAM 2019",2019-10-01,Conference Paper,"Zhang, Fanlong",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84960942189,10.1109/AIRE.2015.7337622,Measuring Requirement Quality to Predict Testability,"Software bugs contribute to the cost of ownership for consumers in a software-driven society and can potentially lead to devastating failures. Software testing, including functional testing and structural testing, remains a common method for uncovering faults and assessing dependability of software systems. To enhance testing effectiveness, the developed artifacts (requirements, code) must be designed to be testable. Prior work has developed many approaches to address the testability of code when applied to structural testing, but to date no work has considered approaches for assessing and predicting testability of requirements to aid functional testing. In this work, we address requirement testability from the perspective of requirement understandability and quality using a machine learning and statistical analysis approach. We first use requirement measures to empirically investigate the relevant relationship between each measure and requirement testability. We then assess relevant requirement measures for predicting requirement testability. We examined two datasets, each consisting of requirement and code artifacts. We found that several measures assist in delineating between the testable and non-testable requirements, and found anecdotal evidence that a learned model of testability can be used to guide evaluation of requirements for other (non-trained) systems. © 2015 IEEE.",code testability | correlation analysis | human analyst | machine learning | requirement testability | statistical analysis | subjective assessment | supervised classification learning,"2nd International Workshop on Artificial Intelligence for Requirements Engineering, AIRE 2015 - Proceedings",2015-11-25,Conference Paper,"Hayes, Jane Huffman;Li, Wenbin;Yu, Tingting;Han, Xue;Hays, Mark;Woodson, Clinton",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85123494064,10.1109/APCC49754.2021.9609856,Fiber Fault Monitoring for Passive Optical Network using a Kernel-Based Support Vector Machine,"In order to avoid service level agreement violation in a passive optical network, effective failure detection and localization is of paramount importance for early fault identification. In this paper, we focus on a technique that can be used to detect and identify a failure in the physical layer using fiber Bragg grating (FBG) sensor integrated with machine learning (ML) technology. A kernel-based support vector machine trained model is purposely developed to detect and identify faulty optical link in PON. The technique relies on the retrieved optical reflected signal from the FBG sensors which are further elaborated by the ML algorithm using MATLAB. The dataset for the training and testing of the proposed model is generated in a simulated 1 times 4 Gigabit passive optical network, with a monitoring power of -4.16 dBm at a distance of 20 km. Optimal parameters of the support vector are selected with the help of cross-validation, thus leading to the optimized non-linear decision boundary. The proposed approach is tested on a number of datasets generated from the FBG sensors and demonstrates that the model achieves a 97 to 99% accuracy compared with the observed optical reflected spectra from the FBG sensors using an optical spectrum analyzer (OSA). The model which is not too complex, could avoid the use of costly OSA when embedded into the network controller in a software define network and minimizes the monitoring cost. © 2021 IEEE.",FBG Sensor | GPON | Passive Optical Network | Support vector Machine,"Proceeding - 2021 26th IEEE Asia-Pacific Conference on Communications, APCC 2021",2021-01-01,Conference Paper,"Usman, Auwalu;Zulkifli, Nadiatulhuda;Salim, Mohd Rashidi;Khairi, Kharina",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85111159271,10.1109/APR52552.2021.00010,Exploring Plausible Patches Using Source Code Embeddings in JavaScript,"Despite the immense popularity of the Automated Program Repair (APR) field, the question of patch validation is still open. Most of the present-day approaches follow the so-called Generate-and-Validate approach, where first a candidate solution is being generated and after validated against an oracle. The latter, however, might not give a reliable result, because of the imperfections in such oracles; one of which is usually the test suite. Although (re-) running the test suite is right under one's nose, in real life applications the problem of over-and underfitting often occurs, resulting in inadequate patches. Efforts that have been made to tackle with this problem include patch filtering, test suite expansion, careful patch producing and many more. Most approaches to date use post-filtering relying either on test execution traces or make use of some similarity concept measured on the generated patches. Our goal is to investigate the nature of these similarity-based approaches. To do so, we trained a Doc2Vec model on an open-source JavaScript project and generated 465 patches for 10 bugs in it. These plausible patches alongside with the developer fix are then ranked based on their similarity to the original program. We analyzed these similarity lists and found that plain document embeddings may lead to misclassification-it fails to capture nuanced code semantics. Nevertheless, in some cases it also provided useful information, thus helping to better understand the area of Automated Program Repair. © 2021 IEEE.",Automatic Program Repair | Code Embeddings | Doc2vec | Machine learning | Patch Correctness,"Proceedings - 2021 IEEE/ACM International Workshop on Automated Program Repair, APR 2021",2021-06-01,Conference Paper,"Csuvik, Viktor;Horváth, Dániel;Lajkó, Márk;Vidács, László",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84946574741,10.1109/APSEC.2014.43,A novel developer ranking algorithm for automatic bug triage using topic model and developer relations,"Recently, bug resolution has become a pivotal issue for software maintenance where recommendations for appropriate fixers are an important task. Some approaches (e.g., social network and machine learning techniques) exist that can achieve automatic bug triage (i.e., developer recommendation). This paper proposes a new method to recommend the most suitable fixer for bug resolution. Different from previous approaches, the proposed approaches combine topic model and developer relations (e.g., bug reporter and assignee) to capture developers' interest and experience on specific bug reports, we can arrange for the most appropriate developer to fix a new bug when it comes in. We evaluate the performance of our method using three large-scale open-source projects, including Eclipse, Mozilla Firefox, and Netbeans. The experimental results reveal that our approach outperforms other recommendation methods for developers. © 2014 IEEE.",Automatic bug triage | Developer recommendation | Developer relations | Software maintenance | Topic model,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",2014-01-01,Conference Paper,"Zhang, Tao;Yang, Geunseok;Lee, Byungjeong;Lua, Eng Keong",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84951840920,10.1109/APSEC.2014.81,Reducing false alarms from an industrial-strength static analyzer by SVM,"Static analysis tools are useful to find potential bugs and security vulnerabilities in a source code, however, false alarms from such tools lower their usability. In order to reduce various kinds of false alarms and enhance the performance of the tools, we propose a machine learning based false alarm reduction method. Abstract syntax trees (AST) are used to represent structural characteristics and support vector machine (SVM) is used to learn models and classify new alarms using probability. This probability is used to remove false alarms. To evaluate the proposed method, we performed experiments using a static analysis tool, SPARROW, and Java open source projects. As a result, 37.33% of false alarms were reduced, with only removing 3.16% of true alarms. © 2014 IEEE.",False alarm detection | Machine learning | Static analysis,"Proceedings - Asia-Pacific Software Engineering Conference, APSEC",2014-01-01,Conference Paper,"Yoon, Jongwon;Jin, Minsik;Jung, Yungbum",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85114049671,10.1109/ARGENCON49523.2020.9505355,User Stories identification in software's issues records using natural language processing,"Nowadays most of software development companies have adopted agile development methodologies, which suggest capturing requirements through user stories. The use of these good practices improves the organization of work teams and the quality of the resulting software product. However, user stories are too often poorly written in practice and exhibit inherent quality defects. In addition, it is common to find the user stories of a software project immersed in large volumes of issues request logs from software quality tracking systems, which makes difficult to process them later. In order to solve these defects and to formulate high quality requirements, a current trend is the application of computational linguistic techniques to identify and then process user stories. In this work, we present two recurrent neural network models that were developed for the identification of user stories in issue records from software quality tracking systems for further processing. © 2020 IEEE",Machine Learning | Natural Language Processing | Recurrent Neural Networks | Software Engineering,"2020 IEEE Congreso Bienal de Argentina, ARGENCON 2020 - 2020 IEEE Biennial Congress of Argentina, ARGENCON 2020",2020-12-01,Conference Paper,"Peña Veitía, Francisco J.;Roldán, Luciana;Vegetti, Marcela",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84982871189,10.1109/ASE.2002.1115019,What makes finite-state models more (or less) testable?,"This paper studies how details of a particular model can effect the efficacy of a search for detects. We find that if the test method is fixed, we can identity classes of software that are more or less testable. Using a combination of model mutators and machine learning, we find that we can isolate topological features that significantly change the effectiveness of a defect detection tool. More specifically, we show that for one defect detection tool (a stochastic search engine) applied to a certain representation (finite state machines), we can increase the average odds of finding a defect from 69% to 91%. The method used to change those odds is quite general and should apply to other defect detection tools being applied to other representations. © 2002 IEEE.",,Proceedings - ASE 2002: 17th IEEE International Conference on Automated Software Engineering,2002-01-01,Conference Paper,"Owen, D.;Menzies, T.;Cukic, B.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84963795380,10.1109/ASE.2015.73,Combining deep learning with information retrieval to localize buggy files for bug reports,"Bug localization refers to the automated process of locating the potential buggy files for a given bug report. To help developers focus their attention to those files is crucial. Several existing automated approaches for bug localization from a bug report face a key challenge, called lexical mismatch, in which the terms used in bug reports to describe a bug are different from the terms and code tokens used in source files. This paper presents a novel approach that uses deep neural network (DNN) in combination with rVSM, an information retrieval (IR) technique. rVSM collects the feature on the textual similarity between bug reports and source files. DNN is used to learn to relate the terms in bug reports to potentially different code tokens and terms in source files and documentation if they appear frequently enough in the pairs of reports and buggy files. Our empirical evaluation on real-world projects shows that DNN and IR complement well to each other to achieve higher bug localization accuracy than individual models. Importantly, our new model, HyLoc, with a combination of the features built from DNN, rVSM, and project's bug-fixing history, achieves higher accuracy than the state-of-the-art IR and machine learning techniques. In half of the cases, it is correct with just a single suggested file. Two out of three cases, a correct buggy file is in the list of three suggested files. © 2015 IEEE.",Bug Localization | Bug Reports | Deep Learning | Deep Neural Network | Information Retrieval,"Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015",2016-01-04,Conference Paper,"Lam, An Ngoc;Nguyen, Anh Tuan;Nguyen, Hoan Anh;Nguyen, Tien N.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85086316093,10.1109/AUPEC48547.2019.211845,A protection scheme for AC microgrids based on multi-agent system combined with machine learning,"Traditional protection schemes at the distribution level designed for unidirectional power flow will be compromised due to bi-directional flow of power with the increased penetration of distributed generation (DG) sources, resulting in miscoordination between protection devices. This paper proposes a new microgrid protection method based on the multi-agent system (MAS) combined with machine learning (ML) for fault detection in autonomous and grid-connected modes, protection coordination and updating relay settings to achieve adaptive protection. MAS framework with various layers and roles of each agent are described in detail. A meshed microgrid model is developed in Simulink to collect fault data for training and testing ML algorithms, while the behaviour of individual agents and interactions between them are validated in AnyLogic simulation software. The simulation results confirmed that the proposed MAS algorithm could provide primary and backup protection in both modes of microgrid. © 2019 IEEE.",Adaptive | AnyLogic | Machine learning | MAS | Microgrid | Multi-agent system | Protection | Simulink,"2019 29th Australasian Universities Power Engineering Conference, AUPEC 2019",2019-11-01,Conference Paper,"Uzair, Muhammad;Li, Li;Zhu, Jian Guo;Eskandari, Mohsen",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85063898878,10.1109/BDCloud.2018.00136,"ISTEP, an integrated self-tuning engine for predictive maintenance in industry 4.0","The recent expansion of IoT-enabled (Internet of Things) devices in manufacturing contexts and their subsequent data-driven exploitation paved the way to the advent of the Industry 4.0, promoting a full integration of IT services, smart devices, and control systems with physical objects, their electronics and sensors. The real-time transmission and analysis of collected data from factories has the potential to create manufacturing intelligence, of which predictive maintenance is an expression. Hence the need to design new approaches able to manage not only the data volume, but also the variety and velocity, extracting actual value from the humongous amounts of collected data. To this aim, we present iSTEP, an integrated Self-Tuning Engine for Predictive maintenance, based on Big Data technologies and designed for Industry 4.0 applications. The proposed approach targets some of the most common needs of manufacturing enterprises: compatibility with both the on-premises and the in-the-cloud environments, exploitation of reliable and largely supported Big Data platforms, easy deployment through containerized software modules, virtually unlimited horizontal scalability, fault-tolerant self-reconfiguration, flexible yet friendly streaming-KPI computations, and above all, the integrated provisioning of self-tuning machine learning techniques for predictive maintenance. The current implementation of iSTEP exploits a distributed architecture based on Apache Kafka, Spark Streaming, MLlib, and Cassandra; iSTEP provides (i) a specific feature engineering block aimed at automatically extracting metrics from the production monitoring time series, which improves the predictive performance by 77% on average, and (ii) a self-tuning approach that dynamically selects the best prediction algorithm, which improves the predictive performance up to 60%. The iSTEP engine provides transparent predictive models, able to provide end users with insights into the knowledge learned, and it has been experimentally evaluated on a public unbalanced failure dataset, whose extensive results are discussed in the paper. © 2018 IEEE.",Machine learning | Predictive maintenance,"Proceedings - 16th IEEE International Symposium on Parallel and Distributed Processing with Applications, 17th IEEE International Conference on Ubiquitous Computing and Communications, 8th IEEE International Conference on Big Data and Cloud Computing, 11th IEEE International Conference on Social Computing and Networking and 8th IEEE International Conference on Sustainable Computing and Communications, ISPA/IUCC/BDCloud/SocialCom/SustainCom 2018",2018-07-02,Conference Paper,"Apiletti, Daniele;Barberis, Claudia;Cerquitelli, Tania;Macii, Alberto;Macii, Enrico;Poncino, Massimo;Ventura, Francesco",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85060656267,10.1109/BigData.2018.8622211,Opinion Mining at Scale: A Case Study of the First Self-driving Car Fatality,"We present a comprehensive pipeline for large-scale opinion mining via a case study of the first self-driving car fatality, in an effort to qualitatively and quantitatively evaluate trending techniques in web searching as well as sentiment analysis. We first perform a scalable and fault-resilient web scraping with a partially-stateful data model. We then apply recent advances in deep learning comparing with a commercial software for sentiment detection. Not only do we measure the performances of the models by numerical metrics, we subsequently align the prediction results with amid economic indices and impactful social events. We further discuss trade-offs of above models from perspectives of both performance improvements of computer systems and accuracy enhancements of machine learning models, and provide deeper insights for stakeholders in the autonomous vehicle industry and the computational social science community. © 2018 IEEE.",,"Proceedings - 2018 IEEE International Conference on Big Data, Big Data 2018",2018-07-02,Conference Paper,"Li, Tao;Choi, Minsoo;Guo, Yuntao;Lin, Lei",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85081409953,10.1109/BigData47090.2019.9006554,KryptoOracle: A Real-Time Cryptocurrency Price Prediction Platform Using Twitter Sentiments,"Cryptocurrencies, such as Bitcoin, are becoming increasingly popular, having been widely used as an exchange medium in areas such as financial transaction and asset transfer verification. However, there has been a lack of solutions that can support real-time price prediction to cope with high currency volatility, handle massive heterogeneous data volumes, including social media sentiments, while supporting fault tolerance and persistence in real time, and provide real-time adaptation of learning algorithms to cope with new price and sentiment data. In this paper we introduce KryptoOracle, a novel real-time and adaptive cryptocurrency price prediction platform based on Twitter sentiments. The integrative and modular platform is based on (i) a Spark-based architecture which handles the large volume of incoming data in a persistent and fault tolerant way; (ii) an approach that supports sentiment analysis which can respond to large amounts of natural language processing queries in real time; and (iii) a predictive method grounded on online learning in which a model adapts its weights to cope with new prices and sentiments. Besides providing an architectural design, the paper also describes the KryptoOracle platform implementation and experimental evaluation. Overall, the proposed platform can help accelerate decision-making, uncover new opportunities and provide more timely insights based on the available and ever-larger financial data volume and variety. © 2019 IEEE.",cryptocurrency | machine learning | price prediction | real time | sentiment analysis | social media | software platform | Spark,"Proceedings - 2019 IEEE International Conference on Big Data, Big Data 2019",2019-12-01,Conference Paper,"Mohapatra, Shubhankar;Ahmed, Nauman;Alencar, Paulo",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84959488619,10.1109/BigDataService.2015.12,Power efficient MapReduce workload acceleration using integrated-GPU,"With the pervasiveness of MapReduce - one of the most prominent programming models for data parallelism in Apache Hadoop-, many researchers and developers have spent tremendous effort attempting to boost the computational speed and energy efficiency of MapReduce-based big data processing. However, the scalable and fault-tolerant nature of MapReduce introduces additional costs in disk IO and data transfer, caused by streaming intermediate outputs to disk. In light of these issues, many interesting research projects have been initiated with the goal of improving the compute speed and power efficiency of compute-intensive cloud computing workloads, several with the addition of discrete GPUs. In this work, we present a modified MapReduce approach focused on the iterative clustering algorithms in the Apache Mahout machine learning library that leverage the acceleration potential of the Intel integrated GPU in a multi-node cluster environment. The accelerated framework shows varying levels of speed-up (?45x for Map tasks-only, ?4.37x for the entire K-means clustering) as evaluated using the HiBench benchmark suite. Based on various experiments and in-depth analysis, we find that utilizing the integrated GPU via OpenCL offers significant performance and power efficiency gains over the original CPU based approach. Further analysis is also done to understand the correlations between compute, IO and power efficiency. As such, our results show that embracing the integrated GPU in the Hadoop MapReduce framework represents a promising advance in adding cost and energy efficient compute parallelism to a data parallel multinode environment. © 2015 IEEE.",Big Data | GPGPU | Hadoop | Integrated Graphics | Machine Learning | Mahout | OpenCL,"Proceedings - 2015 IEEE 1st International Conference on Big Data Computing Service and Applications, BigDataService 2015",2015-08-10,Conference Paper,"Kim, Sungye;Bottleson, Jeremy;Jin, Jingyi;Bindu, Preeti;Sakhare, Snehal C.;Spisak, Joseph S.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85072909282,10.1109/BotSE.2019.00020,Building sankie: An AI platform for DevOps,"There has been a fundamental shift amongst software developers and engineers in the past few years. The software development life cycle (SDLC) for a developer has increased in complexity and scale. Changes that were developed and deployed over a matter of days or weeks are now deployed in a matter of hours. Due to greater availability of compute, storage, better tooling, and the necessity to react, developers are constantly looking to increase their velocity and throughput of developing and deploying changes. Consequently, there is a great need for more intelligent and context sensitive DevOps tools and services that help developers increase their efficiency while developing and debugging. Given the vast amounts of heterogeneous data available from the SDLC, such intelligent tools and services can now be built and deployed at a large scale to help developers achieve their goals and be more productive. In this paper, we present Sankie, a scalable and general service that has been developed to assist and impact all stages of the modern SDLC. Sankie provides all the necessary infrastructure (back-end and front-end bots) to ingest data from repositories and services, train models based on the data, and eventually perform decorations or provide information to engineers to help increase the velocity and throughput of changes, bug fixes etc. This paper discusses the architecture as well as some of the key observations we have made from wide scale deployment of Sankie within Microsoft. © 2019 IEEE.",Azure | Bot | DevOps | Empirical software engineering | Infrastructure | Machine learning | Pull request | Scale | Software development life cycle,"Proceedings - 2019 IEEE/ACM 1st International Workshop on Bots in Software Engineering, BotSE 2019",2019-05-01,Conference Paper,"Kumar, Rahul;Bansal, Chetan;Maddila, Chandra;Sharma, Nitin;Martelock, Shawn;Bhargava, Ravi",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85114475585,10.1109/CCEM.2018.00010,False Positive Analysis of Software Vulnerabilities Using Machine Learning,"Dynamic Application Security Testing is conducted with the help of automated tools that have built-in scanners which automatically crawl all the webpages of the application and report security vulnerabilities based on certain set of pre-defined scan rules. Such pre-defined rules cannot fully determine the accuracy of a vulnerability and very often one needs to manually validate these results to remove the false positives. Eliminating false positives from such results can be a quite painful and laborious task. This article proposes an approach of eliminating false positives by using machine learning. Based on the historic data available on false positives, suitable machine learning models are deployed to predict if the reported defect is a real vulnerability or a false positive. © 2018 IEEE.",False Positive Analysis; Machine Learning; Software Security; vulnerabilities,"Proceedings - 7th IEEE International Conference on Cloud Computing in Emerging Markets, CCEM 2018",2019,,"Gowda S., Prajapati D., Singh R., Gadre S.S.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85050980364,10.1109/CCGRID.2018.00075,Understanding scale-dependent soft-error behavior of scientific applications,"Analyzing application fault behavior on large-scale systems is time-consuming and resource-demanding. Currently, researchers need to perform fault injection campaigns at full scale to understand the effects of soft errors on applications and whether these faults result in silent data corruption. Both time and resource requirements greatly limit the scope of the resilience studies that can be currently performed. In this work, we propose a methodology to model application fault behavior at large scale based on a reduced set of experiments performed at small scale. We employ machine learning techniques to accurately model application fault behavior using a set of experiments that can be executed in parallel at small scale. Our methodology drastically reduces the set and the scale of the fault injection experiments to be performed and provides a validated methodology to study application fault behavior at large scale. We show that our methodology can accurately model application fault behavior at large scale by using only small scale experiments. In some cases, we can model the fault behavior of a parallel application running on 4,096 cores with about 90% accuracy based on experiments on a single core. © 2018 IEEE.",Fault tolerance | HPC | Machine learning | Modeling | Resilience | Scalability,"Proceedings - 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, CCGRID 2018",2018-07-13,Conference Paper,"Kestor, Gokcen;Peng, Ivy Bo;Gioiosa, Roberto;Krishnamoorthy, Sriram",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85103459360,10.1109/CCWC51732.2021.9375841,Deep Learning-Based Resolution Prediction of Software Enhancement Reports,"The automatic resolution prediction of newly submitted enhancement reports is an important task during the bug triage process. It can help developers automatically predict the resolution status of enhancement reports. The resolution prediction is still a manual process which is very time-consuming, costly, and laborious. To help software applications for the timely implementation of enhancement reports, we introduce a deep learning-based technique to predict the resolution of newly submitted enhancement reports automatically by using a summary and description of enhancement reports. We use Word2Vec and a deep-learning-based classifier that can learn the deep syntactical and semantical relationship between the words of enhancement reports. We use additional novel features from enhancement reports and customized tokenizer to save useful features. Experimental results show the proposed approach enhances the performance as compared to state-of-the-art approaches in resolution prediction and has an effective ability to predict the resolution status of enhancement reports. © 2021 IEEE.",computational intelligence | document classification | enhancement reports | machine learning | natural language processing,"2021 IEEE 11th Annual Computing and Communication Workshop and Conference, CCWC 2021",2021-01-27,Conference Paper,"Arshad, Muhammad Ali;Huang, Zhiqiu;Riaz, Adnan;Hussain, Yasir",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-77949742123,10.1109/CISE.2009.5363473,Model bootstrapping for auto-diagnosis of enterprise systems,"Models for fault diagnosis can help reduce the time taken to accurately identify faults, but the complexity of modern enterprise systems means that the process of manually model-building is itself very time-consuming. We study here the relevance of bootstrapping a diagnostic model that can then be manually refined and augmented by domain experts. We present an approach to model construction, developed by analyzing log traces from a real data center. We compare the automatically-bootstrapped model against a manually-constructed reference model for the same problem set in order to measure what amount of the model can be automatically built. An experiment with an Oracle Enterprise System shows that approximately 15% of the model, diagnosing 30% of the related issues, can be automatically built. ©2009 IEEE.",Enterprise systems | Machine learning | System diagnostics,"Proceedings - 2009 International Conference on Computational Intelligence and Software Engineering, CiSE 2009",2009-12-01,Conference Paper,"Gaudin, Benoit;Nixon, Paddy;Bines, Keith;Busacca, Fulvio;Casey, Niall",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84959266084,10.1109/CLUSTER.2015.31,Fast fault injection and sensitivity analysis for collective communications,"The collective communication operations, which are widely used in parallel applications for global communication and synchronization are critical for application's performance and scalability. However, how faulty collective communications impact the application and how errors propagate between the application processes is largely unexplored. One of the critical reasons for this situation is the lack of fast evaluation method to investigate the impacts of faulty collective operations. The traditional random fault injection methods relying on a large amount of fault injection tests to ensure statistical significance require a significant amount of resources and time. These methods result in prohibitive evaluation cost when applied to the collectives. In this paper, we introduce a novel tool named Fast Fault Injection and Sensitivity Analysis Tool (FastFIT) to conduct fast fault injection and characterize the application sensitivity to faulty collectives. The tool achieves fast exploration by reducing the exploration space and predicting the application sensitivity using Machine Learning (ML) techniques. A basis for these techniques are implicit correlations between MPI semantics, application context, critical application features, and application responses to faulty collective communications. The experimental results show that our approach reduces the fault injection points and tests by 97% for representative benchmarks (NAS Parallel Benchmarks (NPB)) and a realistic application (Large-scale Atomic/Molecular Massively Parallel Simulator (LAMMPS)) on a production supercomputer. Further, we statistically generalize the application sensitivity to faulty collective communications for these workloads, and present correlation between application features and the sensitivity. © 2015 IEEE.",Accuracy | Context | Correlation | Decision trees | Error analysis | Predictive models | Sensitivity,"Proceedings - IEEE International Conference on Cluster Computing, ICCC",2015-10-26,Conference Paper,"Feng, Kun;Venkata, Manjunath Gorentla;Li, Dong;Sun, Xian He",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85074146880,10.1109/COMITCon.2019.8862212,ACT Testbot and 4S Quality Metrics in XAAS Framework,"The purpose of this paper is to analyze all Cloud based Service Models, Continuous Integration, Deployment and Delivery process and propose an Automated Continuous Testing and testing as a service based TestBot and metrics dashboard which will be integrated with all existing automation, bug logging, build management, configuration and test management tools. Recently cloud is being used by organizations to save time, money and efforts required to setup and maintain infrastructure and platform. Continuous Integration and Delivery is in practice nowadays within Agile methodology to give capability of multiple software releases on daily basis and ensuring all the development, test and Production environments could be synched up quickly. In such an agile environment there is need to ramp up testing tools and processes so that overall regression testing including functional, performance and security testing could be done along with build deployments at real time. To support this phenomenon, we researched on Continuous Testing and worked with industry professionals who are involved in architecting, developing and testing the software products. A lot of research has been done towards automating software testing so that testing of software product could be done quickly and overall testing process could be optimized. As part of this paper we have proposed ACT TestBot tool, metrics dashboard and coined 4S quality metrics term to quantify quality of the software product. ACT testbot and metrics dashboard will be integrated with Continuous Integration tools, Bug reporting tools, test management tools and Data Analytics tools to trigger automation scripts, continuously analyze application logs, open defects automatically and generate metrics reports. Defect pattern report will be created to support root cause analysis and to take preventive action. © 2019 IEEE.",4S Quality Metrics | ACT (Automated Continuous Testing) | Auto Bug Logging and Tracking | Cloud | Continuous Delivery | Continuous Deployment | Continuous Integration | Continuous Testing | T-Model | TestBot | XaaS (Everything as a Service),"Proceedings of the International Conference on Machine Learning, Big Data, Cloud and Parallel Computing: Trends, Prespectives and Prospects, COMITCon 2019",2019-02-01,Conference Paper,"Chhillar, Dheeraj;Sharma, Kalpana",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84870834685,10.1109/CONSEG.2012.6349519,Determining Bug severity using machine learning techniques,"Software Bug reporting is an integral part of software development process. Once the Bug is reported on Bug Tracking System, their attributes are analyzed and subsequently assigned to various fixers for their resolution. During the last two decades Machine-Learning Techniques (MLT) has been used to create self-improving software. Supervised machine learning technique is widely used for prediction of patterns in various applications but, we have found very few for software repositories. Bug severity, an attribute of a software bug report is the degree of impact that a defect has on the development or operation of a component or system. Bug severity can be classified into different levels based on their impact on the system. In this paper, an attempt has been made to demonstrate the applicability of machine learning algorithms namely Naïve Bayes, k-Nearest Neighbor, Naïve Bayes Multinomial, Support Vector Machine, J48 and RIPPER in determining the class of bug severity of bug report data of NASA from PROMISE repository. The applicability of algorithm in determining the various levels of bug severity for bug repositories has been validated using various performance measures by applying 5-fold cross validation1. © 2012 IEEE.",Bug Severity | Feature Selection | Machine Learning | Supervised Classification,"2012 CSI 6th International Conference on Software Engineering, CONSEG 2012",2012-12-14,Conference Paper,"Chaturvedi, K. K.;Singh, V. B.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85106182864,10.1109/CSICC52343.2021.9420548,Learning to Predict Software Testability,"Software testability is the propensity of code to reveal its existing faults, particularly during automated testing. Testing success depends on the testability of the program under test. On the other hand, testing success relies on the coverage of the test data provided by a given test data generation algorithm. However, little empirical evidence has been shown to clarify whether and how software testability affects test coverage. In this article, we propose a method to shed light on this subject. Our proposed framework uses the coverage of Software Under Test (SUT), provided by different automatically generated test suites, to build machine learning models, determining the testability of programs based on many source code metrics. The resultant models can predict the code coverage provided by a given test data generation algorithm before running the algorithm, reducing the cost of additional testing. The predicted coverage is used as a concrete proxy to quantify source code testability. Experiments show an acceptable accuracy of 81.94% in measuring and predicting software testability. © 2021 IEEE.",machine learning | software analytics | software metrics | software testability | Software testing,"26th International Computer Conference, Computer Society of Iran, CSICC 2021",2021-03-03,Conference Paper,"Nasrabadi, Morteza Zakeri;Parsa, Saeed",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85050694111,10.1109/CYBERNETICSCOM.2017.8311708,Software metrics for fault prediction using machine learning approaches: A literature review with PROMISE repository dataset,"Software testing is an important and critical phase of software development life cycle to find software faults or defects and then correct those faults. However, testing process is a time-consuming activity that requires good planning and a lot of resources. Therefore, technique and methodology for predicting the testing effort is important process prior the testing process to significantly increase efficiency of time, effort and cost usage. Correspond to software metric usage for measuring software quality, software metric can be used to identify the faulty modules in software. Furthermore, implementing machine learning technique will allow computer to 'learn' and able to predict the fault prone modules. Research in this field has become a hot issue for more than ten years ago. However, considering the high importance of software quality with support of machine learning methods development, this research area is still being highlighted until this year. In this paper, a survey of various software metric used for predicting software fault by using machine learning algorithm is examined. According to our review, this is the first study of software fault prediction that focuses to PROMISE repository dataset usage. Some conducted experiments from PROMISE repository dataset are compared to contribute a consensus on what constitute effective software metrics and machine learning method in software fault prediction. © 2017 IEEE.",machine learning algorithm | software defect prediction | Software metric | software testing,"2017 IEEE International Conference on Cybernetics and Computational Intelligence, CyberneticsCOM 2017 - Proceedings",2017-07-02,Conference Paper,"Meiliana, M.;Karim, Syaeful;Warnars, Harco Leslie Hendric Spits;Gaol, Ford Lumban;Abdurachman, Edi;Soewito, Benfano",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85092737018,10.1109/ComPE49325.2020.9200103,A Systematic Review on Application of Deep Learning Techniques for Software Quality Predictive Modeling,"Software quality prediction is the process of evaluating the software developed for various metrics like defect prediction, bug localisation, effort estimation etc. To evaluate these metrics a myriad of techniques have been developed in the literature, from manual assessment to application of machine learning and statistical testing. These methodologies, however, had lower accuracy in determining SQPMs due to their inability to model the complex relationships in the training data. With the wide emergence of deep learning, not only has the accuracy of the pre-existing models enhanced, but it has also opened doors for new metrics that could be evaluated and automated. This study performs a systematic literature review of research papers published from January 1990 to January 2019 that used deep learning to evaluate software quality prediction metrics (SQPM). The paper identifies 20 primary studies and 7 categories of application of deep learning in SQPM. Models using deep learning techniques significantly outperform other traditional methodologies in almost all studies. The concept and external threats to the models are limited, however the time taken to train these models is large. The techniques, currently predominantly applied for defect prediction, have shown promising results in other diverse software engineering fields like code search and effort estimation by modeling the source code efficiently. There is, hence, scope for incorporating deep learning further with pragmatic use and diverse application. The need to find scalable solutions, however, still persists. © 2020 IEEE.",deep learning | defect prediction | effort estimation | fault localisation | software quality prediction metrics | systematic literature review,"2020 International Conference on Computational Performance Evaluation, ComPE 2020",2020-07-01,Conference Paper,"Malhotra, Ruchika;Gupta, Shreya;Singh, Tanishq",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85078736768,10.1109/CyberC.2019.00060,The future of broadband access network architecture and intelligent operations,"This paper presents an overview of the evolution towards a broadband optical access network. Network operations transformation with emphasis on automated broadband service monitoring, fault detection, fault recovery, and fault prediction is also discussed. However, in order to support broadband access network evolution and to accelerate telco operations transformation, traditional network operations, administration and maintenance (OAM) methods will not be sufficient. Therefore, we describe our key research contributions regarding new approaches to support closed-loop broadband network OAM that will eliminate human touches and automate non-physical fault recovery for daily routine tasks. We have designed and implemented a network knowledge engine (NKE) centered software framework to enable data collection, correlation, and analysis for troubleshooting complex issues in the broadband access network. Our solutions will address the following network management domains, including (a) knowledge management, (b) surveillance management, (c) incident management, and (d) problem management. In this paper, we focus on auto proactive and predictive trouble management which can verify and confirm a passive device problem location by NKE. We will describe a step-by-step methodology to control data process flow, optical fiber path discovery management, and auto-detection of failed passive devices by using the following techniques: (a) big data analytics, (b) knowledge graph, (c) machine learning (ML), and (d) artificial intelligence (AI) prediction rule and learning policy involving the spatial-temporal algorithm. With combinations of these technique, our preliminary study indicates that the accuracy of auto fault detection is 93%, while the false positive rate is ∼0.01%. As results, we demonstrated the promise of this novel technology framework in supporting transformation from expert's domain knowledge into machine knowledge. © 2019 IEEE.",Big Data Analytics | Broadband Access Network | Knowledge Graph | ML and AI,"Proceedings - 2019 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery, CyberC 2019",2019-10-01,Conference Paper,"Yang, Charlie Chen Yui;Li, Guangzhi;Liu, Xiang;Wu, Zonghuan;Zhang, Kaiyu",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85072017345,10.1109/DSN-S.2019.00021,On the Estimation of Complex Circuits Functional Failure Rate by Machine Learning Techniques,"De-Rating or Vulnerability Factors are a major feature of failure analysis efforts mandated by today's Functional Safety requirements. Determining the Functional De-Rating of sequential logic cells typically requires computationally intensive fault-injection simulation campaigns. In this paper a new approach is proposed which uses Machine Learning to estimate the Functional De-Rating of individual flip-flops and thus, optimising and enhancing fault injection efforts. Therefore, first, a set of per-instance features is described and extracted through an analysis approach combining static elements (cell properties, circuit structure, synthesis attributes) and dynamic elements (signal activity). Second, reference data is obtained through first-principles fault simulation approaches. Finally, one part of the reference dataset is used to train the Machine Learning algorithm and the remaining is used to validate and benchmark the accuracy of the trained tool. The intended goal is to obtain a trained model able to provide accurate per-instance Functional De-Rating data for the full list of circuit instances, an objective that is difficult to reach using classical methods. The presented methodology is accompanied by a practical example to determine the performance of various Machine Learning models for different training sizes. © 2019 IEEE.",Fault Injection | Gate-Level Netlist | k-Nearest Neighbors | Linear Least Squares | Machine Learning | Single Event Effects | Support Vector Regression | Transient Faults,"Proceedings - 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks - Supplemental Volume, DSN-S 2019",2019-06-01,Conference Paper,"Lange, Thomas;Balakrishnan, Aneesh;Glorieux, Maximilien;Alexandrescu, Dan;Sterpone, Luca",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85114899686,10.1109/DSN48987.2021.00022,Plinius: Secure and Persistent Machine Learning Model Training,"With the increasing popularity of cloud based machine learning (ML) techniques there comes a need for privacy and integrity guarantees for ML data. In addition, the significant scalability challenges faced by DRAM coupled with the high access-times of secondary storage represent a huge performance bottleneck for ML systems. While solutions exist to tackle the security aspect, performance remains an issue. Persistent memory (PM) is resilient to power loss (unlike DRAM), provides fast and fine-granular access to memory (unlike disk storage) and has latency and bandwidth close to DRAM (in the order of ns and GB/s, respectively). We present PLINIUS, a ML framework using Intel SGX enclaves for secure training of ML models and PM for fault tolerance guarantees. PLINIUS uses a novel mirroring mechanism to create and maintain (i) encrypted mirror copies of ML models on PM, and (ii) encrypted training data in byte-addressable PM, for near-instantaneous data recovery after a system failure. Compared to disk-based checkpointing systems, PLINIUS is 3.2× and 3.7× faster respectively for saving and restoring models on real PM hardware, achieving robust and secure ML model training in SGX enclaves. © 2021 IEEE.",AI Security | Fault Tolerance | Intel Software Guard Extensions | Machine Learning | Model Training | Non Volatile Memory | Persistent Memory,"Proceedings - 51st Annual IEEE/IFIP International Conference on Dependable Systems and Networks, DSN 2021",2021-06-01,Conference Paper,"Yuhala, Peterson;Felber, Pascal;Schiavoni, Valerio;Tchana, Alain",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85123210088,10.1109/EPEPS51341.2021.9609118,Prediction of De-embedded Eye Height/Width Parameters using Machine Learning in High-Speed Serial Link Characterization,"Post silicon analog validation and characterization refers to a set of activities done on a lab test bench setup to test and debug analog IPs in an SoC (System-on chip). An analog validation engineer typically uses a combination of hardware and software tools to validate an IP. Machine Learning (ML) as a tool has been successfully utilized by engineers in pre-silicon activities like highspeed channel modelling and Eye height/Width prediction. However, very limited research can be found regarding the use of ML algorithms in post silicon analog validation and characterization. Since, post silicon validation's main objective is to find out bugs in system, the accuracy to techniques used in test are of utmost importance. This paper investigates the use of ML in post silicon characterization. This has been done by taking an example use-case: computation of de-embedded signal integrity parameters (Eye Hight and Eye width) from non-de-embedded parameters in the characterization activity of high speed SERDES. It has been shown that by using ML algorithms, one can reduce the SI parameter computation timing with less than 2.1% average error. © 2021 IEEE.",Analog Bench Validation | De-embedding | high-speed channel | insertion loss | Linear Regression | Machine Learning | Signal Integrity | support vector regression,EPEPS 2021 - IEEE 30th Conference on Electrical Performance of Electronic Packaging and Systems,2021-01-01,Conference Paper,"Goyal, Mohit;Pandey, Maneesh;Kumar, Sharad;Sharma, Rohit",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84893243344,10.1109/ESEM.2013.7,PROMISE 2013: The 9th international conference on predictive models in software engineering,"PROMISE conference is an annual forum for researchers and practitioners to present, discuss and exchange ideas, results, expertise and experiences in construction and/or application of prediction models in software engineering. Such models could be targeted at: planning, design, implementation, testing, maintenance, quality assurance, evaluation, process improvement, management, decision making, and risk assessment in software and systems development. PROMISE is distinguished from similar forums with its public data repository and focus on methodological details, providing a unique interdisciplinary venue for software engineering and machine learning communities, and seeking for verifiable and repeatable prediction models that are useful in practice. © 2013 IEEE.",predictive models; defect prediction; effort estimation; software analytics; empirical software engineering,International Symposium on Empirical Software Engineering and Measurement,2013-12-01,Conference Paper,"Turhan, Burak;Wagner, Stefan;Bener, Ayse;Di Penta, Massimiliano;Yang, Ye",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85071192017,10.1109/ETS.2019.8791512,LearnX: A hybrid deterministic-statistical defect diagnosis methodology,"Software-based diagnosis analyzes the observed response of a failing circuit to pinpoint potential defect locations and deduce their respective behaviors. It plays a crucial role in finding the root cause of failure, and subsequently facilitates yield analysis, learning and optimization. This paper describes a two-phase, physically-aware diagnosis methodology called LearnX to improve the quality of diagnosis, and in turn the quality of design, test and manufacturing. The first phase attempts to diagnose a defect that manifests as a well-established fault behavior (e.g., stuck or bridge fault models). The second phase uses machine learning to build a model (separate for each defect type) that learns the characteristics of defect candidates to distinguish correct candidates from incorrect ones. Results from 30,000 fault injection experiments indicate that LearnX returns an ideal diagnosis result (i.e., a single candidate correctly representing the injected fault) for 73.2% of faulty circuits, which is 86.6% higher than state-of-the-art commercial diagnosis. Silicon experiments further demonstrate the value of LearnX. © 2019 IEEE.",,Proceedings of the European Test Workshop,2019-05-01,Conference Paper,"Mittal, Soumya;Shawn Blanton, R. D.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85057754334,10.1109/FiCloud.2018.00017,ReMot Reputation and Resource-Based Model to Estimate the Reliability of the Host Machines in Volunteer Cloud Environment,"Due to the need of green computing and low cost, emerging paradigms such as edge and fog computing, and volunteer cloud have recently been introduced. In general, the volunteer cloud model targets globally distributed volunteer, highly heterogeneous, and non-dedicated machines. The inherent high degree of resource heterogeneity leads to varying levels of hardware and software failures and configuration faults on the unreliable and volatile volunteer hosts. As a result, the performance of deployed tasks is detrimentally impacted and is a key challenge, particularly in the case of scheduling algorithms. Most of the reputation models that have been used for reliability evaluation only evaluate the reliability of host machines by simply using the ratio of successfully completed tasks to total tasks requested. These models do not consider the resource utilization and the daily or weekly patterns of job behaviors or characteristics (e.g. priority of a job). Thus, the performance of tasks that run on these resources suffers and may take a substantial time to complete. Therefore, there is a need to proactively consider the reliability of host machines for effective and efficient management of resources in highly heterogeneous and distributed cloud environments. To address these challenges, this paper proposes a reputation and resource-based reliability model called ReMot. ReMot is an intelligent machine learning based model that utilizes historical data of the tasks and host machines to extract their resource usage patterns, in addition to other metrics such as task failure rate and resource utilization to predict the reliability of host machines. To validate ReMots approach, the researchers have utilized a large usage trace of real world applications made available by Google Inc. The results indicate that ReMot obtained more accurate reliability estimation than existing models and dynamically adapts to workload variations. © 2018 IEEE.",Cloud Computing | Reliability | ReMot | Reputation | Volunteer Cloud,"Proceedings - 2018 IEEE 6th International Conference on Future Internet of Things and Cloud, FiCloud 2018",2018-09-07,Conference Paper,"Alsenani, Yousef;Crosby, Garth V.;Velasco, Tomas;Alahmadi, Abdulrahman",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85123364715,10.1109/GCCE53005.2021.9621355,Adversarial Multi-task Learning-based Bug Fixing Time and Severity Prediction,"Bug reports contain important information for software quality assurance. Conventionally, engineers complete bug report analysis tasks, which are extremely burdensome. Recently, researchers and companies have been working towards the automation of bug report analysis. Many machine learning and deep learning models are utilized for triage and prediction of bug report attributes such as bug fixing time and bug severity based on the description and comment text in bug reports. However, due to the rapid growth of data size in bug reporting systems, the prediction accuracy in single-task machine learning models is neither efficient nor effective. Multi-task learning (MTL) is a transfer learning scheme, which can train multiple related tasks together, reducing the training time while improving the overall performance. In our study, we utilize adversarial multi-task learning, which addresses the problem of contaminated shared feature space in common MTL models towards a purer shared feature space. Our adversarial convolutional neural network model (ADV-CNN) improved the validation accuracy of the bug fixing time prediction from 83.67% of a ST-CNN model to 89.25%. © 2021 IEEE.",Adversarial Learning | Bug Fixing Time Prediction | Bug Report Analysis | Multi-task Learning,"2021 IEEE 10th Global Conference on Consumer Electronics, GCCE 2021",2021-01-01,Conference Paper,"Liu, Qicong;Washizaki, Hironori;Fukazawa, Yoshiaki",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84973910944,10.1109/IACS.2016.7476092,Severity prediction of software bugs,"We target the problem of identifying the severity of a bug report. Our main aim is to develop an intelligent system that is capable of predicting the severity of a newly submitted bug report through a bug tracking system. For this purpose, we build a dataset consisting of 59 features characterizing 163 instances that belong to two classes: severe and non-severe. We combine the proposed feature set with strong classification algorithms to assist in predicting the severity of bugs. Moreover, the proposed algorithms are integrated within a boosting algorithm for an enhanced performance. Our results show that the proposed technique has proved successful with a classification performance accuracy of more than 76% with the AdaBoost algorithm and cross validation test. Moreover, boosting has been effective in enhancing the performance of its base classifiers with improvements of up to 4.9%. © 2016 IEEE.",Adaboost | machine learning | severity predection | software bugs,"2016 7th International Conference on Information and Communication Systems, ICICS 2016",2016-05-20,Conference Paper,"Otoom, Ahmed Fawzi;Al-Shdaifat, Doaa;Hammad, Maen;Abdallah, Emad E.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85018501119,10.1109/IC3.2016.7880218,Visheshagya: Time based expertise model for bug report assignment,"The brisk escalation in scale of software systems has made bug triaging an imperative step in bug fixing process. A huge amount of bug reports is submitted daily to bug tracking repositories. Although this practice assists in building a reliable and error-free software product but handling a large amount of work becomes challenging. Bug assignment, an essential step in bug triaging, is the process of designating a suitable developer for the bug report who could make code changes in order to fix the bug. Various approaches ranging from semi to fully automatic bug assignment are proposed in literature. These approaches are mostly based on machine learning and information retrieval techniques. Since the information retrieval based activity profiling approach achieves higher accuracy, they are more often used in recent studies. Time factor based normalization in activity profiling could play a vital role in analyzing the level of expertise (or knowledge) of developers as the knowledge decays with time. This paper proposes a time oriented expertise model, Visheshagya, which utilizes the meta-fields of bug reports for developer selection. The proposed technique is used to prioritize the developers actively participating in software bug repository on the basis of their current knowledge. The proposed approach has been validated on two popular projects of Bugzilla repository, Mozilla and Eclipse. The result shows that time based activity profiling of developers outperforms existing information retrieval based bug report assignment and achieves an improvement of 14.3% and 9.95% in the accuracy of top-10 list size in Mozilla and Eclipse projects respectively. © 2016 IEEE.",Activity profiling of developers | Bug assignment | Bug triaging | Mining software repositories | Time based expertise,"2016 9th International Conference on Contemporary Computing, IC3 2016",2017-03-16,Conference Paper,"Anjali, ;Mohan, Devina;Sardana, Neetu",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85073230143,10.1109/IC3.2019.8844876,Empirical Analysis of Ensemble Machine Learning Techniques for Bug Triaging,"Bug reports are an inescapable part of the software product framework. Nowadays, software advancements have led to the creation of beta versions of software in order to assemble the bug reports from clients. The assembled bug reports are then handled by software developers to make consequent software more reliable as well as robust. However, high recurrence of approaching bug reports forges the process of bug fixing to be a troublesome tedious process. Bug triaging is an essential component of issue handling process and it deals with the selection of a suitable software developer for handling of reported bug such that the assigned developer is able to fix the reported issue. In the literature, different semi and fully mechanized procedures are proposed to facilitate the endeavor of developer selection in bug repositories. These techniques use historically fixed information from bug repositories to classify any new incoming bugs. In the recent years, ensemble-based classification techniques have gained popularity. These techniques use multiple classifiers for making a prediction and has proved to be outperforming classical machine learning classification. In this paper, we present an empirical study of ensemble-based techniques for classification of new incoming bug reports. We studied 5 ensemble classification techniques, namely Bagging, Boosting, Majority Voting, Average Voting, and Stacking using 25 different machine learning classifiers as base classifiers. The experimental results showed that ensemble classifiers outperform classical machine learning algorithms for selection of suitable developer for handling the bug report. © 2019 IEEE.",Bug Report | Bug Repository | Classification | Developer Selection | Ensemble Learning | Machine Learning | Mining Software Repositories | Triaging,"2019 12th International Conference on Contemporary Computing, IC3 2019",2019-08-01,Conference Paper,"Goyal, Anjali;Sardana, Neetu",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85069188180,10.1109/ICACCCN.2018.8748582,Severity Prediction of Bug Reports using Text Mining: A Systematic Review,"Bugs are abundant in software systems. The impact that a bug has on operation of a product is defined by its severity. Bugs with high severity such as blocker or critical needs to be fixed instantly as compared to bugs with low severity. Thus predicting the severity of bugs is crucial. Therefore, text mining techniques are applied on the summary of bug reports and prediction is done using various machine learning techniques. In this study, a systematic review is done from 2008 to 2017, in which prediction of severity is done using text mining. 29 relevant research studies are found following a systematic review process. The study deduces that textual data can be used to predict the severity of bugs using various machine learning techniques. After the analysis of results, it was concluded that Bayesian learner techniques are commonly used machine learning technique and Information Gain as feature selection method used. The study also analyses the most common dataset and tools used to predict the severity of reported bugs. The performance of various machine learning techniques is evaluated and results can be generalized to other studies. This study will benefit the researchers and guide them in further research in the area of severity prediction using textual attributes of software bugs. © 2018 IEEE.",Bug reports | Machine Learning techniques | Severity Prediction | Text Mining,"Proceedings - IEEE 2018 International Conference on Advances in Computing, Communication Control and Networking, ICACCCN 2018",2018-10-01,Conference Paper,"Kaur, Arvinder;Goyal Jindal, Shubhra",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84907389222,10.1109/ICACCI.2015.7275620,Mining defect reports for predicting software maintenance effort,"Software Maintenance is the crucial phase of software development lifecycle, which begins once the software has been deployed at the customer's site. It is a very broad activity and includes almost everything that is done to change the software if required, to keep it operational after its delivery at the customer's end. A lot of maintenance effort is required to change the software after it is in operation. Therefore, predicting the effort and cost associated with the maintenance activities such as correcting and fixing the defects has become one of the key issues that need to be analyzed for effective resource allocation and decision-making. In view of this issue, we have developed a model based on text mining techniques using the statistical method namely, Multi-nominal Multivariate Logistic Regression (MMLR). We apply text mining techniques to identify the relevant attributes from defect reports and relate these relevant attributes to software maintenance effort prediction. The proposed model is validated using 'Camera' application package of Android Operating System. Receiver Operating Characteristics (ROC) analysis is done to interpret the results obtained from model prediction by using the value of Area Under the Curve (AUC), sensitivity and a suitable threshold criterion known as the cut-off point. It is evident from the results that the performance of the model is dependent on the number of words considered for classification and therefore shows the best results with respect to top-100 words. The performance is irrespective of the type of effort category. © 2015 IEEE.",Defect reports; Machine Learning; Receiver Operating Characteristics; Software Maintenance Effort Prediction; Text mining,"2015 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2015",2015,,"Jindal R., Malhotra R., Jain A.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85070599364,10.1109/ICACTM.2019.8776830,Implementation of Machine Learning Techniques in Software Reliability: A framework,"In this paper review of existing literature in the field of software reliability models based on machine learning techniques presented. Software reliability is very useful tool in determining the software quality. By using machine learning techniques for getting unhidden parameters affecting software fault prediction for exploring various parameters leading to obsoleteness of software by presenting category of papers of software reliability, software fault prediction, software trustworthiness, software reusability, using machine learning techniques based on statistical inferences which could predict useful pattern on hidden data of faulty software database of empirical datasets related to software testing. After studying plenary relevant papers on faults generated during fault removal, faults already present, we proposed a novel approach based on identifying most relevant parameter affecting the software reliability using Machine Learning Techniques. © 2019 IEEE.",Failures | Faults | Feature Selection | Intelligent Software | Machine Learning Techniques | Software Reliability,"2019 International Conference on Automation, Computational and Technology Management, ICACTM 2019",2019-04-01,Conference Paper,"Banga, Manu;Bansal, Abhay;Singh, Archana",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85100096979,10.1109/ICAI50593.2020.9311385,Load Balancing model based on Machine Learning and Segment Routing in SDN,"Because of increased number of network devices in the world, as well as the need for the network environment to provide dynamics of faults and adaptability to load changes, it is difficult and complicated network to be managed. By applying an approach called Software Defined Networks (SDN), the separation of the control and data planes is achieved. This allows the creation and deployment of new network applications to be easier, as well as provides simplification and flexibility in network policy enforcement, facilitating network configuration and management. The main problems in SDN are load balancing and segment routing. This paper proposes a model which aims to reduce not only the overall load on the network, but also to reduce the bandwidth and improve the routing mechanism on the SDN networks. It combines segment routing algorithm and load balancing mechanisms based on machine learning. © 2020 IEEE.",load balancing | machine learning | segment routing | software defined networks,"2020 International Conference Automatics and Informatics, ICAI 2020 - Proceedings",2020-10-01,Conference Paper,"Todorov, Dimitar;Valchanov, Hristo;Aleksieva, Veneta",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85089345974,10.1109/ICAIBD49809.2020.9137461,An Architecture Based on Digital Twins for Smart Power Distribution System,"Constructing an effective architecture based on digital twins using advanced artificial intelligent technologies remains a key challenge in smart power distribution system. Despite recent advances in important domains such as device health maintenance and manufacturing process, the conventional architecture does not offer a satisfactory solution for rapidly providing data models, a set of analytics or algorithms, and knowledge. In this work, we employ ideas from distributed network based on digital twins and from recent advances that augment artificial intelligent with big data. Our architecture provides a paradigm that uses a distributed network makes computer programming, software, and data spread out across more than one terminal, but communicate complex messages through terminal nodes, and are dependent upon each other, obviating the pressure from master station. We demonstrate the usefulness of digital twins that are used to track the performance of devices such as transformer during normal operation to detect possible defects and fix them before they result in failure. We also conclude that using digital twin technology will understand, predict, and optimize performance in order to achieve improved business outcomes, and apply machine learning and knowledge to predict the future. © 2020 IEEE.",big data | digital twin | machine learning | predictive maintenance | smart power distribution system,"2020 3rd International Conference on Artificial Intelligence and Big Data, ICAIBD 2020",2020-05-01,Conference Paper,"Zhang, Ganghong;Huo, Chao;Zheng, Libin;Li, Xinjun",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85123530378,10.1109/ICCAD51958.2021.9643539,Towards Energy-Efficient and Secure Edge AI: A Cross-Layer Framework,"The security and privacy concerns along with the amount of data that is required to be processed on regular basis has pushed processing to the edge of the computing systems. Deploying advanced Neural Networks (NN), such as deep neural networks (DNNs) and spiking neural networks (SNNs), that offer state-of-the-art results on resource-constrained edge devices is challenging due to the stringent memory and power/energy constraints. Moreover, these systems are required to maintain correct functionality under diverse security and reliability threats. This paper first discusses existing approaches to address energy efficiency, reliability, and security issues at different system layers, i.e., hardware (HW) and software (SW). Afterward, we discuss how to further improve the performance (latency) and the energy efficiency of Edge AI systems through HW/SW-level optimizations, such as pruning, quantization, and approximation. To address reliability threats (like permanent and transient faults), we highlight cost-effective mitigation techniques, like fault-aware training and mapping. Moreover, we briefly discuss effective detection and protection techniques to address security threats (like model and data corruption). Towards the end, we discuss how these techniques can be combined in an integrated cross-layer framework for realizing robust and energy-efficient Edge AI systems. ©2021 IEEE",Accuracy | Artificial intelligence | Deep neural networks | Edge AI | Edge computing | Energy efficiency | Latency | Machine learning | Reliability | Robustness | Security | Spiking neural networks | TinyML,"IEEE/ACM International Conference on Computer-Aided Design, Digest of Technical Papers, ICCAD",2021-01-01,Conference Paper,"Shafique, Muhammad;Marchisio, Alberto;Putra, Rachmad Vidya Wicaksana;Hanif, Muhammad Abdullah",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85105020930,10.1109/ICCCI50826.2021.9402272,Class Imbalanced Data: Open Issues and Future Research Directions,"Since last two decades, imbalanced data is becoming a hot topic to do research or to determine meaningful results. One of the problems of machine learning and data mining areas is class imbalance. Data sets with imbalances have hindered the efficiency of algorithms for data mining and machine learning (in terms of overall accuracy, decision making). In the big data era, the expansion of data mining and machine learning has raised new challenges with the nature of data. In class imbalanced data, majority class lead to problem, i.e., having an imbalance between minority and majority class samples created several problems for researchers. In result, researchers are unable to learn much from systems or they are unable to find or determine prediction or take decision for respective applications like fraud detection, rare diseases identification/ prediction, approval of credit card, software defect prediction, etc. A survey for class imbalance problem is proposed in this paper with discussing several applications (where this problem getting attention). For solving this famous problem or balance this imbalanced data, three methods like Data-level, algorithm-level and hybrid methods are being considered/ used. Among these methods, a hybrid method is receiving much popularity. This paper also discusses several open issues and challenges (which are required to be developed in near future for efficient/ imbalanced learning). Also, in last several (essential) future research directions have been also discussed in this work, which makes this work as important one for research community. © 2021 IEEE.",Big Data | Binary Class Imbalanced Data | Imbalanced Clustering | Imbalanced data | Machine learning | Multi-Class Imbalanced Data,"2021 International Conference on Computer Communication and Informatics, ICCCI 2021",2021-01-27,Conference Paper,"Rekha, G.;Tyagi, Amit Kumar;Sreenath, N.;Mishra, Shashvi",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84992650855,10.1109/ICCPEIC.2016.7557229,Data reduction for bug triage using effective prediction of reduction order techniques,"A large open source project consists of a wide range of bug reports. In open source project, bug reports are available and these reports can be modified by anyone. Bugs are software defects whose prediction is highly difficult. To detect the bugs, machine learning classifier has been proposed. It segregates the bug reports and developers and it learns the type of report suitable for each developer. Bug triage is the process of assigning the bug for the appropriate developer. The techniques include preprocessing, machine learning classifier, instance selection and feature selection. The aim of this paper is to attain a data set reduction in bug triage by including the representative values along with the statistical values of the bug data set. Our work considers the dataset from the open source project Eclipse. We focus on reducing the data scale and thereby improving the accuracy. This can be achieved by building a representative model for prediction of reduction orders by including the summary, metadata. Our proposed work attains an accuracy result of 96.5% that is better when compared with existing work. © 2016 IEEE.",feature selection | instance selection | machine learning classifier | Open source projects | representative values | selection techniques,"2016 International Conference on Computation of Power, Energy, Information and Communication, ICCPEIC 2016",2016-08-31,Conference Paper,"Govindasamy, V.;Akila, V.;Anjanadevi, G.;Deepika, H.;Sivasankari, G.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85104600959,10.1109/ICCQ51190.2021.9392977,Learning to Find Bugs and Code Quality Problems-What Worked and What not?,"The recent growth of open source repositories and deep learning models brought big promises for the next generation of programming tools that can automate or significantly improve the software development process. Yet, such tools are still rare and the machine learning components in them are not always apparent to their users. The current most useful techniques in machine learning for code are also not coming from the organizations such as Microsoft, Google, DeepMind, Facebook, OpenAI or NVIDIA that invested the most in deep neural techniques such as huge neural networks. This probably means that either many of these coding problems are significantly different from other hot topics in deep learning such as image processing or that it is much more difficult to collect datasets that would result in similarly successful tools. In this work, we study the results in the literature on the topic and discuss ways to address these shortcomings. © 2021 IEEE.",,"2021 International Conference on Code Quality, ICCQ 2021",2021-03-27,Conference Paper,"Raychev, Veselin",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85070247711,10.1109/ICCW.2019.8757169,Fast and adaptive failure recovery using machine learning in software defined networks,"Fast failure recovery is a critically important problem in networks. To address this problem in software-defined networks (SDN), backup paths can be chosen in a proactive and adaptive manner in accordance with the traffic dynamics. Existing proactive approaches make use of only the network topology knowledge or a combined knowledge of the topology and static load to compute the backup paths. This, however, does not reflect the traffic dynamics in the network, making the links congested over the period of time when traffic varies or when a failure occurs. In this paper, we develop a traffic engineering (TE)-based machine learning approach that can learn the traffic dynamics, estimate the goodness of a path and update the backup path adaptively. A backup path is proactively configured in the SDN switches, thus enabling a fast failure recovery. We train, test and validate the learning model using different machine learning algorithms such as gradient boosting, linear regression, decision tree, neural network, support vector machine, and random forest. We implement the proposed approach and carry out experiments on the Mininet emulation platform. The results show that our proposed approach significantly reduces the failure recovery time by up to 50% and improves the network bandwidth utilization by up to 24% compared to a baseline approach. © 2019 IEEE.",Failure recovery | Fault tolerance | Machine learning | Proactive rerouting | Software defined networking,"2019 IEEE International Conference on Communications Workshops, ICC Workshops 2019 - Proceedings",2019-05-01,Conference Paper,"Truong-Huu, Tram;Prathap, Prarthana;Mohan, Purnima Murali;Gurusamy, Mohan",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84980416420,10.1109/ICDE.2016.7498352,Java2SDG: Stateful big data processing for the masses,"Big data processing is no longer restricted to specially-trained engineers. Instead, domain experts, data scientists and data users all want to benefit from applying data mining and machine learning algorithms at scale. A considerable obstacle towards this democratisation of big data are programming models: current scalable big data processing platforms such as Spark, Naiad and Flink require users to learn custom functional or declarative programming models, which differ fundamentally from popular languages such as Java, Matlab, Python or C++. An open challenge is how to provide a big data programming model for users that are not familiar with functional programming, while maintaining performance, scalability and fault tolerance. We describe JAVA2SDG, a compiler that translates annotated Java programs to stateful dataflow graphs (SDGs) that can execute on a compute cluster in a data-parallel and fault-tolerant fashion. Compared to existing distributed dataflow models, a distinguishing feature of SDGs is that their computational tasks can access distributed mutable state, thus allowing SDGs to capture the semantics of stateful Java programs. As part of the demonstration, we provide examples of machine learning programs in Java, including collaborative filtering and logistic regression, and we explain how they are translated to SDGs and executed on a large set of machines. © 2016 IEEE.",,"2016 IEEE 32nd International Conference on Data Engineering, ICDE 2016",2016-06-22,Conference Paper,"Fernandez, Raul Castro;Garefalakis, Panagiotis;Pietzuch, Peter",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85116573570,10.1109/ICDI3C53598.2021.00045,ESPY Carting Systems:The Real -Time Tracking Mechanism Architecture,"The massive rise in delivery service has necessitated the development of optimized and efficient smart transport operations. To manage engineering applications and anticipate product faults or service failures, a close integration of Internet of Things technology (IoT) and Big Data analytics approaches have become important. Existent process monitoring mechanisms, on the other hand, are not designed for package vendors. The introduction of smart network monitoring is especially useful for shippers in the field of good transportation to ensure trustworthy parcel deliveries.This eSPY Service is an advanced system for surveillance and managing deliveries that have been shipped. Back-end infrastructure for intelligent data transfer, governance, processing, and insights is made available and it's centered on the eSPY Servicing network of IoT-enabled software, which allows courier users to quickly collect parcel identity for each delivery phase. We look at a real-world dataset that includes package delivery case data and a machine-learning algorithm is used to quickly interpret the features representing incident data to forecast possible package splits and to test if the developed system provides information which could be used to determine shipment status (i.e., Tampered or not). The findings further suggested that the systems derived from event-related features own a strong degree of prediction accuracy. This is the first way to mitigate remote monitoring in smart successful supply chains using real-world model to estimate shipment splits with advanced methods employed © 2021 IEEE.",Algorithms | BigData | IoT | Machine Learning | Raspberry | Spark,"Proceedings - 2021 International Conference on Design Innovations for 3Cs Compute Communicate Control, ICDI3C 2021",2021-06-01,Conference Paper,"Sugathan, Divya;Revanth, R. M.;Kumar, V. Praveen;Desai, Sachin;Keshava, L.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85123420167,10.1109/ICECCT52121.2021.9616861,Software Defects Detection and Prevention Through Virtualization,"With more and more digital machines consolidated on fewer and fewer cloud servers, the software running in those servers needs safety. Virtualization poses unique software defects which must be detected and prevented specific software requirements designed for virtualization environments. In this research thesis, software virtualization technology became used to transparently record the allocation and release of memory resources implemented to a database connection on a virtual machine in the cloud, and these records provided the information to detect memory leaks hiding in the code. Memory leaks account for lack of a self-adaptive handy cloud computing structure due to consistent use ordinary static and dynamic memory leak analysis tools. Most of the available tools for defects detection do not provide for consistency of memory leak prevention. The main intention of the research developed a self-adaptive virtualization model for software defects detection and prevention of software Memory Leaks Using Deep Learning and Machine Learning Methods. Data sampling used was code-based sampling based on Low-Density Parity Checks which avoided overestimating false positives for the variables used. There were a total population of 35 variables for the study, out of these; seven variables were selected as a sample. The sample objects, classes and class loaders access for the 4-database test connection used a minimum 0.1% sampling rate which had 4 database connection references out of every 7 variables used. The approach used gave an accuracy of 98% security rate when compared with other existing methods like Long Short-Term Memory which achieved 82.3%, Self-organizing Maps was 85.5% and Boltzmann Approach was 93.5%. © 2021 IEEE.",Long Short-Term Memory | Low Density Parity Checks | Memory leaks | Software defects | virtualization,"2021 4th International Conference on Electrical, Computer and Communication Technologies, ICECCT 2021",2021-01-01,Conference Paper,"Turikumwe, Jean Paul;Wilson, Cheruiyot;Kibe, Anne",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85078954844,10.1109/ICECTA48151.2019.8959566,A Systematic Review of Feature Selection Techniques in Software Quality Prediction,"Background: Feature selection techniques are important factors for improving machine learning models because they increase prediction accuracy and decrease the time to create a model. Recently, feature selection techniques have been employed on software quality prediction problems with different results and no clear indication of which techniques are frequently used.Objective: This study aims to conduct a systematic review of the application of feature selection techniques in software quality prediction and answers eight research questions.Method: The review evaluates 15 papers in 9 journals and 6 conference proceedings from 2007 to 2017 using the standard systematic literature review method.Results: The results obtained from this study reveal that the filter feature selection method was the most commonly used in the studies (60%) and RELIEF was the most employed among this method, and a limited number of studies employed an ensemble method. Several studies used public datasets available in the PROMISE software project repository (60%). Most studies focused on software defect prediction (classification problem) using area under curve (AUC) as a primary evaluation measure, whereas only two studies focused on software maintainability prediction (regression problem) using mean magnitude of relative error (MMRE) as a primary evaluation measure. All selected studies performed k-fold cross-validation to evaluate model accuracy. Individual prediction models were mostly employed and ensemble models appeared only in three studies. Naive Bayes was the most investigated among individual models, whereas Random forest was the most investigated among ensemble models.Conclusion: Feature selection techniques used by selected primary studies have a positive impact on the performance of the prediction models. Further, both ensemble feature selection method and ensemble models have the ability for increasing prediction accuracy over single methods or individual models and have reported improvement in the prediction accuracy; however, the application of these techniques in software quality prediction is still limited. © 2019 IEEE.",feature selection | prediction | software defect | software maintainability | Systematic literature review,"2019 International Conference on Electrical and Computing Technologies and Applications, ICECTA 2019",2019-11-01,Conference Paper,"Alsolai, Hadeel;Roper, Marc",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85089242743,10.1109/ICEECCOT46775.2019.9114712,Evolutionary Computing Assisted Heterogenous Ensemble Model for Web-of-Service Software Reusability Prediction,"Software being one of the inevitable needs of industries today requires ensuring reliability of computation. On the other hand, to enhance cost of development and productivity software developers use Component based Software Development (CSD) and Free Open Source Software (FOSS) element. Existing functions or software components are used frequently to perform the task. On contrary, the uncontrolled, excessive or improper use of such components lead software faults, aging or smells, which makes overall software vulnerable. To alleviate it, software reusability estimation can be a potential solution. The classical machine learning models can be confined to perform optimal reusability prediction with highly complicate and large size software design such as Web-of-Service (WoS) software. In this paper evolutionary computing assisted ensemble model is percolated that significantly mitigates the key issues of regional minima and convergence. Being OOP software reusability prediction model, six software metrics have been extracted from WoS software, which have been processed with different base learning models. Majority voting scheme in conjunction with Genetic Algorithm based ensemble decision enables the proposed model to exhibit better performance than any other base classifiers. © 2019 IEEE.",Diversified Ensemble | Evolutionary Classifier of Ensemble | Reusability Software Prediction | Web-Service,"4th International Conference on Electrical, Electronics, Communication, Computer Technologies and Optimization Techniques, ICEECCOT 2019",2019-12-01,Conference Paper,"Parande, Prakash V.;Banga, M. K.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85083106752,10.1109/ICENCO48310.2019.9027412,A modified finger vein identification approach based on niching genetic algorithm,"Finger vein engineering is the most modern biometric software using vein recognition patterns. Since these patterns are veiled under the skin surface, they offer great privateness, and therefore provide tremendous protection and are therefore extremely difficult to shape. The detection of the finger vein has received more consideration because previous methods have Highly vulnerable such as imbalanced finger vein dataset, and the extraction of salient feature within the low-quality images. Such defect has led the optimization algorithm not to be converged or its output to be reduced due to the limitations of the static means of finger vein identification, the need for intelligent approaches to finger vein identification is thus imperative. One of such intelligent approaches is machine learning which can be seen as the acquisition of structure description from examples. The kind of descriptions found can be used for identification, explanation, and understanding. The main contribution of the work presented in this thesis is to investigate the effect of genetic algorithm in selecting optimal finger vein features vector by adding niching concept in form of Context Based Clearing (CBC) procedure to increase the heterogeneity of items within the feature vector with the aim of removing the correlation between items of features vector. The enhanced feature selection approach produces an optimal vector that able to deal efficiently with the massive intra-class variations and the diminutive inter-class similarity. Besides, it yields the concept of feature set reduction to remove redundancy without degradation of the accuracy. The performance analysis of suggested model is carried through several experiments and the results show an improvement on an average by 6% in terms of accuracy compared with some Up-to-date distinguishing finger vein mechanisms available in the literature. © 2019 IEEE.",Context Based Clearing (NGA) | Enhanced feature selection | Finger Vein | Gabor Transformation,ICENCO 2019 - 2019 15th International Computer Engineering Conference: Utilizing Machine Intelligence for a Better World,2019-12-01,Conference Paper,"Alhadethy, Ahmed H.;Darwish, Saad",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84985902350,10.1109/ICICCS.2016.7542332,Analyzing and assessing the security-related defects,"The use of the Internet has become an integral part of everyone's life. Due to this, the introduction of virus and other malicious crackers is increasing everyday. This in turn leads to the introduction of defects which adversely affect the security. Thus, protecting vital information in this cyber world is not an easy task. We need to deal with security related defects to ensure failure free and smooth functioning of the software. Thus, in this paper, we intend to study and analyze various aspects of security-related defects by analyzing the defect reports available in various open-source software repositories. Besides this, prediction models can also be constructed which can be used by researchers and practitioners to predict various aspects of security-related defects. Such prediction models are especially beneficial for large-scale systems, where testing experts need to focus their attention and resources to the problem areas of the system under development. Thus, application of software prediction models in the early phases of the software life cycle contributes to efficient defect removal and results in delivering more reliable and better quality software products. Empirical studies lack the use of proper research methodology and thus result in reporting inconsistent results. This study will review the sequence of steps followed in the research process for carrying empirical and replicated studies. The steps include a) literature survey and definition of variables b) data collection c) report findings using statistical and machine learning techniques d) analyzing performance measures for evaluating the performance of the predicted models and e) interpretation of the obtained results for developing a software prediction model. These steps are explained with the help of experimental public domain data set. In addition, the paper provides an overview of repositories for mining software engineering data, tools for analyzing this data and various categories of machine learning methods. It also discusses existing research avenues and provides future research directions in this area. © 2016 IEEE.",Empirical Validation | Machine Learning | Security Vulnerabilities | Security-Related Defects | Statistical Methods,"2016 1st International Conference on Innovation and Challenges in Cyber Security, ICICCS 2016",2016-08-11,Conference Paper,"Bansal, Ankita;Malhotra, Ruchika;Raje, Kimaya",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84879874384,10.1109/ICICES.2013.6508369,Software defect prediction using software metrics - A survey,"Traditionally software metrics have been used to define the complexity of the program, to estimate programming time. Extensive research has also been carried out to predict the number of defects in a module using software metrics. If the metric values are to be used in mathematical equations designed to represent a model of the software process, metrics associated with a ratio scale may be preferred, since ratio scale data allow most mathematical operations to meaningfully apply. Work on the mechanics of implementing metrics programs. The goal of this research is to help developers identify defects based on existing software metrics using data mining techniques and thereby improve software quality which ultimately leads to reducing the software development cost in the development and maintenance phase. This research focuses in identifying defective modules and hence the scope of software that needs to be examined for defects can be prioritized. This allows the developer to run test cases in the predicted modules using test cases. The proposed methodology helps in identifying modules that require immediate attention and hence the reliability of the software can be improved faster as higher priority defects can be handled first. Our goal in this research focuses to improve the classification accuracy of the Data mining algorithm. To initiate this process we initially propose to evaluate the existing classification algorithms and based on its weakness we propose a novel Neural network algorithm with a degree of fuzziness in the hidden layer to improve the classification accuracy. © 2013 IEEE.",machine learning | scheme evaluation | Software defect prediction | software defectproneness prediction,"2013 International Conference on Information Communication and Embedded Systems, ICICES 2013",2013-07-12,Conference Paper,"Punitha, K.;Chitra, S.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85101263959,10.1109/ICIIS51140.2020.9342715,Automatic Quality Enhancement of Medical Diagnostic Scans with Deep Neural Image Super-Resolution Models,"In modern healthcare, diagnostic imaging is an essential component for diagnosing ailments and delivering quality healthcare. Given the variety in medical scanning techniques, a recurring issue across different modalities is that the scan quality is often affected by artifacts introduced by hardware and software faults in the imaging equipment. Significant challenges in the 3D Imaging Techniques include low quality/low-resolution scan images or the addition of unwanted artifacts due to patient movement. Researchers have put forth solutions ranging from machine learning algorithms like Gradient Descent to more complex Deep CNN models for rectifying these faults. In this paper, we aim to benchmark deep learning models for improving the quality of diagnostic images, through Super-resolution, for enabling faster and easier detection of anomalies that may be missed otherwise. Super-resolution CNN and Deep CNN architectures were employed for up-sampling medical scans for enhancing their quality. The CNN models were trained to learn motion artifact characteristics that are a result of patient movement and negate its effects in the super-resolved output. We present comparative results of six super-resolution models on a standard dataset and metrics. During the experimental evaluation, it was observed that the ResNet SRCNN model outperformed all other models used for comparison by a large margin, with an improvement of 4.87 to 8.68% over the other state-of-the-art models. © 2020 IEEE.",Deep Convolutional Neural Networks | Medical Imaging | Quality Enhancement,"2020 IEEE 15th International Conference on Industrial and Information Systems, ICIIS 2020 - Proceedings",2020-11-26,Conference Paper,"Karthik, K.;Sowmya Kamath, S.;Kamath, Surendra U.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85097651183,10.1109/ICIMU49871.2020.9243459,Design and Development of Machine Learning Technique for Software Project Risk Assessment - A Review,"Accurate assessment of software project risk is amongst the key activities in a software project. It directly impacts the time and cost of software projects. This paper presents a literature review of designing developing machine learning techniques for software project risk assessment. The results of the review have concluded prominent trends of machine learning approaches, size metrics, and study findings in the growth and advancement of machine learning in project management. Besides that, this research provides a deeper insight and an important framework for future work in the software project risk assessment. Furthermore, we demonstrated that the assessment of project risk using machine-learning is more efficient in reducing a project's fault. It also increases the probability for the software project's prediction and response, provides a further way to reduce the probability chances of failure effectively and to increase the software development performance ratio. © 2020 IEEE.",Assessment | formatting | Machine Learning Technique | Risk | Software Project Management,"2020 8th International Conference on Information Technology and Multimedia, ICIMU 2020",2020-08-24,Conference Paper,"Mahdi, Mohamed Najah;Mohamed, Mohamed Zabil;Yusof, Azlan;Cheng, Lim Kok;Mohd Azmi, Muhammad Sufyian;Ahmad, Abdul Rahim",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84873601067,10.1109/ICMLA.2012.226,The state of machine learning methodology in software fault prediction,The aim of this paper is to investigate the quality of methodology in software fault prediction studies using machine learning. Over two hundred studies of fault prediction have been published in the last 10 years. There is evidence to suggest that the quality of methodology used in some of these studies does not allow us to have confidence in the predictions reported by them. We evaluate the machine learning methodology used in 21 fault prediction studies. All of these studies use NASA data sets. We score each study from 1 to 10 in terms of the quality of their machine learning methodology (e.g. whether or not studies report randomising their cross validation folds). Only 10 out of the 21 studies scored 5 or more out of 10. Furthermore 1 study scored only 1 out of 10. When we plot these scores over time there is no evidence that the quality of machine learning methodology is better in recent studies. Our results suggest that there remains much to be done by both researchers and reviewers to improve the quality of machine learning methodology used in software fault prediction. We conclude that the results reported in some studies need to be treated with caution. © 2012 IEEE.,experimental techniques | fault prediction | machine learning | methodology | software engineering,"Proceedings - 2012 11th International Conference on Machine Learning and Applications, ICMLA 2012",2012-12-01,Conference Paper,"Hall, Tracy;Bowes, David",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84899418230,10.1109/ICMLA.2013.114,Bug reports prioritization: Which features and classifier to use?,"Large open source bug tracking systems receives large number of bug reports daily. Managing these huge numbers of incoming bug reports is a challenging task. Dealing with these reports manually consumes time and resources which leads to delaying the resolution of important bugs which are crucial and need to be identified and resolved earlier. Bug triaging is an important process in software maintenance. Some bugs are important and need to be fixed right away, whereas others are minor and their fixes could be postponed until resources are available. Most automatic bug assignment approaches do not take the priority of bug reports in their consideration. Assigning bug reports based on their priority may play an important role in enhancing the bug triaging process. In this paper, we present an approach to predict the priority of a reported bug using different machine learning algorithms namely Naive Bayes, Decision Trees, and Random Forest. We also investigate the effect of using two feature sets on the classification accuracy. We conduct experimental evaluation using open-source projects namely Eclipse and Fire fox. The experimental evaluation shows that the proposed approach is feasible in predicting the priority of bug reports. It also shows that feature-set-2 outperformsfeature-set-1. Moreover, both Random Forests and Decision Trees outperform Naive Bayes. © 2013 IEEE.",Bug priority | bug triaging | predictive model | text classification,"Proceedings - 2013 12th International Conference on Machine Learning and Applications, ICMLA 2013",2013-01-01,Conference Paper,"Alenezi, Mamdouh;Banitaan, Shadi",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84969651482,10.1109/ICMLA.2015.147,Superposed naive bayes for accurate and interpretable prediction,"Background: Data mining and machine learning techniques have been widely applied in software engineering research. However, past research has mainly focused on only prediction accuracy. Aim: The interpretability of prediction results should be accorded greater emphasis in software engineering research. A prediction model that has high accuracy and explanatory power is required. Method: We propose a new algorithm of naïve Bayes ensemble, called superposed naïve Bayes (SNB), which firstly builds an ensemble model with high prediction accuracy and then transforms it into an interpretable naïve Bayes model. Results: We conducted an experiment with the NASA MDP datasets, in which the performance and interpretability of the proposed method were compared with those of other classification techniques. The results of the experiment indicate that the proposed method can produce balanced outputs that satisfy both performance and interpretability criteria. Conclusion: We confirmed the effectiveness of the proposed method in an experiment using software defect data. The model can be extensively applied to other application areas, where both performance and interpretability are required. © 2015 IEEE.",Bagging | Boosting | Ensemble method | Interpretability | Model transformatiomn | Naive Bayes classifier | Performance,"Proceedings - 2015 IEEE 14th International Conference on Machine Learning and Applications, ICMLA 2015",2016-03-02,Conference Paper,"Mori, Toshiki",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85048467519,10.1109/ICMLA.2017.00008,Rank learning by ordinal gerrymandering,"Many applications, from ordering search engine results to medical triage, rely on learning to accurately rank a set of objects by combining a given collection of ranking or preference functions. We propose a technique for rank-learning using a boosting approach which merges accurate regions of poor-quality metrics into a single accurate metric. We show an improvement in accuracy for general similarity-ranking tasks across a variety of benchmark datasets and apply this technique to the prediction of software bug severity and resolution time from error report text, showing a significant improvement in bug triage accuracy over the state of the art. © 2017 IEEE.",boosting | computer applications | machine learning | ranking,"Proceedings - 16th IEEE International Conference on Machine Learning and Applications, ICMLA 2017",2017-01-01,Conference Paper,"Fenu, Stefano;Rozell, Chris",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85062245280,10.1109/ICMLA.2018.00234,Bug Report Classification Using LSTM Architecture for More Accurate Software Defect Locating,"Recently many information retrieval (IR)-based approaches have been proposed to help locate software defects automatically by using information from bug report contents. However, some bug reports that do not semantically related to the relevant code are not helpful to IR-based systems. Running an IR-based system on these reports may produce false positives. In this paper, we propose a classification model for classifying a bug report as either helpful or unhelpful using a LSTM-network. By filtering our unhelpful reports before running an IR-based bug locating system, our approach helps reduce false positives and improve the ranking performance. We test our model over 9,000 bug reports from three software projects. The evaluation result shows that our model helps improve a state-of-the-art IR-based system's ranking performance under a trade-off between the precision and the recall. Our comparison experiments show that the LSTM-network achieves the best trade-off between precision and recall than other classification models including CNN, multilayer perceptron, and a simple baseline approach that classifies a bug report based its length. In the situation that precision is more important than recall, our classification model helps for bug locating. © 2018 IEEE.",Bug locating | Bug report | Convolutional neural network | Long short-term memory,"Proceedings - 17th IEEE International Conference on Machine Learning and Applications, ICMLA 2018",2018-07-02,Conference Paper,"Ye, Xin;Fang, Fan;Wu, John;Bunescu, Razvan;Liu, Chang",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85080877813,10.1109/ICMLA.2019.00154,T-REC: Towards accurate bug triage for technical groups,"With ever-larger software development systems involving more people with different skills, it is necessary to think about the process of bug assignment to groups of developers and not just to a developer alone. This work aims to leverage Bug Triage Process by suggesting a list of specialized groups of developers, or Technical Groups (TG's), to be attributed to a new bug report, based on other bugs that are similar and have been resolved by these TG's in the past. In the dataset used in our experiments, the mean time to correctly assign bug reports to their proper TG is 14 days, and just by then, the bug fixing process starts. This is a critical problem for software development and management since issues tend to accumulate a high-resolution time, which compromises developer performance and deliveries. In order to enhance the Bug Triage Process, we propose T-REC, an auxiliary SW Project Management system that accurately and efficiently analyzes similar issues to provide personalized TG recommendation. T-REC is a method that ensemble Machine Learning (ML) and Information Retrieval (IR) algorithms to recommend a list of TG's to handle an issue. Our experiments show that T-REC recommendation reaches an overall Acc@1 of 50.9%, Acc@2 of 63.2%, Acc@5 of 76.1%, Acc@10 of 83.6%, and Acc@20 of 89.7%. To the best of our knowledge, our work is the first to associate multiple machine learning strategies (classifiers, attributes, and training history) on the prediction of specialized groups of developers. We validate our approach on a real-world dataset from a large company that comprises 9.5M mobile-related bug reports from January 2001 to January 2019. © 2019 IEEE.",Bug | Group | Recommendation | Triage,"Proceedings - 18th IEEE International Conference on Machine Learning and Applications, ICMLA 2019",2019-12-01,Conference Paper,"De Lara Pahins, Cicero Augusto;D'Morison, Fabricio;Rocha, Thiago M.;Almeida, Larissa M.;Batista, Arthur F.;Souza, Diego F.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85080913349,10.1109/ICMLA.2019.00161,"A multi-label, dual-output deep neural network for automated bug triaging","Bug tracking enables the monitoring and resolution of issues and bugs within organizations. Bug triaging, or assigning bugs to the owner(s) who will resolve them, is a critical component of this process because there are many incorrect assignments that waste developer time and reduce bug resolution throughput. In this work, we explore the use of a novel two-output deep neural network architecture (Dual DNN) for triaging a bug to both an individual team and developer, simultaneously. Dual DNN leverages this simultaneous prediction by exploiting its own guess of the team classes to aid in developer assignment. A multi-label classification approach is used for each of the two outputs to learn from all interim owners, not just the last one who closed the bug. We make use of a heuristic combination of the interim owners (owner-importance-weighted labeling) which is converted into a probability mass function (pmf). We employ a two-stage learning scheme, whereby the team portion of the model is trained first and then held static to train the team-developer and bug-developer relationships. The scheme employed to encode the team-developer relationships is based on an organizational chart (org chart), which renders the model robust to organizational changes as it can adapt to role changes within an organization. There is an observed average lift (with respect to both team and developer assignment) of 13%-points in 11-fold incremental-learning cross-validation (IL-CV) accuracy for Dual DNN utilizing owner-weighted labels compared with the traditional multi-class classification approach. Furthermore, Dual DNN with owner-weighted labels achieves average 11-fold IL-CV accuracies of 76% (team assignment) and 55% (developer assignment), outperforming reference models by 14%- and 25%-points, respectively, on a proprietary dataset with 236,865 entries. © 2019 IEEE.",Bug triaging | Computer science | Deep neural network | Machine learning | Multi label | Software engineering | Statistics,"Proceedings - 18th IEEE International Conference on Machine Learning and Applications, ICMLA 2019",2019-12-01,Conference Paper,"Choquette-Choo, Christopher A.;Sheldon, David;Proppe, Jonny;Alphonso-Gibbs, John;Gupta, Harsha",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85102511116,10.1109/ICMLA51294.2020.00183,Fiji-FIN: A Fault Injection Framework on Quantized Neural Network Inference Accelerator,"In recent years, the big data booming has boosted the development of highly accurate prediction models driven from machine learning (ML) and deep learning (DL) algorithms. These models can be orchestrated on the customized hardware in the safety-critical missions to accelerate the inference process in ML/DL -powered IoT. However, the radiation-induced transient faults and black/white -box attacks can potentially impact the individual parameters in ML/DL models which may result in generating noisy data/labels or compromising the pre-trained model. In this paper, we propose Fiji-FIN 1, a suitable framework for evaluating the resiliency of IoT devices during the ML/DL model execution with respect to the major security challenges such as bit perturbation attacks and soft errors. Fiji-FIN is capable of injecting both single bit/event flip/upset and multi-bit flip/upset faults on the architectural ML/DL accelerator embedded in ML/DL -powered IoT. Fiji-FIN is significantly more accurate compared to the existing software-level fault injections paradigms on ML/DL -driven IoT devices. © 2020 IEEE.",Binarized Neural Network Accelerator | Black/white -box Attack | Fault Injection | Machine Learning | ML/DL -powered IoT | Soft Error,"Proceedings - 19th IEEE International Conference on Machine Learning and Applications, ICMLA 2020",2020-12-01,Conference Paper,"Khoshavi, Navid;Broyles, Connor;Bi, Yu;Roohi, Arman",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85039907708,10.1109/ICMLANT50963.2020.9355968,Bug Severity Prediction System Using XGBoost Framework,"The field of Bug Reporting and Triaging has of late been a hot area of study among researchers trying to improve system management techniques. There is an increasing importance for developers to consider and address the various issues faced by users in order to not only ensure the delivery of quality service but also to understand the performance of the system under real-life scenarios. Hence, in this project, there is an attempt to develop a system that can improve the process of handling bug reports submitted by users of a software. This will be done through Bug Severity Prediction using the eXtreme Gradient Boosting (XGBoost) algorithm and the inclusion of a class balancing function to offset the bias due to the presence of majority and minority classes. The project would also include a study on the work that has already been done along with a proposal of the system architecture, methodologies used and the various hardware and software requirements. The main aim of the project is to shed light on the advantages of developing a Bug Severity Prediction system that can help reduce the dependence on users for providing accurate information. With the help of models built based on the history of bug reports received till date, the system should be able to take up some of the responsibilities of the user reporting the bug. © 2020 IEEE.",Bug Reports; Dataset Balancing; eXtreme Gradient Boosting (XGBoost); Support Vector Machine (SVM); Synthetic Minority Over-sampling Technique (SMOTE); Text Mining,"Proceedings of the 2020 IEEE International Conference on Machine Learning and Applied Network Technologies, ICMLANT 2020",2020,,"Mondreti V., Satish C.J.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85113726700,10.1109/ICMLC51923.2020.9469568,A three-stage method for classification of binary imbalanced big data,"In the real world, there are many imbalanced data classification problems, such as extreme weather prediction, software defect prediction, machinery fault diagnosis, spam filtering, etc. It has important theoretical and practical value to study the problem of imbalanced data classification. In the framework of binary imbalanced data classification, a three-stage method for classification of binary imbalanced big data was proposed in this paper. Specifically, in the first stage, the negative class big data was clustered into K clusters by K-means algorithm on Hadoop platform. In the second stage, we use instance selection method to select important samples from each cluster in parallel, and obtain K negative class subsets. In the third stage, we first construct K balanced training sets which consist of negative class subset and positive class subset, and then train K classifiers, and finally we integrate these classifiers to classify the unseen samples. Some experiments are conducted to compare the proposed method with two state-of-the-art methods on G-means. The experimental results demonstrate that the proposed method is more effective and efficient than the compared approaches. © 2020 IEEE.",Big data | Clustering | Ensemble | Imbalanced big data | Instance selection,Proceedings - International Conference on Machine Learning and Cybernetics,2020-12-02,Conference Paper,"Zhai, Jun Hai;Zhang, Su Fang;Wang, Mo Han;Li, Yan",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85008178951,10.1109/ICOIN50884.2021.9333902,Learning Graph Representation of Bug Reports to Triage Bugs using Graph Convolution Network,"Bug triage is a software engineering problem, which is being solved by classification methods. The social network analysis, mining repositories, statistical modeling, topic modeling, machine learning, and deep learning techniques have been used to triage the bugs. These existing methods showed promising results but still far from perfection, which requires improvement. This paper proposes a graph representation method for the bug reports dataset, which solves the bug triage problem with the node classification problem. The heterogeneous graph is built using the word to word and word to document co-occurrences for the whole bug dataset. The graph convolution network (GCN) is trained on the generated graph to learn the bug reports' graph representation. The proposed method is validated on the open-source project's bug data. Top-K accuracy is used as an evaluation metric to evaluate the performance of the model. The reported results show promising results compared to previous studies. © 2021 IEEE.",bug fixer recommendation; bug reports; Bug triage; graph representation,International Conference on Information Networking,2021,,"Zaidi S.F.A., Lee C.-G.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85006968329,10.1109/ICOTEN52080.2021.9493515,Automation of Bug-Report Allocation to Developer using a Deep Learning Algorithm,"Software bug maintenance is an important aspect of all software projects. The assignment of bug reports is essential in order to resolve bugs efficiently. In the case of open-source software developments and large projects, where many developers are engaged on different aspects of software development, it can be difficult to assign bug removal tasks to an appropriate developer. An increase in reported bugs, coupled with an increase in the number of software developers, will complicate the bug triage process. In these situations, bug triaging might be slow and may increase the Bug Tossing Length (BTL). An automated system to triage bug reports could potentially reduce BTL, as manual assignment of bug reports is tiresome, costly, and very time-consuming. The assignment of bug reporting to an irrelevant developer who does not possess sufficient experience to deal with the bug will adversely impact BTL and customer satisfaction. Text-based classification methods have the potential to make a strong contribution to automating the bug triaging process. In this research, different types of Information Retrieval and Machine Learning algorithms are used to determine the appropriate developer/s to rectify the reported bugs. This study used deep learning algorithms, such as the Bidirectional Long Short-Term Memory Network, to automate the bug triaging process. Bug reports contain textual data related to the bug information. In this research, the pretrained GloVe model is employed for word-to-vector representation of bug reports' textual information. In this framework, developers' activities are monitored based on their working history. To test the proposed approach, three large datasets, Net-Beans, Eclipse, and Mozilla, are used. It was observed that the proposed technique produced better results in terms of accuracy, recall, precision and f-measure compared to traditional Machine Learning algorithms for bug report recommendation. © 2021 IEEE.",BiLSTM; Bug report assignment; Embedding; Glove; Machine Learning; Supervised Machine Learning,"2021 International Congress of Advanced Technology and Engineering, ICOTEN 2021",2021,,Mian T.S.,Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84961370853,10.1109/ICPC.2015.15,Active Semi-supervised Defect Categorization,"Defects are inseparable part of software development and evolution. To better comprehend problems affecting a software system, developers often store historical defects and these defects can be categorized into families. IBM proposes Orthogonal Defect Categorization (ODC) which include various classifications of defects based on a number of orthogonal dimensions (e.g., Symptoms and semantics of defects, root causes of defects, etc.). To help developers categorize defects, several approaches that employ machine learning have been proposed in the literature. Unfortunately, these approaches often require developers to manually label a large number of defect examples. In practice, manually labelling a large number of examples is both time-consuming and labor-intensive. Thus, reducing the onerous burden of manual labelling while still being able to achieve good performance is crucial towards the adoption of such approaches. To deal with this challenge, in this work, we propose an active semi-supervised defect prediction approach. It is performed by actively selecting a small subset of diverse and informative defect examples to label (i.e., Active learning), and by making use of both labeled and unlabeled defect examples in the prediction model learning process (i.e., Semi-supervised learning). Using this principle, our approach is able to learn a good model while minimizing the manual labeling effort. To evaluate the effectiveness of our approach, we make use of a benchmark dataset that contains 500 defects from three software systems that have been manually labelled into several families based on ODC. We investigate our approach's ability in achieving good classification performance, measured in terms of weighted precision, recall, F-measure, and AUC, when only a small number of manually labelled defect examples are available. Our experiment results show that our active semi-supervised defect categorization approach is able to achieve a weighted precision, recall, F-measure, and AUC of 0.651, 0.669, 0.623, and 0.710, respectively, when only 50 defects are manually labelled. Furthermore, it outperforms an existing active multi-class classification algorithm, proposed in the machine learning community, by a substantial margin. © 2015 IEEE.",active learning | clustering | defect categorization | semi supervised learning | support vector machine,IEEE International Conference on Program Comprehension,2015-08-05,Conference Paper,"Thung, Ferdian;Le, Xuan Bach D.;Lo, David",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85123378670,10.1109/ICRITO51393.2021.9596509,Analyzing Software Vulnerabilities Using Machine Learning,"The driving force of the digital world is written code that is known as software. Software engineers meticulously write thousands of lines of code over a period of time according to the problem statement. Unfortunately, software however written is prone to both cyber attacks and other discrepancies that can render any software useless. A software vulnerability is a defect in a software program that gives any attacker the opportunity to manipulate it and use it in their favour. Software vulnerabilities are defects that are a result of a problem with the design or the way programs are coded. Vulnerabilities make a software prone to all sorts of attacks. Planting malware or stealing sensitive data are major issues resulting from vulnerable software. Vulnerabilities can be categorised on the basis of types of attacks, geographical origin, programming language used in the development. It can be concluded after various experiments that inculcating certain routines and practices is imperative for manufacturing secure software free of as many vulnerabilities as possible. Researchers have been able to devise as many vulnerability detection models as possible but there is always a scope for improvement. In this survey we review how state of art and up to date machine learning techniques can be used to detect and analyse the ever increasing software vulnerabilities. © 2021 IEEE.",cyberattacks | dynamic analysis | Machine learning | Software vulnerabilities | static analysis,"2021 9th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions), ICRITO 2021",2021-01-01,Conference Paper,"Peerzada, Bareen;Kumar, Deepak",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85115627445,10.1109/ICSE-NIER52604.2021.00022,What Are We Really Testing in Mutation Testing for Machine Learning? A Critical Reflection,"Mutation testing is a well-established technique for assessing a test suite's quality by injecting artificial faults into production code. In recent years, mutation testing has been extended to machine learning (ML) systems, and deep learning (DL) in particular; researchers have proposed approaches, tools, and statistically sound heuristics to determine whether mutants in DL systems are killed or not. However, as we will argue in this work, questions can be raised to what extent currently used mutation testing techniques in DL are actually in line with the classical interpretation of mutation testing. We observe that ML model development resembles a test-driven development (TDD) process, in which a training algorithm ('programmer') generates a model (program) that fits the data points (test data) to labels (implicit assertions), up to a certain threshold. However, considering proposed mutation testing techniques for ML systems under this TDD metaphor, in current approaches, the distinction between production and test code is blurry, and the realism of mutation operators can be challenged. We also consider the fundamental hypotheses underlying classical mutation testing: the competent programmer hypothesis and coupling effect hypothesis. As we will illustrate, these hypotheses do not trivially translate to ML system development, and more conscious and explicit scoping and concept mapping will be needed to truly draw parallels. Based on our observations, we propose several action points for better alignment of mutation testing techniques for ML with paradigms and vocabularies of classical mutation testing. © 2021 IEEE.",machine learning | mutation operators | mutation testing | software testing,Proceedings - International Conference on Software Engineering,2021-05-01,Conference Paper,"Panichella, Annibale;Liem, Cynthia C.S.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85026830618,10.1109/ICSE-SEIP.2017.8,Domain adaptation for test report classification in crowdsourced testing,"In crowdsourced testing, it is beneficial to automatically classify the test reports that actually reveal a fault - a true fault, from the large number of test reports submitted by crowd workers. Most of the existing approaches toward this task simply leverage historical data to train a machine learning classifier and classify the new incoming reports. However, our observation on real industrial data reveals that projects under crowdsourced testing come from various domains, and the submitted reports usually contain different technical terms to describe the software behavior for each domain. The different data distribution across domains could significantly degrade the performance of classification models when utilized for cross-domain report classification. To build an effective cross-domain classification model, we leverage deep learning to discover the intermediate representation that is shared across domains, through the co-occurrence between domain-specific terms and domain-unaware terms. Specifically, we use the Stacked Denoising Autoencoders to automatically learn the high-level features from raw textual terms, and utilize these features for classification. Our evaluation on 58 commercial projects of 10 domains from one of the Chinese largest crowdsourced testing platforms shows that our approach can generate promising results, compared to three commonly-used and state-of-the-art baselines. Moreover, we also evaluate its usefulness using real-world case studies. The feedback from real-world testers demonstrates its practical value. © 2017 IEEE.",Crowdsourced testing | deep learning | domain adaptation | test report classification,"Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",2017-06-30,Conference Paper,"Wang, Junjie;Cui, Qiang;Wang, Song;Wang, Qing",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85072111655,10.1109/ICSE-SEIP.2019.00042,Software Engineering for Machine Learning: A Case Study,"Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components-models may be 'entangled' in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations. © 2019 IEEE.",Artifical Intelligence | Data | Machine Learning | Process | Software Engineering,"Proceedings - 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice, ICSE-SEIP 2019",2019-05-01,Conference Paper,"Amershi, Saleema;Begel, Andrew;Bird, Christian;DeLine, Robert;Gall, Harald;Kamar, Ece;Nagappan, Nachiappan;Nushi, Besmira;Zimmermann, Thomas",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85111447656,10.1109/ICSE43902.2021.00046,Prioritizing test inputs for deep neural networks via mutation analysis,"Deep Neural Network (DNN) testing is one of the most widely-used ways to guarantee the quality of DNNs. However, labeling test inputs to check the correctness of DNN prediction is very costly, which could largely affect the efficiency of DNN testing, even the whole process of DNN development. To relieve the labeling-cost problem, we propose a novel test input prioritization approach (called PRIMA) for DNNs via intelligent mutation analysis in order to label more bug-revealing test inputs earlier for a limited time, which facilitates to improve the efficiency of DNN testing. PRIMA is based on the key insight: a test input that is able to kill many mutated models and produce different prediction results with many mutated inputs, is more likely to reveal DNN bugs, and thus it should be prioritized higher. After obtaining a number of mutation results from a series of our designed model and input mutation rules for each test input, PRIMA further incorporates learning-to-rank (a kind of supervised machine learning to solve ranking problems) to intelligently combine these mutation results for effective test input prioritization. We conducted an extensive study based on 36 popular subjects by carefully considering their diversity from five dimensions (i.e., different domains of test inputs, different DNN tasks, different network structures, different types of test inputs, and different training scenarios). Our experimental results demonstrate the effectiveness of PRIMA, significantly outperforming the state-of-the-art approaches (with the average improvement of 8.50%~131.01% in terms of prioritization effectiveness). In particular, we have applied PRIMA to the practical autonomous-vehicle testing in a large motor company, and the results on 4 real-world scene-recognition models in autonomous vehicles further confirm the practicability of PRIMA. © 2021 IEEE.",Deep Learning Testing | Deep Neural Network | Label | Mutation | Test Prioritization,Proceedings - International Conference on Software Engineering,2021-05-01,Conference Paper,"Wang, Zan;You, Hanmo;Chen, Junjie;Zhang, Yingyi;Dong, Xuyuan;Zhang, Wenbin",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85115717137,10.1109/ICSE43902.2021.00086,MuDelta: Delta-oriented mutation testing at commit time,"To effectively test program changes using mutation testing, one needs to use mutants that are relevant to the altered program behaviours. In view of this, we introduce MuDelta, an approach that identifies commit-relevant mutants; mutants that affect and are affected by the changed program behaviours. Our approach uses machine learning applied on a combined scheme of graph and vector-based representations of static code features. Our results, from 50 commits in 21 Coreutils programs, demonstrate a strong prediction ability of our approach; yielding 0.80 (ROC) and 0.50 (PR Curve) AUC values with 0.63 and 0.32 precision and recall values. These predictions are significantly higher than random guesses, 0.20 (PR-Curve) AUC, 0.21 and 0.21 precision and recall, and subsequently lead to strong relevant tests that kill 45%more relevant mutants than randomly sampled mutants (either sampled from those residing on the changed component(s) or from the changed lines). Our results also show that MuDelta selects mutants with 27% higher fault revealing ability in fault introducing commits. Taken together, our results corroborate the conclusion that commit-based mutation testing is suitable and promising for evolving software. © 2021 IEEE.",Commit-relevant mutants | Continuous integration | Machine learning | Mutation testing | Regression testing,Proceedings - International Conference on Software Engineering,2021-05-01,Conference Paper,"Ma, Wei;Titcheu Chekam, Thierry;Papadakis, Mike;Harman, Mark",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85109572803,10.1109/ICSE43902.2021.00109,InferCode: Self-supervised learning of code representations by predicting subtrees,"Learning code representations has found many uses in software engineering, such as code classification, code search, comment generation, and bug prediction, etc. Although representations of code in tokens, syntax trees, dependency graphs, paths in trees, or the combinations of their variants have been proposed, existing learning techniques have a major limitation that these models are often trained on datasets labeled for specific downstream tasks, and as such the code representations may not be suitable for other tasks. Even though some techniques generate representations from unlabeled code, they are far from being satisfactory when applied to the downstream tasks. To overcome the limitation, this paper proposes InferCode, which adapts the self-supervised learning idea from natural language processing to the abstract syntax trees (ASTs) of code. The novelty lies in the training of code representations by predicting subtrees automatically identified from the contexts of ASTs. With InferCode, subtrees in ASTs are treated as the labels for training the code representations without any human labelling effort or the overhead of expensive graph construction, and the trained representations are no longer tied to any specific downstream tasks or code units. We have trained an instance of InferCode model using Tree-Based Convolutional Neural Network (TBCNN) as the encoder of a large set of Java code. This pre-trained model can then be applied to downstream unsupervised tasks such as code clustering, code clone detection, cross-language code search, or be reused under a transfer learning scheme to continue training the model weights for supervised tasks such as code classification and method name prediction. Compared to prior techniques applied to the same downstream tasks, such as code2vec, code2seq, ASTNN, using our pre-trained InferCode model higher performance is achieved with a significant margin for most of the tasks, including those involving different programming languages. The implementation of InferCode and the trained embeddings are available at the link: https://github.com/bdqnghi/infercode. © 2021 IEEE.",Code clone detection | Code retrieval | Code search | Cross language | Fine tuning | Self supervised | Unlabel data | Unlabelled data,Proceedings - International Conference on Software Engineering,2021-05-01,Conference Paper,"Bui, Nghi D.Q.;Yu, Yijun;Jiang, Lingxiao",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85063633493,10.1109/ICSESS.2018.8663824,A Fault Diagnosis Method of Tread Production Line Based on Convolutional Neural Network,"As an important part of automobile, tire is the foundation of the development of automobile industry. The stable and optimized control of compound extrusion process of automobile tire tread is the premise for producing good quality tires. Convolutional neural network is a model of deep learning which was widely used in image recognition, speech processing and other fields and has a very good classification capacity. In this paper, a convolutional neural network model is proposed. The data of the rotating speed, pressure and main current of Φ 150 and Φ 120 extruder compounds tread extrusion lines are selected as the model input. The convolutional neural network is applied to the fault diagnosis of the tread compound extrusion line and compared with the traditional machine learning algorithm-BP neural network, decision tree and Logistic regression. The superiority of the algorithm is verified from the result data. A new method of fault detection and diagnosis for tire tread production line based on convolution neural network is proposed. © 2018 IEEE.",component | Convolutional Neural Network | fault diagnosis | tread production line,"Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",2018-07-02,Conference Paper,"Lihao, Wen;Yanni, Deng",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-78650138468,10.1109/ICSM.2010.5609736,Fine-grained incremental learning and multi-feature tossing graphs to improve bug triaging,"Software bugs are inevitable and bug fixing is a difficult, expensive, and lengthy process. One of the primary reasons why bug fixing takes so long is the difficulty of accurately assigning a bug to the most competent developer for that bug kind or bug class. Assigning a bug to a potential developer, also known as bug triaging, is a labor-intensive, time-consuming and faultprone process if done manually. Moreover, bugs frequently get reassigned to multiple developers before they are resolved, a process known as bug tossing. Researchers have proposed automated techniques to facilitate bug triaging and reduce bug tossing using machine learning-based prediction and tossing graphs. While these techniques achieve good prediction accuracy for triaging and reduce tossing paths, they are vulnerable to several issues: outdated training sets, inactive developers, and imprecise, singleattribute tossing graphs. In this paper we improve triaging accuracy and reduce tossing path lengths by employing several techniques such as refined classification using additional attributes and intra-fold updates during training, a precise ranking function for recommending potential tossees in tossing graphs, and multi-feature tossing graphs. We validate our approach on two large software projects, Mozilla and Eclipse, covering 856,259 bug reports and 21 cumulative years of development. We demonstrate that our techniques can achieve up to 83.62% prediction accuracy in bug triaging. Moreover, we reduce tossing path lengths to 1.5-2 tosses for most bugs, which represents a reduction of up to 86.31% compared to original tossing paths. Our improvements have the potential to significantly reduce the bug fixing effort, especially in the context of sizable projects with large numbers of testers and developers. © 2010 IEEE.",,"IEEE International Conference on Software Maintenance, ICSM",2010-12-20,Conference Paper,"Bhattacharya, Pamela;Neamtiu, Iulian",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84891717700,10.1109/ICSM.2013.31,DRONE: Predicting priority of reported bugs by multi-factor analysis,"Bugs are prevalent. To improve software quality, developers often allow users to report bugs that they found using a bug tracking system such as Bugzilla. Users would specify among other things, a description of the bug, the component that is affected by the bug, and the severity of the bug. Based on this information, bug triagers would then assign a priority level to the reported bug. As resources are limited, bug reports would be investigated based on their priority levels. This priority assignment process however is a manual one. Could we do better? In this paper, we propose an automated approach based on machine learning that would recommend a priority level based on information available in bug reports. Our approach considers multiple factors, temporal, textual, author, related-report, severity, and product, that potentially affect the priority level of a bug report. These factors are extracted as features which are then used to train a discriminative model via a new classification algorithm that handles ordinal class labels and imbalanced data. Experiments on more than a hundred thousands bug reports from Eclipse show that we can outperform baseline approaches in terms of average F-measure by a relative improvement of 58.61%. © 2013 IEEE.",,"IEEE International Conference on Software Maintenance, ICSM",2013-12-01,Conference Paper,"Tian, Yuan;Lo, David;Sun, Chengnian",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85040611476,10.1109/ICSME.2017.69,Towards accurate duplicate bug retrieval using deep learning techniques,"Duplicate Bug Detection is the problem of identifying whether a newly reported bug is a duplicate of an existing bug in the system and retrieving the original or similar bugs from the past. This is required to avoid costly rediscovery and redundant work. In typical software projects, the number of duplicate bugs reported may run into the order of thousands, making it expensive in terms of cost and time for manual intervention. This makes the problem of duplicate or similar bug detection an important one in Software Engineering domain. However, an automated solution for the same is not quite accurate yet in practice, in spite of many reported approaches using various machine learning techniques. In this work, we propose a retrieval and classification model using Siamese Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) for accurate detection and retrieval of duplicate and similar bugs. We report an accuracy close to 90% and recall rate close to 80%, which makes possible the practical use of such a system. We describe our model in detail along with related discussions from the Deep Learning domain. By presenting the detailed experimental results, we illustrate the effectiveness of the model in practical systems, including for repositories for which supervised training data is not available. © 2017 IEEE.",Convolutional neural networks | Deep learning | Duplicate bug detection | Information retrieval | Long short term memory | Natural language processing | Siamese networks | Word embeddings,"Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",2017-11-02,Conference Paper,"Deshmukh, Jayati;Annervaz, K. M.;Podder, Sanjay;Sengupta, Shubhashis;Dubash, Neville",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85040626682,10.1109/ICSME.2017.79,Deep green: Modelling time-series of software energy consumption,"Inefficient mobile software kills battery life. Yet, developers lack the tools necessary to detect and solve energy bugs in software. In addition, developers are usually tasked with the creation of software features and triaging existing bugs. This means that most developers do not have the time or resources to research, build, or employ energy debugging tools. We present a new method for predicting software energy consumption to help debug software energy issues. Our approach enables developers to align traces of software behavior with traces of software energy consumption. This allows developers to match run-time energy hot spots to the corresponding execution. We accomplish this by applying recent neural network models to predict time series of energy consumption given a software's behavior. We compare our time series models to prior state-of-the-art models that only predict total software energy consumption. We found that machine learning based time series based models, and LSTM based time series based models, can often be more accurate at predicting instantaneous power use and total energy consumption. © 2017 IEEE.",,"Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",2017-11-02,Conference Paper,"Romansky, Stephen;Borle, Neil C.;Chowdhury, Shaiful;Hindle, Abram;Greiner, Russ",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85077214670,10.1109/ICSME.2019.00018,Improving Bug Triaging with High Confidence Predictions at Ericsson,"Correctly assigning bugs to the right developer or team, i.e. bug triaging, is a costly activity. A concerted effort at Ericsson has been done to adopt automated bug triaging to reduce development costs. In this work, we replicate the research approaches that have been widely used in the literature. We apply them on over 10k bug reports for 9 large products at Ericsson. We find that a logistic regression classifier including the simple textual and categorical attributes of the bug reports has the highest precision and recall of 78.09% and 79.00%, respectively. Ericsson's bug reports often contain logs that have crash dumps and alarms. We add this information to the bug triage models. We find that this information does not improve the precision and recall of bug triaging in Ericsson's context. Although our models perform as well as the best ones reported in the literature, a criticism of bug triaging at Ericsson is that the accuracy is not sufficient for regular use. We develop a novel approach where we only triage bugs when the model has high confidence in the triage prediction. We find that we improve the accuracy to 90%, but we can make predictions for 62% of the bug reports. © 2019 IEEE.",Bug Triaging | Incremental Learning | Log Analysis | Machine Learning,"Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019",2019-09-01,Conference Paper,"Sarkar, Aindrila;Rigby, Peter C.;Bartalos, Bela",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85096673171,10.1109/ICSME46990.2020.00082,Efficient Bug Triage for Industrial Environments,"Bug triage is an important task for software maintenance, especially in the industrial environment, where timely bug fixing is critical for customer experience. This process is usually done manually and often takes significant time. In this paper, we propose a machine-learning-based solution to address the problem efficiently. We argue that in the industrial environment, it is more suitable to assign bugs to software components (then to responsible developers) than to developers directly. Because developers can change their roles in industry, they may not oversee the same software module as before. We also demonstrate experimentally that assigning bugs to components rather than developers leads to much higher accuracy. Our solution is based on text-projection features extracted from bug descriptions. We use a Deep Neural Network to train the classification model. The proposed solution achieves state-of-the-art performance based on extensive experiments using multiple data sets. Moreover, our solution is computationally efficient and runs in near real-time. © 2020 IEEE.",Automatic bug triage | machine learning | text classification,"Proceedings - 2020 IEEE International Conference on Software Maintenance and Evolution, ICSME 2020",2020-09-01,Conference Paper,"Zhang, Wei",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85123361929,10.1109/ICSME52107.2021.00030,Automated Recovery of Issue-Commit Links Leveraging Both Textual and Non-textual Data,"An issue report documents the discussions around required changes in issue-tracking systems, while a commit contains the change itself in the version control systems. Recovering links between issues and commits can facilitate many software evolution tasks such as bug localization, defect prediction, software quality measurement, and software documentation. A previous study on over half a million issues from GitHub reports only about 42.2% of issues are manually linked by developers to their pertinent commits. Automating the linking of commit-issue pairs can contribute to the improvement of the said tasks. By far, current state-of-the-art approaches for automated commit-issue linking suffer from low precision, leading to unreliable results, sometimes to the point that imposes human supervision on the predicted links. The low performance gets even more severe when there is a lack of textual information in either commits or issues. Current approaches are also proven computationally expensive. We propose Hybrid-Linker, an enhanced approach that overcomes such limitations by exploiting two information channels; (1) a non-textual-based component that operates on non-textual, automatically recorded information of the commit-issue pairs to predict a link, and (2) a textual-based one which does the same using textual information of the commit-issue pairs. Then, combining the results from the two classifiers, Hybrid-Linker makes the final prediction. Thus, every time one component falls short in predicting a link, the other component fills the gap and improves the results. We evaluate Hybrid-Linker against competing approaches, namely FRLink and DeepLink on a dataset of 12 projects. Hybrid-Linker achieves 90.1%, 87.8%, and 88.9% based on recall, precision, and F-measure, respectively. It also outperforms FRLink and DeepLink by 31.3%, and 41.3%, regarding the F-measure. Moreover, the proposed approach exhibits extensive improvements in terms of performance as well. Finally, our source code and data are publicly available. © 2021 IEEE.",Commit | Ensemble Methods | Issue Report | Link Recovery | Machine Learning | Software Maintenance,"Proceedings - 2021 IEEE International Conference on Software Maintenance and Evolution, ICSME 2021",2021-01-01,Conference Paper,"Mazrae, Pooya Rostami;Izadi, Maliheh;Heydarnoori, Abbas",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84862335065,10.1109/ICST.2012.124,Towards automated anomaly report assignment in large complex systems using stacked generalization,"Maintenance costs can be substantial for organizations with very large and complex software systems. This paper describes research for reducing anomaly report turnaround time which, if successful, would contribute to reducing maintenance costs and at the same time maintaining a good customer perception. Specifically, we are addressing the problem of the manual, laborious, and inaccurate process of assigning anomaly reports to the correct design teams. In large organizations with complex systems this is particularly problematic because the receiver of the anomaly report from customer may not have detailed knowledge of the whole system. As a consequence, anomaly reports may be wrongly routed around in the organization causing delays and unnecessary work. We have developed and validated machine learning approach, based on stacked generalization, to automatically route anomaly reports to the correct design teams in the organization. A research prototype has been implemented and evaluated on roughly one year of real anomaly reports on a large and complex system at Ericsson AB. The prediction accuracy of the automation is approaching that of humans, indicating that the anomaly report handling time could be significantly reduced by using our approach. © 2012 IEEE.",Automatic Fault Localization | Bayesian Networks | Bug Assignment | Large Software Systems | Machine Learning | Naive Bayes | Stacked generalization | Support Vector Machines,"Proceedings - IEEE 5th International Conference on Software Testing, Verification and Validation, ICST 2012",2012-06-21,Conference Paper,"Jonsson, Leif;Broman, David;Sandahl, Kristian;Eldh, Sigrid",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84983364773,10.1109/ICST.2016.50,Self-Healing Cloud Applications,"Cloud computing offers on-demand services to deploy and run applications with flexible and scalable resource pooling. The techniques adopted by cloud systems introduce reliability issues that challenge the design of cloud applications. In my PhD I work on the key problem of improving the reliability of cloud applications. In particular, I am investigating on the definition of effective and efficient self-healing approaches that integrate failure prediction, fault localization and fault fixing mechanisms in the context of cloud-based systems. In the first part of my research I investigated the problem of automatic failure prediction, which constitutes the first step of a complete self-healing approach. I identified an original approach based on a combination of data analytics and machine learning techniques, and developed an early prototype to collect experimental data about the proposed approach. The data collected so far indicate that the approach has both high precision and recall rate. © 2016 IEEE.",,"Proceedings - 2016 IEEE International Conference on Software Testing, Verification and Validation, ICST 2016",2016-07-18,Conference Paper,"Xin, Rui",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85048438492,10.1109/ICST.2018.00034,Localizing Faults in Cloud Systems,"By leveraging large clusters of commodity hardware, the Cloud offers great opportunities to optimize the operative costs of software systems, but impacts significantly on the reliability of software applications. The lack of control of applications over Cloud execution environments largely limits the applicability of state-of-the-art approaches that address reliability issues by relying on heavyweight training with injected faults. In this paper, we propose LOUD, a lightweight fault localization approach that relies on positive training only, and can thus operate within the constraints of Cloud systems. LOUD relies on machine learning and graph theory. It trains machine learning models with correct executions only, and compensates the inaccuracy that derives from training with positive samples, by elaborating the outcome of machine learning techniques with graph theory algorithms. The experimental results reported in this paper confirm that LOUD can localize faults with high precision, by relying only on a lightweight positive training. © 2018 IEEE.",cloud | fault localization,"Proceedings - 2018 IEEE 11th International Conference on Software Testing, Verification and Validation, ICST 2018",2018-05-25,Conference Paper,"Mariani, Leonardo;Monni, Cristina;Pezze, Mauro;Riganelli, Oliviero;Xin, Rui",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85048401472,10.1109/ICST.2018.00037,Virtualized-Fault Injection Testing: A Machine Learning Approach,"We introduce a new methodology for virtualized fault injection testing of safety critical embedded systems. This approach fully automates the key steps of test case generation, fault injection and verdict construction. We use machine learning to reverse engineer models of the system under test. We use model checking to generate test verdicts with respect to safety requirements formalised in temporal logic. We exemplify our approach by implementing a tool chain based on integrating the QEMU hardware emulator, the GNU debugger GDB and the LBTest requirements testing tool. This tool chain is then evaluated on two industrial safety critical applications from the automotive sector. © 2018 IEEE.",automotive software | black box testing | emulation platform | fault injection | GDB | learning based testing | machine learning | Model based testing | QEMU | requirements testing | temporal logic | virtual hardware,"Proceedings - 2018 IEEE 11th International Conference on Software Testing, Verification and Validation, ICST 2018",2018-05-25,Conference Paper,"Khosrowjerdi, Hojat;Meinke, Karl;Rasmusson, Andreas",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85067981414,10.1109/ICST.2019.00024,An RBM anomaly detector for the cloud,"Failures are unavoidable in complex software systems, and the intrinsic characteristics of cloud systems amplify the problem. Predicting failures before their occurrence by detecting anomalies in system metrics is a viable solution to enable failure preventing or mitigating actions. The most promising approaches for predicting failures exploit statistical analysis or machine learning to reveal anomalies and their correlation with possible failures. Statistical analysis approaches result in far too many false positives, which severely hinder their practical applicability, while accurate machine learning approaches need extensive training with seeded faults, which is often impossible in operative cloud systems. In this paper, we propose EmBeD, Energy-Based anomaly Detection in the cloud, an approach to detect anomalies at runtime based on the free energy of a Restricted Boltzmann Machine (RBM) model. The free energy is a stochastic function that can be used to efficiently score anomalies for detecting outliers. EmBeD analyzes the system behavior from raw metric data, does not require extensive training with seeded faults, and classifies the relation of anomalous behaviors with future failures with very few false positives. The experimental results presented in this paper confirm that EmBeD can precisely predict failure-prone behavior without training with seeded faults, thus overcoming the main limitations of current approaches. © 2019 IEEE.",Anomaly detection | Cloud reliability | Failure prediction | Machine learning,"Proceedings - 2019 IEEE 12th International Conference on Software Testing, Verification and Validation, ICST 2019",2019-04-01,Conference Paper,"Monni, Cristina;Pezze, Mauro;Prisco, Gaetano",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85067997104,10.1109/ICST.2019.00025,An extensive study on cross-project predictive mutation testing,"Mutation testing is a powerful technique for evaluating the quality of test suite which plays a key role in ensuring software quality. The concept of mutation testing has also been widely used in other software engineering studies, e.g., test generation, fault localization, and program repair. During the process of mutation testing, large number of mutants may be generated and then executed against the test suite to examine whether they can be killed, making the process extremely computational expensive. Several techniques have been proposed to speed up this process, including selective, weakened, and predictive mutation testing. Among those techniques, Predictive Mutation Testing (PMT) tries to build a classification model based on an amount of mutant execution records to predict whether coming new mutants would be killed or alive without mutant execution, and can achieve significant mutation cost reduction. In PMT, each mutant is represented as a list of features related to the mutant itself and the test suite, transforming the mutation testing problem to a binary classification problem. In this paper, we perform an extensive study on the effectiveness and efficiency of the promising PMT technique under the cross-project setting using a total 654 real world projects with more than 4 Million mutants. Our work also complements the original PMT work by considering more features and the powerful deep learning models. The experimental results show an average of over 0.85 prediction accuracy on 654 projects using cross validation, demonstrating the effectiveness of PMT. Meanwhile, a clear speed up is also observed with an average of 28.7X compared to traditional mutation testing with 5 threads. In addition, we analyze the importance of different groups of features in classification model, which provides important implications for the future research. © 2019 IEEE.",Deep learning | Machine learning | Mutation testing | Software quality | Software testing,"Proceedings - 2019 IEEE 12th International Conference on Software Testing, Verification and Validation, ICST 2019",2019-04-01,Conference Paper,"Mao, Dongyu;Chen, Lingchao;Zhang, Lingming",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-80051656116,10.1109/ICSTW.2011.73,Identifying infeasible GUI test cases using support vector machines and induced grammars,"Model-based GUI software testing is an emerging paradigm for automatically generating test suites. In the context of GUIs, a test case is a sequence of events to be executed which may detect faults in the application. However, a test case may be infeasible if one or more of the events in the event sequence are disabled or made inaccessible by a previously executed event (e.g., a button may be disabled until another GUI widget enables it). These infeasible test cases terminate prematurely and waste resources, so software testers would like to modify the test suite execution to run only feasible test cases. Current techniques focus on repairing the test cases to make them feasible, but this relies on executing all test cases, attempting to repair the test cases, and then repeating this process until a stopping condition has been met. We propose avoiding infeasible test cases altogether by predicting which test cases are infeasible using two supervised machine learning methods: support vector machines (SVMs) and grammar induction. We experiment with three feature extraction techniques and demonstrate the success of the machine learning algorithms for classifying infeasible GUI test cases in several subject applications. We further demonstrate a level of robustness in the algorithms when training and classifying test cases of different lengths. © 2011 IEEE.",Event based testing | Grammar induction | GUI testing | Machine learning | Software testing | Support vector machines,"Proceedings - 4th IEEE International Conference on Software Testing, Verification, and Validation Workshops, ICSTW 2011",2011-08-18,Conference Paper,"Gove, Robert;Faytong, Jorge",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85050978697,10.1109/ICSTW.2018.00061,A survey of software quality for machine learning applications,"Machine learning (ML) is now widespread. Traditional software engineering can be applied to the development ML applications. However, we have to consider specific problems with ML applications in therms of their quality. In this paper, we present a survey of software quality for ML applications to consider the quality of ML applications as an emerging discussion. From this survey, we raised problems with ML applications and discovered software engineering approaches and software testing research areas to solve these problems. We classified survey targets into Academic Conferences, Magazines, and Communities. We targeted 16 academic conferences on artificial intelligence and software engineering, including 78 papers. We targeted 5 Magazines, including 22 papers. The results indicated key areas, such as deep learning, fault localization, and prediction, to be researched with software engineering and testing. © 2018 IEEE.",Machine Learning | Software Engineering and Testing | Software Quality,"Proceedings - 2018 IEEE 11th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2018",2018-07-16,Conference Paper,"Masuda, Satoshi;Ono, Kohichi;Yasue, Toshiaki;Hosokawa, Nobuhiro",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85108029707,10.1109/ICSTW52544.2021.00019,A combinatorial approach to explaining image classifiers,"Machine Learning (ML) models, a core component to artificial intelligence systems, often come as a black box to the user, leading to the problem of interpretability. Explainable Artificial Intelligence (XAI) is key to providing confidence and trustworthiness for machine learning-based software systems. We observe a fundamental connection between XAI and software fault localization. In this paper, we present an approach that uses BEN, a combinatorial testing-based software fault localization approach, to produce explanations for decisions made by ML models. © 2021 IEEE.",Combinatorial testing | Counterfactual explanation | Debugging DNN models | Deep learning | Explainability | Explainable AI | Image classifiers | Instance-level explanations | Model-agnostic | Software testing,"Proceedings - 2021 IEEE 14th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2021",2021-04-01,Conference Paper,"Chandrasekaran, Jaganmohan;Lei, Yu;Kacker, Raghu;Richard Kuhn, D.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85048484855,10.1109/ICTAI.2017.00151,Intelligent fault analysis in electrical power grids,"Power grids are one of the most important components of infrastructure in today's world. Every nation is dependent on the security and stability of its own power grid to provide electricity to the households and industries. A malfunction of even a small part of a power grid can cause loss of productivity, revenue and in some cases even life. Thus, it is imperative to design a system which can detect the health of the power grid and take protective measures accordingly even before a serious anomaly takes place. To achieve this objective, we have set out to create an artificially intelligent system which can analyze the grid information at any given time and determine the health of the grid through the usage of sophisticated formal models and novel machine learning techniques like recurrent neural networks. Our system simulates grid conditions including stimuli like faults, generator output fluctuations, load fluctuations using Siemens PSS/E software and this data is trained using various classifiers like SVM, LSTM and subsequently tested. The results are excellent with our methods giving very high accuracy for the data. This model can easily be scaled to handle larger and more complex grid architectures. © 2017 IEEE.",Classification | Deep Learning | Fault Analysis | LSTM | Power Grids | PSS/E | SVM,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",2017-07-02,Conference Paper,"Bhattacharya, Biswarup;Sinha, Abhishek",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85062843955,10.1109/IISA.2018.8633699,Photovoltaic array simulation and fault prediction via multilayer perceptron models,"When collecting solar energy via photovoltaic (PV) panel arrays, one common issue is the potential occurrence of faults. Faults arise from panel short-circuit, soiling, shading, ground leakage and other sources. Machine learning algorithms have enabled data-based classification of faults. In this paper, we present an Internet-based PV array fault monitoring simulation using the Java-DSP (J-DSP) simulation environment. We first develop a solar array simulation in J-DSP and then form appropriate graphics to examine V-I curves, maximum power point tracking, and faults. We then introduce a multi-layer perceptron model for PV fault detection. We deploy and assess the simulation by disseminating to a group of users that provide feedback. © 2018 IEEE",,"2018 9th International Conference on Information, Intelligence, Systems and Applications, IISA 2018",2018-07-02,Conference Paper,"Khondoker, Farib;Rao, Sunil;Spanias, Andreas;Tepedelenlioglu, Cihan",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85101382262,10.1109/IKT51791.2020.9345611,Fast duplicate bug reports detector training using sampling for dimension reduction: Using instance-based learning for continous query in real-world,Duplicate bug report detection (DBRD) is a famous problem in software triage systems like Bugzilla. It is vital to update the internal machine learning (ML) models of DBRD for real-world usage and continuous query of new bug reports. The training phase of ML algorithms is time-consumable and dependent on the training dataset volume. Instance-based learning (IbL) is an ML technique that reduces the number of samples in the training dataset to achieve fast learning for the incremental database. This research introduces a hybrid approach using clustering and straight forward sampling to improve the runtime and validation performance of DBRD. Two bug report datasets of Android and Mozilla Firefox are used to evaluate the proposed approach. The experimental evaluation shows acceptable results and improvement in both runtime and validation performance of DBRD versus the traditional approach without IbL. © 2020 IEEE.,Bug Reports | Continuous Query | Duplicate Detection | Incremental Learning | Information Retrieval | Instance-based Learning | Natural Language Processing | Online Query,"2020 11th International Conference on Information and Knowledge Technology, IKT 2020",2020-12-22,Conference Paper,"Neysiani, Behzad Soleimani;Doostali, Saeed;Babamir, Seyed Morteza;Aminoroaya, Zahra",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85082695988,10.1109/IMPACT47228.2019.9024996,The Prediction of Positioning shift for a Robot Arm Using Machine Learning Techniques,"This study presents an Artificial Intelligence (AI) approach for estimating the Cartesian positioning shift of a wafer handling robot arm to prevent the occurrence of unexpected event, drop of wafers. First, a Charge-coupled Device (CCD) based robot arm fault diagnostic system was built to measure the target positions of the robot arm when handling wafers. An ensemble-based machine learning model with time series cross validation technique from a commercial software called Decanter AI (Mobagel Inc.) was applied to predict the quantity of the maximum position shift with respect to X and Y axis for next one minute. The prediction results by the test datasets through 38,417 minutes show that the Root Mean Square Error (RMSE) is 4.351 μm to validate the trained model is appropriate for predicting the positioning shift of the handling robot arm. © 2019 IEEE.",Cross validation | Ensemble machining learning | RMSE | Wafer handling robot arm,"Proceedings of Technical Papers - International Microsystems, Packaging, Assembly, and Circuits Technology Conference, IMPACT",2019-10-01,Conference Paper,"Huang, Ping Wun;Chung, Kuan Jung",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85090567790,10.1109/INCET49848.2020.9154058,Machine learning and its emergence in the modern world and its contribution to artificial intelligence,"Machine learning is known as the scientific study of various algorithms and statistics as well as models which can be used to create or perform certain tasks. These tasks are often based upon the dependability of the Interface as well as the patterns. As machine learning is also known as the subset of Artificial Intelligence, it enables the system to create or perform several tasks without the need for any manual changes. This mechanism relatively related to automatic performance and self- learning as it can be used to detect various faults within a inbuilt system or a software and take necessary steps to debug and run diagnostics to reduce errors as much as it can. Due to the emergence of Artificial Intelligence, various industries and private firms such as Space X and Tesla have induced Machine Learning into their workspace and especially when it comes to industrial usage, Corporations such as Tesla have developed AI based vehicles which run under electricity and automatic debugging. As the emergence of Artificial Intelligence have given ways to Machine Learning, it is inevitable that we may or may even be quite close to be perceiving futuristic technology much earlier than intended. © 2020 IEEE.",Artificial intelligence | Clustering | Computer vision | Concept learning | Filtering | Machine learning,"2020 International Conference for Emerging Technology, INCET 2020",2020-06-01,Conference Paper,"Dhanalaxmi, B.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85015320263,10.1109/INTECH.2016.7845120,Using ML in designing self-healing OS,"Operating systems serve as executing platforms and resource manager and supervisors for the applications in running phase. With the development of more complex computer systems and applications, the required operating systems become complex too. But the proper management of such complex operating systems by human beings has shown to be impractical. Nowadays, self-managing concepts provide the basis for developing appropriate mechanisms to handle complex systems with minimum human interventions. Although the implications of deploying self-managing and autonomic attributes and concepts at the application levels have been studied, their deployment at system software level such as in operating systems have not been fully studied. Self-managed applications may not enjoy the whole benefit of self-management if the platform on which they run, specially its operating system, is not self-managed. Given this requirement, this paper highlights the most frequently occurred faults and anomalies of operating systems, and proposes a tiered operating system architecture and model, and a corresponding self-healing mechanism using machine learning techniques to show how self-managing can be realized at operating system level. Based on the principles of autonomic computing and self-adapting system research, we identify self-healing systems' fundamental principles. The main objective has been to design the operating system resilient to operating system faults without restarting the operating system and less human interaction. © 2016 IEEE.",Operating System | Self-healing | Self-management Component,"2016 6th International Conference on Innovative Computing Technology, INTECH 2016",2017-02-06,Conference Paper,"Ahmad, Maqsood;Samiullah, Muhammad;Pirzada, Muhammad Jawad;Fahad, Muhammad",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85073709663,10.1109/IOLTS.2019.8854423,Machine Learning to Tackle the Challenges of Transient and Soft Errors in Complex Circuits,"The Functional Failure Rate analysis of today's complex circuits is a difficult task and requires a significant investment in terms of human efforts, processing resources and tool licenses. Thereby, de-rating or vulnerability factors are a major instrument of failure analysis efforts. Usually computationally intensive fault-injection simulation campaigns are required to obtain a fine-grained reliability metrics for the functional level. Therefore, the use of machine learning algorithms to assist this procedure and thus, optimising and enhancing fault injection efforts, is investigated in this paper. Specifically, machine learning models are used to predict accurate per-instance Functional De-Rating data for the full list of circuit instances, an objective that is difficult to reach using classical methods. The described methodology uses a set of per-instance features, extracted through an analysis approach, combining static elements (cell properties, circuit structure, synthesis attributes) and dynamic elements (signal activity). Reference data is obtained through first-principles fault simulation approaches. One part of this reference dataset is used to train the machine learning model and the remaining is used to validate and benchmark the accuracy of the trained tool. The presented methodology is applied on a practical example and various machine learning models are evaluated and compared. © 2019 IEEE.",CART | Fault Injection | k-NN | Linear Least Squares | Machine Learning | Ridge Regression | Single-Event Effects | Support Vector Regression | Transient Faults,"2019 IEEE 25th International Symposium on On-Line Testing and Robust System Design, IOLTS 2019",2019-07-01,Conference Paper,"Lange, Thomas;Balakrishnan, Aneesh;Glorieux, Maximilien;Alexandrescu, Dan;Sterpone, Luca",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85088903267,10.1109/IPDPS47924.2020.00115,Aarohi: Making Real-Time Node Failure Prediction Feasible,"Large-scale production systems are well known to encounter node failures, which affect compute capacity and energy. Both in HPC systems and enterprise data centers, combating failures is becoming challenging with increasing hardware and software complexity. Several data mining solutions of logs have been investigated in the context of anomaly detection in such systems. However, with subsequent proactive failure mitigation, the existing log mining solutions are not sufficiently fast for real-time anomaly detection. Machine learning (ML)-based training can produce high accuracy but the inference scheme needs to be enhanced with rapid parsers to assess anomalies in real-time. This work tackles online anomaly prediction in computing systems by exploiting context free grammar-based rapid event analysis.We present our framework Aarohi1, which describes an effective way to predict failures online. Aarohi is designed to be generic and scalable making it suitable as a real-time predictor. Aarohi obtains more than 3 minutes lead times to node failures with an average of 0.31 msecs prediction time for a chain length of 18. The overall improvement obtained w.r.t. the existing state-of-the-art is over a factor of 27.4×. Our compiler-based approach provides new research directions for lead time optimization with a significant prediction speedup required for the deployment of proactive fault tolerant solutions in practice. © 2020 IEEE.",HPC | Node Failures | Online Prediction | Parsing,"Proceedings - 2020 IEEE 34th International Parallel and Distributed Processing Symposium, IPDPS 2020",2020-05-01,Conference Paper,"Das, Anwesha;Mueller, Frank;Rountree, Barry",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85100941169,10.1109/IPDRM51949.2020.00009,Codir: Towards an mlir codelet model dialect,"Since IBM first introduced the Instruction Set Architecture (ISA), there has been tremendous research conducted on novel hardware architectures and new high-level software frameworks, perhaps to a fault. With innovation came an ever growing gap between a program's description and how it is executed on hardware. Successful exploitation of these innovations calls for a unifying contract to be in place which distinguishes the program description from the low-level execution on target hardware; namely, a Program Execution Model (PXM). By decoupling the programming API from the hardware description the obstacles imposed by low-level architectures and high-level language interoperability are avoided by utilizing program manipulation, source-level optimizations, and eventual lowering into machine code across a wide variety of different heterogeneous hardware. To achieve these goals in software, compilers must adapt to the ever-changing climate of hardware and software development. They must be able to represent data at varying levels so as to provide maximum opportunities for optimizations based on information which would otherwise be lost without utilizing progressive lowering. The target hardware should also not limit the amount of possible software-based optimizations. We propose CODIR, the Codelet Model domain-specific language extension and intermediate representation built using the already existing Multi-Level Intermediate Representation (MLIR) compiler infrastructure. It aims to progressively lower a high-level language to an intermediate representation well suited for maximizing optimization opportunities available to the compiler while additionally providing a plethora of hardware support. Underlying CODIR is the Codelet Model which we believe to be a sound program execution model for mapping computation to the machine and utilizing all available hardware resources. © 2020 IEEE.",Codelet model | Compiler | Darts | Dataflow model | Exa-scale | Machine learning | Many-core architecture | Program execution model | Software-hardware co-design,"Proceedings of IPDRM 2020: 4th Annual Workshop on Emerging Parallel and Distributed Runtime Systems and Middleware, Held in conjunction with SC 2020: The International Conference for High Performance Computing, Networking, Storage and Analysis",2020-11-01,Conference Paper,"Kabrick, Ryan;Perdomo, Diego A.Roa;Raskar, Siddhisanket;Diaz, Jose M.Monsalve;Fox, Dawson;Gao, Guang R.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84937539294,10.1109/ISCBI.2014.19,Business Intelligence &amp; Geo Tracking - A Novel Mining Technique to Identify Alerts and Pattern Analysis,"Web mining plays vital role in day-to-day applications to improve intelligence of web in the context of business must be able to identify useful business intelligence. To achieve our model in web engineering, we are using mining techniques for next generation business intelligence development. In this research our approach identifies the weblogs error reports using comprehensive algorithms, applies the mining techniques to detect noisy and integrates the different models, finally our information patterns satisfies the need of client inputs. For web engineering retrieval system, list of web log bugs and web architecture, the system uses mining techniques to explore valuable web data patterns in order to meet better projects inputs and higher quality web systems that delivered on time. Our research uses association and machine learning applied to web architecture model pertaining to source code mining implementation tools improves software debugging business rules for novel projects and also presents strategies for efficient study text, graph mining. Presents the Geo Tracking system to identify messages from terrorist or threat persons and also from hackers detects the negative rates and improves the high positive which increases the quality of Government Private and Public sectors. © 2014 IEEE.",Business Intelligence | Geo-Tracking | Pattern Analysis | Text Mining | Web mining,"Proceedings - 2014 2nd International Symposium on Computational and Business Intelligence, ISCBI 2014",2014-01-01,Conference Paper,"Vijaya Kamal, M.;Vasumathi, D.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84874375128,10.1109/ISDA.2012.6416595,Predicting the priority of a reported bug using machine learning techniques and cross project validation,"In bug repositories, we receive a large number of bug reports on daily basis. Managing such a large repository is a challenging job. Priority of a bug tells that how important and urgent it is for us to fix. Priority of a bug can be classified into 5 levels from PI to P5 where PI is the highest and P5 is the lowest priority. Correct prioritization of bugs helps in bug fix scheduling/assignment and resource allocation. Failure of this will result in delay of resolving important bugs. This requires a bug prediction system which can predict the priority of a newly reported bug. Cross project validation is also an important concern in empirical software engineering where we train classifier on one project and test it for prediction on other projects. In the available literature, we found very few papers for bug priority prediction and none of them dealt with cross project validation. In this paper, we have evaluated the performance of different machine learning techniques namely Support Vector Machine (SVM), Naive Bayes (NB), K-Nearest Neighbors (KNN) and Neural Network (NNet) in predicting the priority of the newly coming reports on the basis of different performance measures. We performed cross project validation for 76 cases of five data sets of open office and eclipse projects. The accuracy of different machine learning techniques in predicting the priority of a reported bug within and across project is found above 70% except Naive Bayes technique. © 2012 IEEE.",10-fold Cross Validation | Bug priority | Bug repositories | Classifiers | KNN | Naive Bayes | Neural Net | SVM | Triager,"International Conference on Intelligent Systems Design and Applications, ISDA",2012-12-01,Conference Paper,"Sharma, Meera;Bedi, Punam;Chaturvedi, K. K.;Singh, V. B.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84964896882,10.1109/ISSRE.2015.7381836,Should fixing these failures be delegated to automated program repair?,"Program repair constitutes one of the major components of software maintenance that usually incurs a significant cost in software production. Automated program repair is supposed to help in reducing the software maintenance cost by automatically fixing software defects. Despite the recent advances in automated software repair, it is still very costly to wait for repair tools to produce valid repairs of defects. This paper addresses the following question: ""Will an automated program repair technique find a repair for a defect within a reasonable time?"". To answer this question, we build an oracle that can predict whether fixing a failure should be delegated to an automated repair technique. If the repair technique is predicted to take too long to produce a repair, the bug fixing process should rather be assigned to a developer or other appropriate techniques available. Our oracle is built for genetic-programming-based automated program repair approaches, which have recently received considerable attention due to their capability to automatically fix real-world bugs. These approaches search for a valid repair over a large number of variants that are syntactically mutated from the original program. At an early stage of running a repair tool, we extract a number of features that are potentially related to the effectiveness of the tool. Leveraging advances in machine learning, we process the values of these features to learn a discriminative model that is able to predict whether continuing a genetic programming search will lead to a repair within a desired time limit. We perform experiments to evaluate the ability of our approach to predict the effectiveness of GenProg, a well-known genetic-programming-based automated program repair approach, in fixing 105 real bugs. Our experiments show that our approach can identify effective cases from ineffective ones (i.e., bugs for which GenProg cannot produce correct fixes after a long period of time) with a precision, recall, F-measure, and AUC of 72%, 74%, 73%, and 76% respectively. © 2015 IEEE.",Automated Program Repair | Classification Techniques | Effective Feature Design | Effectiveness Prediction,"2015 IEEE 26th International Symposium on Software Reliability Engineering, ISSRE 2015",2016-01-13,Conference Paper,"Le, Xuan Bach D.;Le, Tien Duy B.;Lo, David",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85040772740,10.1109/ISSRE.2017.29,Simultaneous Fault Models for the Generation of Efficient Error Detection Mechanisms,"The application of machine learning to software fault injection data has been shown to be an effective approach for the generation of efficient error detection mechanisms (EDMs). However, such approaches to the design of EDMs have invariably adopted a fault model with a single-fault assumption, limiting the practical relevance of the detectors and their evaluation. Software containing more than a single fault is commonplace, with prominent safety standards recognising that critical failures are often the result of unlikely or unforeseen combinations of faults. This paper addresses this shortcoming, demonstrating that it is possible to generate similarly efficient EDMs under more realistic fault models. In particular, it is shown that (i) efficient EDMs can be designed using fault data collected under models accounting for the occurrence of simultaneous faults, (ii) exhaustive fault injection under a simultaneous bit flip model can yield improvements to EDM efficiency, and (iii) exhaustive fault injection under a simultaneous bit flip model can made non-exhaustive, reducing the resource costs of experimentation to practicable levels, without sacrificing resultant EDM efficiency. © 2017 IEEE.",Detection | Error | Fault | Injection | Machine Learning,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2017-11-14,Conference Paper,"Leeke, Matthew",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85081112062,10.1109/ISSRE.2019.00032,Textout: Detecting Text-Layout Bugs in Mobile Apps via Visualization-Oriented Learning,"Layout bugs commonly exist in mobile apps. Due to the fragmentation issues of smartphones, a layout bug may occur only on particular versions of smartphones. It is quite challenging to detect such bugs for state-of-the-art commercial automated testing platforms, although they can test an app with thousands of different smartphones in parallel. The main reason is that typical layout bugs neither crash an app nor generate any error messages. In this paper, we present our work for detecting text-layout bugs, which account for a large portion of layout bugs. We model text-layout bug detection as a classification problem. This then allows us to address it with sophisticated image processing and machine learning techniques. To this end, we propose an approach which we call Textout. Textout takes screenshots as its input and adopts a specifically-tailored text detection method and a convolutional neural network (CNN) classifier to perform automatic text-layout bug detection. We collect 33,102 text-region images as our training dataset and verify the effectiveness of our tool with 1,481 text-region images collected from real-world apps. Textout achieves an AUC (area under the curve) of 0.956 on the test dataset and shows an acceptable overhead. The dataset is open-source released for follow-up research. © 2019 IEEE.",Deep learning | GUI bug detection | GUI testing | Mobile application testing | Text-layout bug detection,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2019-10-01,Conference Paper,"Wang, Yaohui;Xu, Hui;Zhou, Yangfan;Lyu, Michael R.;Wang, Xin",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85097350528,10.1109/ISSRE5003.2020.00020,Fault injection to generate failure data for failure prediction: A case study,"Due to the complexity of modern software, identifying every fault before deployment is extremely difficult or even not possible. Such residual faults can ultimately lead to failures, often incurring considerable risks or costs. Online Failure Prediction (OFP) is a fault-tolerance technique that attempts to predict the occurrence of failures in the near future and thus prevent/mitigate their consequences. Combined with recent technological developments, Machine Learning (ML) has been successfully used to create predictive models for OFP. However, as failures are rare events, failure data are often not available for building accurate models. Although fault injection has been accepted as a viable solution to generate realistic failure data, fault injectors are difficult to implement/update and thus research on Operating System (OS)-level OFP has become stale, with most works using data from outdated OSs. In this paper, we conduct a comprehensive fault injection campaign on an up-to-date Linux kernel and thoroughly study its behavior in the presence of faults. We then transform the data to explore and assess the predictive performance of various ML techniques for OFP. Finally, we study the influence of different OFP parameters (i.e., lead-time, prediction-window) and compare the results with existing related work. Results suggest that the various failures observed can be grouped into categories that can then be accurately predicted and distinguished by diverse ML models. ©2020 IEEE.",Dependability | Failure Prediction | Fault Injection | Machine Learning,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2020-10-01,Conference Paper,"Campos, Joao R.;Costa, Ernesto",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85097334928,10.1109/ISSRE5003.2020.00021,Modelling machine learning components for mapping and scheduling of AUTOSAR runnables,"The race towards autonomous driving provoked a paradigm shift as safety became a critical objective in the development of novel functionalities. The safety-critical part of these functionalities is predominantly realized in software complying to the AUTOSAR standard in which code fragments called runnables are configured at design-time to run according to a certain order and on a certain core. As a key technology that enables autonomous driving, machine learning is expected to play a significant role in automotive applications. Since machine learning algorithms inherently exhibit faults, e.g. a classifier's prediction is wrong with a relatively high rate, to enforce safety, fault tolerance techniques have to be used. Therefore, this paper proposes that this information is systematically used in the automatic configuration of an AUTOSAR system. Not to disrupt the usual software development process, the information is appended to already mapped and scheduled runnables. Then, a heuristic is presented to generate execution alternatives during design-time which are then selected at run-time to skip the intervals reserved for fault tolerance mechanisms in the prevailing case when no fault occurred. This novel idea considerably reduces execution time as demonstrated on real-world engine control software. ©2020 IEEE.",AUTOSAR | Machine learning | Multi-core | Safety-critical | Software fault tolerance,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2020-10-01,Conference Paper,"Copic, Milan;Leupers, Rainer;Ascheid, Gerd",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85097350140,10.1109/ISSRE5003.2020.00047,Tensorfi: A flexible fault injection framework for tensorflow applications,"As machine learning (ML) has seen increasing adoption in safety-critical domains (e.g., autonomous vehicles), the reliability of ML systems has also grown in importance. While prior studies have proposed techniques to enable efficient error-resilience (e.g., selective instruction duplication), a fundamental requirement for realizing these techniques is a detailed understanding of the application's resilience. In this work, we present TensorFI, a high-level fault injection (FI) framework for TensorFlow-based applications. TensorFI is able to inject both hardware and software faults in general TensorFlow programs. TensorFI is a configurable FI tool that is flexible, easy to use, and portable. It can be integrated into existing TensorFlow programs to assess their resilience for different fault types (e.g., faults in particular operators). We use TensorFI to evaluate the resilience of 12 ML programs, including DNNs used in the autonomous vehicle domain. The results give us insights into why some of the models are more resilient. We also present two case studies to demonstrate the usefulness of the tool. TensorFI is publicly available at https://github.com/DependableSystemsLab/TensorFI. © 2020 IEEE Computer Society. All rights reserved.",Fault Injection | Machine Learning | Resilience,"Proceedings - International Symposium on Software Reliability Engineering, ISSRE",2020-10-01,Conference Paper,"Chen, Zitao;Narayanan, Niranjhana;Fang, Bo;Li, Guanpeng;Pattabiraman, Karthik;DeBardeleben, Nathan",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85040613398,10.1109/ISSREW.2017.42,Analytics-enhanced automated code verification for dependability of software-defined networks,"Software-defined networking (SDN) is a key emerging technology that enables networks to be programmed and dynamically reconfigured through software-based network applications. This programability also significantly increases the exposure of these networks to software application faults, which can compromise or crash the underlying SDN network. It is thus imperative to detect subtle faulty or malicious behaviors of network applications prior to deployment.Automated code verification based on software model checking provides promise for early identification of such behaviors; however, it faces inherent challenges w.r.t. scalability, soft real-time behaviors, and the need for pre-specified thresholds, that significantly limits its usefulness for verifying SDN networks in practice. We describe an approach that enhances automated verification with machine learning-based analytics to detect and identify faulty or malicious behaviors that can compromise network reliability, performance, and security. A novel aspect of our work is that the analytics algorithms learn on information provided by automated verification, coupled with real-time inputs and outputs, to learn thresholds that can be used in software model checking, and to identify anomalous execution paths of network applications that may compromise the underlying SDN network. We demonstrate our approach with a proof-of-concept case study on the ONOS open-source SDN network operating system, using our customization of the Java Path Finder tool and the application of our machine learning algorithms. © 2017 IEEE.",#NAME?,"Proceedings - 2017 IEEE 28th International Symposium on Software Reliability Engineering Workshops, ISSREW 2017",2017-11-14,Conference Paper,"Jagadeesan, Lalita J.;Mendiratta, Veena",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85080930285,10.1109/ISSREW.2019.00087,Software framework for data fault injection to test machine learning systems,"Data-intensive systems are sensitive to the quality of data. Data often has problems due to faulty sensors or network problems, for instance. In this work, we develop a software framework to emulate faults in data and use it to study how machine learning (ML) systems work when the data has problems. We aim for flexibility: users can use predefined or their own dedicated fault models. Likewise, different kind of data (e.g.Text, time series, video) can be used and the system under test can vary from a single ML model to a complicated software system. Our goal is to show how data faults can be emulated and how that can be used in the study and development of ML solutions. © 2019 IEEE.",fault injection | machine learning | Testing,"Proceedings - 2019 IEEE 30th International Symposium on Software Reliability Engineering Workshops, ISSREW 2019",2019-10-01,Conference Paper,"Nurminen, Jukka K.;Halvari, Tuomas;Harviainen, Juha;Myllari, Juha;Roysko, Antti;Silvennoinen, Juuso;Mikkonen, Tommi",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85099203094,10.1109/IWAPS51164.2020.9286791,Reducing Systematic Defects using Calibre Wafer Defect Engineering and Machine Learning Solutions,"As the semiconductor manufacturing continues its march towards more advanced technology nodes, design and process introduced systematic defects become significant yield limiters [1]. Therefore, identification and characterization of these systematic defects becomes increasingly important. The design systematic defect analysis is normally done by combining both inline inspection results and physical layout (design) information. In the full flow, from preparing inspection care area to performing systematic defect root cause analysis, utilizing EDA software plays an important role. Especially with machine learning technique combining with OPC feature vector extraction, we can have a more precise analysis of the weak pattern on wafers. In this paper, we will introduce how we utilize these techniques for Process Window Qualification (PWQ), focus mainly on how we perform BFI to SEM down sampling, and full chip hotspot prediction to verify potential hotspots on PWQ wafer in order to obtain an accurate process window, and identify systematic weak patterns with increased SEM defect hit rate. © 2020 IEEE.",BFI to SEM sampling | machine learning | OPC feature vector | PWQ analysis | systematic defects,"2020 4th International Workshop on Advanced Patterning Solutions, IWAPS 2020",2020-11-05,Conference Paper,"Jiang, Jet;Hou, Frank;Li, Gavin;Chen, Summy;Fei, Marfei;Xie, Qian;Cao, Liang;Wan, Qijian;Hu, Xinyi;Du, Chunshan;Wang, David;Huang, Elven;Paninjath, Sankaranarayanan;Madhusudhan, Saikiran;Tian, Leo",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85046710212,10.1109/JBHI.2018.2835405,Learning to Detect Blue-White Structures in Dermoscopy Images With Weak Supervision,"We propose a novel approach to identify one of the most significant dermoscopic criteria in the diagnosis of cutaneous Melanoma: the blue-white structure (BWS). In this paper, we achieve this goal in a multiple instance learning (MIL) framework using only image-level labels indicating whether the feature is present or not. To this aim, each image is represented as a bag of (nonoverlapping) regions, where each region may or may not be identified as an instance of BWS. A probabilistic graphical model is trained (in MIL fashion) to predict the bag (image) labels. As output, we predict the classification label for the image (i.e., the presence or absence of BWS in each image) and we also localize the feature in the image. Experiments are conducted on a challenging dataset with results outperforming state-of-the-art techniques, with BWS detection besting competing methods in terms of performance. This study provides an improvement on the scope of modeling for computerized image analysis of skin lesions. In particular, it propounds a framework for identification of dermoscopic local features from weakly labeled data. © 2018 IEEE.",Biomedical image processing | computer aided diagnosis | dermatology | feature extraction | microscopy,IEEE Journal of Biomedical and Health Informatics,2019-03-01,Article,"Madooei, Ali;Drew, Mark S.;Hajimirsadeghi, Hossein",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85037554389,10.1109/JSYST.2017.2771349,A TAS-model-based algorithm for rule redundancy detection and scene scheduling in smart home systems,"In smart home systems, users enjoy the comfort and convenience by rule subscription and execution. However, with the increase in the complex and the number of rules, there is an increased risk of redundancy within the process of rule customization and execution. Obviously, redundancy will add weight to the system, affect the administrative operation, and reduce the system efficiency. To address the above-mentioned issues, a formal model Trigger-Actuator-Status for rules is devised. Such rules are defined as a tuple, which contains triggers, actuators, and states. Then, rules are processed with the methods of classification, combination, and analytics to generate redundant types so as to describe the redundancy relationship among rules. After that, a redundancy detection algorithm is proposed according to the determined relationship. Besides, in order to eliminate and avoid the occurrence of redundancy, redundancy for scenarios based on rule redundancy is detected and solved. The experimental results show that the proposed scheme can provide more accurate classification results and identify part or full redundancy within and among rules. Meanwhile, our scheme enables redundancy elimination in the phase of rule setting and scenario setup as well as redundancy avoidance in the stage of rule execution and scenario start-up, which significantly reduces the number of rules to greatly enhance the efficiency of the systems. © 2018 IEEE.",Dataset preprocessing | Machine learning | Software defect predict | Software metric | Software quality assurance,IEEE Systems Journal,2018-09-01,Article,"Lin, Zhaowen;Wu, Tin Yu;Sun, Yan;Xu, Jie;Obaidat, Mohammad S.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85049132394,10.1109/KBEI.2017.8324986,Fault injection framework for fault diagnosis based on machine learning in heating and demand-controlled ventilation systems,"The main purpose of a heating, ventilation and air-conditioning system is to provide thermal comfort and acceptable indoor air quality through adequate natural ventilation by exchanging the building zone air with the outside fresh air, as an option. This air exchange during the cold seasons can be considered as a heating load for the heating system which increases the energy consumption of the heating system. The use of the demand-controlled ventilation system can be counted as a potential energy saving method by its automatic adjustment of ventilation, which modifies the amount of fresh air coming from outside and causes less energy consumption by less heating load, as a result. Faults in system components such as sensors and actuators can result in different types of failures and severe implications on the efficiency of the heating and demand-controlled ventilation system. However, tracing the component and system behavior back to the faults is a challenging task. This study demonstrates a successful approach for finding the nature, value, time of occurrence, and locality of these faults using a mapping from failures to faults with knowledge-based diagnostic techniques. We introduce fault models for online diagnosis using machine learning. These fault models are established for a heating and demand-controlled ventilation system with fault injection blocks design using a simulation framework based on MATLAB/Simulink. © 2017 IEEE.",Demand-Controlled Ventilation | failure | fault detection and diagnosis | fault injection | heating modeling and simulation | machine learning | MATLAB/Simulink,"2017 IEEE 4th International Conference on Knowledge-Based Engineering and Innovation, KBEI 2017",2017-07-02,Conference Paper,"Behravan, Ali;Obermaisser, Roman;Basavegowda, Deepak Hanike;Mallak, Ahlam;Weber, Christian;Fathi, Madjid",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85006974296,10.1109/KSE.2016.7758034,Exploiting tree structures for classifying programs by functionalities,"Analyzing source code to solve software engineering problems such as fault prediction, cost, and effort estimation always receives attention of researchers as well as companies. The traditional approaches are based on machine learning, and software metrics obtained by computing standard measures of software projects. However, these methods have faced many challenges due to limitations of using software metrics which were not enough to capture the complexity of programs. The aim of this paper is to apply several natural language processing techniques, which deal with software engineering problems by exploring information of programs' abstract syntax trees (ASTs) instead of software metrics. To speed up computational time, we propose a pruning tree technique to eliminate redundant branches of ASTs. In addition, the k-Nearest Neighbor (kNN) algorithm was adopted to compare with other methods whereby the distance between programs is measured by using the tree edit distance (TED) and the Levenshtein distance. These algorithms are evaluated based on the performance of solving 104-label program classification problem. The experiments show that due to the use of appropriate data structures although kNN is a simple machine learning algorithm, the classifiers achieve the promising results. © 2016 IEEE.",,"Proceedings - 2016 8th International Conference on Knowledge and Systems Engineering, KSE 2016",2016-11-28,Conference Paper,"Phan, Viet Anh;Chau, Ngoc Phuong;Nguyen, Minh Le",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85084218746,10.1109/LATS49555.2020.9093688,Soft error reliability predictor based on a Deep Feedforward Neural Network,"Statistical fault injection is a widely used methodology to early evaluation of soft error reliability of microprocessor based systems. Due to the increasing complexity of the software and hardware stack, the simulation of faults on modern processors is becoming a computationally demanding task even for ISA-equivalent models and virtualization tools. This paper proposes and explores the use of a supervised machine learning technique, Deep Feedforward Neural Network, to design a predictor which drastically reduces the computing time of fault injection campaigns. In addition, a novel approach is presented to increase the training data from a limited set of benchmarks. Thanks to this approach, the predictor can be modeled with an extensive data set comprising not only of millions of fault injections but also thousands of different benchmarks. Experiments show promising results for estimating the applications fault tolerance when they run on state of the art ARM processor. © 2020 IEEE.",deep learning; Fault injection tool; microprocessor; radiation effects; reliability,"21st IEEE Latin-American Test Symposium, LATS 2020",2020,,"Falco D.R., Serrano-Cases A., Martinez-Alvarez A., Cuenca-Asensi S.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85084042023,10.1109/LES.2017.2783240,FIFA: Exploring a Focally Induced Fault Attack Strategy in Near-Threshold Computing,"In this letter, we explore the emerging security threats of near-threshold computing (NTC). Researchers have shown that the delay sensitivity of a circuit to supply voltage variation tremendously increases, as the circuit's operating conditions shift from traditional super-threshold values to NTC values. As a result, NTC systems become extremely vulnerable to timing fault attacks, jeopardizing trustworthy computing. Inspired by the operation of a polymorphic virus, we propose a novel threat model for NTC, referred to as a focally induced fault attack (FIFA). FIFA employs a machine learning framework to ascertain the circuit vulnerabilities and generates targeted software modules to cause a breach of end-user privacy. Our experimental results, obtained from a rigorous machine learning approach, indicate the efficacy of FIFA, in a low-power mobile platform. © 2009-2012 IEEE.",Computer security; fault diagnosis; system-on-chip,IEEE Embedded Systems Letters,2018,,"Basu P., Rajamanikkam C., Bal A., Pandey P., Carter T., Chakraborty K., Roy S.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85088525339,10.1109/MECO49872.2020.9134279,Composing Graph Theory and Deep Neural Networks to Evaluate SEU Type Soft Error Effects,"Rapidly shrinking technology node and voltage scaling increase the susceptibility of Soft Errors in digital circuits. Soft Errors are radiation-induced effects while the radiation particles such as Alpha, Neutrons or Heavy Ions, interact with sensitive regions of microelectronic devices/circuits. The particle hit could be a glancing blow or a penetrating strike. A well apprehended and characterized way of analyzing soft error effects is the fault-injection campaign, but that typically acknowledged as time and resource-consuming simulation strategy. As an alternative to traditional fault injection-based methodologies and to explore the applicability of modern graph based neural network algorithms in the field of reliability modeling, this paper proposes a systematic framework that explores gate-level abstractions to extract and exploit relevant feature representations at low-dimensional vector space. The framework allows the extensive prediction analysis of SEU type soft error effects in a given circuit. A scalable and inductive type representation learning algorithm on graphs called GraphSAGE has been utilized for efficiently extracting structural features of the gate-level netlist, providing a valuable database to exercise a downstream machine learning or deep learning algorithm aiming at predicting fault propagation metrics. Functional Failure Rate (FFR): the predicted fault propagating metric of SEU type fault within the gate-level circuit abstraction of the 10-Gigabit Ethernet MAC (IEEE 802.3) standard circuit. © 2020 IEEE.",Deep Neural Networks | Functional Failure Rate (FFR) | Gate-level Circuit Abstraction | GraphSAGE (Graph Based Neural Network) | Single Event Transient (SET) and Soft Errors | Single Event Upset (SEU),"2020 9th Mediterranean Conference on Embedded Computing, MECO 2020",2020-06-01,Conference Paper,"Balakrishnan, Aneesh;Lange, Thomas;Glorieux, Maximilien;Alexandrescu, Dan;Jenihhin, Maksim",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85091395185,10.1109/MECO49872.2020.9134336,"Decentralized, Secure and Cognitive Architecture for Automotive CyberPhysical System of Systems","In this paper, a holistic, decentralized and cognitive design and operation approach that supports CyberPhysical System of Systems (CPSoS) autonomic (without human intervention) behavior, making such systems aware of their physical and cyber environment and reacting to it accordingly so that they constantly match their intended purpose. We propose using a model based design approach to describe a CPSoS in a holistic and abstract way and to allocate computational power/resources to the CPS end devices of the System by determining and generating autonomously what cyber-physical processes will be handled by a device's each heterogenous component (processor cores, GPUs, FPGA fabric) and software components (software stacks). The proposed solution uses this methodology to strengthen reliability, fault tolerance and security at system level but also to support CPS designs that work in a decentralized way, collaboratively, in an equilibrium, by sharing tasks and data with minimal central intervention. Also, the proposed system supports the interaction of the CPSoS with their human users/operators through extended reality modules (AR glasses, haptics interfaces) to increase human situational awareness but also to include human behavior in the CPSoS design and operation phase. The proposal key points are highlighted in this paper and their usage in an automotive use case that involves connected cars is presented. © 2020 IEEE.",CyberPhysical Systems of Systems; Machine Learning; Security,"2020 9th Mediterranean Conference on Embedded Computing, MECO 2020",2020,,"Fournaris A.P., Lalos A., Kapsalas P., Koulamas C.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85075542870,10.1109/MIS.2019.2918092,A Data-analytics Approach for Enterprise Resilience,"Enterprise resilience plays an important role to prevent business services from disruptions caused by human-induced disasters such as failed change implementations and software bugs. Traditional expert-centric approach has difficulty to maintain continued critical business functions because the disasters can often only be handled after their occurrence. This paper introduces a data-analytics approach, which leverages system monitoring data for the enterprise resilience. With the power of data mining and machine learning techniques, we build an intelligent business analytics system to detect the potential disruptions proactively, and to assist the operational team for enterprise resilience enhancement. We demonstrate the effectiveness of our approach on a real enterprise system monitoring dataset in simulation. IEEE",data mining | enterprise resilience | machine learning | predictive data-analytics,IEEE Intelligent Systems,2019-05-01,Article,"Xu, Donna;Tsang, Ivor W.;Chew, Eng K.;Siclari, Cosimo;Kaul, Varun",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85050998097,10.1109/MLDS.2017.15,An Empirical Investigation to Overcome Class-Imbalance in Inspection Reviews,Background: software inspection results in reviews that report the presence of faults. Requirements author must manually read through the reviews and differentiate between true-faults and false-positives. Problem: post-inspection decisions (fault or nonfault) are difficult and time consuming. It is difficult to employ machine learning (ML) techniques directly to raw (unstructured) data because of class imbalance problem and possible fault-slippage through misclassification of fault. Aim: The aim of this research is to solve this problem with the help of ensemble approach and priority analysis to achieve significant accuracy in determining true-fault and false-positive reviews without losing any listed fault. Method: We conducted empirical experiment using two trained models (with reviews from inspection domain vs. movies domain) to address class imbalance problem. Our approach uses ensemble methods to develop classification confidence of inspection reviews and assigns them to appropriate priority class. Results: The results showed that movies trained model performed better than inspection trained and restricted any possible fault-slippage. © 2017 IEEE.,class imbalance | ensemble | Fault priority | fault slippage | inspections reviews | machine learning | part of speech | sampling,"Proceedings - 2017 International Conference on Machine Learning and Data Science, MLDS 2017",2017-07-02,Conference Paper,"Singh, Maninder;Walia, Gursimran S.;Goswami, Anurag",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85051007543,10.1109/MLDS.2017.16,Validation of Inspection Reviews over Variable Features Set Threshold,Background: Mining software requirement reviews involve natural language processing (NLP) to efficiently validate a true-fault as useful and false-positive as non-useful. Aim: The aim of this paper is to evaluate our proposed mining approach to automate the validation of requirement reviews generated during an inspection of NL requirements document. Method: Our approach utilized two training models; one from requirement reviews and other from online movies. We conducted an empirical study to test our approach using part of speech (POS) against these two trained models and observed trends w.r.t. F-measure and G-mean along with percentage of features used to train two models. Results: The results showed that using training reviews from two different domains report similar trend across evaluation metrics. Our results show that the most stable and promising validation results for F-measure and G-mean are obtained when a model over inspection and movies reviews are trained using feature set threshold value 65% and 45% respectively. © 2017 IEEE.,class imbalance | faults | feature sets | inspection reviews | machine learning | part of speech | sampling,"Proceedings - 2017 International Conference on Machine Learning and Data Science, MLDS 2017",2017-07-02,Conference Paper,"Singh, Maninder;Walia, Gursimran S.;Goswami, Anurag",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84991769772,10.1109/MMAR.2016.7575171,Machine learning based power quality event classification using wavelet - Entropy and basic statistical features,"Today's industrial environment is smarter than ever before. Most production lines include electrical devices which are able to communicate each other and controlled from a single station with automation systems. Most of those elements have an internet connection link known as industrial internet. Development of smart technology with industrial internet comes with a need of monitoring. Monitoring technologies are emergent systems that focus on fault detection, grid self - healings and online tracking of power quality issues. Present study deals with one of the essential part of an electricity grid monitoring system called power quality event classification in a manner of machine learning topic. Power quality events to be processed are generated synthetically by means of a comprehensive software tool. Classification of real-like dataset is executed using extreme learning machine which is an extremely fast learning algorithm applied to single layer neural networks. Basic statistical criteria and wavelet - entropy methods are handled to achieve distinctive features of dataset. As a performance evaluation instrument, conventional artificial neural network structure is run too. Detailed results are discussed to prove the satisfactory performance of proposed pattern recognition model. © 2016 IEEE.",extreme learning machine | pattern recognition | power quality events | smart grid | wavelet transform,"2016 21st International Conference on Methods and Models in Automation and Robotics, MMAR 2016",2016-09-22,Conference Paper,"Ucar, Ferhat;Alcin, Omer Faruk;Dandil, Besir;Ata, Fikret",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85112588113,10.1109/MNET.011.2000552,Making a Case for Federated Learning in the Internet of Vehicles and Intelligent Transportation Systems,"With the incoming introduction of 5G networks and the advancement in technologies such as network function virtualization and software defined networking, new and emerging networking technologies and use cases are taking shape. One such technology is the Internet of Vehicles (IoV), which describes an interconnected system of vehicles and infrastructure. Coupled with recent developments in artificial intelligence and machine learning, IoV is transformed into an intelligent transportation system (ITS). There are, however, several operational considerations that hinder the adoption of ITSs, including scalability, high availability, and data privacy. To address these challenges, federated learning, a collaborative and distributed intelligence technique, is suggested. Through an ITS case study, the ability of a federated model deployed on roadside infrastructure throughout the network to recover from faults by leveraging group intelligence while reducing recovery time and restoring acceptable system performance is highlighted. With a multitude of use cases and benefits, federated learning is a key enabler for ITS and is poised to achieve widespread implementation in 5G and beyond networks and applications. © 1986-2012 IEEE.",,IEEE Network,2021-05-01,Article,"Manias, DImitrios Michael;Shami, Abdallah",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85067804872,10.1109/MS.2019.2923408,Can a Machine Learn through Customer Sentiment?: A Cost-Aware Approach to Predict Support Ticket Escalations,"Given the connection between customer happiness and support ticket escalation, we describe an approach that 1) analyzes the emotions in conversations between a customer and a support analyst and 2) provides organizations with a cost-based mechanism to evaluate machine-learning algorithms trained on emotion-related features to predict support ticket escalations. © 1984-2012 IEEE.",Affective Computing | cost-sensitive learning | data mining | defect escalation | Machine learning | machine learning | Modeling and prediction | Natural language | Sentiment analysis | software defect escalation prediction,IEEE Software,2019-09-01,Article,"Werner, Colin;Li, Ze Shi;Damian, Daniela",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85111024778,10.1109/MS.2021.3098670,Ground truth deficiencies in software engineering: when codifying the past can be counterproductive,"Many software engineering tools build and evaluate their models based on historical data to support development and process decisions. These models help us answer numerous interesting questions, but have their own caveats. In a real-life setting, the objective function of human decision-makers for a given task might be influenced by a whole host of factors that stem from their cognitive biases, subverting the ideal objective function required for an optimally functioning system. Relying on this data as ground truth may give rise to systems that end up automating software engineering decisions by mimicking past sub-optimal behaviour. We illustrate this phenomenon and suggest mitigation strategies to raise awareness. IEEE",,IEEE Software,2022-01-01,Article,"Tuzun, Eray;Erdogmus, Hakan;Baldassarre, Maria Teresa;Felderer, Michael;Feldt, Robert;Turhan, Burak",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84957103031,10.1109/MSR.2015.16,Using developer-interaction trails to triage change requests,"The paper presents an approach, namely iHDev, to recommend developers who are most likely to implement incoming change requests. The basic premise of iHDev is that the developers who interacted with the source code relevant to a given change request are most likely to best assist with its resolution. A machine-learning technique is first used to locate source code entities relevant to the textual description of a given change request. Ihdev then mines interaction trails (i.e., Mylyn sessions) associated with these source code entities to recommend a ranked list of developers. Ihdev integrates the interaction trails in a unique way to perform its task, which was not investigated previously. An empirical study on open source systems Mylyn and Eclipse Project was conducted to assess the effectiveness of iHDev. A number of change requests were used in the evaluated bench-mark. Recall for top one to five recommended developers and Mean Reciprocal Rank (MRR) values are reported. Furthermore, a comparative study with two previous approaches that use commit histories and/or the source code authorship information for developer recommendation was performed. Results show that iHDev could provide a recall gain of up to 127.27% with equivalent or improved MRR values by up to 112.5%. © 2015 IEEE.",Computer bugs | Context | Data mining | History | Mathematical model | Software | XML,IEEE International Working Conference on Mining Software Repositories,2015-08-04,Conference Paper,"Zanjani, Motahareh Bahrami;Kagdi, Huzefa;Bird, Christian",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85026548340,10.1109/MSR.2017.53,Predicting likelihood of requirement implementation within the planned iteration: An empirical study at IBM,"There has been a significant interest in the estimation of time and effort in fixing defects among both software practitioners and researchers over the past two decades. However, most of the focus has been on prediction of time and effort in resolving bugs, without much regard to predicting time needed to complete high-level requirements, a critical step in release planning. In this paper, we describe a mixed-method empirical study on three large IBM projects in which we developed and evaluated a process of training a predictive model constituting a set of 29 features in nine categories in order to predict if a requirement will be completed within its planned iteration. We conducted feature engineering through iterative interviews with IBM practitioners as well as analysis of large development repositories of these three projects. Using machine learning techniques, we were able to make predictions on completion time of requirements at four different stages of their lifetime. Using our industrial partner's interest in high precision over recall, we then adopted a cost sensitive learning method and maximized precision of predictions (ranging from 0.8 to 0.97) while maintaining an acceptable recall. We also ranked the features based on their relative importance to the optimized predictive model. We show that although satisfying predictions can be made at early stages, performance of predictions improves over time by taking advantage of requirements' progress data. Furthermore, feature importance ranking results show that although importance of features are highly dependent on project and prediction stage, there are certain features (e.g. requirement creator, time remained to the end of iteration, time since last requirement summary change and number of times requirement has been replanned for a new iteration) that emerge as important across most projects and stages, implying future worthwhile research directions for both researchers and practitioners. © 2017 IEEE.",Completion Time Prediction | Machine Learning | Mining Software Repositories | Release Planning,IEEE International Working Conference on Mining Software Repositories,2017-06-29,Conference Paper,"Dehghan, Ali;Neal, Adam;Blincoe, Kelly;Linaker, Johan;Damian, Daniela",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85072306720,10.1109/MSR.2019.00052,What do developers know about machine learning: A study of ML discussions on stackoverflow,"Machine learning, a branch of Artificial Intelligence, is now popular in software engineering community and is successfully used for problems like bug prediction, and software development effort estimation. Developers' understanding of machine learning, however, is not clear, and we require investigation to understand what educators should focus on, and how different online programming discussion communities can be more helpful. We conduct a study on Stack Overflow (SO) machine learning related posts using the SOTorrent dataset. We found that some machine learning topics are significantly more discussed than others, and others need more attention. We also found that topic generation with Latent Dirichlet Allocation (LDA) can suggest more appropriate tags that can make a machine learning post more visible and thus can help in receiving immediate feedback from sites like SO. © 2019 IEEE.",Machine learning | Stackoverflow | Topic modeling,IEEE International Working Conference on Mining Software Repositories,2019-05-01,Conference Paper,"Bangash, Abdul Ali;Sahar, Hareem;Chowdhury, Shaiful;Wong, Alexander William;Hindle, Abram;Ali, Karim",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85072331325,10.1109/MSR.2019.00073,Style-analyzer: Fixing code style inconsistencies with interpretable unsupervised algorithms,"Source code reviews are manual, time-consuming, and expensive. Human involvement should be focused on analyzing the most relevant aspects of the program, such as logic and maintainability, rather than amending style, syntax, or formatting defects. Some tools with linting capabilities can format code automatically and report various stylistic violations for supported programming languages. They are based on rules written by domain experts, hence, their configuration is often tedious, and it is impractical for the given set of rules to cover all possible corner cases. Some machine learning-based solutions exist, but they remain uninterpretable black boxes. This paper introduces style-analyzer, a new open source tool to automatically fix code formatting violations using the decision tree forest model which adapts to each codebase and is fully unsupervised. style-analyzer is built on top of our novel assisted code review framework, Lookout. It accurately mines the formatting style of each analyzed Git repository and expresses the found format patterns with compact human-readable rules. style-analyzer can then suggest style inconsistency fixes in the form of code review comments. We evaluate the output quality and practical relevance of style-analyzer by demonstrating that it can reproduce the original style with high precision, measured on 19 popular JavaScript projects, and by showing that it yields promising results in fixing real style mistakes. style-analyzer includes a web application to visualize how the rules are triggered. We release style-analyzer as a reusable and extendable open source software package on GitHub for the benefit of the community. © 2019 IEEE.",Assisted code review | Code style | Decision tree forest | Interpretable machine learning,IEEE International Working Conference on Mining Software Repositories,2019-05-01,Conference Paper,"Markovtsev, Vadim;Long, Waren;Mougard, Hugo;Slavnov, Konstantin;Bulychev, Egor",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85069945166,10.1109/MysuruCon52639.2021.9641090,A Systematic Study on Machine Learning Techniques for Predicting Software Faults,"Fault tolerance and prediction are some of the interesting quality assurance activities of software engineering. Fault prediction algorithms assist the project manager in identifying the parts of the system that are prone to failure and provide the benefit of reducing the time required to test the entire application. The fault-prediction method attempts to detect faulty software modules so that they are used subsequently in the software development process. There are numerous prediction approaches available in the domain of software engineering for detecting defect-prone blocks. Few of them are stable models, and a few are unstable models, which may not be enough to handle all of the possible scenarios and thus fail to produce the earliest prediction. This paper provides an extensive review of existing predictive models for software defect prediction, identify flaws in existing techniques, and address the need for a new prediction approach in the era of fault prediction. The Classification was commonly used to differentiate among faulty and non-faulty modules. This paper proposes two sub modules for prediction of faults. One submodule to identify the accuracy are implemented and discussed. To achieve better and more accurate results, we intend to build a machine learning model, which can automate the complete process. © 2021 IEEE.",classification; evaluation model; fault prediction; faulty systems; machine learning; object-oriented; prediction metrics,"2021 IEEE Mysore Sub Section International Conference, MysuruCon 2021",2021,,"Shathish Kumar T., Booba B.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85080874348,10.1109/NAPS46351.2019.8999972,Arcing Fault Detection with Interpretable Learning Model under the Integration of Renewable Energy,"Under the trend of deeper renewable energy integration, active distribution networks are facing increasing uncertainty and security issues, among which the arcing fault detection (AFD) has baffled researchers for years. Existing machine learning based AFD methods are deficient in feature extraction and model interpretability. To overcome these limitations in learning algorithms, we have designed a way to translate the non-transparent machine learning prediction model into an implementable logic for AFD. Moreover, the AFD logic is tested under different fault scenarios and realistic renewable generation data, with the help of our self-developed AFD software. The performance from various tests shows that the interpretable prediction model has high accuracy, dependability, security and speed under the integration of renewable energy. © 2019 IEEE.",Arcing Fault Detection | Distribution Networks | Power System Protection | Renewable Energy,"51st North American Power Symposium, NAPS 2019",2019-10-01,Conference Paper,"Hashmy, Yousaf;Cui, Qiushi;Ma, Zhihao;Weng, Yang",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85123493289,10.1109/NorCAS53631.2021.9599646,Machine Learning Approach for Accelerating Simulation-based Fault Injection,"Simulation-based fault injection is an approach, which is usually conducted in the design phase to evaluate the reliability of circuits. However, this approach is frequently time-consuming, especially for large designs. This paper explores the possibilities of applying machine learning (ML) as enhancement to the fault injection to reduce the runtime of the fault verification. The proposed approach separates the target gates for fault injection into two subsets. Standard simulation-based fault injection is only performed for first subset to get the result of fault propagation in the circuit, and enables machine learning process. Based on such partial classical fault injection campaign, a dataset is built to train and validate machine learning models that are able to predict the fault injection result of the remaining gates. The machine learning approach is based on a set of relevant features that could be easily extracted based on the netlist. The dataset in this study, used to validate the approach, is obtained from the ADC digital core. We have evaluated this netlist and provided ML-driven assessment whether the fault injection in some gate will propagate to primary outputs. We have evaluated several machine learning models with different loss functions. The best prediction accuracy is 95% and achieved by a 4 layer neural network with weighted Cross Entropy. © 2021 IEEE.",Logistic Regression | Machine Learning | Neural Networks | Simulation-based Fault Injection,"2021 IEEE Nordic Circuits and Systems Conference, NORCAS 2021 - Proceedings",2021-01-01,Conference Paper,"Lu, Li;Chen, Junchao;Breitenreiter, Anselm;Schrape, Oliver;Ulbricht, Markus;Krstic, Milos",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85015690290,10.1109/PHM.2016.7819780,Prognostics and health management solution development in LabVIEW: Watchdog agent® toolkit and case study,"Prognostics and Health Management (PHM) research has been intensively studied in various industries in order to improve the overall performance of the critical assets and avoid unexpected failure. For data-driven approach, many data analysis methods have been applied for PHM system development including digital signal processing, data mining, machine learning and pattern recognition. And three major goals for PHM system, including health assessment, fault diagnosis and remaining useful life prediction, are achieved by combining different algorithms. However, the challenges for industry are how to efficiently select appropriate tools to develop a suitable PHM solution and how to quickly demonstrate PHM concepts for different applications. The concept of a reconfigurable algorithm toolset titled the Watchdog Agent® for PHM was first presented in 2003 and now is a commercialized toolbox in LabVIEW. The Watchdog Agent® toolbox consists of selected tools/algorithms from four categories: signal processing, health assessment, fault diagnosis and remaining useful life prediction. LabVIEW, a system design software developed by National Instruments, has been used for measurement, testing, control and data analytics in various areas including wind energy, automobile manufacturing, aerospace, etc. This paper first presents an introduction about Watchdog Agent® (WDA) Toolkit and then provides a systematic approach for PHM solutions development in LabVIEW environment. A detailed discussion about data acquisition, data pre-processing, feature extraction, model training and result visualization are provided with case studies. © 2016 IEEE.",LabVIEW | PHM | Reconfigurable | System development | Systematic approach | Watchdog aegnt toolkit,"Proceedings of 2016 Prognostics and System Health Management Conference, PHM-Chengdu 2016",2017-01-16,Conference Paper,"Shi, Zhe;Lee, Jay;Cui, Peng",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85055755951,10.1109/PHOSST.2018.8456700,Machine Learning-Assisted Optical Performance Monitoring in Fiber-Optic Networks,"We review machine learning (ML)-based optical performance monitoring (OPM) techniques in optical communications. Recent applications of ML-Assisted OPM in different aspects of fiber-optic networking including cognitive fault detection and management, network equipment failure prediction, and dynamic planning and optimization of software-defined networks are also discussed. © 2018 IEEE.",fiber-optic networks; machine learning; Optical performance monitoring; software-defined networks,"IEEE Photonics Society Summer Topicals Meeting Series, SUM 2018",2018,,"Khan F.N., Fan Q., Lu C., Lau A.P.T.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85078493174,10.1109/PRDC47002.2019.00058,On the diversity of machine learning models for system reliability,"The diversity of system components is one of the important contributing factors of reliable and secure software systems. In a software fault-tolerant system using diverse versions of software components, a component failure caused by defects or malicious attacks can be covered by other versions. Machine learning systems can also benefit from such a multi-version approach to improve the system reliability. Nevertheless, there are few studies addressing this issue. In this paper, we experimentally analyze how outputs of machine learning modules can be diversified by using different versions of machine learning algorithms, neural network architectures and perturbated input data. The experiments are conducted on image classification tasks of MNIST data set and Belgian Traffic Sign data set. Different neural network architectures, support vector machines and random forests are used for constructing diverse machine learning models. The diversity is characterized by the coverage of errors over the test samples. We observe that the different machine learning models have quite different error coverages that can be leveraged for system reliability design. Based on the experimental results, we construct the reliability model for three-version machine learning architecture with a diversity measure defined as the intersection of error spaces in the sample space. From the presented reliability model, we derive a necessary condition under which three-version architecture achieves a higher system reliability than a single machine learning module. © 2019 IEEE.",Diversity | Image classification | Machine learning | Reliability | Software fault-tolerance,"Proceedings of IEEE Pacific Rim International Symposium on Dependable Computing, PRDC",2019-12-01,Conference Paper,"Machida, Fumio",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85015293873,10.1109/PyHPC.2016.014,Mrs: High performance MapReduce for iterative and asynchronous algorithms in python,"Mrs [1] is a lightweight Python-based MapReduce implementation designed to make MapReduce programs easy to write and quick to run, particularly useful for research and academia. A common set of algorithms that would benefit from Mrs are iterative algorithms, like those frequently found in machine learning; however, iterative algorithms typically perform poorly in the MapReduce framework, meaning potentially poor performance in Mrs as well.Therefore, we propose four modifications to the original Mrs with the intent to improve its ability to perform iterative algorithms. First, we used direct task-to-task communication for most iterations and only occasionally write to a distributed file system to preserve fault tolerance. Second, we combine the reduce and map tasks which span successive iterations to eliminate unnecessary communication and scheduling latency. Third, we propose a generator-callback programming model to allow for greater flexibility in the scheduling of tasks. Finally, some iterative algorithms are naturally expressed in terms of asynchronous message passing, so we propose a fully asynchronous variant of MapReduce.We then demonstrate Mrs' enhanced performance in the context of two iterative applications: particle swarm optimization (PSO), and expectation maximization (EM). © 2016 IEEE.",High-level parallel programming frameworks | Iterative algorithms | MapReduce,"Proceedings of PyHPC 2016: 6th Workshop on Python for High-Performance and Scientific Computing - Held in conjunction with SC16: The International Conference for High Performance Computing, Networking, Storage and Analysis",2017-01-30,Conference Paper,"Lund, Jeffrey;Ashcraft, Chace;McNabb, Andrew;Seppi, Kevin",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84991824134,10.1109/QRS-C.2016.8,Test-Suite Reduction Does Not Necessarily Require Executing the Program under Test,"Removing redundancies from test-suites is an important task of software testing in order to keep test-suites as small as possible, but not to harm the test-suite's fault detection capabilities. A straightforward algorithm for test-suite reduction would select elements of the test-suite randomly and remove them if and only if the reduced test-suite fulfills the same or similar coverage or mutation score. Such algorithms rely on the execution of the program and the repeated computation of coverage or mutation score. In this paper, we present an alternative approach that purely relies on a model learned from the original test-suite without requiring the execution of the program under test. The idea is to remove those tests that do not change the learned model. In order to evaluate the approach we carried out an experimental study showing that reductions of 60-99% are possible while still keeping coverage and mutation score almost the same. © 2016 IEEE.",Coverage | Machine learning | Mutation Score | Redundancy | Software testing,"Proceedings - 2016 IEEE International Conference on Software Quality, Reliability and Security-Companion, QRS-C 2016",2016-09-21,Conference Paper,"Felbinger, Hermann;Wotawa, Franz;Nica, Mihai",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85052536544,10.1109/QRS-C.2018.00019,Defect Prediction Based on the Characteristics of Multilayer Structure of Software Network,"Software defect prediction can help us identify software defect modules and improve software quality. The existing defect prediction mainly analyzes the software code or the development process and uses the statistical feature data on the files or categories related to the software defects as the metrics. The method disregards the macroscopic integrity of software programs and the relevance of local defects to the surrounding program elements. For this reason, this study introduces a complex network technology into defect prediction, establishes a software network model, uses a complex network metric to design a set of metrics that can reflect the local and global features of defects, and proposes a dynamic prediction model optimization method based on the threshold filter algorithm. The effectiveness of the proposed metric and method is verified through comparison with the Predictive Model in Software Engineering dataset experiment and a practical engineering software data prediction experiment. © 2018 IEEE.",Complex network | Defect prediction | Machine learning | Software metrics,"Proceedings - 2018 IEEE 18th International Conference on Software Quality, Reliability, and Security Companion, QRS-C 2018",2018-08-09,Conference Paper,"Yang, Yiwen;Ai, Jun;Wang, Fei",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85099358626,10.1109/QRS-C51114.2020.00014,Online Prediction of Server Crash Based on Running Data,"For web servers, the most typical and common failure is that the client's web page requests surge in a certain period of time, resulting in the server's collapse under extreme pressure. The early warning of server crash time provides the possibility to avoid crash loss. In this paper, we propose an early-warning method for Web service failure. Firstly, we obtain the key nodes on the running path of the server, collect and analyze the running status data of the path under various pressures through program instrumentation, and inform the occurrence of failure in advance through LSTM-SVM (the algorithm combining SVM and LSTM). We apply this method to Nginx, a widely used server, and the accuracy of crash warning is over 95%. Experiments show that the method of acquiring target data has little effect on the performance of server, achieves high degree of automation, and realizes high-precision fault warning. The innovation of this paper is that we realize the fault warning through the change of the parameters in the software. © 2020 IEEE.",machine learning | online prediction | server crash | software reliability,"Proceedings - Companion of the 2020 IEEE 20th International Conference on Software Quality, Reliability, and Security, QRS-C 2020",2020-12-01,Conference Paper,"Zou, Zhuoliang;Ai, Jun",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85099374821,10.1109/QRS-C51114.2020.00078,A Threat Analysis Methodology for Security Requirements Elicitation in Machine Learning Based Systems,"Machine learning (ML) models are now a key component for many applications. However, machine learning based systems (MLBSs), those systems that incorporate them, have proven vulnerable to various new attacks as a result. Currently, there exists no systematic process for eliciting security requirements for MLBSs that incorporates the identification of adversarial machine learning (AML) threats with those of a traditional non-MLBS. In this research study, we explore the applicability of traditional threat modeling and existing attack libraries in addressing MLBS security in the requirements phase. Using an example MLBS, we examined the applicability of 1) DFD and STRIDE in enumerating AML threats; 2) Microsoft SDL AI/ML Bug Bar in ranking the impact of the identified threats; and 3) the Microsoft AML attack library in eliciting threat mitigations to MLBSs. Such a method has the potential to assist team members, even with only domain specific knowledge, to collaboratively mitigate MLBS threats. © 2020 IEEE.",Adversarial Machine Learning | Attack Libraries | Model Inference and Perturbation and Evasion Attacks | Requirements Elicitation Using Threat Modeling | Security Requirements Engineering | STRIDE,"Proceedings - Companion of the 2020 IEEE 20th International Conference on Software Quality, Reliability, and Security, QRS-C 2020",2020-12-01,Conference Paper,"Wilhjelm, Carl;Younis, Awad A.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84962129143,10.1109/QRS.2015.25,An Anomaly Detection System Based on Ensemble of Detectors with Effective Pruning Techniques,"Anomaly detection systems rely on machine learning techniques to model the normal behavior of the system. This model is used during operation to detect anomalies due to attacks or design faults. Ensemble methods have been used to improve the overall detection accuracy by combining the outputs of several accurate and diverse models. Existing Boolean combination techniques either require an exponential number of combinations or sequential combinations that grow linearly with the number of iterations, which make them difficult to scale up and analyze. In this paper, we propose PBC (Pruning Boolean Combination), an efficient approach for selecting and combining anomaly detectors. PBC relies on two novel pruning techniques that we have developed to aggressively prune redundant and trivial detectors. Compared to existing work, PBC reduces significantly the number of detectors to combine, while keeping similar accuracy. We show the effectiveness of PBC when applying it to a large dataset. © 2015 IEEE.",Anomaly Detection Systems | Boolean Combination | Intrusion Detection Systems | Multiple-Detector Systems | Pruning Techniques,"Proceedings - 2015 IEEE International Conference on Software Quality, Reliability and Security, QRS 2015",2015-09-21,Conference Paper,"Soudi, Amirreza;Khreich, Wael;Hamou-Lhadj, Abdelwahab",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84961328724,10.1109/QRS.2016.30,Using a Distributed Representation of Words in Localizing Relevant Files for Bug Reports,"Once a bug in software is reported, developers have to determine which source files are related to the bug. This process is referred to as bug localization, and an automatic way of bug localization is important to improve developers' productivity. This paper proposes an approach called DrewBL to efficiently localize faulty files for a given bug report using a natural language processing tool, word2vec. In DrewBL, we first build a vector space model named semantic-VSM which represents a distributed representation of words in the bug report and source code files and next compute the relevance between them by feeding the constructed model to word2vec. We also present an approach called CombBL to further improve the accuracy of bug localization which employs not only the proposed DrewBL but also existing bug localization techniques, such as BugLocator based on textual similarity and Bugspots based on bug-fixing history, in a combinational manner. This paper gives our early experimental results to show the effectiveness and efficiency of the proposed approaches using two open source projects. © 2016 IEEE.",Bug localization; Bug report; Information Retrieval; Machine learning; Natural language processing; Vector space model,"Proceedings - 2016 IEEE International Conference on Software Quality, Reliability and Security, QRS 2016",2016,,"Uneno Y., Mizuno O., Choi E.-H.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84937701467,10.1109/QRS.2018.00044,Manifesting bugs in machine learning code: An explorative study with mutation testing,"Nowadays statistical machine learning is widely adopted in various domains such as data mining, image recognition and automated driving. However, software quality assurance for machine learning is still in its infancy. While recent efforts have been put into improving the quality of training data and trained models, this paper focuses on code-level bugs in the implementations of machine learning algorithms. In this explorative study we simulated program bugs by mutating Weka implementations of several classification algorithms. We observed that 8%-40% of the logically non-equivalent executable mutants were statistically indistinguishable from their golden versions. Moreover, other 15%-36% of the mutants were stubborn, as they performed not significantly worse than a reference classifier on at least one natural data set. We also experimented with several approaches to killing those stubborn mutants. Preliminary results indicate that bugs in machine learning code may have negative impacts on statistical properties such as robustness and learning curves, but they could be very difficult to detect, due to the lack of effective oracles. © 2018 IEEE.",Explorative study; Machine learning programs; Mutation testing,"Proceedings - 2018 IEEE 18th International Conference on Software Quality, Reliability, and Security, QRS 2018",2018,,"Cheng D., Cao C., Xu C., Ma X.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85073795911,10.1109/QRS.2019.00059,TFCheck : A TensorFlow Library for Detecting Training Issues in Neural Network Programs,"The increasing inclusion of Machine Learning (ML) models in safety-critical systems like autonomous cars have led to the development of multiple model-based ML testing techniques. One common denominator of these testing techniques is their assumption that training programs are adequate and bug-free. These techniques only focus on assessing the performance of the constructed model using manually labeled data or automatically generated data. However, their assumptions about the training program are not always true as training programs can contain inconsistencies and bugs. In this paper, we examine training issues in ML programs and propose a catalog of verification routines that can be used to detect the identified issues, automatically. We implemented the routines in a Tensorflow-based library named TFCheck. Using TFCheck, practitioners can detect the aforementioned issues automatically. To assess the effectiveness of TFCheck, we conducted a case study with real-world, mutants, and synthetic training programs. Results show that TFCheck can successfully detect training issues in ML code implementations. © 2019 IEEE.",Deep Learning | TensorFlow program | Testing | Training issues,"Proceedings - 19th IEEE International Conference on Software Quality, Reliability and Security, QRS 2019",2019-07-01,Conference Paper,"Braiek, Houssem Ben;Khomh, Foutse",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85099293301,10.1109/QRS51102.2020.00041,Pull Request Prioritization Algorithm based on Acceptance and Response Probability,"Pull requests (PRs) prioritization is one of the main challenges faced by integrators in pull-based development. This is especially true for large open-source projects where hundreds of pull requests are submitted daily. Indeed, managing these pull requests manually consumes time and resources and may lead to delays in the reaction (i.e., acceptance or response) to enhancements or bug fixes suggested in the codebase by contributors. We propose an approach, called AR-Prioritizer (Acceptance and Response based Prioritizer), integrating a PRs prioritization mechanism that considers these two aspects. The results of our study demonstrate that our approach can recommend top@5, top@10, and top@20 most likely to be accepted and responded pull requests with Mean Average Precision of 95.3%, 89.6%, and 79.6% and Average Recall of 40%, 65.7%, and 92.9%. Moreover, AR-Prioritizer has outperformed the baseline models with a statistical significance in prioritizing the most likely to be accepted and responded to PRs. © 2020 IEEE.",machine learning models | prioritization | Pull requests | pull-based development,"Proceedings - 2020 IEEE 20th International Conference on Software Quality, Reliability, and Security, QRS 2020",2020-12-01,Conference Paper,"Azeem, Muhammad Ilyas;Peng, Qiang;Wang, Qing",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85061305992,10.1109/QUATIC.2018.00050,Improving la redoute's CI/CD pipeline and devops processes by applying machine learning techniques,"The complexity inherent to software development and maintenance - not only in technical terms, but also from a human perspective - entails challenges that can be addressed as learning problems. Machine learning techniques may be employed as tools to gain insight about strategies that can lead to the improvement of the quality of software processes and products. Defect proneness prediction, in particular, may be identified as an active research field. As stated by DevOps guidelines, the possibility of obtaining quick feedback allows teams to operate in an agile mode in which communication, decision taking and problem solving are expeditious, allowing companies to boost business value. This paper describes ongoing research for applying machine learning techniques to improve the quality of processes and products inside the DevOps pipeline of the La Redoute's IT department. © 2018 IEEE.",CI/CD Pipeline | DevOps | Machine Learning | Software Defect Prediction | Software Development Life Cycle | Software Quality,"Proceedings - 2018 International Conference on the Quality of Information and Communications Technology, QUATIC 2018",2018-12-26,Conference Paper,"Nogueira, Ana Filipa;Ribeiro, José C.B.;Zenha-Rela, Mario;Craske, Antoine",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84886707767,10.1109/RAISE.2013.6615204,Handling missing attributes using matrix factorization,Predictive models that use machine learning techniques has been useful tools to guide software project managers in making decisions under uncertainty. However in practice collecting metrics or defect data has been a troublesome job and researchers often have to deal with incomplete datasets in their studies. As a result both researchers and practitioners shy away from implementing such models. Missing data is a common problem in other domains to build recommender systems. We believe that the techniques used to overcome missing data problem in other domains can also be employed in software engineering. In this paper we propose Matrix Factorization algorithm to tackle with missing data problem in building predictive models in software development domain. © 2013 IEEE.,matrix factorization | missing data | Software defect prediction,"2013 2nd International Workshop on Realizing Artificial Intelligence Synergies in Software Engineering, RAISE 2013 - Proceedings",2013-11-04,Conference Paper,"Bozcan, Övünç;Bener, Ayşe Başar",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85090454422,10.1109/RAMS48030.2020.9153718,Architecture-based software reliability incorporating fault tolerant machine learning,"With the increased interest to incorporate machine learning into software and systems, methods to characterize the impact of the reliability of machine learning are needed to ensure the reliability of the software and systems in which these algorithms reside. Towards this end, we build upon the architecture-based approach to software reliability modeling, which represents application reliability in terms of the component reliabilities and the probabilistic transitions between the components. Traditional architecture-based software reliability models consider all components to be deterministic software. We therefore extend this modeling approach to the case, where some components represent learning enabled components. Here, the reliability of a machine learning component is interpreted as the accuracy of its decisions, which is a common measure of classification algorithms. Moreover, we allow these machine learning components to be fault-tolerant in the sense that multiple diverse classifier algorithms are trained to guide decisions and the majority decision taken. We demonstrate the utility of the approach to assess the impact of machine learning on software reliability as well as illustrate the concept of reliability growth in machine learning. Finally, we validate past analytical results for a fault tolerant system composed of correlated components with real machine learning algorithms and data, demonstrating the analytical expression's ability to accurately estimate the reliability of the fault tolerant machine learning component and subsequently the architecture-based software within which it resides. © 2020 IEEE.",Architecture-based software | Correlation | Fault-tolerance | Machine learning | Software reliability,Proceedings - Annual Reliability and Maintainability Symposium,2020-01-01,Conference Paper,"Nafreen, Maskura;Bhattacharya, Saikath;Fiondella, Lance",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85032831281,10.1109/RE.2017.61,What do Support Analysts Know about Their Customers? On the Study and Prediction of Support Ticket Escalations in Large Software Organizations,"Understanding and keeping the customer happy is a central tenet of requirements engineering. Strategies to gather, analyze, and negotiate requirements are complemented by efforts to manage customer input after products have been deployed. For the latter, support tickets are key in allowing customers to submit their issues, bug reports, and feature requests. Whenever insufficient attention is given to support issues, however, their escalation to management is time-consuming and expensive, especially for large organizations managing hundreds of customers and thousands of support tickets. Our work provides a step towards simplifying the job of support analysts and managers, particularly in predicting the risk of escalating support tickets. In a field study at our large industrial partner, IBM, we used a design science methodology to characterize the support process and data available to IBM analysts in managing escalations. Through iterative cycles of design and evaluation, we translated our understanding of support analysts' expert knowledge of their customers into features of a support ticket model to be implemented into a Machine Learning model to predict support ticket escalations. We trained and evaluated our Machine Learning model on over 2.5 million support tickets and 10,000 escalations, obtaining a recall of 79.9% and an 80.8% reduction in the workload for support analysts looking to identify support tickets at risk of escalation. Further on-site evaluations, through a prototype tool we developed to implement our Machine Learning techniques in practice, showed more efficient weekly support-ticket-management meetings. The features we developed in the Support Ticket Model are designed to serve as a starting place for organizations interested in implementing our model to predict support ticket escalations, and for future researchers to build on to advance research in escalation prediction. © 2017 IEEE.",Customer relationship management | customer support ticket | escalation prediction | machine learning,"Proceedings - 2017 IEEE 25th International Requirements Engineering Conference, RE 2017",2017-09-22,Conference Paper,"Montgomery, Lloyd;Damian, Daniela",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85118441328,10.1109/REW53955.2021.00024,Issue Link Label Recovery and Prediction for Open Source Software,"Modern open source software development heavily relies on the issue tracking systems to manage their feature requests, bug reports, tasks, and other similar artifacts. Together, those 'issues' form a complex network with links to each other. The heterogeneous character of issues inherently results in varied link types and therefore poses a great challenge for users to create and maintain the label of the link manually. The goal of most existing automated issue link construction techniques ceases with only examining the existence of links between issues. In this work, we focus on the next important question of whether we can assess the type of issue link automatically through a data-driven method. We analyze the links between issues and their labels used the issue tracking system for 66 open source projects. Using three projects, we demonstrate promising results when using supervised machine learning classification for the task of link label recovery with careful model selection and tuning, achieving F1 scores of between 0.56-0.70 for the three studied projects. Further, the performance of our method for future link label prediction is convincing when there is sufficient historical data. Our work signifies the first step in systematically manage and maintain issue links faced in practice. © 2021 IEEE.",Issue Tracking Systems | Open Source Software | Software Maintenance and Evolution | Software Traceability,Proceedings of the IEEE International Conference on Requirements Engineering,2021-09-01,Conference Paper,"Nicholson, Alexander;Jin, Guo L.C.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85079224206,10.1109/RPA47751.2019.8958213,Software Package for Improving Financial and Technological Performance of Microgrid Networks,"In modern world, the use of the most advanced digital technologies in any industry and business directly effect on financial and technological indicators, in other words, the more advanced digital technologies are used to solve various tasks, the greater the profit or benefits that can be gained. The electric power industry is not an exception. Nowadays, more and more of the electric power sector is moving from large networks to small, often isolated, so-called Microgrid. Such networks generally have generation based on renewable energy sources (RE): wind power plants, solar power plants, small hydropower plants, tidal power plants, etc.In view of the fact that generation is stochastic, networks with generation relying on renewable energy sources have energy storage. It is also worth noting that isolated and non-isolated Microgrid even if generation does not base on renewable energy, energy storage devices can bring them a certain benefits connected with the changing cost of electricity during the day or year. Thus, a consumer of Microgrid has much greater capabilities than a consumer of a 'traditional', centralized power supply system, but he is also subject to much greater responsibility, because many of the functions that the system operator used to perform now fall on his shoulders. So, the following customer features in Microgrid can be distinguished: the ability to disconnect themselves from the mains supply for the period when consuming is not profitable for them, the ability to sell electricity to the power supply network, independently maintain equipment (including generating), calculate and forecast their consumption and generation, make profit from the sale of electricity to the network.Obviously, in the past, the average consumer was not capable meeting the greatest part of the needs of his own electrical 'industry' independently, but using modem digital technologies, most of the tasks that previously were impossible to fulfill could be automated without the direct participation of the consumer. That is why, it is proposed to use software systems which will be based on neural networks. The task of these software systems is to collect and process monitoring data, each consuming or generating unit in Microgrid, to perform a large number of tasks. One of such tasks is classification and creation characteristics of generating and consuming equipment by collecting and analyzing data from Microgrid participants. The algorithm determines the characteristics of consumption and generation. Based on these characteristics under various external conditions and factors, load and generations schedules (especially important for renewable energy sources) and possible emergency events are predicted. In addition, such software systems make it possible to optimize the algorithms for determining the most profitable hours for consumption or selling of electricity to the network. It is more convenient to operate the described software systems as a cloud services. In other words, in order to start implement a software package into operation, the consumer will only need an Internet connection, so there is no need for computers with high computational abilities, all calculations occur remotely.This work describe a software package which include the automation, forecasting and optimization of the financial and technological performance of Microgrid networks. It considers the data that the software package needs for complete analysis and further prediction, methods and algorithms that underpin this software package and the possible benefit from its use. RTDS hardware and software system was used to model the power system; the prediction methodology was based on recurrent neural networks (RNN).Machine learningWhat exactly is machine learning? It is obvious that 'learning' is when a certain model' learns'in a some way and then begins to return results, that is, most likely, to predict something. A very general definition of'learnability' is roughly the same as that given by Thomas Mitchell in his book 'Machine learning' [4]: 'A computer program is said to learn from experience with some class of tasks T and performance measure P, if its performance at tasks (as measured by P) improves with experience' [5].The main classification of machine learning tasks is shown in Fig. 1. The two main classes of machine learning tasks are supervised learning and unsupervised learning. To fulfill the purposes-to detect faults in power transformers on the basis of PMU, it is necessary to carry out supervised learning tasks such as data classification. In the work presented here, data is a set of features that will be fed to the neural network input with the expected output: 'true'- interwinding fault occurrence, 'false'- without inter-winding fault. In order for a trained neural network to accurately detect turn-to-turn faults in a transformer it is highly important to prepare a sufficient set of data and to select the features of this type of damage as accurately as possible. © 2019 IEEE.",,"2019 2nd International Youth Scientific and Technical Conference on Relay Protection and Automation, RPA 2019",2019-10-01,Conference Paper,"Usachev, S.;Voloshin, A.;Ententeev, A.;Maksudov, B.;Maksimov, R.;Livshits, S.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85124547808,10.1109/RTSS52674.2021.00018,AegisDNN: Dependable and Timely Execution of DNN Tasks with SGX,"With the rising demand for emerging DNN applications in safety-critical systems, much attention has been given to the reliability and trustworthiness of DNN inference output against malicious attacks. Although prior work has been conducted to improve the privacy of DNN inference by executing the entire DNN model inside Intel SGX enclaves, existing approaches pose severe performance challenges to achieve dependable and timely execution simultaneously. In this paper, we propose AegisDNN, a DNN inference framework to address this problem. AegisDNN leverages secure SGX enclaves for protecting only the critical part of real-time DNN tasks which are vulnerable to potential fault injection attacks. To choose the right set of layers for protection while ensuring the timeliness of task execution, AegisDNN includes a dynamic-programming based algorithm that finds a layer protection configuration for each task to meet the real-time and dependability requirements based on the layer-wise DNN time and SDC (Silent Data Corruption) profiling mechanism. AegisDNN also utilizes a machine-learning based SDC prediction method to significantly reduce the time for estimating SDC rates for all possible layer protection configurations. We implemented AegisDNN on Caffe, PyTorch, and Tensorflow with Eigen BLAS ported into SGX enclaves to comprehensively demonstrate the effectiveness of AegisDNN against state-of-the-art DNN fault-injection attacks. Experiment results indicate that AegisDNN could satisfy both dependability and real-time requirements simultaneously, when none of the other compared approaches could do so. © 2021 IEEE",,Proceedings - Real-Time Systems Symposium,2021-01-01,Conference Paper,"Xiang, Yecheng;Wang, Yidi;Choi, Hyunjong;Karimi, Mohsen;Kim, Hyoseung",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85074229872,10.1109/RUSAUTOCON.2019.8867740,Parametric Model of Pipe Defect Description for Generation of Training Set for Machine Learning in Data-Poor Conditions,"The article addresses the problem of the lack of real data in training and testing of machine learning algorithms. The issue is presented below through the case of identifying the defect on the pipe inner surface. In this paper, the authors present approaches to the formation of a training set using synthetic images obtained from various sources. The article considers in detail the method of image generation based on the recommended parametric representation of the defect on the inner surface of a pipe. For the defect description, the following parameters were selected: area coefficient, HSV color model, texture, shape and boundary of the defect. The determination of each of the selected parameters is described in the paper. The experimental results on the synthetic image generation based on parametric representation of the defect using the developed software in the Matlab environment are noted. The article considers the method of detecting defects on the inner surface of the pipe using the presented defect parametric description. Based on the developed model, there was formed a sample of tube images for the neural network training and testing. © 2019 IEEE.",data preparation | image processing | machine learning | neural networks | visual inspection,"Proceedings - 2019 International Russian Automation Conference, RusAutoCon 2019",2019-09-01,Conference Paper,"Mantserov, S. A.;Fedosova, L. O.;Tsapaev, A. P.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85083565109,10.1109/SANER48275.2020.9054821,Are SonarQube Rules Inducing Bugs?,"The popularity of tools for analyzing Technical Debt, and particularly the popularity of SonarQube, is increasing rapidly. SonarQube proposes a set of coding rules, which represent something wrong in the code that will soon be reflected in a fault or will increase maintenance effort. However, our local companies were not confident in the usefulness of the rules proposed by SonarQube and contracted us to investigate the fault-proneness of these rules. In this work we aim at understanding which SonarQube rules are actually fault-prone and to understand which machine learning models can be adopted to accurately identify fault-prone rules. We designed and conducted an empirical study on 21 well-known mature open-source projects. We applied the SZZ algorithm to label the fault-inducing commits. We analyzed the fault-proneness by comparing the classification power of seven machine learning models. Among the 202 rules defined for Java by SonarQube, only 25 can be considered to have relatively low fault-proneness. Moreover, violations considered as 'bugs' by SonarQube were generally not fault-prone and, consequently, the fault-prediction power of the model proposed by SonarQube is extremely low. The rules applied by SonarQube for calculating technical debt should be thoroughly investigated and their harmfulness needs to be further confirmed. Therefore, companies should carefully consider which rules they really need to apply, especially if their goal is to reduce fault-proneness. © 2020 IEEE.",architectural smells | code smells | coding style | machine learning | SonarQube | static analysis | Technical Debt,"SANER 2020 - Proceedings of the 2020 IEEE 27th International Conference on Software Analysis, Evolution, and Reengineering",2020-02-01,Conference Paper,"Lenarduzzi, Valentina;Lomio, Francesco;Huttunen, Heikki;Taibi, Davide",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85106590420,10.1109/SANER50967.2021.00058,Adaptive Immunity for Software: Towards Autonomous Self-healing Systems,"Testing and code reviews are known techniques to improve the quality and robustness of software. Unfortunately, the complexity of modern software systems makes it impossible to anticipate all possible problems that can occur at runtime, which limits what issues can be found using testing and reviews. Thus, it is of interest to consider autonomous self-healing software systems, which can automatically detect, diagnose, and contain unanticipated problems at runtime. Most research in this area has adopted a model-driven approach, where actual behavior is checked against a model specifying the intended behavior, and a controller takes action when the system behaves outside of the specification. However, it is not easy to develop these specifications, nor to keep them up-to-date as the system evolves. We pose that, with the recent advances in machine learning, such models may be learned by observing the system. Moreover, we argue that artificial immune systems (AISs) are particularly well-suited for building self-healing systems, because of their anomaly detection and diagnosis capabilities. We present the state-of-the-art in self-healing systems and in AISs, surveying some of the research directions that have been considered up to now. To help advance the state-of-the-art, we develop a research agenda for building self-healing software systems using AISs, identifying required foundations, and promising research directions. © 2021 IEEE.",anomaly detection | artificial immune systems | dependability | fault containment | runtime diagnosis | self-healing,"Proceedings - 2021 IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2021",2021-03-01,Conference Paper,"Ali Naqvi, Moeen;Astekin, Merve;Malik, Sehrish;Moonen, Leon",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85048863253,10.1109/SDS.2018.8370415,Towards an autonomic Bayesian fault diagnosis service for SDN environments based on a big data infrastructure,"Software Defined Networks (SDN) are gaining momentum as a solution for current and future networking issues. Its programmability and centralised control enables a more dynamic management of the network. But this feature introduces the cost of a potential increase in failures, since every modification introduced on the control plane is a new possibility for failures to appear and cause a decrement of the quality for the offered service. Following a classical approach, this kind of problems could be solved increasing the number of high skilled human operators, which would dramatically increase network operation cost. Our approach is to apply Machine Learning and Data Analysis for monitoring and diagnosis SDN networks with the goal of automating these tasks. In this paper, we present an architecture for a self-diagnosis service which is deployed on top of a SDN management platform. In addition, a prototype of the proposed service with different diagnosis models for SDN networks has been developed. The evaluation shows encouraging results which will be explored in future works. © 2018 IEEE.",Bayesian network | fault diagnosis | Machine Learning | SDN,"2018 5th International Conference on Software Defined Systems, SDS 2018",2018-05-31,Conference Paper,"Benayas, Fernando;Carrera, Alvaro;Iglesias, Carlos A.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84998836308,10.1109/SEAA.2016.20,Automated Bug Triaging in an Industrial Context,"There is an increasing need to introduce some formof automation within the bug triaging process, so that no timeis wasted on the initial assignment of issues. However, there is agap in current research, as most of the studies deal with opensource projects, ignoring the industrial context and needs. In this paper, we report our experience in dealing with theautomation of the bug triaging process within a research-industrycooperation. After reporting the requirements and needs thatwere set within the industrial project, we compare the analysisresults with those from an open source project used frequentlyin related research (Firefox). In spite of the fact that theprojects have different size and development process, the datadistributions are similar and the best models as well. We foundout that more easily configurable models (such as SVM+TF - IDF) are preferred, and that top-x recommendations, number of issuesper developers, and online learning can all be relevant factorswhen dealing with an industrial collaboration. © 2016 IEEE.",Bug Assignment | Bug Reports | Industrial Scale | Machine Learning | Software Bug Triaging | Text Classification,"Proceedings - 42nd Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2016",2016-10-14,Conference Paper,"Dedik, Vaclav;Rossi, Bruno",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85096580313,10.1109/SEAA51224.2020.00075,A Taxonomy of Metrics for Software Fault Prediction,"Researchers in the field of Software Fault Prediction (SFP) make use of software metrics to build predictive models, for example, by means of machine learning and statistical techniques. The number of metrics used for SFP has increased dramatically in the last few decades. Therefore, a taxonomy of metrics for SFP could be useful to standardize the lexicon, to simplify the communication among researchers/practitioners, and to organize and classify such metrics. In this research, we built a taxonomy of metrics for SFP with the aim of making it as comprehensive as possible. We exploited and extended two Systematic Literature Reviews (SLRs) to collect and classify a total of 512 metrics for SFP and then to build our taxonomy. We also provide information on the metrics in this taxonomy in terms of: acronym(s), extended name, description, granularity of the prediction, category, and research papers in which they were used. To allow the taxonomy to be constantly updated over time, we provide external contributors the possibility to ask for changes via pull-requests on GitHub. © 2020 IEEE.",fault prediction | software metrics | taxonomy,"Proceedings - 46th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2020",2020-08-01,Conference Paper,"Caulo, Maria;Scanniello, Giuseppe",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85096529855,10.1109/SEAA51224.2020.00077,Trends in Software Engineering Processes using Deep Learning: A Systematic Literature Review,"In recent years, several researchers have applied machine learning techniques to several knowledge areas achieving acceptable results. Thus, a considerable number of deep learning models are focused on a wide range of software processes. This systematic review investigates the software processes supported by deep learning models, determining relevant results for the software community. This research identified that the most extensively investigated sub-processes are software testing and maintenance. In such sub-processes, deep learning models such as CNN, RNN, and LSTM are widely used to process bug reports, malware classification, libraries and commits recommendations generation. Some solutions are oriented to effort estimation, classify software requirements, identify GUI visual elements, identification of code authors, the similarity of source codes, predict and classify defects, and analyze bug reports in testing and maintenance processes. © 2020 IEEE.",Deep Learning | Machine Learning | Software Processes | Systematic Review,"Proceedings - 46th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2020",2020-08-01,Conference Paper,"Del Carpio, Alvaro Fernandez;Angarita, Leonardo Bermon",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85119208370,10.1109/SEAA53835.2021.00022,NLP4IP: Natural Language Processing-based Recommendation Approach for Issues Prioritization,"This paper proposes a recommendation approach for issues (e.g., a story, a bug, or a task) prioritization based on natural language processing, called NLP4IP. The proposed semi-automatic approach takes into account the priority and story points attributes of existing issues defined by the project stakeholders and devises a recommendation model capable of dynamically predicting the rank of newly added or modified issues. NLP4IP was evaluated on 19 projects from 6 repositories employing the JIRA issue tracking software with a total of 29,698 issues. A comprehensive benchmark study was also conducted to compare the performance of various machine learning models. The results of the study showed an average top@3 accuracy of 81% and a mean squared error of 2.2 when evaluated on the validation set. The applicability of the proposed approach is demonstrated in the form of a JIRA plug-in illustrating predictions made by the newly developed machine learning model. The dataset has also been made publicly available in order to support other researchers working in this domain. © 2021 IEEE.",Agile software development | issues prioritization | natural language processing,"Proceedings - 2021 47th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2021",2021-09-01,Conference Paper,"Shafiq, Saad;Mashkoor, Atif;Mayr-Dorn, Christoph;Egyed, Alexander",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-56749169872,10.1109/SICE.2008.4654736,Learning from errors: A bio-inspired approach for hypothesis-based machine learning,"This contribution present an approach extending existing learning strategies based on Situation-Operator-Modeling (SOM), which can be used to model interactions with the environment and to represent the knowledge of cognitive systems. The approach proposes a planning process using hypotheses to bridge the gap of knowledge, which is refined by a following check of the applied hypothesis. The hypotheses are inspired by human errors according to Dörner's classification, which is related to the interaction within complex dynamic systems. The programmed implementation of the approach is based on an experimental environment using a software tool for high-level Petri Nets. © 2008 SICE.",Autonomous systems | Cognitive technical systems | Human error,Proceedings of the SICE Annual Conference,2008-12-01,Conference Paper,"Gamrad, Dennis;Söffker, Dirk",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85070915051,10.1109/SMARTCOMP.2019.00026,Identifying security bug reports based solely on report titles and noisy data,"Identifying security bug reports (SBRs) is a vital step in the software development life-cycle. In supervised machine learning based approaches, it is usual to assume that entire bug reports are available for training and that their labels are noise free. To the best of our knowledge, this is the first study to show that accurate label prediction is possible for SBRs even when solely the title is available and in the presence of label noise. © 2019 IEEE.",Bug Repositories | Machine Learning | Mislabeling | Noise | Security Bug Report,"Proceedings - 2019 IEEE International Conference on Smart Computing, SMARTCOMP 2019",2019-06-01,Conference Paper,"Pereira, Mayana;Kumar, Alok;Cristiansen, Scott",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85098861637,10.1109/SMC42975.2020.9283289,Fast Detection of Duplicate Bug Reports using LDA-based Topic Modeling and Classification,"A bug tracking system continuously monitors the status of a software environment, like an Operating System (OS) or a user application. Whenever it detects an anomaly situation, it generates a bug report and sends it to the software developer or maintenance center. However, the newly reported bug can be an already existing issue that was reported earlier and may have a solution in the master report repository. This condition brings an avalanche of duplicate bug reports, posing a big challenge to the software development life cycle. Thus, early detection of duplicate bug reports has become an extremely important task in the software industry. To address this issue, this work proposes a double-tier approach using clustering and classification, whereby it exploits Latent Dirichlet Allocation (LDA) for topic-based clustering, multimodal text representation using Word2Vec (W2V), FastText (FT) and Global Vectors for Word Representation (GloVe), and a unified text similarity measure using Cosine and Euclidean metrics. The proposed model is tested on the Eclipse dataset consisting over 80,000 bug reports, which is the amalgamation of both master and duplicate reports. This work considers only the description of the reports for detecting duplicates. The experimental results show that the proposed two-tier model achieves a recall rate of 67% for Top-N recommendations with 3 times faster computation than the conventional one-on-one classification model. © 2020 IEEE.",Bug report detection | information retrieval | machine learning | natural language processing | topic modeling,"Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics",2020-10-11,Conference Paper,"Akilan, Thangarajah;Shah, Dhruvit;Patel, Nishi;Mehta, Rinkal",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85022179262,10.1109/SOSE.2017.19,SOA Based Integrated Software to Develop Fault Diagnosis Models Using Machine Learning in Rotating Machinery,"Fault detection and diagnostic software (FDDS) supports technicians and engineers to deal with operational matters, in major cases related to complicated systems and advanced technology that require higher performance expectation. Information and communication technologies play an importantrole for implementing efficient maintenance software, therefore, the development of FDDS is posed as an industrial necessity. In case of industrial rotating machinery, data-driven FDDS using available vibration signals, or other related signals monitored from sensors, is currently viewed as an industrial informatics requirement. This paper proposes the application of a Service Oriented Architecture (SOA) to implement an integrated tool for automatically developing and testing machine learning based fault diagnosis models in rotating machinery. As a result, a generic architecture is obtained which is able to build and implement diagnosis models in similar devices or processes. A condition monitoring software application, using the proposed SOA, was implemented in Java and deployed on a computational environment to test its performance in a experimental test bed, under realistic fault mechanical conditions in a gearbox. © 2017 IEEE.",e-Maintenance | Fault diagnosis | Industrial supervision | Machine Learning | Rotating machinery | SOA,"Proceedings - 11th IEEE International Symposium on Service-Oriented System Engineering, SOSE 2017",2017-06-07,Conference Paper,"Cerrada, Mariela;Pacheco, Fannia;Sanchez, Rene Vinicio;Cabrera, Diego;MacAncela, Jean;Lucero, Pablo",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85051027221,10.1109/SP.2018.00016,Learning from Mutants: Using Code Mutation to Learn and Monitor Invariants of a Cyber-Physical System,"Cyber-physical systems (CPS) consist of sensors, actuators, and controllers all communicating over a network; if any subset becomes compromised, an attacker could cause significant damage. With access to data logs and a model of the CPS, the physical effects of an attack could potentially be detected before any damage is done. Manually building a model that is accurate enough in practice, however, is extremely difficult. In this paper, we propose a novel approach for constructing models of CPS automatically, by applying supervised machine learning to data traces obtained after systematically seeding their software components with faults ('mutants'). We demonstrate the efficacy of this approach on the simulator of a real-world water purification plant, presenting a framework that automatically generates mutants, collects data traces, and learns an SVM-based model. Using cross-validation and statistical model checking, we show that the learnt model characterises an invariant physical property of the system. Furthermore, we demonstrate the usefulness of the invariant by subjecting the system to 55 network and code-modification attacks, and showing that it can detect 85% of them from the data logs generated at runtime. © 2018 IEEE.",anomaly detection | attacks | attestation | cyber physical systems | invariants | machine learning | mutation testing | system modelling | water treatment systems,Proceedings - IEEE Symposium on Security and Privacy,2018-07-23,Conference Paper,"Chen, Yuqi;Poskitt, Christopher M.;Sun, Jun",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85013870251,10.1109/SYNASC.2016.072,A combined analytical modeling machine learning approach for performance prediction of MapReduce jobs in cloud environment,"Nowadays MapReduce and its open source implementation, Apache Hadoop, are the most widespread solutions for handling massive dataset on clusters of commodity hardware. At the expense of a somewhat reduced performance in comparison to HPC technologies, the MapReduce framework provides fault tolerance and automatic parallelization without any efforts by developers. Since in many cases Hadoop is adopted to support business critical activities, it is often important to predict with fair confidence the execution time of submitted jobs, for instance when SLAs are established with end-users. In this work, we propose and validate a hybrid approach exploiting both queuing networks and support vector regression, in order to achieve a good accuracy without too many costly experiments on a real setup. The experimental results show how the proposed approach attains a 21% improvement in accuracy over applying machine learning techniques without any support from analytical models. © 2016 IEEE.",Analytical performance modeling | cloud computing | machine learning | MapReduce,"Proceedings - 18th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing, SYNASC 2016",2017-01-23,Conference Paper,"Ataie, Ehsan;Gianniti, Eugenio;Ardagna, Danilo;Movaghar, Ali",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85077616115,10.1109/SYSTOL.2019.8864766,Process Monitoring Platform based on Industry 4.0 tools: A waste-to-energy plant case study,"This work presents a process data analytics platform built around the concept of industry 4.0. The platform utilizes the state-of-the-art industry internet of things (IIoT) platforms, machine learning (ML) algorithms and big-data software tools. The industrial applicability of the platform was demonstrated by the development of soft sensors for use in a waste-to-energy (WTE) plant. In the case study, the work studied data-driven soft sensors to predict syngas heating value and hot flue gas temperature. From data-driven models, the neural network based nonlinear autoregressive with external input (NARX) model demonstrated better performance in prediction of both syngas heating value and flue gas temperature in a WTE process. © 2019 IEEE.",Industrial internet of things | machine learning | soft sensor | waste-to-energy,"Conference on Control and Fault-Tolerant Systems, SysTol",2019-09-01,Conference Paper,"Kabugo, James Clovis;Jamsa-Jounela, Sirkka Liisa;Schiemann, Robert;Binder, Christian",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85077615178,10.1109/SYSTOL.2019.8864797,Control strategy of a Multiple Hearth Furnace enhanced by machine learning algorithms,"An enhanced control strategy for a multiple hearth furnace for the purpose of kaolin production is developed and presented in this paper. Mineralogy-driven machine learning algorithms play a key role in the optimization strategy of the furnace. First, the capacity and temperature setpoints for furnace control are determined based on the feed ore mineralogy. Next, the capacity is optimized by combining the prediction of soluble alumina content and mullite content, while maintaining the quality of the product. The stabilizing control level compensates the disturbances with a feedforward control, which uses a spinel phase reaction rate soft sensor, aimed at minimizing the energy use of the furnace. The control concept is successfully tested by simulation using industrial data. Finally, a sampling campaign and software testing of the soft sensors and machine learning algorithms are performed at the industrial site. The results are presented and discussed in the paper. © 2019 IEEE.",,"Conference on Control and Fault-Tolerant Systems, SysTol",2019-09-01,Conference Paper,"Fuentes, Jose Valentin Gomez;Jamsa-Jounela, Sirkka Liisa;Moseley, David;Skuse, Tom",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85098916785,10.1109/SysCon47679.2020.9275863,A baseline accuracy classification method to over-come the over-fitting problem for class- imbalanced defect-prone datasets model,"Software Engineers are battling always with defects from the earliest starting point of the software history. The defect has significant commitment and assumes an important job in the hour of freeing software. Software defect-prone might be recognized before the software announcement or after the announcement. Although, the desire for designers is to give defect free software to their customers. Hence, recognizing the software defect-prone before announcement makes a decent preferred position for the designers. Here, the difficult assignment is to discover the defect-prone region or defective module of software. On the off chance that defective modules are distinguished before the arrival of software, at that point it could be effectively counteracted and fixed by the designer. In most recent two decades scientists have concentrated on software deformity prophecy issue by applying a few statistical and machine learning systems. The software defect-prone information experiences the class-imbalance issue due to the slanted conveyance of defective and non-defective software modules. Generally, machine learning calculations consider equivalent circulation of information tests in each class and expect the misclassification cost of each class is similarly significant. We have used SMOTE with ONE-R by using its MinBucketsize number n=1, 2, 3, 4, 5 & 6 for over-fit the data and resultant we got that minbucketsize n= 1 & 2 are highly good for base accuracy and efficiency for defect-prone datasets model. We have used datasets model in three different ways and we observed that use training datasets is very suitable for our experiments as compare to other two ways. We also observed that al evaluation measures results are highly good at use of training datasets model. © 2020 IEEE",Class-Imbalanced | Classification | Defect-Prone | Over-fitting | Over-Sampling | Software quality | Under-sapling,"SYSCON 2020 - 14th Annual IEEE International Systems Conference, Proceedings",2020-08-24,Conference Paper,"Shaikh, Salahuddin;Changan, Liu;Malik, Maaz Rasheed",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84949644119,10.1109/TAI.2000.889847,JADE - AI support for debugging java programs,"Model-based diagnosis is a successjid AI technique for locating and identifying faults in technical syslems. Extending previous research on model-based diagnosis support for fault search in technical designs, we are building a model-based debugger for Java programs to provide intelligent support for the programmer trying to locate the source of an error. By using one or more models derived from the source code of the program without additional specifications except the Java semantics, the debugger guides the user towards potential sources for incorrect program behaviors, i.e., bugs. © 2000 IEEE.",Artificial intelligence | Artificial neural networks | Constraint optimization | Data mining | Knowledge engineering | Logic | Machine learning | Multimedia databases | Multimedia systems | Software tools,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",2000-01-01,Conference Paper,"Mateis, Cristinel;Stumptner, Markus;Wieland, Dominik;Wotawa, Franz",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84949640598,10.1109/TAI.2000.889902,The application of a machine learning tool to the validation of an air traffic control domain theory,"In this paper we describe a project (IMPRESS) which utilised a machine learning tool for the validation of an air traffic control domain theory. During the project, novel techniques were devised for the automated revision of general clause form theories using training examples. This technique involves focusing in on the parts of a theory which involve ordinal sorts, and applying geometrical revision operators to repair faulty component parts. The method is illustrated with experimental results obtained during the project. © 2000 IEEE.",Air traffic control | Animation | Computer bugs | Fault diagnosis | Machine learning | Mathematical model | Mathematics | Prototypes | Software safety | System testing,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI",2000-01-01,Conference Paper,"West, M. M.;McCluskey, T. L.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85057069769,10.1109/TDC-LA.2018.8511709,Wavelet Transform and Support Vector Machine-Based Current-Only Directional Overcurrent Relay for Transmission Line Protection,"Conventionally, for power protective relay applications, overcurrent relaying is made directional by use of two inputs, the operating current and a reference or polarizing quantity (either voltage or current) that does not change with fault location. This work presents a pattern model, which is capable of providing the characteristic of directionality to a nondirectional overcurrent relay that is installed on a transmission line by monitoring the current signal only, without the reference signal. In addition, a second model which has the ability to identify the type of fault occurred in the protected line is presented. The electrical test system was implemented in ATP/EMTP program, and simulations were driven by a MATLAB program for several short-circuit fault types at different distances. The simulation results constitute the database. These files were processed and analyzed on MATLAB platform mainly using two mathematical tools: Wavelet Transform and Support Vector Machine. Finally, a graphical user interface was implemented for academic purposes, so the user can easily and didactically appreciate the developed methodology, as well as its effectiveness. © 2018 IEEE.",ATP/EMTP | Discrete Wavelet Transform | Machine Learning | MATLAB | Power System Protection | Support Vector Machines,"Proceedings of the 2018 IEEE PES Transmission and Distribution Conference and Exhibition - Latin America, T and D-LA 2018",2018-10-26,Conference Paper,"Travez, Cristina J.;Perez, Fabian E.;Quilumba, Franklin L.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85122301098,10.1109/TDSC.2021.3139201,A Comparative Analysis of Software Aging in Image Classifiers on Cloud and Edge,"Image classifiers for recognizing real-world objects are widely used in the Internet of Things (IoT) and Cyber-Physical Systems(CPSs). A classifier is trained offline by machine learning algorithms with training data sets, and then it is deployed on a cloud or an edge computing system for online label predictions. As the classifier's performance depends on the underlying software infrastructure, it may degrade over time due to software faults causing software aging. In this paper, we address this issue and experimentally investigate software aging observed in an image classification system that continuously runs on cloud and edge computing environments. We apply several statistical techniques to analyze degradation trends in the systems under stress tests. Our statistical trend analysis confirms the degradation trends in the throughput as well as the available memory resources both in the cloud and the edge environments. Contrary to our expectation, the edge computing environment under test had much less impact on the performance degradation than our cloud environment when the workload is high, although the latter one has four times larger allocated memory resources. We also show that the observed performance degradation trends are associated with the memory usage of specific processes by performing correlation analysis. IEEE",Cloud computing | Edge computing | image classifiers | performance analysis | software aging,IEEE Transactions on Dependable and Secure Computing,2023-01-01,Article,"Andrade, Ermeson;Pietrantuono, Roberto;Machida, Fumio;Cotroneo, Domenico",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85077697078,10.1109/TENCON.2019.8929264,Predictive Model for Classification of Power System Faults using Machine Learning,"Power System is a combination of electric power generation, transmission, distribution and utilization systems. In brief, power system is the heart of any electrical system. In an electric power system, a fault or fault current is any abnormal electric current. As a consequence of such fault, the entire system may damage and eventually collapse. The aim of this work is to automatically classify the faults into one of the eleven faulty classes, which includes both balanced and unbalanced faults. The dataset of generated fault in overhead transmission lines is synthetic, which consists of 11 different faults for 100 kilometers. The simulation is done using MATLAB/Simulink software model. The task of classification of faults is implemented using supervised machine learning algorithms in Python and scikit-learn. Comparison is made using three commonly used classification algorithms - Decision Tree (DT), K-Nearest Neighbor (KNN), Support Vector Machine (SVM). SVM performed excellent giving a performance with 91.6% test accuracy for the generated dataset. The predictive model will thus make the system more intelligent in bringing up reliable power supply. © 2019 IEEE.",fault classification | power system | supervised machine learning,"IEEE Region 10 Annual International Conference, Proceedings/TENCON",2019-10-01,Conference Paper,"Goswami, Tilottama;Roy, Uponika Barman",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85098970108,10.1109/TENCON50793.2020.9293918,Hardware accelerators for edge enabled machine learning,"The proliferation of IoT devices in recent years has resulted in an exponential increase in data being transmitted over the internet. The traffic is slated for further increase in the coming years and will result in excessive network congestion and high latency. To alleviate this problem, an alternate approach needs to be considered. A prominent option would be to move the computing domain to the edge device. This option is constrained due to reduced computing, storage and power available on the edge. A novel approach combining both software and hardware solutions is required to perform analytics at the edge. This paper proposes an architecture for analysing data on the edge, combining hardware and software solutions. The proposed methodology explores machine learning algorithms for edge computing combined with the use of hardware accelerators to achieve truly intelligent edge devices. A qualitative and quantitative comparison of performance of various algorithms on CPU, GPU, FPGA platforms is carried out. A machine learning model for predicting Remaining Useful Life (RUL) for a multivariate time series dataset is developed and its deployment on the edge is discussed. The results of the experiments carried out are promising and hold potential for further research. © 2020 IEEE.",Fault prediction | Hardware acceleration | Machine learning,"IEEE Region 10 Annual International Conference, Proceedings/TENCON",2020-11-16,Conference Paper,"Suresh, Arjun;Reddy, Bhargava N.;Renu Madhavi, C. H.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85117067491,10.1109/TETC.2021.3116999,Emulating the Effects of Radiation-Induced Soft-Errors for the Reliability Assessment of Neural Networks,"Convolutional Neural Networks (CNNs) are currently one of the most widely used predictive models in machine learning. Recent studies have demonstrated that hardware faults induced by radiation fields, including cosmic rays, may significantly impact the CNN inference leading to wrong predictions. Therefore, ensuring the reliability of CNNs is crucial, especially for safety-critical systems. In the literature, several works propose reliability assessments of CNNs mainly based on statistically injected faults. This work presents a software emulator capable of injecting real faults retrieved from radiation tests. Specifically, from the device characterisation of a DRAM memory, we extracted event rates and fault models. The software emulator can reproduce their incidence and access their effect on CNN applications with a reliability assessment precision close to the physical one. Radiation-based physical injections and emulator-based injections are performed on three CNNs (LeNet-5) exploiting different data representations. Their outcomes are compared, and the software results evidence that the emulator is able to reproduce the faulty behaviours observed during the radiation tests for the targeted CNNs. This approach leads to a more concise use of radiation experiments since the extracted fault models can be reused to explore different scenarios (e.g., impact on a different application). IEEE",approximate methods | fault injection | Neural nets | radiation effects | reliability,IEEE Transactions on Emerging Topics in Computing,2022-10-01,Article,"Luza, Lucas Matana;Ruospo, Annachiara;Soderstrom, Daniel;Cazzaniga, Carlo;Kastriotou, Maria;Sanchez, Ernesto;Bosio, Alberto;Dilillo, Luigi",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85086081890,10.1109/TII.2019.2958606,Real-Time Anomaly Detection of NoSQL Systems Based on Resource Usage Monitoring,"Today, the emergence of the industry revolution systems such as Industry 4.0, Internet of Things, and big data frameworks poses new challenges in terms of storage and processing of real-time data. As systems scale in humongous sizes, a crucial task is to administer the variety of different subsystems and applications to ensure high performance. This is directly related with the identification and elimination of system failures and errors, while the system runs. In particular, database systems may experience abnormalities related with decreased throughput or increased resource usage, that in turn affects system performance. In this article, we focus on not only SQL (NoSQL) database systems that are ideal for storing sensor data in the concept of Industry 4.0. This typically includes a variety of applications and workloads that are difficult to online monitor, thus making anomaly detection a challenging task. Creating a robust platform to serve such infrastructures with minimum hardware or software failures is a key challenge. In this article, we propose RADAR, an anomaly detection system that works on real time. RADAR is a data-driven decision-making system for NoSQL systems, by providing process information extraction during resource monitoring and by associating resource usage with the top processes, to identify anomalous cases. In this article, we focus on anomalies such as hardware failures or software bugs that could lead to abnormal application runs, without necessarily stopping system functionality, e.g., due to a system crash, but by affecting its performance, e.g., decreased database system throughput. Although different patterns may occur through time, we focus on periodic running workloads (e.g., monitoring daily usage) that are very common for NoSQL systems, and Internet of Things scenarios where data streams are forwarded to the Cloud for storage and processing. We apply various machine learning algorithms such as autoregressive integrated moving average (ARIMA), seasonal ARIMA, and long-short-term memory recurrent neural networks. We experimentally analyze our solution to demonstrate the benefits of supporting online erroneous state identification and characterization for modern applications. © 2005-2012 IEEE.",Anomaly detection | cloud computing | not only SQL (NoSQL) systems | real-time analytics,IEEE Transactions on Industrial Informatics,2020-09-01,Article,"Chouliaras, Spyridon;Sotiriadis, Stelios",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85058094225,10.1109/TMI.2018.2885319,Personalized Models for Injected Activity Levels in SPECT Myocardial Perfusion Imaging,"We propose a patient-specific ('personalized') approach for tailoring the injected activities to individual patients in order to achieve dose reduction in SPECT-myocardial perfusion imaging (MPI). First, we develop a strategy to determine the minimum dose levels required for each patient in a large set of clinical acquisitions (857 subjects) such that the reconstructed images are sufficiently similar to that obtained at conventional clinical dose. We then apply machine learning models to predict the required dose levels on an individual basis based on a set of patient attributes which include body measurements and various clinical variables. We demonstrate the personalized dose models for two commonly used reconstruction methods in clinical SPECT-MPI: 1) conventional filtered backprojection (FBP) with post-filtering and 2) ordered-subsets expectation-maximization (OS-EM) with corrections for attenuation, scatter and resolution, and evaluate their performance in perfusion-defect detection by using the clinical Quantitative Perfusion SPECT software package. The results indicate that the achieved dose reduction can vary greatly among individuals from their conventional clinical dose and that the personalized dose models can achieve further reduction on average compared with a global (non-patient specific) dose reduction approach. In particular, the average personalized dose level can be reduced to 58% and 54% of the full clinical dose, respectively, for FBP and OS-EM reconstruction, while without deteriorating the accuracy in perfusion-defect detection. Furthermore, with the average personalized dose further reduced to only 16% of full dose, OS-EM can still achieve a detection accuracy level comparable to that of FBP with full dose. © 1982-2012 IEEE.",Dose reduction | personalized imaging | SPECT-MPI,IEEE Transactions on Medical Imaging,2019-06-01,Article,"Juan Ramon, Albert;Yang, Yongyi;Pretorius, P. Hendrik;Johnson, Karen L.;King, Michael A.;Wernick, Miles N.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85124089584,10.1109/TNSM.2022.3146869,Learning Long-and Short-Term Temporal Patterns for ML-driven Fault Management in Optical Communication Networks,"The deployment of 5G and network slicing has challenged the current network management requirements, triggering the need for programmable and software-driven architectures. Thus, automated real-time fault management for self-managed networks with machine learning and artificial intelligence at the forefront has become necessary. This is especially the case of optical communication systems, accountable for most of the data traffic worldwide. This study introduces the application of a novel failure detection and localization framework capable of forecasting failures in optical systems based on an unsupervised learning strategy. In this approach, the Long-and Short-term Time-series Network (LSTNet) is exploited for modeling the normal behavior of optical systems. Then, failure conditions are properly forecast without explicitly training the model for such cases, easing the data acquisition process. Later, forecast values and actual measurements from optical equipment are used to derive an outlier detection method to detect and locate failures to improve the decision-making process at the network orchestrator. Laboratory experiments comparing the proposed approach with the Recurrent and Long Short-Term Memory models in terms of failure detection and forecasting performance show that using the LSTNet reduces the mean squared errors in 95% for unseen data, indicating robustness and suitability for real-world environments. IEEE",deep learning | Digital orchestration | ETSI zero-touch networks | novelty detection | optical communication systems,IEEE Transactions on Network and Service Management,2022-09-01,Article,"Silva, Moisés Felipe;Pacini, Alessandro;Sgambelluri, Andrea;Valcarenghi, Luca",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84883321067,10.1109/TSE.2013.11,Data quality: Some comments on the NASA software defect datasets,"Background-Self-evidently empirical analyses rely upon the quality of their data. Likewise, replications rely upon accurate reporting and using the same rather than similar versions of datasets. In recent years, there has been much interest in using machine learners to classify software modules into defect-prone and not defect-prone categories. The publicly available NASA datasets have been extensively used as part of this research. Objective-This short note investigates the extent to which published analyses based on the NASA defect datasets are meaningful and comparable. Method-We analyze the five studies published in the IEEE Transactions on Software Engineering since 2007 that have utilized these datasets and compare the two versions of the datasets currently in use. Results-We find important differences between the two versions of the datasets, implausible values in one dataset and generally insufficient detail documented on dataset preprocessing. Conclusions-It is recommended that researchers 1) indicate the provenance of the datasets they use, 2) report any preprocessing in sufficient detail to enable meaningful replication, and 3) invest effort in understanding the data prior to applying machine learners. © 1976-2012 IEEE.",data quality | defect prediction | Empirical software engineering | machine learning,IEEE Transactions on Software Engineering,2013-09-09,Article,"Shepperd, Martin;Song, Qinbao;Sun, Zhongbin;Mair, Carolyn",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85028922080,10.1109/TSE.2017.2731308,Authors' reply to 'comments on 'researcher bias: The use of machine learning in software defect prediction'',"In 2014 we published a meta-analysis of software defect prediction studies [1] . This suggested that the most important factor in determining results was Research Group, i.e., who conducts the experiment is more important than the classifier algorithms being investigated. A recent re-analysis [2] sought to argue that the effect is less strong than originally claimed since there is a relationship between Research Group and Dataset. In this response we show (i) the re-analysis is based on a small (21 percent) subset of our original data, (ii) using the same re-analysis approach with a larger subset shows that Research Group is more important than type of Classifier and (iii) however the data are analysed there is compelling evidence that who conducts the research has an effect on the results. This means that the problem of researcher bias remains. Addressing it should be seen as a matter of priority amongst those of us who conduct and publish experiments comparing the performance of competing software defect prediction systems. © 1976-2012 IEEE.",Defect prediction | Researcher bias | Software quality assurance,IEEE Transactions on Software Engineering,2018-11-01,Article,"Shepperd, Martin;Hall, Tracy;Bowes, David",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85042870809,10.1109/TSE.2018.2809496,Predictive Mutation Testing,"Test suites play a key role in ensuring software quality. A good test suite may detect more faults than a poor-quality one. Mutation testing is a powerful methodology for evaluating the fault-detection ability of test suites. In mutation testing, a large number of mutants may be generated and need to be executed against the test suite under evaluation to check how many mutants the test suite is able to detect, as well as the kind of mutants that the current test suite fails to detect. Consequently, although highly effective, mutation testing is widely recognized to be also computationally expensive, inhibiting wider uptake. To alleviate this efficiency concern, we propose Predictive Mutation Testing (PMT): The first approach to predicting mutation testing results without executing mutants. In particular, PMT constructs a classification model, based on a series of features related to mutants and tests, and uses the model to predict whether a mutant would be killed or remain alive without executing it. PMT has been evaluated on 163 real-world projects under two application scenarios (cross-version and cross-project). The experimental results demonstrate that PMT improves the efficiency of mutation testing by up to 151.4X while incurring only a small accuracy loss. It achieves above 0.80 AUC values for the majority of projects, indicating a good tradeoff between the efficiency and effectiveness of predictive mutation testing. Also, PMT is shown to perform well on different tools and tests, be robust in the presence of imbalanced data, and have high predictability (over 60 percent confidence) when predicting the execution results of the majority of mutants. © 1976-2012 IEEE.",binary classification | machine learning | mutation testing | PMT,IEEE Transactions on Software Engineering,2019-09-01,Article,"Zhang, Jie;Zhang, Lingming;Harman, Mark;Hao, Dan;Jia, Yue;Zhang, Lu",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85072973761,10.1109/TSE.2019.2945020,How to 'DODGE' Complex Software Analytics,"Machine learning techniques applied to software engineering tasks can be improved by hyperparameter optimization, i.e., automatic tools that find good settings for a learner's control parameters. We show that such hyperparameter optimization can be unnecessarily slow, particularly when the optimizers waste time exploring 'redundant tunings', i.e., pairs of tunings which lead to indistinguishable results. By ignoring redundant tunings, DODGE( E)E), a tuning tool, runs orders of magnitude faster, while also generating learners with more accurate predictions than seen in prior state-of-the-art approaches. © 1976-2012 IEEE.",defect prediction | hyperparameter optimization | Software analytics | text mining,IEEE Transactions on Software Engineering,2021-10-01,Article,"Agrawal, Amritanshu;Fu, Wei;Chen, Di;Shen, Xipeng;Menzies, Tim",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85122869706,10.1109/TSE.2022.3141758,ASTRAEA: Grammar-based Fairness Testing,"Software often produces biased outputs. In particular, machine learning (ML) based software is known to produce erroneous predictions when processing discriminatory inputs. Such unfair program behavior can be caused by societal bias. In the last few years, Amazon, Microsoft and Google have provided software services that produce unfair outputs, mostly due to societal bias (e.g. gender or race). In such events, developers are saddled with the task of conducting fairness testing. Fairness testing is challenging; developers are tasked with generating discriminatory inputs that reveal and explain biases. We propose a grammar-based fairness testing approach (called ASTRAEA) which leverages context-free grammars to generate discriminatory inputs that reveal fairness violations in software systems. Using probabilistic grammars, ASTRAEA also provides fault diagnosis by isolating the cause of observed software bias. ASTRAEAs diagnoses facilitate the improvement of ML fairness. ASTRAEA was evaluated on 18 software systems that provide three major natural language processing (NLP) services. In our evaluation, ASTRAEA generated fairness violations at a rate of about 18%. ASTRAEA generated over 573K discriminatory test cases and found over 102K fairness violations. Furthermore, ASTRAEA improves software fairness by about 76% via model-retraining, on average. IEEE",machine learning | natural language processing | program debugging | software fairness | software testing,IEEE Transactions on Software Engineering,2022-12-01,Article,"Soremekun, Ezekiel;Udeshi, Sakshi;Chattopadhyay, Sudipta",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85124188450,10.1109/TSE.2022.3147265,Neural Transfer Learning for Repairing Security Vulnerabilities in C Code,"In this paper, we address the problem of automatic repair of software vulnerabilities with deep learning. The major problem with data-driven vulnerability repair is that the few existing datasets of known confirmed vulnerabilities consist of only a few thousand examples. However, training a deep learning model often requires hundreds of thousands of examples. In this work, we leverage the intuition that the bug fixing task and the vulnerability fixing task are related and that the knowledge learned from bug fixes can be transferred to fixing vulnerabilities. In the machine learning community, this technique is called transfer learning. In this paper, we propose an approach for repairing security vulnerabilities named VRepair which is based on transfer learning. VRepair is first trained on a large bug fix corpus and is then tuned on a vulnerability fix dataset, which is an order of magnitude smaller. In our experiments, we show that a model trained only on a bug fix corpus can already fix some vulnerabilities. Then, we demonstrate that transfer learning improves the ability to repair vulnerable C functions. We also show that the transfer learning model performs better than a model trained with a denoising task and fine-tuned on the vulnerability fixing task. To sum up, this paper shows that transfer learning works well for repairing security vulnerabilities in C compared to learning on a small dataset. Author",seq2seq learning | transfer learning | Vulnerability fixing,IEEE Transactions on Software Engineering,2023-01-01,Article,"Chen, Zimin;Kommrusch, Steve;Monperrus, Martin",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85079240121,10.1109/UBMYK48245.2019.8965451,Vibration Signal Processing Based Bearing Defect Diagnosis with Transfer Learning,"It is very important to diagnose the fault condition of the bearing machines in order to make the machine run healthier. There are many successful studies in the detection of failures in the bearing machine made by conventional machine learning methods. However, these studies produce successful results in cases where the machines operate under the same condition and feature space is the same. Therefore, a deep learning-based diagnostic has been proposed for changing machine operating conditions. In the scope of our study, Keras and Tensorflow libraries, CNN network from scratch, VGG16 model and VGG19 deep learning models have been used for the classification of vibration images. Freeze the weights for transfer learning and remove the last fully connected layer to update the network in a problem-specific manner. The number of iterations and batch size has been determined by experimental studies. In this study, four faulty conditions have been successfully classified. While the accuracy rate of CNN network from scratch is 25%, the accuracy rate obtained by the VGG16 transfer learning method is 93% and the loss rate is 0.17% and the accuracy rate obtained by the VGG19 transfer learning method is 95% and the loss rate is 0.13%. © 2019 IEEE.",Bearing | Deep learning | Fault diagnosis | Transfer Learning | Vibration signal,"1st International Informatics and Software Engineering Conference: Innovative Technologies for Digital Transformation, IISEC 2019 - Proceedings",2019-11-01,Conference Paper,"Tastimur, Canan;Karakose, Mehmet;Akin, Erhan",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85061831841,10.1109/UCC-Companion.2018.00034,A review of adversarial behaviour in distributed multi-Agent optimisation,"This paper addresses the challenges of distributed multi-Agent optimisation, in environments with potential adversarial agents. In distributed multi-Agent optimisation, each agent has a local cost function and the collective goal is to optimise the global cost function, which takes as input, the output of each agent's local cost function. Such optimisation algorithms are used in different fields such as distributed machine learning, distributed robotics, and recently, distributed energy planning. A prominent assumption in most of these algorithms is that all the agents are cooperative, non-faulty, and non-Adversarial. Yet, in scenarios with multi-Agent systems, such assumptions are not always valid. Recently, there has been some research in the area of resilience and fault-Tolerant distributed multi-Agent optimisation. However, these studies either assume a fully Byzantine environment, or have their own adversary model with different sets of assumptions about adversary capabilities. This makes the analysis and comparison between the results challenging. This paper presents a review of such algorithms which helps in defining and investigating restricted adversarial behaviour in distributed multi-Agent optimisation. In comparison to the Byzantine environments which carry no assumptions about the agent's capability or purpose, in this paper adversaries have restrictions in regard to their knowledge, capabilities, purpose, and impact. This approach makes it easier to provide guarantees about the system performance depending on the specific adversary model and avoid the overly conservative, 'one size fits all' algorithms needed in fully Byzantine environments. Moreover, using the gained insights, previously unexplored aspects of adversarial behaviour in distributed multi-Agent optimisation are discovered and indicated as possible future research directions. © 2018 IEEE.",Adversarial Distributed Optimisation | Adversary Modelling | Byzantine Agent | Multi Agent System,"Proceedings - 11th IEEE/ACM International Conference on Utility and Cloud Computing Companion, UCC Companion 2018",2018-07-02,Conference Paper,"Fanitabasi, Farzam",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85081080020,10.1109/UEMCON47517.2019.8992983,Multi-Objective Optimization for Size and Resilience of Spiking Neural Networks,"Inspired by the connectivity mechanisms in the brain, neuromorphic computing architectures model Spiking Neural Networks (SNNs) in silicon. As such, neuromorphic architectures are designed and developed with the goal of having small, low power chips that can perform control and machine learning tasks. However, the power consumption of the developed hardware can greatly depend on the size of the network that is being evaluated on the chip. Furthermore, the accuracy of a trained SNN that is evaluated on chip can change due to voltage and current variations in the hardware that perturb the learned weights of the network. While efforts are made on the hardware side to minimize those perturbations, a software based strategy to make the deployed networks more resilient can help further alleviate that issue. In this work, we study Spiking Neural Networks in two neuromorphic architecture implementations with the goal of decreasing their size, while at the same time increasing their resiliency to hardware faults. We leverage an evolutionary algorithm to train the SNNs and propose a multiobjective fitness function to optimize the size and resiliency of the SNN. We demonstrate that this strategy leads to well-performing, small-sized networks that are more resilient to hardware faults. © 2019 IEEE.",Evolutionary Optimization | Fault Tolerance | Multi-objective | Neuromorphic Computing | Spiking Neural Networks,"2019 IEEE 10th Annual Ubiquitous Computing, Electronics and Mobile Communication Conference, UEMCON 2019",2019-10-01,Conference Paper,"Dimovska, Mihaela;Johnston, Travis;Schuman, Catherine D.;Parker Mitchell, J.;Potok, Thomas E.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84864996579,10.1109/VTS.2012.6231068,Towards spatial fault resilience in array processors,"Computing with large die-size graphical processors (that need huge arrays of identical structures) in the late CMOS era is abounding with challenges due to spatial non-idealities arising from chip-to-chip and within-chip variation of MOSFET threshold voltage. In this paper, we propose a machine learning based software-framework for in-situ prediction and correction of computation corrupted due to threshold voltage variation of transistors. Based on semi-supervised training imparted to a fully connected cascade feed-forward neural network (FCCFF-NN), the NN makes an accurate prediction of the underlying hardware, creating a spatial map of faulty processing elements (PE). The faulty elements identified by the NN are avoided in future computing. Further, any transient faults occurring over and above these spatial faults are tracked, and corrected if the number of PEs involved in a particle strike is above a preset threshold. For the purposes of experimental validation, we consider a 256 x 256 array of PE. Each PE is comprised of a multiply-accumulate (MAC) block with three 8 bit registers (two for inputs and one for storing the computed result). One thousand instances of this processor array are created and PEs in each instance are randomly perturbed with threshold voltage variation. Common image processing operations such as low pass filtering and edge enhancement are performed on each of these 1000 instances. A fraction of these images (about 10%) is used to train the NN for spatial non-idealities. Based on this training, the NN is able to accurately predict the spatial extremities in 95% of all the remaining 90% of the cases. The proposed NN based error tolerance results in superior quality images whose degradation is no longer visually perceptible. © 2012 IEEE.",,Proceedings of the IEEE VLSI Test Symposium,2012-08-20,Conference Paper,"Sindia, Suraj;Agrawal, Vishwani D.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84973866705,10.1109/VTS.2016.7477301,Online soft-error vulnerability estimation for memory arrays,"Radiation-induced soft errors are a major reliability concern in circuits fabricated at advanced technology nodes. Online soft-error vulnerability estimation offers the flexibility of exploiting dynamic fault-tolerant mechanisms for cost-effective reliability enhancement. We propose a generic run-time method with low area and power overhead to predict the soft-error vulnerability of on-chip memory arrays. The vulnerability prediction is based on signal probabilities (SPs) of a small set of flip-flops, chosen at design time, by studying the correlation between the soft-error vulnerability and the flip-flop SPs for representative workloads. We exploit machine learning to develop a predictive model that can be deployed in the system in software form. Simulation results on two processor designs show that the proposed technique can accurately estimate the soft-error vulnerability of on-chip memory arrays that constitute the instruction cache, the data cache, and the register file. © 2016 IEEE.",,Proceedings of the IEEE VLSI Test Symposium,2016-05-23,Conference Paper,"Vijayan, Arunkumar;Koneru, Abhishek;Ebrahimit, Mojtaba;Chakrabarty, Krishnendu;Tahoori, Mehdi B.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85086500033,10.1109/VTS48691.2020.9107603,A Deterministic-Statistical Multiple-Defect Diagnosis Methodology,"Software diagnosis is the process of locating and characterizing a defect in a failing chip. It is the cornerstone of failure analysis that consequently enables yield learning and monitoring. However, multiple-defect diagnosis is challenging due to error masking and unmasking effects, and exponential complexity of the solution search process. This paper describes a three-phase, physically-aware diagnosis methodology called MDLearnX to effectively diagnose multiple defects, and in turn, aid in accelerating the design and process development. The first phase identifies a defect that resembles traditional fault models. The second and the third phases utilize the X-fault model and machine learning to identify correct candidates. Results from a thorough fault injection and simulation experiment demonstrate that MD-LearnX returns an ideal diagnosis 2X more often than commercial diagnosis. Its effectiveness is further evidenced through a silicon experiment, where, on average, MD-LearnX returns 5.3 fewer candidates per diagnosis as compared to state-of-the-art commercial diagnosis without losing accuracy. © 2020 IEEE.",,Proceedings of the IEEE VLSI Test Symposium,2020-04-01,Conference Paper,"Mittal, Soumya;Shawn Blanton, R. D.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85119830021,10.1109/WF-IoT51360.2021.9595075,An Overview of Laser Injection against Embedded Neural Network Models,"For many IoT domains, Machine Learning and more particularly Deep Learning brings very efficient solutions to handle complex data and perform challenging and mostly critical tasks. However, the deployment of models in a large variety of devices faces several obstacles related to trust and security. The latest is particularly critical since the demonstrations of severe flaws impacting the integrity, confidentiality and accessibility of neural network models. However, the attack surface of such embedded systems cannot be reduced to abstract flaws but must encompass the physical threats related to the implementation of these models within hardware platforms (e.g., 32-bit microcontrollers). Among physical attacks, Fault Injection Analysis (FIA) are known to be very powerful with a large spectrum of attack vectors. Most importantly, highly focused FIA techniques such as laser beam injection enable very accurate evaluation of the vulnerabilities as well as the robustness of embedded systems. Here, we propose to discuss how laser injection with state-of-the-art equipment, combined with theoretical evidences from Adversarial Machine Learning, highlights worrying threats against the integrity of deep learning inference and claims that join efforts from the theoretical AI and Physical Security communities are a urgent need. © 2021 IEEE.",Adversarial Machine Example | Deep Learning | Hardware Security | Laser Fault Injection,"7th IEEE World Forum on Internet of Things, WF-IoT 2021",2021-06-14,Conference Paper,"Dumont, Mathieu;Moellic, Pierre Alain;Viera, Raphael;Dutertre, Jean Max;Bernhard, Remi",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85079510124,10.1109/iSPEC48194.2019.8974897,Mining Partially Labeled Data from Edge Devices to Detect and Locate High Impedance Faults,"The security of active distribution systems is critical to grid modernization along with deep renewable penetration, where the protection plays a vital role. Among various security issues in protection, conventional protection clears only 17.5% of staged high impedance faults (HIFs) due to the limited electrical data utilization. For resolving this problem, a detection and location scheme based on μ-PMUs is presented to enhance data processing capability for HIF detection through machine learning and big data analytics. To detect HIFs with reduced cost on data labeling, we choose expectation-maximization (EM) algorithm for semi-supervised learning (SSL) since it is capable of expressing complex relationships between the observed and target variables by fitting Gaussian models. As one of the generative models, EM algorithm is compared with two discriminative models to highlight its detection performance. To make HIF location robust to HIF impedance variation, we adopt a probabilistic model embedding parameter learning into the physical line modeling. The location accuracy is validated at multiple locations of a distribution line. Numerical results show that the proposed EM algorithm greatly saves labeling cost and outperforms other SSL methods. Hardware-in-the-loop simulation proves a superior HIF location accuracy and detection time to complement the HIF's probabilistic model. With outstanding performance, we develop software for our utility partner to integrate the proposed scheme. © 2019 IEEE.",,"iSPEC 2019 - 2019 IEEE Sustainable Power and Energy Conference: Grid Modernization for Energy Revolution, Proceedings",2019-11-01,Conference Paper,"Cui, Qiushi;Weng, Yang",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84878378991,10.1117/12.2009966,Multiple defect interpretation based on Gaussian processes for MFL technology,"Magnetic Flux Leakage (MFL) technology has been used in non-destructive testing for more than three decades. There have been several publications in detecting and sizing defects on metal pipes using machine learning techniques. Most of these literature focus on isolated defects, which is far from the real scenario. This study is towards the generalization of interpretation of the leakage flux in the presence of multiple defects based on simulation models, together with data-driven inference methodologies, such as Gaussian Process (GP) models. A MFL device has been simulated using both COMSOL Multiphysics and ANSYS software followed by prototyping the same device for experimental validations. Multiple defects with different geometrical configurations were introduced on a cast iron pipe sample and both radial and axial components of the leakage field have been measured. It was observed that both axial and radial components differ with different defect configurations. We propose to use GP to solve the inverse model problem by capturing such behaviors, i.e. to recover the profille of a cluster of defects from the measurements of a MFL device. The data was used to learn the non-parametric GP model with squared exponential covariance function and automatic relevance determination to solve this regression problem. Extensive quantitative and qualitative evaluations are presented using simulated and experimental data that validate the success of the proposed non-parametric methodology for interpreting the profiling of clusters of defects with MFL technology. © 2013 SPIE.",Data driven inference | Finite element analysis | Gaussian processes | Magnetic flux leakage | Nondestructive testing,Proceedings of SPIE - The International Society for Optical Engineering,2013-06-05,Conference Paper,"Wijerathna, Buddhi;Vidal-Calleja, Teresa;Kodagoda, Sarath;Zhang, Qiang;Miro, Jaime Valls",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85046466397,10.1117/12.2300952,Deep machine learning based Image classification in hard disk drive manufacturing (Conference Presentation),"A key sensor element in a Hard Disk Drive (HDD) is the read-write head device. The device is complex 3D shape and its fabrication requires over thousand process steps with many of them being various types of image inspection and critical dimension (CD) metrology steps. In order to have high yield of devices across a wafer, very tight inspection and metrology specifications are implemented. Many images are collected on a wafer and inspected for various types of defects and in CD metrology the quality of image impacts the CD measurements. Metrology noise need to be minimized in CD metrology to get better estimate of the process related variations for implementing robust process controls. Though there are specialized tools available for defect inspection and review allowing classification and statistics. However, due to unavailability of such advanced tools or other reasons, many times images need to be manually inspected. SEM Image inspection and CD-SEM metrology tools are different tools differing in software as well. SEM Image inspection and CD-SEM metrology tools are separate tools differing in software and purpose. There have been cases where a significant numbers of CD-SEM images are blurred or have some artefact and there is a need for image inspection along with the CD measurement. Tool may not report a practical metric highlighting the quality of image. Not filtering CD from these blurred images will add metrology noise to the CD measurement. An image classifier can be helpful here for filtering such data. This paper presents the use of artificial intelligence in classifying the SEM images. Deep machine learning is used to train a neural network which is then used to classify the new images as blurred and not blurred. Figure 1 shows the image blur artefact and contingency table of classification results from the trained deep neural network. Prediction accuracy of 94.9 % was achieved in the first model. Paper covers other such applications of the deep neural network in image classification for inspection, review and metrology. © 2018 SPIE.",Artificial Intelligence | CD Metrology | CD-SEM | Deep Machine Learning | Defect Inspection | Hard Disk Drive | Neural Networks | Offline Image Analysis | Tensor flow™,Proceedings of SPIE - The International Society for Optical Engineering,2018-01-01,Conference Paper,"Rana, Narender;Chien, Chester",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85072551675,10.1117/12.2519226,Learning to rank faulty source files for dependent bug reports,"With the rise of autonomous systems, the automation of faults detection and localization becomes critical to their reliability. An automated strategy that can provide a ranked list of faulty modules or files with respect to how likely they contain the root cause of the problem would help in the automation bug localization. Learning from the history if previously located bugs in general, and extracting the dependencies between these bugs in particular, helps in building models to accurately localize any potentially detected bugs. In this study, we propose a novel fault localization solution based on a learning-To-rank strategy, using the history of previously localized bugs and their dependencies as features, to rank files in terms of their likelihood of being a root cause of a bug. The evaluation of our approach has shown its efficiency in localizing dependent bugs. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.",bug localization | machine learning | mining software repositories | software quality,Proceedings of SPIE - The International Society for Optical Engineering,2019-01-01,Conference Paper,"Safdari, Nasir;Alrubaye, Hussein;Aljedaani, Wajdi;Baez Baez, Bladimir;Distasi, Andrew;Mkaouer, Mohamed Wiem",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85107510460,10.1117/12.2584369,Modeling of guided waves for aerospace applications,"Advancements in computer hardware has led to new possibilities for rapid modeling and simulation capabilities across many scientific fields. Nondestructive evaluation (NDE) can benefit from increased use of simulation tools to guide optimization of inspection and health monitoring methods, enhance understanding of data, aid in development of defect characterization methods, and generate data sets for use with machine learning and model-assisted probability of detection. Recent work at NASA has entailed development and benchmarking of both custom simulation codes and commercial simulation tools for ultrasonic wave propagation. This paper describes recent work at NASA in modeling of guided waves in composites and other aerospace materials. Results and computational speeds for a composite benchmark case are reported for a custom finite difference Rotated Staggered Grid code and for the commercial finite element software package, Pogo. Recent progress in linking NDE models to parametric analysis tools is also discussed. © 2021 SPIE",Guided wave | Modeling | Simulation | Ultrasound,Proceedings of SPIE - The International Society for Optical Engineering,2021-01-01,Conference Paper,"Leckey, Cara;Frankforter, Erik;Horne, Michael;Schneck, William",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-17644427017,10.1117/12.580587,Conceptual framework design for integrated visual inspection system,"This paper describes a framework for automatic generation of an image processing algorithm that consists of preprocessing, feature extraction, classification and algorithm evaluation modules based on machine learning. With a view to applying the generated algorithm to industrial visual inspection system, we intend to offer a framework model equipped with the below-mentioned features. Also, we want to report on the experimental result of die offered model. 1. Automatically generate by machine learning an image processing algorithm to extract regions that have same characteristics as specified by users. 2. Generate in particular a high-precision image processing algorithm, improving the level of statistical separation between true and false defects that may cause a deterioration actor in classification accuracy. 3. Optimize an image improving filter sequence in preprocessing modules by means of GA(Genetic Algorithm).",Automatic Generation | Genetic Algorithm | Image Processing Algorithm | Machine Learning | Visual Inspection System,Proceedings of SPIE - The International Society for Optical Engineering,2004-12-01,Conference Paper,"Ueda, Yasuhiro;Yamamoto, Shuhei;Iden, Tamon;Yanase, Masakazu;Shigeyama, Yoshihide;Nakamura, Atsuyoshi",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85046038318,10.1142/S0218194018500158,Predicting the Severity of Bug Reports Based on Feature Selection,"In software maintenance process, it is a fairly important activity to predict the severity of bug reports. However, manually identifying the severity of bug reports is a tedious and time-consuming task. So developing automatic judgment methods for predicting the severity of bug reports has become an urgent demand. In general, a bug report contains a lot of descriptive natural language texts, thus resulting in a high-dimensional feature set which poses serious challenges to traditionally automatic methods. Therefore, we attempt to use automatic feature selection methods to improve the performance of the severity prediction of bug reports. In this paper, we introduce a ranking-based strategy to improve existing feature selection algorithms and propose an ensemble feature selection algorithm by combining existing ones. In order to verify the performance of our method, we run experiments over the bug reports of Eclipse and Mozilla and conduct comparisons with eight commonly used feature selection methods. The experiment results show that the ranking-based strategy can effectively improve the performance of the severity prediction of bug reports by up to 54.76% on average in terms of F-measure, and it also can significantly reduce the dimension of the feature set. Meanwhile, the ensemble feature selection method can get better results than a single feature selection algorithm. © 2018 World Scientific Publishing Company.",Bug report | feature selection | machine learning | severity prediction,International Journal of Software Engineering and Knowledge Engineering,2018-04-01,Article,"Liu, Wenjie;Wang, Shanshan;Chen, Xin;Jiang, He",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85095973810,10.1142/S0218194020500394,Method for Predicting Mobile Service Evolution from User Reviews and Update Logs,"Because of rapid growth in mobile application markets, competition between companies that provide similar applications has become fierce. To improve user satisfaction for keeping existing users and attracting new users, application developers need to quickly respond to customer feedback regarding functionality and performance defects. In software engineering, specifying an accurate evolution plan according to user feedback is useful but quite difficult. Hence, we propose an approach for predicting and recommending evolution plans to application developers that includes: (1) when a new version of an App should be released; (2) which features should be updated in the next version and (3) if a new version is released, to what degree users would like or dislike it. This approach is based on an elaborate text analysis of massive numbers of user reviews and App update histories. A collocation-based mRAKE method is presented to extract requested and updated features from user reviews and update logs, and the intensity and sentiment scores of each feature are calculated to quantitatively represent time-series histories of App updates and user requests. Machine learning algorithms including linear support vector, Gaussian naïve Bayes and logistic regression are employed to discover the underlying correlation between user opinions embedded in their reviews and the App update behaviors of developers, and rich experiments were conducted on real data to validate the effectiveness of the proposed approach. Overall, our approach can achieve an average accuracy of 72.8% and 93.7% in release time recommendation and content updates of successive versions, respectively, and it can predict user reactions to a planned version with an average accuracy of above 89.0%. © 2020 World Scientific Publishing Company.",machine learning | Mobile Apps | prediction | release plan | user reviews,International Journal of Software Engineering and Knowledge Engineering,2020-10-01,Article,"Song, Jiafei;Wang, Zhongjie;Tu, Zhiying;Xu, Xiaofei",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85124009371,10.1142/S0218194021400192,Automatically Generating Release Notes with Content Classification Models,"Release notes are admitted as an essential technical document in software maintenance. They summarize the main changes, e.g. bug fixes and new features, that have happened in the software since the previous release. Manually producing release notes is a time-consuming and challenging task. For that reason, sometimes developers neglect to write release notes. For example, we collect data from GitHub with over 1900 releases, and among them, 37% of the release notes are empty. To mitigate this problem, we propose an automatic release notes generation approach by applying the text summarization techniques, i.e. TextRank. To improve the keyword extraction method of traditional TextRank, we integrate the GloVe word embedding technique with TextRank. After generating release notes automatically, we apply machine learning algorithms to classify the release note contents (or sentences). We classify the contents into six categories, e.g. bug fixes and performance improvements, to represent the release notes better for users. We use the evaluation metric, e.g. ROUGE, to evaluate the automatically generated release notes. We also compare the performance of our technique with two popular extractive algorithms, e.g. Luhn's and latent semantic analysis (LSA). Our evaluation results show that the improved TextRank method outperforms the two algorithms. © 2021 World Scientific Publishing Company.",Extractive text summarization | GloVe | Machine learning | Software documentation | Software release notes | TextRank,International Journal of Software Engineering and Knowledge Engineering,2021-12-01,Article,"Nath, Sristy Sumana;Roy, Banani",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84892152565,10.1142/S0218539313500253,Predicting software change in an open source software using machine learning algorithms,"Due to various reasons such as ever increasing demands of the customer or change in the environment or detection of a bug, changes are incorporated in a software. This results in multiple versions or evolving nature of a software. Identification of parts of a software that are more prone to changes than others is one of the important activities. Identifying change prone classes will help developers to take focused and timely preventive actions on the classes of the software with similar characteristics in the future releases. In this paper, we have studied the relationship between various object oriented (OO) metrics and change proneness. We collected a set of OO metrics and change data of each class that appeared in two versions of an open source dataset, 'Java TreeView', i.e., version 1.1.6 and version 1.0.3. Besides this, we have also predicted various models that can be used to identify change prone classes, using machine learning and statistical techniques and then compared their performance. The results are analyzed using Area Under the Curve (AUC) obtained from Receiver Operating Characteristics (ROC) analysis. The results show that the models predicted using both machine learning and statistical methods demonstrate good performance in terms of predicting change prone classes. Based on the results, it is reasonable to claim that quality models have a significant relevance with OO metrics and hence can be used by researchers for early prediction of change prone classes. © 2013 World Scientific Publishing Company.",Change proneness | Empirical validation | Machine learning | Software quality,"International Journal of Reliability, Quality and Safety Engineering",2013-12-01,Article,"Malhotra, Ruchika;Bansal, Ankita Jain",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85075536205,10.1142/S0219720019500264,Glycine-induced formation and druggability score prediction of protein surface pockets,"Nowadays, it is well established that most of the human diseases which are not related to pathogen infections have their origin from DNA disorders. Thus, DNA mutations, waiting for the availability of CRISPR-like remedies, will propagate into proteomics, offering the possibility to select natural or synthetic molecules to fight against the effects of malfunctioning proteins. Drug discovery, indeed, is a flourishing field of biotechnological research to improve human health, even though the development of a new drug is increasingly more expensive in spite of the massive use of informatics in Medicinal Chemistry. CRISPR technology adds new alternatives to cure diseases by removing DNA defects responsible of genome-related pathologies. In principle, the same technology, however, could also be exploited to induce protein mutations whose effects are controlled by the presence of suitable ligands. In this paper, a new idea is proposed for the realization of mutated proteins, on the surface of which more spacious transient pockets are formed and, therefore, are more suitable for hosting drugs. In particular, new allosteric sites are obtained by replacing amino-acids with bulky side chains with glycine, Gly, the smallest natural amino-acid. We also present a machine learning approach to evaluate the druggability score of new (or enlarged) pockets. Preliminary experimental results are very promising, showing that 10% of the sites created by the Gly-pipe software are druggable. © 2019 World Scientific Publishing Europe Ltd.",druggability score | neural networks | Protein glycinization | protein-ligand interaction | transient pockets,Journal of Bioinformatics and Computational Biology,2019-10-01,Article,"Bongini, Pietro;Niccolai, Neri;Bianchini, Monica",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-77949344285,10.1145/1595696.1595715,Improving bug triage with bug tossing graphs,"A bug report is typically assigned to a single developer who is then responsible for fixing the bug. In Mozilla and Eclipse, between 37%-44% of bug reports are ""tossed"" (reassigned) to other developers, for example because the bug has been assigned by accident or another developer with additional expertise is needed. In any case, tossing increases the time-to-correction for a bug. In this paper, we introduce a graph model based on Markov chains, which captures bug tossing history. This model has several desirable qualities. First, it reveals developer networks which can be used to discover team structures and to find suitable experts for a new task. Second, it helps to better assign developers to bug reports. In our experiments with 445,000 bug reports, our model reduced tossing events, by up to 72%. In addition, the model increased the prediction accuracy by up to 23 percentage points compared to traditional bug triaging approaches. Copyright 2009 ACM.",Bug report assignment | Bug tossing | Bug triage | Issue tracking | Machine learning | Problem tracking,ESEC-FSE'09 - Proceedings of the Joint 12th European Software Engineering Conference and 17th ACM SIGSOFT Symposium on the Foundations of Software Engineering,2009-12-01,Conference Paper,"Jeong, Gaeul;Kim, Sunghun;Zimmermann, Thomas",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-80054066380,10.1145/2020390.2020402,Selecting discriminating terms for bug assignment: A formal analysis,"Background. The bug assignment problem is the problem of triaging new bug reports to the most qualified developer. The qualified developer is the one who has enough knowledge in a specific area that is relevant to the reported bug. In recent years, bug triaging has received a considerable amount of attention from researchers. In previous work, bugs were represented as vectors of terms extracted from the bug reports' description. Once the bugs are represented as vectors in the terms space, traditional machine learning techniques are employed for the bug assignment. Most of the previous algorithms are marred by low accuracy values. Aims. This paper formulates the bug assignment problem as a classification task, and then examines the impact of several term selection approaches on the classification effectiveness. Method. Three variants selection methods that are based on the Log Odds Ratio (LOR) score are compared against methods that are based on the Information Gain (IG) score and Latent Semantic Analysis (LSA). The main difference in the methods that are based on the LOR score is in the process of selecting the terms. Results. Term selection techniques that are based on the Log Odds Ratio achieved up to 30% improvement in the precision and up to 5% higher in recall compared to other term selection methods such as Latent Semantic Analysis and Information Gain. Conclusions. Experimental results showed that the effectiveness of bug assignment methods is directly affected by the selected terms that are used in the classification methods. Copyright © 2011 ACM.",Bug assignment | Bug reports | Classification | Machine learning,ACM International Conference Proceeding Series,2011-10-19,Conference Paper,"Aljarah, Ibrahim;Banitaan, Shadi;Abufardeh, Sameer;Jin, Wei;Salem, Saeed",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-80054083336,10.1145/2020390.2020403,Empirical validation of human factors in predicting issue lead time in open source projects,"[Context] Software developers often spend a significant portion of their resources resolving submitted evolution issue reports. Classification or prediction of issue lead time is useful for prioritizing evolution issues and supporting human resources allocation in software maintenance. However, the predictability of issue lead time is still a research gap that calls for more empirical investigation. [Aim] In this paper, we empirically assess different types of issue lead time prediction models using human factor measures collected from issue tracking systems. [Method] We conduct an empirical investigation of three active open source projects. A machine learning based classification and statistical univariate and multivariate analyses are performed. [Results] The accuracy of classification models in ten-fold cross-validation varies from 75.56% to 91%. The R2 value of linear multivariate regression models ranges from 0.29 to 0.60. Correlation analysis confirms the effectiveness of collaboration measures, such as the number of stakeholders and number of comments, in prediction models. The measures of assignee past performance are also an effective indicator of issue lead time. [Conclusions] The results indicate that the number of stakeholders and average past issue lead time are important variables in constructing prediction models of issue lead time. However, more variables should be explored to achieve better prediction performance. Copyright © 2011 ACM.",Bug lead time | Bug prediction | Bug triage | Classification model | Empirical study | Issue lead time | Issue resolution time | Regression model,ACM International Conference Proceeding Series,2011-10-19,Conference Paper,"Anh, Nguyen Duc;Cruzes, Daniela S.;Conradi, Reidar;Ayala, Claudia",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84867708403,10.1145/2365324.2365326,The scientific basis for prediction research,"In recent years there has been a huge growth in using statistical and machine learning methods to find useful prediction systems for software engineers. Of particular interest is predicting project effort and duration and defect behaviour. Unfortunately though results are often promising no single technique dominates and there are clearly complex interactions between technique, training methods and the problem domain. Since we lack deep theory our research is of necessity experimental. Minimally, as scientists, we need reproducible studies. We also need comparable studies. I will show through a meta-analysis of many primary studies that we are not presently in that situation and so the scientific basis for our collective research remains in doubt. By way of remedy I will argue that we need to address these issues of reporting protocols and expertise plus ensure blind analysis is routine. Copyright © 2012 ACM.",Defect prediction | Empirical research | Machine learning | Software metrics,ACM International Conference Proceeding Series,2012-10-26,Conference Paper,"Shepperd, Martin",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84867733834,10.1145/2365324.2365338,Comparing the performance of fault prediction models which report multiple performance measures: Recomputing the confusion matrix,"There are many hundreds of fault prediction models published in the literature. The predictive performance of these models is often reported using a variety of different measures. Most performance measures are not directly comparable. This lack of comparability means that it is often difficult to evaluate the performance of one model against another. Our aim is to present an approach that allows other researchers and practitioners to transform many performance measures of categorical studies back into a confusion matrix. Once performance is expressed in a confusion matrix alternative preferred performance measures can then be derived. Our approach has enabled us to compare the performance of 600 models published in 42 studies. We demonstrate the application of our approach on several case studies, and discuss the advantages and implications of doing this. Copyright © 2012 ACM.",Confusion matrix | Fault | Machine learning,ACM International Conference Proceeding Series,2012-10-26,Conference Paper,"Bowes, David;Hall, Tracy;Gray, David",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84871326149,10.1145/2393596.2393628,Has this bug been reported?,"Bug reporting is an uncoordinated process that is often the cause of redundant workload in triaging and fixing bugs due to many duplicated bug reports. Furthermore, quite often, same bugs are repeatedly reported as users or testers are unaware of whether they have been reported from the search query results. In order to reduce both the users and developers' efforts, the quality of search in a bug tracking system is crucial. However, all existing search functions in a bug tracking system produce results with undesired relevance and ranking. Hence, it is essential to provide an effective search function to any bug tracking system. Learning to rank (LTR) is a supervised machine learning technique that is used to construct a ranking model from training data. We propose a novel approach by using LTR to search for potentially related bug reports in a bug tracking system. Our method uses a set of proposed features of bug reports and queries. A preliminary evaluation shows that our approach can enhance the quality of searching for similar bug reports, therefore, relieving the burden of developers in dealing with duplicate bug reports. © 2012 ACM.",bug report | bug tracking system | duplicate | learning to rank | search engine | search quality,"Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering, FSE 2012",2012-12-24,Conference Paper,"Liu, Kaiping;Tan, Hee Beng Kuan;Chandramohan, Mahinthan",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84993661583,10.1145/2568225.2568303,Data-guided repair of selection statements,"Database-centric programs form the backbone of many enterprise systems. Fixing defects in such programs takes much human effort due to the interplay between imperative code and database-centric logic. This paper presents a novel data-driven approach for automated fixing of bugs in the selection condition of database statements (e.g., WHERE clause of SELECT statements) - a common form of bugs in such programs. Our key observation is that in real-world data, there is information latent in the distribution of data that can be useful to repair selection conditions efficiently. Given a faulty database program and input data, only a part of which induces the defect, our novelty is in determining the correct behavior for the defect-inducing data by taking advantage of the information revealed by the rest of the data. We accomplish this by employing semi-supervised learning to predict the correct behavior for defect-inducing data and by patching up any inaccuracies in the prediction by a SAT-based combinatorial search. Next, we learn a compact decision tree for the correct behavior, including the correct behavior on the defect-inducing data. This tree suggests a plausible fix to the selection condition. We demonstrate the feasibility of our approach on seven realworld examples. © 2014 ACM.",ABAP | data-centric programs | Databases | Machine Learning | Program Repair | SAT | Support Vector Machines,Proceedings - International Conference on Software Engineering,2014-05-31,Conference Paper,"Gopinath, Divya;Khurshid, Sarfraz;Saha, Diptikalyan;Chandra, Satish",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84908630925,10.1145/2597073.2597090,New features for duplicate bug detection,"Issue tracking software of large software projects receive a large volume of issue reports each day. Each of these issues is typically triaged by hand, a time consuming and error prone task. Additionally, issue reporters lack the necessary understanding to know whether their issue has previously been reported. This leads to issue trackers containing a lot of duplicate reports, adding complexity to the triaging task. Duplicate bug report detection is designed to aid developers by automatically grouping bug reports concerning identical issues. Previous work by Alipour et al. has shown that the textual, categorical, and contextual information of an issue report are effective measures in duplicate bug report detection. In our work, we extend previous work by introducing a range of metrics based on the topic distribution of the issue reports, relying only on data taken directly from bug reports. In particular, we introduce a novel metric that measures the first shared topic between two topic-document distributions. This paper details the evaluation of this group of pair-based metrics with a range of machine learning classifiers, using the same issues used by Alipour et al. We demonstrate that the proposed metrics show a significant improvement over previous work, and conclude that the simple metrics we propose should be considered in future studies on bug report deduplication, as well as for more general natural language processing applications.",Duplicate bug reports | Machine learning | Topic model,"11th Working Conference on Mining Software Repositories, MSR 2014 - Proceedings",2014-05-31,Conference Paper,"Klein, Nathan;Corley, Christopher S.;Kraft, Nicholas A.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84938807326,10.1145/2597073.2597100,Finding patterns in static analysis alerts: Improving actionable alert ranking,"Static analysis (SA) tools that find bugs by inferring programmer beliefs (e.g., FindBugs) are commonplace in today's software industry. While they find a large number of actual defects, they are often plagued by high rates of alerts that a developer would not act on (unactionable alerts) because they are incorrect, do not significantly affect program execution, etc. High rates of unactionable alerts decrease the utility of static analysis tools in practice. We present a method for differentiating actionable and unactionable alerts by finding alerts with similar code patterns. To do so, we create a feature vector based on code characteristics at the site of each SA alert. With these feature vectors, we use machine learning techniques to build an actionable alert prediction model that is able to classify new SA alerts. We evaluate our technique on three subject programs using the FindBugs static analysis tool and the Faultbench benchmark methodology. For a developer inspecting the top 5% of all alerts for three sample projects, our approach is able to identify 57 of 211 actionable alerts, which is 38 more than the FindBugs priority measure. Combined with previous actionable alert identification techniques, our method finds 75 actionable alerts in the top 5%, which is four more actionable alerts (a 6% improvement) than previous actionable alert identification techniques. Copyright 2014 ACM.",Alert classification | Alert patterns | Bug detection | Machine learning | Static analysis,"11th Working Conference on Mining Software Repositories, MSR 2014 - Proceedings",2014-05-31,Conference Paper,"Hanam, Quinn;Tan, Lin;Holmes, Reid;Lam, Patrick",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84905662458,10.1145/2639490.2639504,A proposed method to evaluate and compare fault predictions across studies,"Studies on fault prediction often pay little attention to empirical rigor and presentation. Researchers might not have full command over the statistical method they use, full understanding of the data they have, or tend not to report key details about their work. What does it happen when we want to compare such studies for building a theory on fault prediction? There are two issues that if not addressed, we believe, prevent building such theory. The first concerns how to compare and report prediction performance across studies on different data sets. The second regards fitting performance of prediction models. Studies tend not to control and report the performance of predictors on historical data underestimating the risk that good predictors may poorly perform on past data. The degree of both fitting and prediction performance determines the risk managers are requested to take when they use such predictors. In this work, we propose a framework to compare studies on categorical fault prediction that aims at addressing the two issues. We propose three algorithms that automate our framework. We finally review baseline studies on fault prediction to discuss the application of the framework. Copyright 2014 ACM.",Confusion matrix | Fault | Machine learning | Model comparison,ACM International Conference Proceeding Series,2014-01-01,Conference Paper,"Russo, Barbara",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84929572619,10.1145/2741948.2741965,Malt: Distributed data-parallelism for existing ML applications,"Machine learning methods, such as SVM and neural networks, often improve their accuracy by using models with more parameters trained on large numbers of examples. Building such models on a single machine is often impractical because of the large amount of computation required. We introduce MALT, a machine learning library that integrates with existing machine learning software and provides data parallel machine learning. MALT provides abstractions for fine-grained in-memory updates using one-sided RDMA, limiting data movement costs during incremental model updates. MALT allows machine learning developers to specify the dataflow and apply communication and representation optimizations. Through its general-purpose API, MALT can be used to provide data-parallelism to existing ML applications written in C++ and Lua and based on SVM, matrix factorization and neural networks. In our results, we show MALT provides fault tolerance, network efficiency and speedup to these applications.",,"Proceedings of the 10th European Conference on Computer Systems, EuroSys 2015",2015-04-17,Conference Paper,"Li, Hao;Kadav, Asim;Kruus, Erik;Ungureanu, Cristian",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84978484033,10.1145/2915970.2916007,The Jinx on the NASA software defect data sets,"Background: The NASA datasets have previously been used extensively in studies of software defects. In 2013 Shepperd et al. presented an essential set of rules for removing erroneous data from the NASA datasets making this data more reliable to use. Objective: We have now found additional rules necessary for removing problematic data which were not identified by Shepperd et al. Results: In this paper, we demonstrate the level of erroneous data still present even after cleaning using Shepperd et al.'s rules and apply our new rules to remove this erroneous data. Conclusion: Even after systematic data cleaning of the NASA MDP datasets, we found new erroneous data. Data quality should always be explicitly considered by researchers before use. © 2016 ACM.",Data quality | Machine learning | Software defect prediction,ACM International Conference Proceeding Series,2016-06-01,Conference Paper,"Petrić, Jean;Bowes, David;Hall, Tracy;Christianson, Bruce;Baddoo, Nathan",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84978763067,10.1145/2933267.2933271,Industry paper: Proactive disruption management system: How not to be surprised by upcoming situations,"In most industrial processing scenarios the value of a product increases over time in the value chain. To avoid unnecessary processing steps, it is of immense importance to detect defects as early as possible in the value creating process. These situations of interest can be distinguished as specified and unspecified situations, dependent on whether the cause-effect relation is known and defined or not. In this article we describe ongoing work on a proactive disruption management system for manufacturing environments, which helps being prepared for the unexpected by applying a combination of unsupervised and supervised machine learning for the identification and prediction of unspecified situations and adopting data mining techniques to derive predictive patterns for specified situations. We also introduce a real-world use case from the field of semiconductor manufacturing and present first preliminary results. © 2016 ACM.",Complex event processing | Disruption management system | Machine learning | Proactive event-driven computing,DEBS 2016 - Proceedings of the 10th ACM International Conference on Distributed and Event-Based Systems,2016-06-13,Conference Paper,"Sejdovic, Suad;Hegenbarth, Yvonne;Ristow, Gerald H.;Schmidt, Roland",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84989168220,10.1145/2970276.2975934,Towards efficient and effective automatic program repair,"Automatic Program Repair (APR) has recently been an emerging research area, addressing an important challenge in software engineering. APR techniques, if effective and efficient , can greatly help software debugging and maintenance. Recently proposed APR techniques can be generally classified into two families, namely searchbased and semantics-based APR methods. To produce repairs, searchbased APR techniques generate huge populations of possible repairs, i.e., search space, and lazily search for the best one among the search space. Semantics-based APR techniques utilize constraint solving and program synthesis to make search space more tractable, and find those repairs that conform to semantics constraints extracted via symbolic execution. Despite recent advances in APR, search-based APR still suffers from search space explosion problem, while the semantics-based APR could be hindered by limited capability of constraint solving and program synthesis. Furthermore, both APR families may be subject to overfitting, in which generated repairs do not generalize to other test sets. This thesis works towards enhancing both effectiveness and efficiency in order for APR to be practically adopted in foreseeable future. To achieve this goal, other than using test cases as the primary criteria for traversing the search space, we designed a new feature used for a new search-based APR technique to effectively traverse the search space, wherein bug fix history is used to evaluate the quality of repair candidates. We also developed a deductivereasoningbased repair technique that combines search-based and semantics-based approaches to enhance the repair capability, while ensuring the soundness of generated repairs. We also leveraged machine-learning techniques to build a predictive model that predicts whether an APR technique is effective in fixing particular bugs. In the future, we plan to synergize many existing APR techniques, improve our predictive model, and adopt the advances of other fields such as test case generation and program synthesis for APR. © 2016 ACM.",Automatic Program Repair | Deductive Reasoning | Genetic Programming | Mining Software Repository,ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering,2016-08-25,Conference Paper,"Le, Xuan Bach D.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85017269987,10.1145/3014586,Configurable detection of SDC-causing errors in programs,"Silent Data Corruption (SDC) is a serious reliability issue in many domains, including embedded systems. However, current protection techniques are brittle and do not allow programmers to trade off performance for SDC coverage. Further, many require tens of thousands of fault-injection experiments, which are highly time- and resource-intensive. In this article, we propose two empirical models, SDCTune and SDCAuto, to predict the SDC proneness of a program's data. Both models are based on static and dynamic features of the program alone and do not require fault injections to be performed. The main difference between them is that SDCTune requires manual tuning while SDCAuto is completely automated, using machine-learning algorithms. We then develop an algorithm using both models to selectively protect the most SDC-prone data in the program subject to a given performance overhead bound. Our results show that both models are accurate at predicting the relative SDC rate of an application compared to fault injection, for a fraction of the time taken. Further, in terms of efficiency of detection (i.e., ratio of SDC coverage provided to performance overhead), our technique outperforms full duplication by a factor of 0.78x to 1.65x with the SDCTune model and 0.62x to 0.96x with SDCAuto model. © 2017 ACM.",Compiler | Error detection | Fault tolerance | Modeling | Reliability,ACM Transactions on Embedded Computing Systems,2017-03-01,Article,"Lu, Qining;Li, Guanpeng;Pattabiraman, Karthik;Gupta, Meeta S.;Rivers, Jude A.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85020901390,10.1145/3019612.3019788,Analyzing emotion words to predict severity of software bugs: A case study of open source projects,"A successful software development project becomes an essential part of a software company's reputation. Thus, lots of project managers focus more on maintenance than on other management processes. Previous works studied how to help the maintenance process by detecting bug duplication and predicting the severity of bugs. This paper continues that kind of special work by analyzing emotion words for bug-severity prediction. In detail, we construct an emotion words-based dictionary for verifying bug reports' textual emotion analyses based on positive and negative terms. Then, we modify a machine learning algorithm, the Naïve Bayes multinomial, calling the new algorithm EWD-Multinomial. We compare this EWD-Multinomial study with our baselines, including Naïve Bayes multinomial and a Lamkanfi study, for open source projects such as Eclipse, Android, and JBoss. The result shows this study's algorithm outperforms the others. Copyright 2017 ACM.",Bug report | Bug severity prediction | Emotion words-based dictionary | Software maintenance,Proceedings of the ACM Symposium on Applied Computing,2017-04-03,Conference Paper,"Yang, Geunseok;Baek, Seungsuk;Lee, Jung Won;Lee, Byungjeong",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85060288902,10.1145/3041021.3051108,Large scale distributed data science from scratch using apache spark 2.0,"Apache Spark is an open-source cluster computing framework. It has emerged as the next generation big data processing engine, overtaking Hadoop MapReduce which helped ignite the big data revolution. Spark maintains MapReduce's linear scalability and fault tolerance, but extends it in a few important ways: it is much faster (100 times faster for certain applications), much easier to program in due to its rich APIs in Python, Java, Scala, SQL and R (MapReduce has 2 core calls), and its core data abstraction, the distributed data frame. In addition, it goes far beyond batch applications to support a variety of compute-intensive tasks, including interactive queries, streaming, machine learning, and graph processing. With massive amounts of computational power, deep learning has been shown to produce state-of-the-art results on various tasks in different fields like computer vision, automatic speech recognition, natural language processing and online advertising targeting. Thanks to the open-source frameworks, e.g. Torch, Theano, Caffe, MxNet, Keras and TensorFlow, we can build deep learning model in a much easier way. Among all these framework, TensorFlow is probably the most popular open source deep learning library. TensorFlow 1.0 was released recently, which provide a more stable, flexible and powerful computation tool for numerical computation using data flow graphs. Keras is a high-level neural networks library, written in Python and capable of running on top of either TensorFlow or Theano. It was developed with a focus on enabling fast experimentation. This tutorial will provide an accessible introduction to large-scale distributed machine learning and data mining, and to Spark and its potential to revolutionize academic and commercial data science practices. It is divided into three parts: the first part will cover fundamental Spark concepts, including Spark Core, functional programming ala map-reduce, data frames, the Spark Shell, Spark Streaming, Spark SQL, MLlib, and more; the second part will focus on hands-on algorithmic design and development with Spark (developing algorithms from scratch such as decision tree learning, association rule mining (aPriori), graph processing algorithms such as pagerank/shortest path, gradient descent algorithms such as support vectors machines and matrix factorization. Industrial applications and deployments of Spark will also be presented.; the third part will introduce deep learning concepts, how to implement a deep learning model through TensorFlow, Keras and run the model on Spark. Example code will be made available in python (pySpark) notebooks. © 2017 International World Wide Web Conference Committee (IW3C2), published under Creative Commons CC BY 4.0 License.",Deep learning | Distributed systems | Hadoop | HDFS | Large scale machine learning | Mobile advertising | Spark,"26th International World Wide Web Conference 2017, WWW 2017 Companion",2017-01-01,Conference Paper,"Shanahan, James G.;Dai, Liang",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85053184137,10.1145/3127005.3127013,The characteristics of false-negatives in file-level fault prediction,"Over the years, a plethora of works has proposed more and more sophisticated machine learning techniques to improve fault prediction models. However, past studies using product metrics from closedsource projects, found a ceiling e?ect in the performance of fault prediction models. On the other hand, other studies have shown that process metrics are signi?cantly better than product metrics for fault prediction. In our case study therefore we build models that include both product and process metrics taken together. We ?nd that the ceiling e?ect found in prior studies exists even when we consider process metrics. We then qualitatively investigate the bug reports, source code ?les, and commit information for the bugs in the ?les that are false-negative in our fault prediction models trained using product and process metrics. Surprisingly, our qualitative analysis shows that bugs related to false-negative ?les and true-positive ?les are similar in terms of root causes, impact and a?ected components, and consequently such similarities might be exploited to enhance fault prediction models. © 2017 Association for Computing Machinery. All rights reserved.",Code metrics | Post-release defects | Process metrics,ACM International Conference Proceeding Series,2017-11-08,Conference Paper,"Valdivia-Garcia, Harold;Nagappan, Meiyappan",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85049826107,10.1145/3177457.3191709,Research on defect detection technology of trusted behavior decision tree based on intelligent data semantic analysis of massive data,"With the rapid development of information technology, software systems' scales and complexity are showing a trend of expansion. The users' needs for the software security, software security reliability and software stability are growing increasingly. At present, the industry has applied machine learning methods to the fields of defect detection to repair and improve software defects through the massive data intelligent semantic analysis or code scanning. The model in machine learning is faced with big difficulty of model building, understanding, and the poor visualization in the field of traditional software defect detection. In view of the above problems, we present a point of view that intelligent semantic analysis technology based on massive data, and using the trusted behavior decision tree model to analyze the soft behavior by layered detection technology. At the same time, it is equipped related test environment to compare the tested software. The result shows that the defect detection technology based on intelligent semantic analysis of massive data is superior to other techniques at the cost of building time and error reported ratio. © 2018 Association for Computing Machinery.",Decision tree | Intelligent semantic analysis | Massive data | Software defect detection,ACM International Conference Proceeding Series,2018-01-08,Conference Paper,"Ren, Yidan;Zhu, Zhengzhou;Chen, Xiangzhou;Ding, Huixia;Zhang, Geng",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85049687459,10.1145/3183440.3183461,"Machine learning for software engineering: Models, methods, and applications","Machine Learning (ML) is the discipline that studies methods for automatically inferring models from data. Machine learning has been successfully applied in many areas of software engineering ranging from behaviour extraction, to testing, to bug fixing. Many more applications are yet be defined. However, a better understanding of ML methods, their assumptions and guarantees would help software engineers adopt and identify the appropriate methods for their desired applications. We argue that this choice can be guided by the models one seeks to infer. In this technical briefing, we review and reflect on the applications of ML for software engineering organised according to the models they produce and the methods they use. We introduce the principles of ML, give an overview of some key methods, and present examples of areas of software engineering benefiting from ML. We also discuss the open challenges for reaching the full potential of ML for software engineering and how ML can benefit from software engineering methods. © 2018 ACM.",,Proceedings - International Conference on Software Engineering,2018-05-27,Conference Paper,"Meinke, Karl;Bennaceur, Amel",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85049673871,10.1145/3183440.3183487,Elixir: An automated repair tool for Java programs,"Object-oriented (OO) languages, by design, make heavy use of method invocations (MI). Unsurprisingly, a large fraction of OO-program bug patches also involve method invocations. However, current program repair techniques incorporate MIs in very limited ways, ostensibly to avoid searching the huge repair space that method invocations afford. To address this challenge, in previous work, we proposed a generate-And-validate repair technique which can effectively synthesize patches from a repair space rich in method invocation expressions, by using a machine-learned model to rank the space of concrete repairs. In this paper we describe the tool Elixir that instantiates this technique for the repair of Java programs. We describe the architecture, user-interface, and salient features of Elixir, and specific use-cases it can be applied in. We also report on our efforts towards practical deployment of Elixir within our organization, including the initial results of a trial of Elixir on a project of interest to potential customers. A video demonstrating Elixir is available at: https://elixir-Tool.github.io/demo-video.HTML. © 2018 Authors.",Automatic program repair | Machine learning | OOP,Proceedings - International Conference on Software Engineering,2018-05-27,Conference Paper,"Saha, Ripon K.;Yoshida, Hiroaki;Prasad, Mukul R.;Tokumoto, Susumu;Takayama, Kuniharu;Nanba, Isao",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85049672108,10.1145/3183440.3195078,LWE: LDA refined word embeddings for duplicate bug report detection,"Bug reporting is a major part of software maintenance and due to its inherently asynchronous nature, duplicate bug reporting has become fairly common. Detecting duplicate bug reports is an important task in order to avoid the assignment of a same bug to different developers. Earlier approaches have improved duplicate bug report detection by using the notions of word embeddings, topic models and other machine learning approaches. In this poster, we attempt to combine Latent Dirichlet Allocation (LDA) and word embeddings to leverage the strengths of both approaches for this task. As a first step towards this idea, we present initial analysis and an approach which is able to outperform both word embeddings and LDA for this task. We validate our hypothesis on a real world dataset of Firefox project and show that there is potential in combining both LDA and word embeddings for duplicate bug report detection. © 2018 Authors.",,Proceedings - International Conference on Software Engineering,2018-05-27,Conference Paper,"Budhiraja, Amar;Reddy, Raghu;Shrivastava, Manish",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85049678355,10.1145/3183440.3195100,Poster: Leveraging product relationships to generate candidate bugs for duplicate bug prediction,"Adaptive Bug Search (ABS) is a service developed by Oracle that uses machine learning to find potential duplicate bugs for a given input bug. ABS leverages the product and component relationships of existing duplicate bug pairs to limit the set of candidate bugs in which it searches for potential duplicates. In this paper, we discuss various approaches for selecting and refining the set of candidate bugs. © 2018 Authors.",Duplicate bug prediction | Human factors | Machine learning,Proceedings - International Conference on Software Engineering,2018-05-27,Conference Paper,"Su, Emily;Joshi, Sameer",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85051086990,10.1145/3184407.3184440,AI techniques in Software Engineering paradigm,"In the next decade, Artificial Intelligent (AI) techniques can see wide adoption in our daily life to release human burden. In our recent Software Engineering research, we investigated on the design of novel AI methods to facilitate all three major phases in software engineering: development, operation, and analysis. In this talk, I will first introduce the AI techniques we employed, including machine learning framework, classification, clustering, matrix factorization, topic modeling, deep learning, and parallel computing platform. Then I will explain the challenges in each phase and describe our recently proposed methodologies. First in development phase, we suggested an automated code completion technique via deep learning. Our technique learns the code style from lots of existing code bases, and recommends the most suitable token based on the trained deep learning model and current coding context. Besides, to help developers in conducting effective logging, we designed a tool named LogAdvisor, which tells developers whether they should write a logging statement in the current code block or not. Secondly, in operation phase, we implemented a continuous and passive authentication method for mobile phones based on user touch biometrics. Different from the traditional password authentication scheme, our method can recognize malicious attackers based on abnormal user behaviors. Moreover, we developed PAID, which automatically prioritizes app issues by mining user reviews. Finally, in analysis phase, we designed systematic data analytics techniques for software reliability prediction. Besides, to make full use of the crucial runtime information, we proposed effective methods for every step in log analysis, including log parsing, feature extraction, and log mining. Furthermore, we developed a CNN-based defect prediction method to help developers find the buggy code. In the end, we expect to establish a comprehensive framework for systematic employment of AI techniques in the Software Engineering paradigm. © 2018 Copyright held by the owner/author(s).",Artificial intelligence | Software engineering,ICPE 2018 - Proceedings of the 2018 ACM/SPEC International Conference on Performance Engineering,2018-03-30,Conference Paper,"Lyu, Michael R.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85051192018,10.1145/3194718.3194730,Predictive analytics for software testing: Keynote paper,"This keynote discusses the use of Predictive Analytics for Software Engineering, and in particular for Software Defect Prediction and Software Testing, by presenting the latest results achieved in these fields leveraging Artificial Intelligence, Search-based and Machine Learning methods, and by giving some directions for future work. © 2018 Author.",predictive analytics | search-based predictive modelling,Proceedings - International Conference on Software Engineering,2018-05-28,Conference Paper,"Sarro, Federica",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85056877266,10.1145/3211346.3211347,Clone-hunter: Accelerated bound checks elimination via binary code clone detection,"Unsafe pointer usage and illegitimate memory accesses are prevalent bugs in software. To ensure memory safety, conditions for array bound checks are inserted into the code to detect out-of-bound memory accesses. Unfortunately, these bound checks contribute to high runtime overheads, and therefore, redundant array bound checks should be removed to improve application performance. In this paper, we propose Clone-Hunter, a practical and scalable framework for redundant bound check elimination in binary executables. Clone-Hunter first uses binary code clone detection, and then employs bound safety verification mechanism (using binary symbolic execution) to ensure sound removal of redundant bound checks. Our results show the Clone-Hunter can swiftly identify redundant bound checks about 90× faster than pure binary symbolic execution, while ensuring zero false positives. © 2018 ACM.",Array bound checks | Binary analysis | Machine learning | Memory safety,"MAPL 2018 - Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, co-located with PLDI 2018",2018-06-18,Conference Paper,"Xue, H. Hongfa;Venkataramani, G. Guru;Lan, T. Tian",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85056736328,10.1145/3275219.3275228,An effective approach for routing the bug reports to the right fixers,"Routing the bug reports to potential fixers (i.e., bug triaging), is an integral step in software development and maintenance. However, manually inspecting and assigning bug reports is tedious and time-consuming, especially in those software projects that have a large amount of bug reports and developers. To make bug triaging more efficient, many machine learning and information retrieval based approaches have been proposed to automatically assign bug reports for suitable developers to fix. However, these techniques typically ignore two important facts in bug fixing. First, for some bug reports, the bug reporter himself/herself is one of the developers in the project, and he/she is likely to fix his/her reported bugs in the future. Second, for some bug reports, there may be a tossing sequence which contains several developers from the first potential fixer to the last actual fixer. Such tossing sequences encode valuable information such as the dependency of developers for the bug triaging task. To make use of the above facts, we propose a sequence to sequence model named SeqTriage to automatically route a given bug report to its responsible fixer. Evaluation results on three different open-source projects show that the proposed approach has significantly improved the accuracy of bug triaging compared with the state-of-the-art approaches (20% at best and 5% at least). © 2018 Association for Computing Machinery.",Bug reporter | Bug triaging | Mining software repositories | Software maintenance | Tossing sequence,ACM International Conference Proceeding Series,2018-09-16,Conference Paper,"Xi, Shengqu;Yao, Yuan;Xiao, Xusheng;Xu, Feng;Lu, Jian",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85058368592,10.1145/3276774.3276795,Scrabble: Transferrable semi-automated semantic metadata normalization using intermediate representation,"Interoperability in the Internet of Things relies on a common data model that captures the necessary semantics for vendor independent application development and data exchange. However, traditional systems such as those in building management are vertically integrated and do not use a standard schema. A typical building can consist of thousands of data points. Third party vendors who seek to deploy applications like fault diagnosis need to manually map the building information into a common schema. This mapping process requires deep domain expertise and a detailed understanding of intricacies of each building's system. Our framework - Scrabble - reduces the mapping effort significantly by using a multi-stage active learning mechanism that exploits the structure present in a standard schema and learns from buildings that have already been mapped to the schema. Scrabble uses conditional random fields with transfer learning to represent unstructured building information in a reusable intermediate representation. This reusable representation is mapped to the schema using a multilayer perceptron. Our novel semantic model based active learning mechanism requires only minimal input from domain experts to interpret esoteric, idiosyncratic data points. We have evaluated Scrabble on five buildings with thousands of different entities and our method outperforms prior work by 59%/162% higher Accuracy/Macro-averaged-F1 in a building when 10 examples are provided by an expert in both cases. Scrabble achieves 99% Accuracy with 100-160 examples for buildings with thousands of points while the other baselines cannot. © 2018 Association for Computing Machinery.",Machine learning | Metadata schema | Smart buildings,BuildSys 2018 - Proceedings of the 5th Conference on Systems for Built Environments,2018-11-07,Conference Paper,"Koh, Jason;Balaji, Bharathan;Sengupta, Dhiman;McAuley, Julian;Gupta, Rajesh;Agarwal, Yuvraj",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85061793120,10.1145/3278186.3278191,A reinforcement learning based approach to automated testing of android applications,"In recent years, researchers have actively proposed tools to automate testing for Android applications. Their techniques, however, still encounter major difficulties. First is the difficulty of achieving high code coverage because applications usually have a large number of possible combinations of operations and transitions, which makes testing all possible scenarios time-consuming and ineffective for large systems. Second is the difficulty of achieving a wide range of application functionalities, because some functionalities can only be reached through a specific sequence of events. Therefore they are tested less often in random testing. Facing these problems, we apply a reinforcement learning algorithm called Q-learning to take advantage of both random and model-based testing. A Q-learning agent interacts with the Android application, builds a behavioral model gradually and generates test cases based on the model. The agent explores the application in an optimal way that reveals as much functionalities of the application as possible. The exploration using Q-learning improves code coverage in comparison to random and model-based testing and is able to detect faults in applications under test. © 2018 Association for Computing Machinery..",Android | Q-learning | Reinforcement learning | Test input generation,"A-TEST 2018 - Proceedings of the 9th ACM SIGSOFT International Workshop on Automating TEST Case Design, Selection, and Evaluation, Co-located with FSE 2018",2018-11-05,Conference Paper,"Vuong, Thi Anh Tuyet;Takada, Shingo",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85065654570,10.1145/3297280.3297411,How do implementation bugs affect the results of machine learning algorithms?,"Applications based on Machine learning (ML) are growing in popularity in a multitude of different contexts such as medicine, bioinformatics, and finance. However, there is a lack of established approaches and strategies able to assure the reliability of this category of software. This has a big impact since nowadays our society relies on (potentially) unreliable applications that could cause, in extreme cases, catastrophic events (e.g., loss of life due to a wrong diagnosis of an ML-based cancer classifier). In this paper, as a preliminary step towards providing a solution to this big problem, we used automatic mutations to mimic realistic bugs in the code of two machine learning algorithms, Multilayer Perceptron and Logistic Regression, with the goal of studying the impact of implementation bugs on their behaviours. Unexpectedly, our experiments show that about 2/3 of the injected bugs are silent since they does not influence the results of the algorithms, while the bugs emerge as runtime errors, exceptions, or modified accuracy of the predictions only in the remaining cases. Moreover, we also discovered that about 1% of the bugs are extremely dangerous since they drastically affect the quality of the prediction only in rare cases and with specific datasets increasing the possibility of going unnoticed. © 2019 Association for Computing Machinery.",Accuracy | Bug | Machine Learning | Oracle Problem | Software Quality Assurance | Testing,Proceedings of the ACM Symposium on Applied Computing,2019-01-01,Conference Paper,"Leotta, Maurizio;Ricca, Filippo;Olianas, Dario;Noceti, Nicoletta",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85086270375,10.1145/3318464.3389696,Complaint-driven Training Data Debugging for Query 2.0,"As the need for machine learning (ML) increases rapidly across all industry sectors, there is a significant interest among commercial database providers to support ""Query 2.0"", which integrates model inference into SQL queries. Debugging Query 2.0 is very challenging since an unexpected query result may be caused by the bugs in training data (e.g., wrong labels, corrupted features). In response, we propose Rain, a complaint-driven training data debugging system. Rain allows users to specify complaints over the query's intermediate or final output, and aims to return a minimum set of training examples so that if they were removed, the complaints would be resolved. To the best of our knowledge, we are the first to study this problem. A naive solution requires retraining an exponential number of ML models. We propose two novel heuristic approaches based on influence functions which both require linear retraining steps. We provide an in-depth analytical and empirical analysis of the two approaches and conduct extensive experiments to evaluate their effectiveness using four real-world datasets. Results show that Rain achieves the highest recall@k among all the baselines while still returns results interactively. © 2020 Association for Computing Machinery.",data debugging | data provenance | machine learning explanation | machine learning workflow debugging | software 2.0,Proceedings of the ACM SIGMOD International Conference on Management of Data,2020-06-14,Conference Paper,"Wu, Weiyuan;Flokas, Lampros;Wu, Eugene;Wang, Jiannan",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85099188843,10.1145/3324884.3416587,Patching as Translation: The Data and the Metaphor,"Machine Learning models from other fields, like Computational Linguistics, have been transplanted to Software Engineering tasks, often quite successfully. Yet a transplanted model's initial success at a given task does not necessarily mean it is well-suited for the task. In this work, we examine a common example of this phenomenon: the conceit that software patching is like language translation. We demonstrate empirically that there are subtle, but critical distinctions between sequence-to-sequence models and translation model: while program repair benefits greatly from the former, general modeling architecture, it actually suffers from design decisions built into the latter, both in terms of translation accuracy and diversity. Given these findings, we demonstrate how a more principled approach to model design, based on our empirical findings and general knowledge of software development, can lead to better solutions. Our findings also lend strong support to the recent trend towards synthesizing edits of code conditional on the buggy context, to repair bugs. We implement such models ourselves as 'proof-of-concept' tools and empirically confirm that they behave in a fundamentally different, more effective way than the studied translation-based architectures. Overall, our results demonstrate the merit of studying the intricacies of machine learned models in software engineering: not only can this help elucidate potential issues that may be overshadowed by increases in accuracy; it can also help innovate on these models to raise the state-of-the-art further. We will publicly release our replication data and materials at https://github.com/ARiSE-Lab/Patch-as-translation. © 2020 ACM.",automated program repair | big code | neural machine translation | sequence-to-sequence model,"Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020",2020-09-01,Conference Paper,"Ding, Yangruibo;Ray, Baishakhi;Devanbu, Premkumar;Hellendoorn, Vincent J.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85072798067,10.1145/3328905.3332514,Reproducible and reliable distributed classification of text streams,"Large-scale classification of text streams is an essential problem that is hard to solve. Batch processing systems are scalable and proved their effectiveness for machine learning but do not provide low latency. On the other hand, state-of-the-art distributed stream processing systems are able to achieve low latency but do not support the same level of fault tolerance and determinism. In this work, we discuss how the distributed streaming computational model and fault tolerance mechanisms can affect the correctness of text classification data flow. We also propose solutions that can mitigate the revealed pitfalls. © 2019 Authors.",Data streams | Exactly once | Reproducibility | Text classification,DEBS 2019 - Proceedings of the 13th ACM International Conference on Distributed and Event-Based Systems,2019-06-24,Conference Paper,"Trofimov, Artem;Shavkunov, Mikhail;Reznick, Sergey;Sokolov, Nikita;Yutman, Mikhail;Kuralenok, Igor E.;Novikov, Boris",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85071904606,10.1145/3338906.3341462,A taxonomy of metrics for software fault prediction,"In the field of Software Fault Prediction (SFP), researchers exploit software metrics to build predictive models using machine learning and/or statistical techniques. SFP has existed for several decades and the number of metrics used has increased dramatically. Thus, the need for a taxonomy of metrics for SFP arises firstly to standardize the lexicon used in this field so that the communication among researchers is simplified and then to organize and systematically classify the used metrics. In this doctoral symposium paper, I present my ongoing work which aims not only to build such a taxonomy as comprehensive as possible, but also to provide a global understanding of the metrics for SFP in terms of detailed information: acronym(s), extended name, univocal description, granularity of the fault prediction (e.g., method and class), category, and research papers in which they were used. © 2019 ACM.",Software fault prediction | Software metrics | Taxonomy,ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2019-08-12,Conference Paper,"Caulo, Maria",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85076425007,10.1145/3340482.3342741,Leveraging mutants for automatic prediction of metamorphic relations using machine learning,"An oracle is used in software testing to derive the verdict (pass/fail) for a test case. Lack of precise test oracles is one of the major problems in software testing which can hinder judgements about quality. Metamorphic testing is an emerging technique which solves both the oracle problem and the test case generation problem by testing special forms of software requirements known as metamorphic requirements. However, manually deriving the metamorphic requirements for a given program requires a high level of domain expertise, is labor intensive and error prone. As an alternative, we consider the problem of automatic detection of metamorphic requirements using machine learning (ML). For this problem we can apply graph kernels and support vector machines (SVM). A significant problem for any ML approach is to obtain a large labeled training set of data (in this case programs) that generalises well. The main contribution of this paper is a general method to generate large volumes of synthetic training data which can improve ML assisted detection of metamorphic requirements. For training data synthesis we adopt mutation testing techniques. This research is the first to explore the area of data augmentation techniques for ML-based analysis of software code. We also have the goal to enhance black-box testing using white-box methodologies. Our results show that the mutants incorporated into the source code corpus not only efficiently scale the dataset size, but they can also improve the accuracy of classification models. © 2019 Association for Computing Machinery.",Data augmentation | Fault Identification | Machine Learning | Metamorphic Testing | Mutation Testing | Source Code Analysis | Test Case Generation,"MaLTeSQuE 2019 - Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation, co-located with ESEC/FSE 2019",2019-08-27,Conference Paper,"Nair, Aravind;Meinke, Karl;Eldh, Sigrid",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85083026787,10.1145/3341105.3373898,Customer requests matter: Early stage software effort estimation using k-grams,"Estimating the software development effort associated with a customer request, immediately after the request has been made, is quite challenging for project managers. Studies on software effort estimation often utilize expert knowledge to build statistical or machine learning models, although it is subjective and human-dependent. Rich text, natural language based descriptions provided by the customers are mostly not incorporated into the models during estimation. The aim of this study is to propose an early stage software effort estimation model that can predict the effort of the related problem immediately after a customer request or bug fix problem is appeared. Our model utilizes the textual descriptions of customer requests collected in the requirement management tool, and other features stored with the customer requests. Our results show that the use of textual data helps to make better predictions for an effort estimation system. Besides the effect of textual data, we asked whether there is a significant difference between the performance of an effort estimation system that can be used for all customers (Unified Model) and Customer Specific Models which is trained using only the related customer's requests. The Pred(25), Standard Accuracy (SA) and Gibbs' A were used during the evaluation. The Unified Model for early stage effort estimation achieves 43% pred(25) and 51% SA values. Customer Specific Models, on the other hand, depending on the customer, obtain 39%-58% Pred(25), and 35%-58% SA values. The results show that there is no significant advantage between the Unified Model and Customer Specific Models, and which model to use should be determined according to the customer. © 2020 ACM.",Feature extraction | K-grams | Project management | Software effort estimation | Text mining,Proceedings of the ACM Symposium on Applied Computing,2020-03-30,Conference Paper,"Eren, Kazlm Klvanç;Ozbey, Can;Eken, Beyza;Tosun, Ayşe",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85076085221,10.1145/3345629.3345635,Leveraging change intents for characterizing and identifying large-review-effort changes,"Code changes to software occur due to various reasons such as bug fixing, new feature addition, and code refactoring. In most existing studies, the intent of the change is rarely leveraged to provide more specific, context aware analysis. In this paper, we present the first study to leverage change intent to characterize and identify Large-Review-Effort (LRE) changes regarding review effort - changes with large review effort. Specifically, we first propose a feedback-driven and heuristics-based approach to obtain change intents. We then characterize the changes regarding review effort by using various features extracted from change metadata and the change intents. We further explore the feasibility of automatically classifying LRE changes. We conduct our study on a large-scale project from Microsoft and three large-scale open source projects, i.e., Qt, Android, and OpenStack. Our results show that, (i) code changes with some intents are more likely to be LRE changes, (ii) machine learning based prediction models can eficiently help identify LRE changes, and (iii) prediction models built for code changes with some intents achieve better performance than prediction models without considering the change intent, the improvement in AUC can be up to 19 percentage points and is 7.4 percentage points on average. The tool developed in this study has already been used in Microsoft to provide the review effort and intent information of changes for reviewers to accelerate the review process. © 2019 Association for Computing Machinery.",Change intent | Code review | Machine learning | Review effort,ACM International Conference Proceeding Series,2019-09-18,Conference Paper,"Wang, Song;Bansal, Chetan;Nagappan, Nachiappan;Philip, Adithya Abraham",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85097195733,10.1145/3368089.3409754,Is neuron coverage a meaningful measure for testing deep neural networks?,"Recent effort to test deep learning systems has produced an intuitive and compelling test criterion called neuron coverage (NC), which resembles the notion of traditional code coverage. NC measures the proportion of neurons activated in a neural network and it is implicitly assumed that increasing NC improves the quality of a test suite. In an attempt to automatically generate a test suite that increases NC, we design a novel diversity promoting regularizer that can be plugged into existing adversarial attack algorithms. We then assess whether such attempts to increase NC could generate a test suite that (1) detects adversarial attacks successfully, (2) produces natural inputs, and (3) is unbiased to particular class predictions. Contrary to expectation, our extensive evaluation finds that increasing NC actually makes it harder to generate an effective test suite: higher neuron coverage leads to fewer defects detected, less natural inputs, and more biased prediction preferences. Our results invoke skepticism that increasing neuron coverage may not be a meaningful objective for generating tests for deep neural networks and call for a new test generation technique that considers defect detection, naturalness, and output impartiality in tandem. © 2020 Owner/Author.",Adversarial Attack | Machine Learning | Neuron Coverage | Software Engineering | Testing,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2020-11-08,Conference Paper,"Harel-Canada, Fabrice;Wang, Lingxiao;Gulzar, Muhammad Ali;Gu, Quanquan;Kim, Miryung",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85097183424,10.1145/3368089.3417043,Testing machine learning code using polyhedral region,"To date, although machine learning has been successful in various practical applications, generic methods of testing machine learning code have not been established yet. Here we present a new approach to test machine learning code using the possible input region obtained as a polyhedron. If an ML system generates different output for multiple input in the polyhedron, it is ensured that there exists a bug in the code. This property is known as one of theoretical fundamentals in statistical inference, for example, sparse regression models such as the lasso, and a wide range of machine learning algorithms satisfy this polyhedral condition, to which our testing procedure can be applied. We empirically show that the existence of bugs in lasso code can be effectively detected by our method in the mutation testing framework. © 2020 Owner/Author.",Lasso | Machine learning code | Mutation Analysis | Polyhedral region | Testing,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2020-11-08,Conference Paper,"Ahmed, Md Sohel;Ishikawa, Fuyuki;Sugiyama, Mahito",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85097166343,10.1145/3368089.3417928,BEE: A tool for structuring and analyzing bug reports,"This paper introduces BEE, a tool that automatically analyzes user-written bug reports and provides feedback to reporters and developers about the system's observed behavior (OB), expected behavior (EB), and the steps to reproduce the bug (S2R). BEE employs machine learning to (i) detect if an issue describes a bug, an enhancement, or a question; (ii) identify the structure of bug descriptions by automatically labeling the sentences that correspond to the OB, EB, or S2R; and (iii) detect when bug reports fail to provide these elements. BEE is integrated with GitHub and offers a public web API that researchers can use to investigate bug management tasks based on bug reports. We evaluated BEE's underlying models on more than 5k existing bug reports and found they can correctly detect OB, EB, and S2R sentences as well as missing information in bug reports. BEE is an open-source project that can be found at <a>https://git.io/JfFnN</a>. A screencast showing the full capabilities of BEE can be found at <a>https://youtu.be/8pC48f_hClw</a>. © 2020 ACM.",Bug report quality | Bug report structure | Bug reporting | Text analysis,ESEC/FSE 2020 - Proceedings of the 28th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2020-11-08,Conference Paper,"Song, Yang;Chaparro, Oscar",Exclude,
10.1016/j.infsof.2022.107128,,10.1145/3377816.3381734,Manifold for Machine Learning Assurance,"The increasing use of machine-learning (ML) enabled systems in critical tasks fuels the quest for novel verification and validation techniques yet grounded in accepted system assurance principles. In traditional system development, model-based techniques have been widely adopted, where the central premise is that abstract models of the required system provide a sound basis for judging its implementation. We posit an analogous approach for ML systems using an ML technique that extracts from the high-dimensional training data implicitly describing the required system, a low-dimensional underlying structure-A manifold. It is then harnessed for a range of quality assurance tasks such as test adequacy measurement, test input generation, and runtime monitoring of the target ML system. The approach is built on variational autoencoder, an unsupervised method for learning a pair of mutually near-inverse functions between a given high-dimensional dataset and a low-dimensional representation. Preliminary experiments establish that the proposed manifold-based approach, for test adequacy drives diversity in test data, for test generation yields fault-revealing yet realistic test cases, and for run-Time monitoring provides an independent means to assess trustability of the target system's output. Ccs Concepts • Software and its engineering ? Software testing and debugging; • Computing methodologies ? Machine learning. © 2020 ACM.",machine learning testing; neural networks; variational autoencoder,"Proceedings - 2020 ACM/IEEE 42nd International Conference on Software Engineering: New Ideas and Emerging Results, ICSE-NIER 2020",2020,,"Byun T., Rayadurgam S.",Exclude,
10.1016/j.infsof.2022.107128,,10.1145/3377816.3381734,Manifold for machine learning assurance,"The increasing use of machine-learning (ML) enabled systems in critical tasks fuels the quest for novel verification and validation techniques yet grounded in accepted system assurance principles. In traditional system development, model-based techniques have been widely adopted, where the central premise is that abstract models of the required system provide a sound basis for judging its implementation. We posit an analogous approach for ML systems using an ML technique that extracts from the high-dimensional training data implicitly describing the required system, a low-dimensional underlying structure-a manifold. It is then harnessed for a range of quality assurance tasks such as test adequacy measurement, test input generation, and runtime monitoring of the target ML system. The approach is built on variational autoencoder, an unsupervised method for learning a pair of mutually near-inverse functions between a given high-dimensional dataset and a low-dimensional representation. Preliminary experiments establish that the proposed manifold-based approach, for test adequacy drives diversity in test data, for test generation yields fault-revealing yet realistic test cases, and for run-time monitoring provides an independent means to assess trustability of the target system's output. © 2020 Association for Computing Machinery.",Machine learning testing; Neural networks; Variational autoencoder,Proceedings - International Conference on Software Engineering,2020,,"Byun T., Rayadurgam S.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85093688307,10.1145/3379597.3387461,A Machine Learning Approach for Vulnerability Curation,"Software composition analysis depends on database of open-source library vulerabilities, curated by security researchers using various sources, such as bug tracking systems, commits, and mailing lists. We report the design and implementation of a machine learning system to help the curation by by automatically predicting the vulnerability-relatedness of each data item. It supports a complete pipeline from data collection, model training and prediction, to the validation of new models before deployment. It is executed iteratively to generate better models as new input data become available. We use self-training to significantly and automatically increase the size of the training dataset, opportunistically maximizing the improvement in the models' quality at each iteration. We devised new deployment stability metric to evaluate the quality of the new models before deployment into production, which helped to discover an error. We experimentally evaluate the improvement in the performance of the models in one iteration, with 27.59% maximum PR AUC improvements. Ours is the first of such study across a variety of data sources. We discover that the addition of the features of the corresponding commits to the features of issues/pull requests improve the precision for the recall values that matter. We demonstrate the effectiveness of self-training alone, with 10.50% PR AUC improvement, and we discover that there is no uniform ordering of word2vec parameters sensitivity across data sources. © 2020 ACM.",application security | classifiers ensemble | machine learning | open-source software | self-training,"Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020",2020-06-29,Conference Paper,"Chen, Yang;Santosa, Andrew E.;Yi, Ang Ming;Sharma, Abhishek;Sharma, Asankhaya;Lo, David",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85097806608,10.1145/3382025.3414976,Machine learning and configurable systems: A gentle introduction,"The goal of this tutorial is to give a gentle introduction to how machine learning can be used to support software product line configuration. This is our second practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance and bug prediction) on real-world systems (Linux, VaryLaTeX, x264). The material is designed for academics and practitioners with basic knowledge in software product lines and machine learning. © 2020 Owner/Author.",configurable systems | machine learning | software product lines,ACM International Conference Proceeding Series,2020-10-19,Conference Paper,"Pereira, Juliana Alves;Martin, Hugo;Temple, Paul;Acher, Mathieu",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85090322801,10.1145/3382734.3405695,Genuinely Distributed Byzantine Machine Learning,"Machine Learning (ML) solutions are nowadays distributed, according to the so-called server/worker architecture. One server holds the model parameters while several workers train the model. Clearly, such architecture is prone to various types of component failures, which can be all encompassed within the spectrum of a Byzantine behavior. Several approaches have been proposed recently to tolerate Byzantine workers. Yet all require trusting a central parameter server. We initiate in this paper the study of the ""general"" Byzantine-resilient distributed machine learning problem where no individual component is trusted. In particular, we distribute the parameter server computation on several nodes. We show that this problem can be solved in an asynchronous system, despite the presence of ĝ..."" Byzantine parameter servers and ĝ..."" Byzantine workers (which is optimal). We present a new algorithm, ByzSGD, which solves the general Byzantine-resilient distributed machine learning problem by relying on three major schemes. The first, Scatter/Gather, is a communication scheme whose goal is to bound the maximum drift among models on correct servers. The second, Distributed Median Contraction (DMC), leverages the geometric properties of the median in high dimensional spaces to bring parameters within the correct servers back close to each other, ensuring learning convergence. The third, Minimum-Diameter Averaging (MDA), is a statistically-robust gradient aggregation rule whose goal is to tolerate Byzantine workers. MDA requires loose bound on the variance of non-Byzantine gradient estimates, compared to existing alternatives (e.g., Krum [12]). Interestingly, ByzSGD ensures Byzantine resilience without adding communication rounds (on a normal path), compared to vanilla non-Byzantine alternatives. ByzSGD requires, however, a larger number of messages which, we show, can be reduced if we assume synchrony. We implemented ByzSGD on top of TensorFlow, and we report on our evaluation results. In particular, we show that ByzSGD achieves convergence in Byzantine settings with around 32% overhead compared to vanilla TensorFlow. Furthermore, we show that ByzSGD's throughput overhead is 24 - 176% in the synchronous case and 28 - 220% in the asynchronous case. © 2020 Owner/Author.",byzantine fault tolerance | byzantine parameter servers | distributed machine learning,Proceedings of the Annual ACM Symposium on Principles of Distributed Computing,2020-07-31,Conference Paper,"El-Mhamdi, El Mahdi;Guerraoui, Rachid;Guirguis, Arsany;Hoang, Lê Nguyên;Rouault, Sébastien",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85094831137,10.1145/3387905.3388605,ReviewViz: Assisting developers perform empirical study on energy consumption related reviews for mobile applications,"Improving energy efficiency of mobile applications is a topic that has gained a lot of attention recently. It has been addressed in a number of ways such as identifying energy bugs and developing a catalog of energy patterns. Previous work shows that users discuss the battery related issues (energy inefficiency or energy consumption) of the apps in their reviews. However, there is no work that addresses the automatic extraction of the battery related issues from users' feedback. In this paper, we report on a visualization tool that is developed to empirically study machine learning algorithms and text features to automatically identify the energy consumption specific reviews with the highest accuracy. Other than the common machine learning algorithms, we utilize deep learning models with different word embeddings to compare the results. Furthermore, to help the developers extract the main topics that are discussed in the reviews, two state of the art topic modeling algorithms are applied. The visualizations of the topics represent the keywords that are extracted for each topic along with a comparison with the results of string matching. The developed web-browser based interactive visualization tool is a novel framework developed with the intention of giving the app developers insights about running time and accuracy of machine learning and deep learning models as well as extracted topics. The tool makes it easier for the developers to traverse through the extensive result set generated by the text classification and topic modeling algorithms. The dynamic-data structure used for the tool stores the baseline-results of the discussed approaches and are updated when applied on new datasets. The tool is open sourced to replicate the research results.1 © 2020 ACM.",app review analysis | data-visualization | energy consumption | machine learning | neural networks | topic modeling,"Proceedings - 2020 IEEE/ACM 7th International Conference on Mobile Software Engineering and Systems, MOBILESoft 2020",2020-07-13,Conference Paper,"Hadi, Mohammad Abdul;Fard, Fatemeh Hendijani",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85093109559,10.1145/3387940.3391460,Manifold-based Test Generation for Image Classifiers,"Neural network image classifiers are being adopted in safety-critical applications, and they must be tested thoroughly to inspire confidence. In doing so, two major challenges remain. First, the thoroughness of testing needs to be measurable by an adequacy criterion that shows a strong correlation to the semantic features of the images. Second, a large amount of diverse test cases needs to be prepared, either manually or automatically. The former can be aided by neural-net-specific coverage criteria such as surprise adequacy [3] or neuron coverage [4], but their correlation to semantic features had not been evaluated. The latter is attempted through metamorphic testing [5], but it is limited to domain-dependent metamorphic relations that requires explicit modeling. This presentation discusses a framework which can address the two challenges together. Our approach is based on the premise that patterns in a large data space can be effectively captured in a smaller manifold space, from which similar yet novel test cases— both the input and the label—can be synthesized. This manifold space can also serve as a basis for judging the adequacy of a given test suite, since the manifold encodes all the necessary information for distinguishing among different data points. For modeling this manifold and creating a pair of encoder and a decoder that maps between manifold space and input space, we utilized a conditional variational autoencoder (VAE). The conditional VAE learns class-dependent manifold which enables class-conditioned test generation, solving the oracle problem by construction. For generating novel test cases, we applied search on the manifold to effectively find fault-revealing test cases. Experiments for test case generation show that this approach enables generation of thousands of realistic yet fault-revealing test cases efficiently even for well-trained models that achieve a high validation accuracy. Experiments for coverage measurement shows that manifold-based coverage exhibits higher correlation to semantic features—represented by class label—compared to neuron coverage or neuron boundary coverage. These results suggest that the concept of manifold-based testing is a promising direction for machine learning testing, and calls for a further investigation. The original work is accepted to be presented in AI Test 2020 [2], and a part of the idea will also be presented in ICSE–NIER 2020 [1]. © 2020 ACM.",,"Proceedings - 2020 IEEE/ACM 42nd International Conference on Software Engineering Workshops, ICSEW 2020",2020-06-27,Conference Paper,"Byun, Taejoon;Rayadurgam, Sanjai",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85093117905,10.1145/3387940.3391463,Deep Learning for Software Defect Prediction: A Survey,"Software fault prediction is an important and beneficial practice for improving software quality and reliability. The ability to predict which components in a large software system are most likely to contain the largest numbers of faults in the next release helps to better manage projects, including early estimation of possible release delays, and affordably guide corrective actions to improve the quality of the software. However, developing robust fault prediction models is a challenging task and many techniques have been proposed in the literature. Traditional software fault prediction studies mainly focus on manually designing features (e.g. complexity metrics), which are input into machine learning classifiers to identify defective code. However, these features often fail to capture the semantic and structural information of programs. Such information is needed for building accurate fault prediction models. In this survey, we discuss various approaches in fault prediction, also explaining how in recent studies deep learning algorithms for fault prediction help to bridge the gap between programs' semantics and fault prediction features and make accurate predictions. © 2020 ACM.",deep learning | machine learning | software defect prediction | software quality assurance | software testing,"Proceedings - 2020 IEEE/ACM 42nd International Conference on Software Engineering Workshops, ICSEW 2020",2020-06-27,Conference Paper,"Omri, Safa;Sinz, Carsten",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85093120247,10.1145/3387940.3391541,Automatic Improvement of Machine Translation Using Mutamorphic Relation: Invited Talk Paper,"This paper introduces Mutamorphic Relation for Machine Learning Testing. Mutamorphic Relation combines data mutation and metamorphic relations as test oracles for machine learning systems. These oracles can help achieve fully automatic testing as well as automatic repair of the machine learning models. The paper takes TransRepair as an example to show the effectiveness of Mutamorphic Relation in automatically testing and improving machine translators, TransRepair detects inconsistency bugs without access to human oracles. It then adopts probability-reference or cross-reference to post-process the translations, in a grey-box or black-box manner, to repair the inconsistencies. Manual inspection indicates that the translations repaired by TransRepair improve consistency in 87% of cases (degrading it in 2%), and that the repairs of have better translation acceptability in 27% of the cases (worse in 8%). © 2020 Owner/Author.",metamorphic testing | mutamorphic relation | mutation testing,"Proceedings - 2020 IEEE/ACM 42nd International Conference on Software Engineering Workshops, ICSEW 2020",2020-06-27,Conference Paper,"Zhang, Jie M.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85093088048,10.1145/3387940.3392199,Digital Twin for Cybersecurity Incident Prediction: A Multivocal Literature Review,"The advancements in the field of internet of things, artificial intelligence, machine learning, and data analytics has laid the path to the evolution of digital twin technology. The digital twin is a high-fidelity digital model of a physical system or asset that can be used e.g. to optimize operations and predict faults of the physical system. To understand different use cases of digital twin and its potential for cybersecurity incident prediction, we have performed a Systematic Literature Review (SLR). In this paper, we summarize the definition of digital twin and state-of-the-art on the development of digital twin including reported work on the usability of a digital twin for cybersecurity. Existing tools and technologies for developing digital twin is discussed. © 2020 ACM.",Cybersecurity | Digital Twin | Fault detection | Incident prediction | IoT | Multivocal literature review,"Proceedings - 2020 IEEE/ACM 42nd International Conference on Software Engineering Workshops, ICSEW 2020",2020-06-27,Conference Paper,"Pokhrel, Abhishek;Katta, Vikash;Colomo-Palacios, Ricardo",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85093095868,10.1145/3387940.3392250,MRpredT: Using Text Mining for Metamorphic Relation Prediction,"Metamorphic relations (MRs) are an essential component of metamorphic testing (MT) that highly affects its fault detection effectiveness. MRs are usually identified with the help of a domain expert, which is a labor-intensive task. In this work, we explore the feasibility of a text classification-based machine learning approach to predict MRs using their program documentation as the sole input. We compare our method to our previously developed graph kernelbased machine learning approach and demonstrate that textual features extracted from program documentation are highly effective for predicting metamorphic relations for matrix calculation programs. © 2020 ACM.",Metamorphic relations | Metamorphic testing | Text classification,"Proceedings - 2020 IEEE/ACM 42nd International Conference on Software Engineering Workshops, ICSEW 2020",2020-06-27,Conference Paper,"Rahman, Karishma;Kahanda, Indika;Kanewala, Upulee",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85088922959,10.1145/3395363.3397356,Scaffle: Bug localization on millions of files,"Despite all efforts to avoid bugs, software sometimes crashes in the field, leaving crash traces as the only information to localize the problem. Prior approaches on localizing where to fix the root cause of a crash do not scale well to ultra-large scale, heterogeneous code bases that contain millions of code files written in multiple programming languages. This paper presents Scaffle, the first scalable bug localization technique, which is based on the key insight to divide the problem into two easier sub-problems. First, a trained machine learning model predicts which lines of a raw crash trace are most informative for localizing the bug. Then, these lines are fed to an information retrieval-based search engine to retrieve file paths in the code base, predicting which file to change to address the crash. The approach does not make any assumptions about the format of a crash trace or the language that produces it. We evaluate Scaffle with tens of thousands of crash traces produced by a large-scale industrial code base at Facebook that contains millions of possible bug locations and that powers tools used by billions of people. The results show that the approach correctly predicts the file to fix for 40% to 60% (50% to 70%) of all crash traces within the top-1 (top-5) predictions. Moreover, Scaffle improves over several baseline approaches, including an existing classification-based approach, a scalable variant of existing information retrieval-based approaches, and a set of hand-tuned, industrially deployed heuristics. © 2020 ACM.",Bug localization | machine learning | software crashes,ISSTA 2020 - Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis,2020-07-18,Conference Paper,"Pradel, Michael;Murali, Vijayaraghavan;Qian, Rebecca;Machalica, Mateusz;Meijer, Erik;Chandra, Satish",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85095865060,10.1145/3425174.3425226,An Experimental Study on Applying Metamorphic Testing in Machine Learning Applications,"Machine learning techniques have been successfully employed in various areas and, in particular, for the development of healthcare applications, aiming to support in more effective and faster diagnostics (such as cancer diagnosis). However, machine learning models may present uncertainties and errors. Errors in the training process, classification, and evaluation can generate incorrect results and, consequently, to wrong clinical decisions, reducing the professionals' confidence in the use of such techniques. Similar to other application domains, the quality should be guaranteed to produce more reliable models capable of assisting health professionals in their daily activities. Metamorphic testing can be an interesting option to validate machine learning applications. Using this testing approach is possible to define relationships that define changes to be made in the application's input data to identify faults. This paper presents an experimental study to evaluate the effectiveness of metamorphic testing to validate machine learning applications. A Machine learning application to verify breast cancer diagnostic was developed, using an available dataset composed of 569 samples whose data were taken from breast cancer images, and used as the software under test, in which the metamorphic testing was applied. The results indicate that metamorphic testing can be an alternative to support the validation of machine learning applications. © 2020 ACM.",Experimental Study | Machine Learning | Metamorphic Test,ACM International Conference Proceeding Series,2020-10-20,Conference Paper,"Santos, Sebastiaõ H.N.;Da Silveira, Beatriz Nogueira Carvalho;Andrade, Stevaõ A.;Delamaro, Márcio;Souza, Simone R.S.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85111043100,10.1145/3437359.3465602,Research Cloud Bazaar: A software defined cloud workflow cost management tool,"Research workflows will benefit from a hybrid computing environment that offers seamless integration between on-campus and off-campus cloud resources. Commercial and Federal and commercial clouds provide researchers a rich set of computing platforms that allow opportunities to improve workflows and reduce the time to research the large number of cloud offerings, however, makes cost management, and workflow transitions to appropriate platforms challenging. Successfully mapping workflows from on-campus resources to the cloud and leveraging the available cost structures to find economical cost models are critical steps to enabling researcher access to this vast resource. To address these concerns, here we introduce the Research Computing Bazaar (RCB) software application for resource mapping and cost estimation. RCB is a software-as-a-service platform that is an elastic, scalable, and fault tolerant system. It is developed using actual data from research computing workloads and can be easily configured to be used by users or system administrators in computing environments that use Slurm. In this pilot, we inform researchers about opportunities offered by RCB to leverage flexible workload orchestration in managing cloud costs on a major cloud service provider. An extension into predictive capacities with machine learning mechanisms is being developed. © 2021 ACM.",Apache Superset | API-led integration | Classification | Cloud | Cloud Cost Management | Docker | Flask | Machine Learning | Resource Mapping | Slurm | Workflow Orchestration,ACM International Conference Proceeding Series,2021-07-17,Conference Paper,"Lau, Michael;Trivedi, Stuti;He, Zhenhua;Pham, Tri;Perez, Lisa;Chakravorty, Dhruva",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85102876306,10.1145/3439961.3439991,Deployment of a Machine Learning System for Predicting Lawsuits against Power Companies: Lessons Learned from an Agile Testing Experience for Improving Software Quality,"The advances in Machine Learning (ML) require software organizations to evolve their development processes in order to improve the quality of ML systems. Within the software development process, the testing stage of an ML system is more critical, considering that it is necessary to add data validation, trained model quality evaluation, and model validation to traditional unit, integration tests and system tests. In this paper, we focus on reporting the lessons learned of using model testing and exploratory testing within the context of the agile development process of an ML system that predicts lawsuits proneness in energy supply companies. Through the development of the project, the SCRUM agile methodology was applied and activities related to the development of the ML model and the development of the end-user application were defined. After the testing process of the ML model, we managed to achieve 93.89 accuracy; 95.58 specificity; 88.84 sensitivity; and 87.09 precision. Furthermore, we focused on the quality of use of the application embedding the ML model, by carrying out exploratory testing. As a result, through several iterations, different types of defects were identified and corrected. Our lessons learned support software engineers willing to develop ML systems that consider both the ML model and the end-user application. © 2020 ACM.",and Testing | and Tools | Methods | Software Processes | Validation | Verification,ACM International Conference Proceeding Series,2020-12-01,Conference Paper,"Rivero, Luis;Diniz, João;Silva, Giovanni;Borralho, Gabriel;Braz Junior, Geraldo;Paiva, Anselmo;Alves, Erika;Oliveira, Milton",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85101149711,10.1145/3442391.3442407,Validating Feature Models with Respect to Textual Product Line Specifications,"Feature models (FM) are a valuable resource in the analysis of software product lines (SPL). They provide a visual abstraction of the variation points in a family of related software products. FMs can be manually created by domain experts or extracted (semi-) automatically from textual documents such as product descriptions or requirements specifications. Nevertheless, there is no way to measure the accuracy of a FM with respect to the information described in the source documents. This paper proposes a method to quantify and visualize whether the elements in a FM (features and relationships) conform to the information available in a set of specification documents. Both the correctness (choice of representative elements) and completeness (no missing elements) of the FM are considered. Designers can use this feedback to fix defects in the FM or to detect incomplete or inconsistent information in the source documents. © 2021 ACM.",Feature Model Validation | Machine Learning | Natural Language Processing | Requirements Engineering | Software Product Line,ACM International Conference Proceeding Series,2021-02-09,Conference Paper,"Sree-Kumar, Anjali;Planas, Elena;Clarisó, Robert",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85105496030,10.1145/3452383.3452400,Association of Defect Log Suitability for Machine Learning with Performance: An Experience Report,"Machine learning (ML) based solutions utilizing textual details in defect logs have been shown to enable automation of defect management process and make it cost effective. In this work, we assess effectiveness of apriori manual analysis of the suitability of applying ML to problems encountered during defect management process. We consider problems of mapping defects to service engineers and business processes for designing experiments. Experimental analysis on these problems using multiple defect logs from practice reveals that a systematic analysis of the defect log data by project experts can provide approximate indication of the eventual performance of the ML model even before they are actually built. We discuss practical significance of the conclusions for designing ML based solutions in-practice. © 2021 ACM.",Assignee Recommendation | Business Process Mapping | Defect Management Life-Cycle | Machine Learning Suitability | Mining Defect Repositories | Text Analysis,ACM International Conference Proceeding Series,2021-02-25,Conference Paper,"Misra, Janardan;Podder, Sanjay",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85116242734,10.1145/3468264.3473122,CrossVul: A cross-language vulnerability dataset with commit data,"Examining the characteristics of software vulnerabilities and the code that contains them can lead to the development of more secure software. We present a dataset (∼1.4 GB) containing vulnerable source code files together with the corresponding, patched versions. Contrary to other existing vulnerability datasets, ours includes vulnerable files written in more than 40 programming languages. Each file is associated to (1) a Common Vulnerability Exposures identifier (CVE ID) and (2) the repository it came from. Further, our dataset can be the basis for machine learning applications that identify defects, as we show in specific examples. We also present a supporting dataset that contains commit messages derived from Git commits that serve as security patches. This dataset can be used to train ML models that in turn, can be used to detect security patch commits as we highlight in a specific use case. © 2021 ACM.",commit messages | Dataset | security patches | vulnerabilities,ESEC/FSE 2021 - Proceedings of the 29th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering,2021-08-20,Conference Paper,"Nikitopoulos, Georgios;Dritsa, Konstantina;Louridas, Panos;Mitropoulos, Dimitris",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85119099815,10.1145/3474124.3474194,MABTriage: Multi armed bandit triaging model approach,"Recommendation of bugs to appropriate developers about whom we have very less or no information is a challenging problem faced in many open source developers community. In most of the reported works, this bug-triaging problem is handled through popular machine learning algorithms. However, in the absence of sufficient information of either a developer or a bug, it is difficult to build, train and test a conventional machine-learning model. One of the possible solutions in such a scenario is a reinforcement-learning model. In this paper, we propose an approach called MABTriage, to help a triager assign bugs to developers under uncertainty. To the best of our knowledge, it is the first work that has formulated bug-triaging process as a MAB problem. Experiments conducted on five publicly available open source datasets have shown that MABTriage approach performed better than a random selection. We have also evaluated the performance of six MAB algorithms -Greedy, -Decay, Softmax, Thompson Sampling, Optimistic Agent and UCB based on cumulative rewards. Results have shown that all five performed well in comparison to random selection. © 2021 ACM.",Bug triaging | Multi Armed Bandit | Random Agent | Rewards,ACM International Conference Proceeding Series,2021-08-05,Conference Paper,"Singh, Neetu;Kumar Singh, Sandeep",Exclude,
10.1016/j.infsof.2022.107128,,10.11591/ijece.v9i2.pp.1122-1130,Proposed T-Model to cover 4S quality metrics based on empirical study of root cause of software failures,"There are various root causes of software failures. Few years ago, software used to fail mainly due to functionality related bugs. That used to happen due to requirement misunderstanding, code issues and lack of functional testing. A lot of work has been done in past on this and software engineering has matured over time, due to which software’s hardly fail due to functionality related bugs. To understand the most recent failures, we had to understand the recent software development methodologies and technologies. In this paper we have discussed background of technologies and testing progression over time. A survey of more than 50 senior IT professionals was done to understand root cause of their software project failures. It was found that most of the softwares fail due to lack of testing of non-functional parameters these days. A lot of research was also done to find most recent and most severe software failures. Our study reveals that main reason of software failures these days is lack of testing of non-functional requirements. Security and Performance parameters mainly constitute non-functional requirements of software. It has become more challenging these days due to lots of development in the field of new technologies like Internet of things (IoT), Cloud of things (CoT), Artificial Intelligence, Machine learning, robotics and excessive use of mobile and technology in everything by masses. Finally, we proposed a software development model called as T-model to ensure breadth and depth of software is considered while designing and testing of software. Copyright © 2019 Institute of Advanced Engineering and Science. All rights reserved.",Performance testing; Recent software failures; Safety; Scalability; Security testing; Serviceability (4S); Stability and; Survey; T-model,International Journal of Electrical and Computer Engineering,2019,,"Chhillar D., Sharma K.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85091074784,10.11591/ijeecs.v20.i1.pp465-474,FIBR-OSS: Fault injection model for bug reports in open-source software,"For assessment of system dependability, fault injection techniques are used to expedite the presence of an error or failure in the system, which helps evaluate fault tolerance and system failure prediction. Defects classification and prediction is the principal significant advance in the trustworthiness evaluation of complex software systems such as open-source software since it can quickly be affected by the reliability of those systems, improves performance, and lessening the product cost. In this context, a new prototype of the fault injection model is presented, FIBR-OSS (Fault Injection for Bug Reports in Open-Source Software). FIBR-OSS can support developers to evaluate the system performance during phase's development for its dependability attributes such as reliability and system dependability means such as fault prediction or forecasting. FIBR-OSS is used for fault speed-up to test the system's failure prediction performance. Some machine learning techniques are implemented on bug reports produced existing by the bug tracking system as datasets for failure prediction techniques, some of those machine learning techniques are used in our approach. Copyright © 2020 Institute of Advanced Engineering and Science. All rights reserved.",Bug | FIBR-OSS | Injection techniques | Open-source software,Indonesian Journal of Electrical Engineering and Computer Science,2020-10-01,Article,"Alazawi, Sundos Abdulameer;Al-Salam, Mohammed Najm",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85014384559,10.11591/ijeecs.v3.i2.pp572-578,Optimum software aging prediction and rejuvenation model for virtualized environment,"Advancement in electronics and hardware has resulted in multiple softwares running on the same hardware. The result is multiuser, multitasking, multithreaded and virtualized environments. However, reliability of such high performance computing system depends both on hardware and software. For hardware, aging can be dealt with replacement. But, software aging needs to be dealt with different techniques. For software aging detection, a new approach using machine learning framework is proposed in this paper. For rejuvenation, the proposed solution uses Adaptive Genetic Algorithm (A-GA) to perform live migration to avoid downtime and SLA violation. The proposed A-GA based rejuvenation controller (A-GARC) has outperformed other heuristic techniques such as Ant Colony Optimization (ACO) and best fit decreasing (BFD) for migration. Results reveal that the proposed aging forecasting method and A-GA based rejuvenation outperforms other approaches to ensure optimal system availability, minimum task migration, performance degradation and SLA violation. © 2016 Institute of Advanced Engineering and Science. All rights reserved.",Fault tolerant | Metrics | Rejuvenation | Software aging | Virtualization,Indonesian Journal of Electrical Engineering and Computer Science,2016-09-01,Article,"Umesh, I. M.;Srinivasan, G. N.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-33746952996,10.1186/1471-2105-7-319,Protein disorder prediction by condensed PSSM considering propensity for order or disorder,"Background: More and more disordered regions have been discovered in protein sequences, and many of them are found to be functionally significant. Previous studies reveal that disordered regions of a protein can be predicted by its primary structure, the amino acid sequence. One observation that has been widely accepted is that ordered regions usually have compositional bias toward hydrophobic amino acids, and disordered regions are toward charged amino acids. Recent studies further show that employing evolutionary information such as position specific scoring matrices (PSSMs) improves the prediction accuracy of protein disorder. As more and more machine learning techniques have been introduced to protein disorder detection, extracting more useful features with biological insights attracts more attention. Results: Thi s paper first studies the effect of a condensed position specific scoring matrix with respect to physicochemical properties (PSSMP) on the prediction accuracy, where the PSSMP is derived by merging several amino acid columns of a PSSM belonging to a certain property into a single column. Next, we decompose each conventional physicochemical property of amino acids into two disjoint groups which have a propensity for order and disorder respectively, and show by experiments that some of the new properties perform better than their parent properties in predicting protein disorder. In order to get an effective and compact feature set on this problem, we propose a hybrid feature selection method that inherits the efficiency of uni-variant analysis and the effectiveness of the stepwise feature selection that explores combinations of multiple features. The experimental results show that the selected feature set improves the performance of a classifier built with Radial Basis Function Networks (RBFN) in comparison with the feature set constructed with PSSMs or PSSMPs that adopt simply the conventional physicochemical properties. Conclusion: Distinguishing disordered regions from ordered regions in protein sequences facilitates the exploration of protein structures and functions. Results based on independent testing data reveal that the proposed predicting model DisPSSMP performs the best among several of the existing packages doing similar tasks, without either under-predicting or over-predicting the disordered regions. Furthermore, the selected properties are demonstrated to be useful in finding discriminating patterns for order/disorder classification. © 2006 Su et al; licensee BioMed Central Ltd.",,BMC Bioinformatics,2006-06-23,Article,"Su, Chung Tsai;Chen, Chien Yu;Ou, Yu Yen",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85086592271,10.1186/s13673-020-00229-7,Improving bug report triage performance using artificial intelligence based document generation model,"Artificial intelligence is one of the key technologies for progression to the fourth industrial revolution. This technology also has a significant impact on software professionals who are continuously striving to achieve high-quality software development by fixing various types of software bugs. During the software development and maintenance stages, software bugs are the major factor that can affect the cost and time of software delivery. To efficiently fix a software bug, open bug repositories are used for identifying bug reports and for classifying and prioritizing the reports for assignment to the most appropriate software developers based on their level of interest and expertise. Owing to a lack of resources such as time and manpower, this bug report triage process is extremely important in software development. To improve the bug report triage performance, numerous studies have focused on a latent Dirichlet allocation (LDA) using the k-nearest neighbors or a support vector machine. Although the existing approaches have improved the accuracy of a bug triage, they often cause conflicts between the combined techniques and generate incorrect triage results. In this study, we propose a method for improving the bug report triage performance using multiple LDA-based topic sets by improving the LDA. The proposed method improves the existing topic sets of the LDA by building two adjunct topic sets. In our experiment, we collected bug reports from a popular bug tracking system, Bugzilla, as well as Android bug reports, to evaluate the proposed method and demonstrate the achievement of the following two goals: increase the bug report triage accuracy, and satisfy the compatibility with other state-of-the-art approaches. © 2020, The Author(s).",Artificial intelligence | Bug report triage | Latent Dirichlet Allocation | Machine learning | Software defect prediction | Software engineering,Human-centric Computing and Information Sciences,2020-12-01,Article,"Lee, Dong Gun;Seo, Yeong Seok",Exclude,
10.1016/j.infsof.2022.107128,,10.1201/b21822,Nonlinear feature extraction for big data analytics,"In recent years, there has been a noticeable incremental increase in the number of available Big Data infrastructures. This increase has promoted the adaptation of traditional machine learning techniques in order to be able to address large-scale problems and to adapt solutions to be launched in distributed environments. These platforms provide scalability, fault tolerance, and highly intuitive programming languages to develop software, but they need to train algorithms that are efficient in terms of computational time and communication. For these reasons, linear models are among the most common predictive modeling techniques in working with Big Data. However, these methods show poor performance when there is a nonlinear relationship between the features and the variables being predicted. To solve this limitation, one alternative is to extract features based on the original ones in order to solve problems that are nonlinear in the original feature space. In this chapter, we explain some feature extraction techniques capable of working in distributed Big Data platforms. © 2018 by Taylor and Francis Group, LLC.",,Big Data Analytics: Tools and Technology for Effective Planning,2017,,"Omari A., Zevallos J.J.C., Morales R.D.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85008950510,10.12988/ces.2016.6695,Improving predictions about bug severity by utilizing bugs classified as normal,"More bugs have been generated because today's software has become large and complex. Some of these bugs are critical, while others are trivial. Because accurate prediction of bug severity enables software developers to effectively solve software problems, that accuracy aids software development and project planning. Therefore, this study presents a reliable approach to improve predictions about bug severity by utilizing bugs classified as normal, which is the default level specified in a submitted report. This approach uses attributes as well as text information in bug reports to produce more accurate prediction results. Bug reports in open source projects such as Mozilla and Eclipse were used in the experiments. The result shows that this approach performs better than other studies.",Bug Severity Prediction | Classification | Machine Learning | Normal Bugs | Project Planning | Software Development,Contemporary Engineering Sciences,2016-01-01,Article,"Jin, Kwanghue;Dashbalbar, Amarmend;Yang, Geunseok;Lee, Byungjeong;Lee, Jung Won",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85110718866,10.1364/JOCN.424654,Machine-learning-based soft-failure localization with partial software-defined networking telemetry,"Soft-failure localization frameworks typically use if-else rules to localize failures based on the received telemetry data. However, in certain cases, particularly in disaggregated networks, some devices may not implement telemetry, or their telemetry may not be readily available. Alternatively, machine-learning-based (ML-based) frameworks can automatically learn complex relationships between telemetry and the fault location, incorporating information from the telemetry data collected network-wide. This paper evaluates an ML-based soft-failure localization framework in scenarios of partial telemetry. The framework is based on an artificial neural network (ANN) trained by optical signal and noise power models that simulate the network telemetry upon all possible failure scenarios. The ANN can be trained in less than 2 min, allowing it to be retrained according to the available partial telemetry data. The ML-based framework exhibits excellent performance in scenarios of partial telemetry, practically interpolating the missing data. We show that in the rare cases of incorrect failure localization, the actual failure is in the localized device's vicinity. We also show that ANN training is accelerated by principal component analysis and can be carried out using cloud-based services. Finally, the evaluated ML-based framework is emulated in a software-defined-networking-based setup using the gNMI protocol for streaming telemetry. © 2009-2012 OSA.",,Journal of Optical Communications and Networking,2021-10-01,Article,"Mayer, Kayol S.;Soares, Jonathan A.;Pinto, Rossano P.;Rothenberg, Christian E.;Arantes, Dalton S.;Mello, Darli A.A.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85059012363,10.14419/ijet.v7i4.38.24316,Automated prediction of critical states of turbogenerators during thermal expansion of a rotor and a stator based on a recurrent neural network,"The present article is devoted to the development of a method and its software implementation for forecasting the critical states of a turbogenerator and its design elements that arise during starting-up & adjustment works and stopping a turbine. The method is based on a short-term prediction of the image of the spectrogram of vibrations during thermal expansion of the rotor and stator. The dependence of the increase in the vibration level in the spectrum with the failure of the turbogenerator design element is substantiated. The model takes into account the influence of thermal expansion on critical states. The technique of training a deep neural network is given in the classification of thermal influences on the level of vibration while a spectrogram receiving. For machine learning of a neural network in software, a recurrent autoencoder is used. The technique of operation is with a time sequence of spectrograms. To test the model is introduced the concept of semantic quality of clustering. Semantic quality, determined as the degree of correspondence between the information that can be extracted from the obtained cluster structure and the formalized presentation of the user. The interpretation of the results of the discovery of turbine generator defects is presented. © 2018 Authors.",Critical state | Deep machine training | Forecasting | Heat effects assessment | Neural network | Recurrent neural network | Rnn | Rotor vibrations | Thermal expansion | Thermal influence | Trouble effects evaluation | Troubleshooting | Turbogenerators | Vibrodiagnostics,International Journal of Engineering and Technology(UAE),2018-01-01,Article,"Akimov, Dmitry Aleksandrovich;Pavelyev, Sergey Aleksandrovich;Ivchenko, Valery Dmitrievich",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84891121890,10.14778/2536274.2536318,REEF: Retainable evaluator execution framework,"In this demo proposal, we describe REEF, a framework that makes it easy to implement scalable, fault-tolerant runtime environments for a range of computational models. We will demonstrate diverse workloads, including extract-transform-load MapReduce jobs, iterative machine learning algorithms, and ad-hoc declarative query processing. At its core, REEF builds atop YARN (Apache Hadoop 2's resource manager) to provide retainable hardware resources with lifetimes that are decoupled from those of computational tasks. This allows us to build persistent (cross-job) caches and cluster-wide services, but, more importantly, supports high-performance iterative graph processing and machine learning algorithms. Unlike existing systems, REEF aims for composability of jobs across computational models, providing significant performance and usability gains, even with legacy code. REEF includes a library of interoperable data management primitives optimized for communication and data movement (which are distinct from storage locality). The library also allows REEF applications to access external services, such as user-facing relational databases. We were careful to decouple lower levels of REEF from the data models and semantics of systems built atop it. The result was two new standalone systems: Tang, a configuration manager and dependency injector, and Wake, a state-of-the-art event-driven programming and data movement framework. Both are language independent, allowing REEF to bridge the JVM and .NET. © 2013 VLDB Endowment.",,Proceedings of the VLDB Endowment,2013-01-01,Article,"Chun, Byung Gon;Condie, Tysontcondie;Curino, Carlo;Douglas, Chris;Matusevych, Sergiy;Myers, Brandon;Narayanamurthy, Shravan;Ramakrishnan, Raghu;Rao, Sriram;Rosen, Josh;Sears, Russell;Weimer, Markus",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85097303115,10.14778/3421424.3421429,Inspector gadget: A data programming-based labeling system for industrial images,"As machine learning for images becomes democratized in the Software 2.0 era, one of the serious bottlenecks is securing enough labeled data for training. This problem is especially critical in a manufacturing setting where smart factories rely on machine learning for product quality control by analyzing industrial images. Such images are typically large and may only need to be partially analyzed where only a small portion is problematic (e.g., identifying defects on a surface). Since manual labeling these images is expensive, weak supervision is an attractive alternative where the idea is to generate weak labels that are not perfect, but can be produced at scale. Data programming is a recent paradigm in this category where it uses human knowledge in the form of labeling functions and combines them into a generative model. Data programming has been successful in applications based on text or structured data and can also be applied to images usually if one can find a way to convert them into structured data. In this work, we expand the horizon of data programming by directly applying it to images without this conversion, which is a common scenario for industrial applications. We propose Inspector Gadget, an image labeling system that combines crowdsourcing, data augmentation, and data programming to produce weak labels at scale for image classification. We perform experiments on real industrial image datasets and show that Inspector Gadget obtains better performance than other weak-labeling techniques: Snuba, GOGGLES, and self-learning baselines using convolutional neural networks (CNNs) without pre-training. © 2020, VLDB Endowment. All rights reserved.",,Proceedings of the VLDB Endowment,2020-09-01,Article,"Heo, Geon;Roh, Yuji;Hwang, Seonghyeon;Lee, Dayun;Whang, Steven Euijong",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85000605879,10.1504/IJCAT.2016.080487,Software change prediction: A literature review,"In industrial organisations, software products are quite large and complex, consisting of a number of classes. Thus, it is not possible to test all the products with a finite number of resources. Hence, it would be beneficial if we could predict in advance some of the attributes associated with the classes such as change proneness, defect proneness, maintenance effort, etc. In this paper, we have dealt with one of the quality attributes, i.e., change proneness. Changes in the software are unavoidable and thus, early prediction of change proneness will help the developers to focus the limited resources on the classes which are predicted to be change-prone. We have conducted a systematic review which evaluates all the available important studies relevant to the area of change proneness. This will help us to identify gaps in the current technology and discuss possible new directions of research in the areas related to change proneness. © Copyright 2016 Inderscience Enterprises Ltd.",Change prediction | Empirical validation | Machine learning | Object-oriented metrics | Software quality,International Journal of Computer Applications in Technology,2016-01-01,Article,"Malhotra, Ruchika;Bansal, Ankita Jain",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85101503804,10.1504/IJCSE.2020.113176,ELBA-NoC: Ensemble learning-based accelerator for 2D and 3D network-on-chip architectures,"Network-on-chips (NoCs) have emerged as a scalable alternative to traditional bus and point-to-point architectures, it has become highly sensitive as the number of cores increases. Simulation is one of the main tools used in NoC for analysing and testing new architectures. To achieve the best performance vs. cost trade-off, simulators have become an essential tool. Software simulators are too slow for evaluating large scale NoCs. This paper presents a framework which can be used to analyse overall performance of 2D and 3D NoC architectures which is fast and accurate. This framework is named as ensemble learning-based accelerator (ELBA-NoC) which is built using random forest regression algorithm to predict parameters of NoCs. On 2D, 3D NoC architectures, ELBA-NoC was tested and the results obtained were compared with extensively used Booksim NoC simulator. The framework showed an error rate of less than 5% and an overall speedup of up to 16 K×. Copyright © 2020 Inderscience Enterprises Ltd.",2D NoC | 3D NoC | Area | Booksim | Ensemble learning | Machine learning | Network-on-chip | NoC | Performance | Performance modelling | Power | Prediction | Random forest | Regression | Router | Simulation | Traffic pattern,International Journal of Computational Science and Engineering,2020-01-01,Article,"Kumar, Anil;Talawar, Basavaraj",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85079735275,10.1504/IJICS.2020.105155,Fault prediction for distributed computing hadoop clusters using real-Time higher order differential inputs to svm: Zedacross,"Hadoop distributed computing clusters are used worldwide for high-performance computations. Often various hardware and software faults occur, leading to both data and computation time losses. This paper proposes the usage of a fault prediction software called Zedacross which uses machine learning principles combined with cluster monitoring tools. Firstly, the paper suggests a model that uses the resource usage statistics of a normally functioning Hadoop cluster to create a machine learning model that can then be used to predict and detect faults in real time. Secondly, the paper explains the novel idea of using higher order differentials as inputs to SVM for highly accurate fault predictions. Predictions of system faults by observing system resource usage statistics in real-Time with minimum delay will play a vital role in deciding the need for job rescheduling tasks or even dynamic up-scaling of the cluster. To demonstrate the effectiveness of the design a Java utility was built to perform cluster fault monitoring. The results obtained after running the system on various test cases demonstrate that the proposed method is accurate and effective. © 2020 Inderscience Enterprises Ltd.",Fault prediction | Ganglia | Hadoop | Higher order differential | Svm,International Journal of Information and Computer Security,2020-01-01,Article,"Pinto, Joey;Jain, Pooja;Kumar, Tapan",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85117926269,10.1515/nleng-2021-0019,Research on vibration monitoring and fault diagnosis of rotating machinery based on internet of things technology,"Recently, researchers are investing more fervently in fault diagnosis area of electrical machines. The users and manufacturers of these various efforts are strong to contain diagnostic features in software for improving reliability and scalability. Internet of Things (IoT) has grown immensely and contributing for the development of recent technological advancements in industries, medical and various environmental applications. It provides efficient processing power through cloud, and presents various new opportunities for industrial automation by implementing IoT and industrial wireless sensor networks. The process of regular monitoring enables early detection of machine faults and hence beneficial for Industrial automation by providing efficient process control. The performance of fault detection and its classification by implementing machine-learning algorithms highly dependent on the amount of features involved. The accuracy of classification will adversely affect by the dimensionality features increment. To address these problems, the proposed work presents the extraction of relevant features based on oriented sport vector machine (FO-SVM). The proposed algorithm is capable for extracting the most relevant feature set and hence presenting the accurate classification of faults accordingly. The extraction of most relevant features before the process of classification results in higher classification accuracy. Moreover it is observed that the lesser dimensionality of propose process consumes less time and more suitable for cloud. The experimental analysis based on the implementation of proposed approach provides and solution for the monitoring of machine condition and prediction of fault accurately based on cloud platform using industrial wireless sensor networks and IoT service. © 2021 Xiaoran Zhang et al., published by De Gruyter.",fault diagnosis | industrial wireless sensor networks (IWSNs) | internet of things (IoT) | support vector machine,Nonlinear Engineering,2021-01-01,Article,"Zhang, Xiaoran;Rane, Kantilal Pitambar;Kakaravada, Ismail;Shabaz, Mohammad",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85039910899,10.15439/2017F536,A case study on machine learning model for code review expert system in software engineering,"Code review is a key tool for quality assurance in software development. It is intended to find coding mistakes overlooked during development phase and lower risk of bugs in final product. In large and complex projects accurate code review is a challenging task. As code review depends on individual reviewer predisposition there is certain margin of source code changes that is not checked as it should. In this paper we propose machine learning approach for pointing project artifacts that are significantly at risk of failure. Planning and adjusting quality assurance (QA) activities could strongly benefit from accurate estimation of software areas endangered by defects. Extended code review could be directed there. The proposed approach has been evaluated for feasibility on large medical software project. Significant work was done to extract features from heterogeneous production data, leading to good predictive model. Our preliminary research results were considered worthy of implementation in the company where the research has been conducted, thus opening the opportunities for the continuation of the studies. © 2017 PTI.",,"Proceedings of the 2017 Federated Conference on Computer Science and Information Systems, FedCSIS 2017",2017-11-10,Conference Paper,"Madera, Michal;Tomon, Rafal",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84969793330,10.18293/SEKE2015-135,Building a large-scale software programming taxonomy from stackoverflow,"Taxonomy is becoming indispensable to a growing number of applications in software engineering such as software repository mining and defect prediction. However, the existing related taxonomies are always manually constructed. The sizes of these taxonomies are small and their depths are limited. In order to show the full potential of taxonomies in software engineering applications, in this paper, we present the first large-scale software programming taxonomy which is more comprehensive than any existing ones. It contains 38,205 concepts and 68,098 subsumption relations. Instead of learning from a open domain, we focus on taxonomy construction from Stackoverflow which is one of the largest QA websites about software programming. We propose a machine learning based method with novel features to create a taxonomy that captures the hierarchical semantic structure of tags in Stackoverflow. This method executes iteratively to find as many relations as possible. Experimental results show that our approach achieves much better accuracy than baselines. Compared with taxonomies related to software programming which are extracted from the general-purpose taxonomies such as WikiTaxonomy, Yago Taxonomy and Schema.org, our taxonomy has the widest coverage of concepts, contains the largest number of subsumption relations, and runs up to the deepest semantic hierarchy.",Software Engineering | Stackoverflow | Taxonomy Construction,"Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",2015-01-01,Conference Paper,"Zhu, Jiangang;Shen, Beijun;Cai, Xuyang;Wang, Haofen",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85060232328,10.18637/jss.v087.i02,Semiparametric regression analysis via Infer.NET,"We provide several examples of Bayesian semiparametric regression analysis via the Infer.NET package for approximate deterministic inference in Bayesian models. The examples are chosen to encompass a wide range of semiparametric regression situations. Infer.NET is shown to produce accurate inference in comparison with Markov chain Monte Carlo via the BUGS package, but to be considerably faster. Potentially, this contribution represents the start of a new era for semiparametric regression, where large and complex analyses are performed via fast Bayesian inference methodology and software, mainly being developed within Machine Learning. © 2018, American Statistical Association. All rights reserved.",Additive mixed models | Expectation propagation | Generalized additive models | Mean field variational Bayes | Measurement error models | Missing data models | Penalized splines | Variational message passing,Journal of Statistical Software,2018-10-01,Article,"Luts, Jan;Wang, Shen S.J.;Ormerod, John T.;Wand, Matt P.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85119589613,10.24425/bpasts.2021.138815,3D scan contour de-featuring for improved measurement accuracy – A case study for a small turbine guide vane component,"3D scanning measurements are gaining popularity every year. Quick inspections on already captured point clouds are easy to prepare with the use of modern software and machine learning. To achieve repeatability and accuracy, some surface and measurement issues should be considered and resolved before the inspection. Large numbers of manufacturing scans are not intended for manual correction. This article is a case study of a small surface inspection of a turbine guide vane based on 3D scans. Small surface errors cannot be neglected as their incorrect inspection can result in serious faults in the final product. Contour recognition and deletion seem to be a rational method for making a scan inspection with the same level of accuracy as we have now for CMM machines. The main reason why a scan inspection can be difficult is that the CAD source model can be slightly different from the inspected part. Not all details are always included, and small chamfers and blends can be added during the production process, based on manufacturing standards and best practices. This problem does not occur during a CMM (coordinate measuring machine) inspection, but it may occur in a general 3D scanning inspection. © 2021 The Author(s). This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).",3d scan | Contour de-featuring | Contour recognition | Flatness | Point clouds | Small surfaces | Turbine guide vane,Bulletin of the Polish Academy of Sciences: Technical Sciences,2021-01-01,Article,"Jamontt, Marcin;Pyrzanowski, Paweł",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85122663624,10.3233/FAIA210348,Neural-Symbolic Learning and Reasoning: A Survey and Interpretation,"The study and understanding of human behaviour is relevant to computer science, artificial intelligence, neural computation, cognitive science, philosophy, psychology, and several other areas. Presupposing cognition as basis of behaviour, among the most prominent tools in the modelling of behaviour are computational-logic systems, connectionist models of cognition, and models of uncertainty. Recent studies in cognitive science, artificial intelligence, and psychology have produced a number of cognitive models of reasoning, learning, and language that are underpinned by computation. In addition, efforts in computer science research have led to the development of cognitive computational systems integrating machine learning and automated reasoning. Such systems have shown promise in a range of applications, including computational biology, fault diagnosis, training and assessment in simulators, and software verification. This joint survey reviews the personal ideas and views of several researchers on neural-symbolic learning and reasoning. The article is organised in three parts: Firstly, we frame the scope and goals of neural-symbolic computation and have a look at the theoretical foundations. We then proceed to describe the realisations of neural-symbolic computation, systems, and applications. Finally we present the challenges facing the area and avenues for further research. © 2022 The authors and IOS Press. All rights reserved.",,Frontiers in Artificial Intelligence and Applications,2022-01-01,Book Chapter,"Hitzler, Pascal;Sarker, Md Kamruzzaman;Besold, Tarek R.;D'Avila Garcez, Artur;Bader, Sebastian;Bowman, Howard;Domingos, Pedro;Hitzler, Pascal;Kühnberger, Kai Uwe;Lamb, Luis C.;Lima, Priscila Mac Hado Vieira;De Penning, Leo;Pinkas, Gadi;Poon, Hoifung;Zaverucha, Gerson",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85059541729,10.3233/KES-180390,Software reusability metrics prediction by using evolutionary algorithms: The interactive mobile learning application RozGaar,"Considering object oriented program based software metrics (cohesion, coupling and complexity) and their significance to characterize software quality, particularly software component reusability, we have considered six important CK matrices. The predominant reason behind using the measurement technique is the individual relationship with the design aspect and fault-proneness or aging-proneness. The key objective of this paper is to generate employment opening to thousands of people who have different skillsets and furthermore to provide hassle-free services by RozGaar service providers to customers with the help of machine learning techniques. In the current century's rapid growth of modernization and automation, manual labor is reduced which gives rise to unemployment at mass. If we need technicians, workers, plumbers or drivers who work on daily wages, it is quite difficult to find one in our locality without having any contact references and knowing the quality of the work they provide. This paper helps in filling the gap between the various customers and the service providers. We aim to introduce this paper as an ocean of opportunities for all where people can get jobs on a daily basis and can earn money for their skills. The used application is a dual-platform application that runs on Android devices and on Internet as a website, promising you to provide unmatched services of daily work. To achieve the goal, we used the novel software prediction model, evolutionary algorithms such as decision tree, Rough Set, and Logistic Regression algorithms, to predict software reusability. © 2018 - IOS Press and the authors. All rights reserved.",architecture and framework reused | reusability prediction | software metrics through regular expression | Software reusability metrics | software reused code from apps,International Journal of Knowledge-Based and Intelligent Engineering Systems,2018-01-01,Article,"Padhy, Neelamadhab;Satapathy, Suresh Chandra;Mohanty, J. R.;Panigrahi, Rasmita",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85099450471,10.32604/cmc.2021.014686,An ontology based test case prioritization approach in regression testing,"Regression testing is a widely studied research area, with the aim of meeting the quality challenges of software systems. To achieve a software system of good quality, we face high consumption of resources during testing. To overcome this challenge, test case prioritization (TCP) as a sub-type of regression testing is continuously investigated to achieve the testing objectives. This study provides an insight into proposing the ontology-based TCP (OTCP) approach, aimed at reducing the consumption of resources for the quality improvement and maintenance of software systems. The proposed approach uses software metrics to examine the behavior of classes of software systems. It uses Binary Logistic Regression (BLR) and AdaBoostM1 classifiers to verify correct predictions of the faulty and non-faulty classes of software systems. Reference ontology is used to match the code metrics and class attributes. We investigated five Java programs for the evaluation of the proposed approach, which was used to achieve code metrics. This study has resulted in an average percentage of fault detected (APFD) value of 94.80%, which is higher when compared to other TCP approaches. In future works, large sized programs in different languages can be used to evaluate the scalability of the proposed OTCP approach. © 2021 Tech Science Press. All rights reserved.",Faults detection | Machine learning | Software code metric | Testing,"Computers, Materials and Continua",2021-01-01,Article,"Hasnain, Muhammad;Jeong, Seung Ryul;Pasha, Muhammad Fermi;Ghani, Imran",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85109048026,10.32604/iasc.2021.017562,Software defect prediction using supervised machine learning techniques: A systematic literature review,"Software defect prediction (SDP) is the process of detecting defect-prone software modules before the testing stage. The testing stage in the software development life cycle is expensive and consumes the most resources of all the stages. SDP can minimize the cost of the testing stage, which can ultimately lead to the development of higher-quality software at a lower cost. With this approach, only those modules classified as defective are tested. Over the past two decades, many researchers have proposed methods and frameworks to improve the performance of the SDP process. The main research topics are association, estimation, clustering, classification, and dataset analysis. This study provides a systematic literature review that highlights the latest research trends in the area of SDP by providing a critical review of papers published between 2016 and 2019. Initially, 1012 papers were shortlisted from three online libraries (IEEE Xplore, ACM, and ScienceDirect); following a systematic research protocol, 22 of these papers were selected for detailed critical review. This review will serve researchers by providing the most current picture of the published work on software defect classification. © 2021, Tech Science Press. All rights reserved.",Machine learning | Software defect prediction | Systematic literature review,Intelligent Automation and Soft Computing,2021-01-01,Article,"Matloob, Faseeha;Aftab, Shabib;Ahmad, Munir;Khan, Muhammad Adnan;Fatima, Areej;Iqbal, Muhammad;Alruwaili, Wesam Mohsen;Elmitwally, Nouh Sabri",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85107110889,10.32604/iasc.2021.017703,Novel power transformer fault diagnosis using optimized machine learning methods,"Power transformer is one of the more important components of electrical power systems. The early detection of transformer faults increases the power system reliability. Dissolved gas analysis (DGA) is one of the most favorite approaches used for power transformer fault prediction due to its easiness and applicability for online diagnosis. However, the imbalanced, insufficient and overlap of DGA dataset impose a challenge towards powerful and accurate diagnosis. In this work, a novel fault diagnosis for power transformers is introduced based on DGA by using data transformation and six optimized machine learning (OML) methods. Four data transformation techniques are used with the dissolved gasses of transformer oils to reduce the high overlap of dataset samples. The OML methods used for transformer fault diagnosis are decision tree, discriminant analysis, Naïve Bayes, support vector machines, K-nearest neighboring, and ensemble classification methods. The six OML methods are implemented by MATLAB/ Software based on 542 dataset samples collected from laboratories and literature. In this regard, 361 dataset samples were used for training, while 181 dataset samples were used for testing. The transformer fault diagnosis based on the OML methods had superior predicting accuracy compared to conventional and artificial intelligence methods. © 2021, Tech Science Press. All rights reserved.",Data transformation | Dissolved gas analysis | Machine learning | Transformer fault diagnosis,Intelligent Automation and Soft Computing,2021-01-01,Article,"Taha, Ibrahim B.M.;Mansour, Diaa Eldin A.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85059033026,10.3389/fninf.2018.00094,An automated pipeline for the analysis of PET data on the cortical surface,"We present a fully automatic pipeline for the analysis of PET data on the cortical surface. Our pipeline combines tools from FreeSurfer and PETPVC, and consists of (i) co-registration of PET and T1-w MRI (T1) images, (ii) intensity normalization, (iii) partial volume correction, (iv) robust projection of the PET signal onto the subject's cortical surface, (v) spatial normalization to a template, and (vi) atlas statistics. We evaluated the performance of the proposed workflow by performing group comparisons and showed that the approach was able to identify the areas of hypometabolism characteristic of different dementia syndromes: Alzheimer's disease (AD) and both the semantic and logopenic variants of primary progressive aphasia. We also showed that these results were comparable to those obtained with a standard volume-based approach. We then performed individual classifications and showed that vertices can be used as features to differentiate cognitively normal and AD subjects. This pipeline is integrated into Clinica, an open-source software platform for neuroscience studies available at www.clinica.run. © 2018 Marcoux, Burgos, Bertrand, Teichmann, Routier, Wen, Samper-González, Bottani, Durrleman, Habert, Colliot and for the Alzheimer's Disease Neuroimaging Initiative.",Brain | Neurodegenerative diseases | PET | Pipeline | Positron emission tomography | Surface analysis | Workflow,Frontiers in Neuroinformatics,2018-12-10,Article,"Marcoux, Arnaud;Burgos, Ninon;Bertrand, Anne;Teichmann, Marc;Routier, Alexandre;Wen, Junhao;Samper-González, Jorge;Bottani, Simona;Durrleman, Stanley;Habert, Marie Odile;Colliot, Olivier",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85103015556,10.3390/app11052072,Protected network architecture for ensuring consistency of medical data through validation of user behavior and DICOM archive integrity,"The problem of consistency of medical data in Hospital Data Management Systems is considered in the context of correctness of medical images stored in a PACS (Picture Archiving and Communication System) and legality of actions authorized users perform when accessing MIS (Medical Information System) facilities via web interfaces. The purpose of the study is to develop a SIEM-like (Security Information and Event Management) architecture for offline analysis of DICOM (Digital Imaging and Communications in Medicine) archive integrity and users’ activity. To achieve amenable accuracy when validating DICOM archive integrity, two aspects are taken into account: correctness of periodicity of the incoming data stream and correctness of the image data (time series) itself for the considered modality. Validation of users’ activity assumes application of model-driven approaches using state-of-the-art machine learning methods. This paper proposes a network architecture with guard clusters to protect sensitive components like the DICOM archive and application server of the MIS. New server roles were designed to perform traffic interception, data analysis and alert management without reconfiguration of production software components. The cluster architecture allows the analysis of incoming big data streams with high availability, providing horizontal scalability and fault tolerance. To minimize possible harm from spurious DICOM files the approach should be considered as an addition to other securing techniques like watermarking, encrypting and testing data conformance with a standard. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Data validation | DICOM archive | Distributed systems | Network traffic interception | User behavior analytics,Applied Sciences (Switzerland),2021-03-01,Article,"Magomedov, Shamil;Lebedev, Artem",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85107796983,10.3390/app11115183,Software project management using machine learning technique-a review,"Project management planning and assessment are of great significance in project performance activities. Without a realistic and logical plan, it isn’t easy to handle project management efficiently. This paper presents a wide-ranging comprehensive review of papers on the application of Machine Learning in software project management. Besides, this paper presents an extensive literature analysis of (1) machine learning, (2) software project management, and (3) techniques from three main libraries, Web Science, Science Directs, and IEEE Explore. One-hundred and eleven papers are divided into four categories in these three repositories. The first category contains research and survey papers on software project management. The second category includes papers that are based on machine-learning methods and strategies utilized on projects; the third category encompasses studies on the phases and tests that are the parameters used in machine-learning management and the final classes of the results from the study, contribution of studies in the production, and the promotion of machine-learning project prediction. Our contribution also offers a more comprehensive perspective and a context that would be important for potential work in project risk management. In conclusion, we have shown that project risk assessment by machine learning is more successful in minimizing the loss of the project, thereby increasing the likelihood of the project success, providing an alternative way to efficiently reduce the project failure probabilities, and increasing the output ratio for growth, and it also facilitates analysis on software fault prediction based on accuracy. © 2021 by the author. Licensee MDPI, Basel, Switzerland.",Machine learning technique | Project risk assessment | Software estimation | Software project estimation | Software project management,Applied Sciences (Switzerland),2021-06-01,Article,"Mahdi, Mohammed Najah;Zabil, Mohd Hazli Mohamed;Ahmad, Abdul Rahim;Ismail, Roslan;Yusoff, Yunus;Cheng, Lim Kok;Mohd Azmi, Muhammad Sufyian Bin;Natiq, Hayder;Naidu, Hushalini Happala",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85108870235,10.3390/app11125533,Article metaheuristic optimized multi-level classification learning system for engineering management,"Multi-class classification is one of the major challenges in machine learning and an ongoing research issue. Classification algorithms are generally binary, but they must be extended to multi-class problems for real-world application. Multi-class classification is more complex than binary classification. In binary classification, only the decision boundaries of one class are to be known, whereas in multiclass classification, several boundaries are involved. The objective of this investigation is to propose a metaheuristic, optimized, multi-level classification learning system for forecasting in civil and construction engineering. The proposed system integrates the firefly algorithm (FA), metaheuristic intelligence, decomposition approaches, the one-against-one (OAO) method, and the least squares support vector machine (LSSVM). The enhanced FA automatically fine-tunes the hyperparameters of the LSSVM to construct an optimized LSSVM classification model. Ten benchmark functions are used to evaluate the performance of the enhanced optimization algorithm. Two binary-class datasets related to geotechnical engineering, concerning seismic bumps and soil liquefaction, are then used to clarify the application of the proposed system to binary problems. Further, this investigation uses multi-class cases in civil engineering and construction management to verify the effectiveness of the model in the diagnosis of faults in steel plates, quality of water in a reservoir, and determining urban land cover. The results reveal that the system predicts faults in steel plates with an accuracy of 91.085%, the quality of water in a reservoir with an accuracy of 93.650%, and urban land cover with an accuracy of 87.274%. To demonstrate the effectiveness of the proposed system, its predictive accuracy is compared with that of a non-optimized baseline model, single multi-class classification algorithms (sequential minimal optimization (SMO), the Multiclass Classifier, the Naïve Bayes, the library support vector machine (LibSVM) and logistic regression) and prior studies. The analytical results show that the proposed system is promising project analytics software to help decision makers solve multi-level classification problems in engineering applications. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Chaotic maps and Lévy flights | Civil and construction engineering | Engineering management | Hybrid computing system | Machine learning | Metaheuristic optimization | Multi-level classification | Swarm and evolutionary algorithm,Applied Sciences (Switzerland),2021-06-02,Article,"Chou, Jui Sheng;Pham, Trang Thi Phuong;Ho, Chia Chun",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85019201235,10.3390/ijms18051029,PCVMZM: Using the probabilistic classification vector machines model combined with a Zernike moments descriptor to predict protein-protein interactions from protein sequences,"Protein-protein interactions (PPIs) are essential for most living organisms’ process. Thus, detecting PPIs is extremely important to understand the molecular mechanisms of biological systems. Although many PPIs data have been generated by high-throughput technologies for a variety of organisms, the whole interatom is still far from complete. In addition, the high-throughput technologies for detecting PPIs has some unavoidable defects, including time consumption, high cost, and high error rate. In recent years, with the development of machine learning, computational methods have been broadly used to predict PPIs, and can achieve good prediction rate. In this paper, we present here PCVMZM, a computational method based on a Probabilistic Classification Vector Machines (PCVM) model and Zernike moments (ZM) descriptor for predicting the PPIs from protein amino acids sequences. Specifically, a Zernike moments (ZM) descriptor is used to extract protein evolutionary information from Position-Specific Scoring Matrix (PSSM) generated by Position-Specific Iterated Basic Local Alignment Search Tool (PSI-BLAST). Then, PCVM classifier is used to infer the interactions among protein. When performed on PPIs datasets of Yeast and H. Pylori, the proposed method can achieve the average prediction accuracy of 94.48% and 91.25%, respectively. In order to further evaluate the performance of the proposed method, the state-of-the-art support vector machines (SVM) classifier is used and compares with the PCVM model. Experimental results on the Yeast dataset show that the performance of PCVM classifier is better than that of SVM classifier. The experimental results indicate that our proposed method is robust, powerful and feasible, which can be used as a helpful tool for proteomics research. © 2017 by the authors. Licensee MDPI, Basel, Switzerland.",Position-specific scoring matrix | Probabilistic classification vector machines | Proteins,International Journal of Molecular Sciences,2017-05-11,Article,"Wang, Yanbin;You, Zhuhong;Li, Xiao;Chen, Xing;Jiang, Tonghai;Zhang, Jingting",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85119105558,10.3390/infrastructures6110152,A machine learning framework for predicting bridge defect detection cost,"Evaluating the cost of detecting bridge defects is a difficult task, but one that is vital to the lifecycle cost analysis of bridges. In this study, a detection cost sample database was established based on practical engineering data, and a bridge defect detection cost prediction model and software were developed using machine learning. First, the random forest method was adopted to evaluate the importance of the seven main factors affecting the detection cost. The most important indicators were selected, and the recent GDP growth rate was employed to account for the impact of social and economic developments on the detection cost. Combining a genetic algorithm with a multilayer neural network, a detection cost prediction model was established. The predictions given by this model were found to have an average relative error of 3.41%. Finally, an intelligent prediction software for bridge defect detection costs was established, providing a reliable reference for bridge lifecycle cost analysis and the evaluation of defect detection costs during the operation period. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Bridge engineering | Intelligent prediction model | Life cycle detection cost | Machine learning,Infrastructures,2021-11-01,Article,"Wang, Chongjiao;Yao, Changrong;Qiang, Bin;Zhao, Siguang;Li, Yadong",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85111045271,10.3390/s21155036,Intelligent network applications monitoring and diagnosis employing software sensing and machine learning solutions,"The article presents a research in the field of complex sensing, detection, and recovery of communications networks applications and hardware, in case of failures, maloperations, or unauthorized intrusions. A case study, based on Davis AI engine operation versus human maintenance operation is performed on the efficiency of artificial intelligence agents in detecting faulty operation, in the context of growing complexity of communications networks, and the perspective of future development of internet of things, big data, smart cities, and connected vehicles. (*). In the second part of the article, a new solution is proposed for the detection of applications faults or unauthorized intrusions in traffic of communications networks. The first objective of the proposed method is to propose an approach for predicting time series. This approach is based on a multi-resolution decomposition of the signals employing the undecimate wavelet transform (UWT). The second approach for assessing traffic flow is based on the analysis of long-range dependence (LRD) (for this case, a long-term dependence). Estimating the degree of long-range dependence is performed by estimating the Hurst parameter of the analyzed time series. This is a relatively new statistical con-cept in communications traffic analysis and can be implemented using UWT. This property has important implications for network performance, design, and sizing. The presence of long-range dependency in network traffic is assumed to have a significant impact on network performance, and the occurrence of LRD can be the result of faults that occur during certain periods. The strategy chosen for this purpose is based on long-term dependence on traffic, and for the prediction of faults occurrence, a predictive control model (MPC) is proposed, combined with a neural network with radial function (RBF). It is demonstrated via simulations that, in the case of communications traffic, time location is the most important feature of the proposed algorithm. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Apdex performance index | Hurst exponent | Intelligent agent | Preventive failure maintenance | Software sensing | Undecimate wavelet transform | Wavelet decomposition,Sensors,2021-08-01,Article,"Minea, Marius;Dumitrescu, Cătălin Marian;Minea, Viviana Laetitia",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85046071580,10.3390/sym9080161,Using knowledge transfer and rough set to predict the severity of android test reports via text mining,"Crowdsourcing is an appealing and economic solution to software application testing because of its ability to reach a large international audience. Meanwhile, crowdsourced testing could have brought a lot of bug reports. Thus, in crowdsourced software testing, the inspection of a large number of test reports is an enormous but essential software maintenance task. Therefore, automatic prediction of the severity of crowdsourced test reports is important because of their high numbers and large proportion of noise. Most existing approaches to this problem utilize supervised machine learning techniques, which often require users to manually label a large number of training data. However, Android test reports are not labeled with their severity level, and manual labeling is time-consuming and labor-intensive. To address the above problems, we propose a Knowledge Transfer Classification (KTC) approach based on text mining and machine learning methods to predict the severity of test reports. Our approach obtains training data from bug repositories and uses knowledge transfer to predict the severity of Android test reports. In addition, our approach uses an Importance Degree Reduction (IDR) strategy based on rough set to extract characteristic keywords to obtain more accurate reduction results. The results of several experiments indicate that our approach is beneficial for predicting the severity of android test reports. © 2017 by the authors.",Crowdsourced testing | Importance degree reduction | Knowledge transfer | Rough set | Test report,Symmetry,2017-01-01,Article,"Guo, Shikai;Chen, Rong;Li, Hui",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85074617854,10.35940/ijeat.A9746.109119,Software vulnerability classification based on deep neural network,"Software vulnerability is most common issues in software engineering, many applications has suffering vulnerability, information leakage, and data hijacking such kind of problems facing since couple of years. Sometimes developers should be making some mistakes during code making which generate vulnerability issues for entire application. In this research work, we carried out an approach to software vulnerability detection using deep learning approach behalf of metadata processing. The system carried software vulnerability detection based on the Deep Neural Network (DNN). a new dynamic vulnerability classification approach has suggested. The model basic build based on TF-IDF as well density based feature selection approach for DNN. basically TF-IDF has used to measured the frequency and weight of specific word of vulnerability description; the Vector Space Model (VSM) is used for feature selection to achieve an finest set of feature term, and; the DNN neural network model is used to built an dynamic weakness classifier to achieve effectiveness into the bug detection. The overall system has categorized into four phases in first phase we detect the code clone to eliminate the data redundancy and execution time complexity, in second we apply Vector Space Model (VSM) recommend the re-factor possibility in entire code while in third section we build DNN module for software vulnerability detection and finally recommend the vulnerability for entire code. The system partial implementation has evaluated in java environment which provide satisfactory results for heterogeneous code modules. ©BEIESP.",Computer security | Data mining | Deep neural networks | Machine learning,International Journal of Engineering and Advanced Technology,2019-10-01,Article,"Vitthalrao, Markad Ashok;Gupta, Mukesh Kumar",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85097657361,10.3844/JCSSP.2020.1558.1569,Machine Learning Techniques for Software Bug Prediction: A Systematic Review,"The goal of software bug prediction is to identify the software modules that will have the likelihood to get bugs by using some fundamental project resources before the real testing starts. Due to high cost in correcting the detected bugs, it is advisable to start predicting bugs at the early stage of development instead of at the testing phase. There are many techniques and approaches that can be used to build the prediction models, such as machine learning. This technique is widely used nowadays because it can give accurate results and analysis. Therefore, we decided to perform a review of past literature on software bug prediction and machine learning so that we can understand better about the process of constructing the prediction model. Not only we want to see the machine learning techniques that past researchers used, we also assess the datasets, metrics and performance measures that are used during the development of the models. In this study, we have narrowed down to 31 main studies and six types of machine learning techniques have been identified. Two public datasets are found to be frequently used and object-oriented metrics are the highly chosen metrics for the prediction model. As for the performance measure, both graphical and numerical measures are often used to evaluate the performance of the models. From the results, we conclude that the machine learning technique can predict the bug, but there are not many applications in this area that exist nowadays. There are a few challenges in constructing the prediction model. Thus, more studies need to be carried out so that a well-formed result is obtained. We also provide a recommendation for future research based on the results we got from this study. ©",Literature Review | Machine Learning Techniques | Software Bug Prediction,Journal of Computer Science,2020-01-01,Article,"Saharudin, Syahana Nur Ain;Wei, Koh Tieng;Na, Kew Si",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84907242292,10.3923/jai.2014.94.112,Survey on graphical user interface and machine learning based testing techniques,"The Graphical User Interface (GUI) is used in building interactive and web based applications. Several components are available for building the front end of t he software. The interaction among components is accomplished through their corresponding events. This survey explains the testing concepts in detail by addressing the various testing techniques, test model structure and detected fault category. Various testing techniques such as data flow testing, object oriented testing, model based testing, web applications testing, user interaction testing, user interface testing, machine learning based testing, state based testing, test suite reduction and specification based testing are discussed in this survey. The survey also presents the parameters used for evaluation. Machine learning algorithms, features and the data set used for classification in the this study are analyzed in survey. Considering of role of direct and indirect metrics in software testing is also addressed in this survey. The testing tools and frameworks used for testing the Graphical User Interface (GUI) applications and their issues are also handled. © 2014 Asian Network for Scientific Information.",Defect prediction | Graphical user interface | Machine learning | Model based testing | Web applications,Journal of Artificial Intelligence,2014-01-01,Article,"Uma Maheswari, B.;Valli, S.",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84926158550,10.4018/978-1-4666-6026-7.ch023,Important issues in software fault prediction: A road map,"Quality assurance tasks such as testing, verification and validation, fault tolerance, and fault prediction play a major role in software engineering activities. Fault prediction approaches are used when a software company needs to deliver a finished product while it has limited time and budget for testing it. In such cases, identifying and testing parts of the system that are more defect prone is reasonable. In fact, prediction models are mainly used for improving software quality and exploiting available resources. Software fault prediction is studied in this chapter based on different criteria that matters in this research field. Usually, there are certain issues that need to be taken care of such as different machine-learning techniques, artificial intelligence classifiers, variety of software metrics, distinctive performance evaluation metrics, and some statistical analysis. In this chapter, the authors present a roadmap for those researchers who are interested in working in this area. They illustrate problems along with objectives related to each mentioned criterion, which could assist researchers to build the finest software fault prediction model. © 2014 by IGI Global. All rights reserved.",,Handbook of Research on Emerging Advancements and Technologies in Software Engineering,2014-04-30,Book Chapter,"Abaei, Golnoush;Selamat, Ali",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85018578673,10.4018/978-1-5225-1759-7.ch083,Prediction of change-prone classes using machine learning and statistical techniques,"For software development, availability of resources is limited, thereby necessitating efficient and effective utilization of resources. This can be achieved through prediction of key attributes, which affect software quality such as fault proneness, change proneness, effort, maintainability, etc. The primary aim of this chapter is to investigate the relationship between object-oriented metrics and change proneness. Predicting the classes that are prone to changes can help in maintenance and testing. Developers can focus on the classes that are more change prone by appropriately allocating resources. This will help in reducing costs associated with software maintenance activities. The authors have constructed models to predict change proneness using various machine-learning methods and one statistical method. They have evaluated and compared the performance of these methods. The proposed models are validated using open source software, Frinika, and the results are evaluated using Receiver Operating Characteristic (ROC) analysis. The study shows that machine-learning methods are more efficient than regression techniques. Among the machine-learning methods, boosting technique (i.e. Logitboost) outperformed all the other models. Thus, the authors conclude that the developed models can be used to predict the change proneness of classes, leading to improved software quality. © 2017 by IGI Global. All rights reserved.",,"Artificial Intelligence: Concepts, Methodologies, Tools, and Applications",2016-12-12,Book Chapter,"Malhotra, Lin Ruchika;Bansal, Ankita Jain",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85041612511,10.4018/978-1-5225-3923-0.ch086,Prediction of change-prone classes using machine learning and statistical techniques,"For software development, availability of resources is limited, thereby necessitating efficient and effective utilization of resources. This can be achieved through prediction of key attributes, which affect software quality such as fault proneness, change proneness, effort, maintainability, etc. The primary aim of this chapter is to investigate the relationship between object-oriented metrics and change proneness. Predicting the classes that are prone to changes can help in maintenance and testing. Developers can focus on the classes that are more change prone by appropriately allocating resources. This will help in reducing costs associated with software maintenance activities. The authors have constructed models to predict change proneness using various machine-learning methods and one statistical method. They have evaluated and compared the performance of these methods. The proposed models are validated using open source software, Frinika, and the results are evaluated using Receiver Operating Characteristic (ROC) analysis. The study shows that machine-learning methods are more efficient than regression techniques. Among the machine-learning methods, boosting technique (i.e. Logitboost) outperformed all the other models. Thus, the authors conclude that the developed models can be used to predict the change proneness of classes, leading to improved software quality. © 2018 by IGI Global. All rights reserved.",,"Computer Systems and Software Engineering: Concepts, Methodologies, Tools, and Applications",2017-01-01,Book Chapter,"Malhotra, Lin Ruchika;Bansal, Ankita Jain",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84900226237,10.4018/978-1-59140-941-1.ch011,Applying rule induction in software prediction,"Recently, the use of machine learning (ML) algorithms has proven to be of great practical value in solving a variety of software engineering problems including software prediction, for example, cost and defect processes. An important advantage of machine learning over statistical analysis as a modelling technique lies in the fact that the interpretation of production rules is more straightforward and intelligible to human beings than, say, principal components and patterns with numbers that represent their meaning. The main focus of this chapter is upon rule induction (RI): providing some background and key issues on RI and further examining how RI has been utilised to handle uncertainties in data. Application of RI in prediction and other software engineering tasks is considered. The chapter concludes by identifying future research work when applying rule induction in software prediction. Such future research work might also help solve new problems related to rule induction and prediction. © 2007, Idea Group Inc.",,Advances in Machine Learning Applications in Software Engineering,2006-12-01,Book Chapter,"Twala, Bhekisipho;Cartwright, Michelle;Shepperd, Martin",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-84898387431,10.4018/978-1-60566-766-9.ch025,Counting the hidden defects in software documents,"The author uses neural networks to estimate how many defects are hidden in a software document. Input for the models are metrics that get collected when effecting a standard quality assurance technique on the document, a software inspection. For inspections, the empirical data sets typically are small. The author identifies two key ingredients for a successful application of neural networks to small data sets: Adapting the size, complexity, and input dimension of the networks to the amount of information available for training; and using Bayesian techniques instead of cross-validation for determining model parameters and selecting the final model. For inspections, the machine learning approach is highly successful and outperforms the previously existing defect estimation methods in software engineering by a factor of 4 in accuracy on the standard benchmark. The author's approach is well applicable in other contexts that are subject to small training data sets. © 2010, IGI Global.",,"Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods, and Techniques",2009-12-01,Book Chapter,"Padberg, Frank",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85093517097,10.4018/IJWSR.2020070101,Fault prediction in SOA-based systems using deep learning techniques,"Fault prediction in Service Oriented Architecture (SOA) based systems is one of the important tasks to minimize the computation cost and time of the software system development. Predicting the faults and discovering their locations in the early stage of the system development lifecycle makes maintenance processes easy and improves the resource utilization. In this paper, the authors proposed the fault prediction model for SOA-based systems by utilizing the deep learning techniques. Twentyone source code metrics are applied to different web services projects. The web services datasets are constructed by injecting the faults into it, and metrics are extracted for both faulty and nonfaulty data for training and testing purpose. Moreover, different deep learning techniques are inspected for fault prediction of web services and performance of different methods are compared by using standard performance measures. From the experimental results, it is observed that deep learning techniques provide effective results and applicable to the real-world SOA-based systems. Copyright © 2020, IGI Global.",Artificial Neural Networks | Convolutional Neural Networks | Fault | Fault Prediction | Machine Learning (ML) | Metrics | Service-Oriented Architecture | SOA-Based Systems,International Journal of Web Services Research,2020-07-01,Article,"Bhandari, Guru Prasad;Gupta, Ratneshwer",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85067431644,10.5220/0007707003730380,A probabilistic approach based on a finite mixture model of multivariate beta distributions,"Model-based approaches specifically finite mixture models are widely applied as an inference engine in machine learning, data mining and related disciplines. They proved to be an effective and advanced tool in discovery, extraction and analysis of critical knowledge from data by providing better insight into the nature of data and uncovering hidden patterns that we are looking for. In recent researches, some distributions such as Beta distribution have demonstrated more flexibility in modeling asymmetric and non-Gaussian data. In this paper, we introduce an unsupervised learning algorithm for a finite mixture model based on multivariate Beta distribution which could be applied in various real-world challenging problems such as texture analysis, spam detection and software modules defect prediction. Parameter estimation is one of the crucial and critical challenges when deploying mixture models. To tackle this issue, deterministic and efficient techniques such as Maximum likelihood (ML), Expectation maximization (EM) and Newton Raphson methods are applied. The feasibility and effectiveness of the proposed model are assessed by experimental results involving real datasets. The performance of our framework is compared with the widely used Gaussian Mixture Model (GMM). Copyright © 2019 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.",Clustering | Maximum likelihood | Mixture models | Multivariate beta distribution,ICEIS 2019 - Proceedings of the 21st International Conference on Enterprise Information Systems,2019-01-01,Conference Paper,"Manouchehri, Narges;Bouguila, Nizar",Exclude,
10.1016/j.infsof.2022.107128,2-s2.0-85073118940,10.5220/0007830700400050,Generation of complex data for AI-based predictive maintenance research with a physical factory model,"Manufacturing systems naturally contain plenty of sensors which produce data primarily used by the control software to detect relevant status information of the actuators. In addition, sensors are included in order to monitor the health status of specific components, which enable to detect certain known, frequently occurring faults or undesired states of the system. While the identification of a failure by using the data of a sensor dedicated explicitly to its detection is a rather straightforward machine learning application, the detection of failures which only have an indirect effect on the data produced by a couple of other sensors is much more challenging. Therefore, a combination of different methods from Artificial Intelligence, in particular, machine learning and knowledge-based (semantic) approaches is required to identify relevant patterns (or failure modes). However, there are currently no appropriate research environments and data sets available that can be used for this kind of research. In this paper, we propose an approach for the generation of predictive maintenance data by using a physical Fischertechnik model factory equipped with several sensors. Different ways of reproducing real failures using this model are presented as well as a general procedure for data generation. Copyright © 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.",Data generation | Industry 4.0 | Machine learning | Predictive maintenance,"ICINCO 2019 - Proceedings of the 16th International Conference on Informatics in Control, Automation and Robotics",2019-01-01,Conference Paper,"Klein, Patrick;Bergmann, Ralph",Exclude,
