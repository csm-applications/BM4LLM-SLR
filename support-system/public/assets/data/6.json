{
  "secondary_study": {
    "Secondary study DOI": "10.1016/j.infsof.2021.106783",
    "Title": "How far are we from reproducible research on code smell detection? A systematic literature review",
    "Inclusion / Exclusion criteria": "Inclusion criteria:\n\n- IC1: Title or abstract of the paper indicates that it is related to software engineering.\n- IC2: Title or abstract of the paper indicates that at least one code smell/anti-pattern plays an important part of the study.\n- IC3: Title or abstract of the paper indicates that it might use machine learning techniques.\n- IC4: Abstract or full text of the paper indicates that it focuses on code smells/anti-patterns in programming languages.\n- IC5: Abstract or full text of the paper indicates that it focuses on code smells/anti-patterns detection using source code.\n\nExclusion criteria:\n\n- EC1: The paper focus on techniques for resolving code smells/anti-patterns.\n- EC2: The paper focus on using code smells/anti-patterns as predictors of other code or project traits.\n- EC3: The paper focuses on detection/prediction of code smells/antipatterns.",
    "Prompt instruction": "The paper is included, if all inclusion criteria match. If the paper matches any exclusion criteria, it is excluded."
  },
  "primary_studies": [
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040092215",
      "Primary study DOI": "10.1109/TSE.2017.2788018",
      "Title": "Automatic Loop Summarization via Path Dependency Analysis",
      "Abstract": "Analyzing loops is very important for various software engineering tasks such as bug detection, test case generation and program optimization. However, loops are very challenging structures for program analysis, especially when (nested) loops contain multiple paths that have complex interleaving relationships. In this paper, we propose the path dependency automaton (PDA) to capture the dependencies among the multiple paths in a loop. Based on the PDA, we first propose a loop classification to understand the complexity of loop summarization. Then, we propose a loop analysis framework, named Proteus, which takes a loop program and a set of variables of interest as inputs and summarizes path-sensitive loop effects (i.e., disjunctive loop summary) on the variables of interest. An algorithm is proposed to traverse the PDA to summarize the effect for all possible executions in the loop. We have evaluated Proteus using loops from five open-source projects and two well-known benchmarks and applying the disjunctive loop summary to three applications: loop bound analysis, program verification and test case generation. The evaluation results have demonstrated that Proteus can compute a more precise bound than the existing loop bound analysis techniques; Proteus can significantly outperform the state-of-the-art tools for loop program verification; and Proteus can help generate test cases for deep loops within one second, while symbolic execution tools KLEE and Pex either need much more time or fail.",
      "Keywords": "Disjunctive loop summary | path dependency automaton | path interleaving",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-06-01",
      "Publication type": "Article",
      "Authors": "Xie, Xiaofei;Chen, Bihuan;Zou, Liang;Liu, Yang;Le, Wei;Li, Xiaohong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040086250",
      "Primary study DOI": "10.1109/TSE.2017.2786286",
      "Title": "Automatic Detection and Removal of Ineffective Mutants for the Mutation Analysis of Relational Database Schemas",
      "Abstract": "Data is one of an organization's most valuable and strategic assets. Testing the relational database schema, which protects the integrity of this data, is of paramount importance. Mutation analysis is a means of estimating the fault-finding strength of a test suite. As with program mutation, however, relational database schema mutation results in many ineffective mutants that both degrade test suite quality estimates and make mutation analysis more time consuming. This paper presents a taxonomy of ineffective mutants for relational database schemas, summarizing the root causes of ineffectiveness with a series of key patterns evident in database schemas. On the basis of these, we introduce algorithms that automatically detect and remove ineffective mutants. In an experimental study involving the mutation analysis of 34 schemas used with three popular relational database management systems-HyperSQL, PostgreSQL, and SQLite-the results show that our algorithms can identify and discard large numbers of ineffective mutants that can account for up to 24 percent of mutants, leading to a change in mutation score for 33 out of 34 schemas. The tests for seven schemas were found to achieve 100 percent scores, indicating that they were capable of detecting and killing all non-equivalent mutants. The results also reveal that the execution cost of mutation analysis may be significantly reduced, especially with heavyweight DBMSs like PostgreSQL.",
      "Keywords": "relational databases | software quality | software testing | software tools",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-05-01",
      "Publication type": "Article",
      "Authors": "McMinn, Phil;Wright, Chris J.;McCurdy, Colton J.;Kapfhammer, Gregory M.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040028248",
      "Primary study DOI": "10.1109/TSE.2017.2787585",
      "Title": "makeSense: Simplifying the Integration of Wireless Sensor Networks into Business Processes",
      "Abstract": "A wide gap exists between the state of the art in developing Wireless Sensor Network (WSN) software and current practices concerning the design, execution, and maintenance of business processes. WSN software is most often developed based on low-level OS abstractions, whereas business process development leverages high-level languages and tools. This state of affairs places WSNs at the fringe of industry. The makeSense system addresses this problem by simplifying the integration of WSNs into business processes. Developers use BPMN models extended with WSN-specific constructs to specify the application behavior across both traditional business process execution environments and the WSN itself, which is to be equipped with application-specific software. We compile these models into a high-level intermediate language-Also directly usable by WSN developers-And then into OS-specific deployment-ready binaries. Key to this process is the notion of meta-Abstraction, which we define to capture fundamental patterns of interaction with and within the WSN. The concrete realization of meta-Abstractions is application-specific; developers tailor the system configuration by selecting concrete abstractions out of the existing codebase or by providing their own. Our evaluation of makeSense shows that i) users perceive our approach as a significant advance over the state of the art, providing evidence of the increased developer productivity when using makeSense; ii) in large-scale simulations, our prototype exhibits an acceptable system overhead and good scaling properties, demonstrating the general applicability of makeSense; and, iii) our prototype-including the complete tool-chain and underlying system support-sustains a real-world deployment where estimates by domain specialists indicate the potential for drastic reductions in the total cost of ownership compared to wired and conventional WSN-based solutions.",
      "Keywords": "Business processes | embedded software | internet of things | wireless sensor networks",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-06-01",
      "Publication type": "Article",
      "Authors": "Mottola, L.;Picco, G. P.;Oppermann, F. J.;Eriksson, J.;Finne, N.;Fuchs, H.;Gaglione, A.;Karnouskos, S.;Montero, P. Moreno;Oertel, N.;Romer, K.;Spies, P.;Tranquillini, S.;Voigt, T.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040055397",
      "Primary study DOI": "10.1109/TSE.2017.2787653",
      "Title": "Text Filtering and Ranking for Security Bug Report Prediction",
      "Abstract": "Security bug reports can describe security critical vulnerabilities in software products. Bug tracking systems may contain thousands of bug reports, where relatively few of them are security related. Therefore finding unlabelled security bugs among them can be challenging. To help security engineers identify these reports quickly and accurately, text-based prediction models have been proposed. These can often mislabel security bug reports due to a number of reasons such as class imbalance, where the ratio of non-security to security bug reports is very high. More critically, we have observed that the presence of security related keywords in both security and non-security bug reports can lead to the mislabelling of security bug reports. This paper proposes FARSEC, a framework for filtering and ranking bug reports for reducing the presence of security related keywords. Before building prediction models, our framework identifies and removes non-security bug reports with security related keywords. We demonstrate that FARSEC improves the performance of text-based prediction models for security bug reports in 90 percent of cases. Specifically, we evaluate it with 45,940 bug reports from Chromium and four Apache projects. With our framework, we mitigate the class imbalance issue and reduce the number of mislabelled security bug reports by 38 percent.",
      "Keywords": "prediction models | ranking | security bug reports | Security cross words | security related keywords | text filtering | transfer learning",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-06-01",
      "Publication type": "Article",
      "Authors": "Peters, Fayola;Tun, Thein Than;Yu, Yijun;Nuseibeh, Bashar",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85039779858",
      "Primary study DOI": "10.1109/TSE.2017.2785841",
      "Title": "Coverage-based Greybox Fuzzing as Markov Chain",
      "Abstract": "Coverage-based Greybox Fuzzing (CGF) is a random testing approach that requires no program analysis. A new test is generated by slightly mutating a seed input. If the test exercises a new and interesting path, it is added to the set of seeds; otherwise, it is discarded. We observe that most tests exercise the same few high-frequency paths and develop strategies to explore significantly more paths with the same number of tests by gravitating towards low-frequency paths. We explain the challenges and opportunities of CGF using a Markov chain model which specifies the probability that fuzzing the seed that exercises path i i generates an input that exercises path j j. Each state (i.e., seed) has an energy that specifies the number of inputs to be generated from that seed. We show that CGF is considerably more efficient if energy is inversely proportional to the density of the stationary distribution and increases monotonically every time that seed is chosen. Energy is controlled with a power schedule. We implemented several schedules by extending AFL. In 24 hours, AFLFast exposes 3 previously unreported CVEs that are not exposed by AFL and exposes 6 previously unreported CVEs 7x faster than AFL. AFLFast produces at least an order of magnitude more unique crashes than AFL. We compared AFLFast to the symbolic executor Klee. In terms of vulnerability detection, AFLFast is significantly more effective than Klee on the same subject programs that were discussed in the original Klee paper. In terms of code coverage, AFLFast only slightly outperforms Klee while a combination of both tools achieves best results by mitigating the individual weaknesses.",
      "Keywords": "automated testing | fuzzing | path exploration | symbolic execution | Vulnerability detection",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-05-01",
      "Publication type": "Article",
      "Authors": "Bohme, Marcel;Pham, Van Thuan;Roychoudhury, Abhik",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85038868789",
      "Primary study DOI": "10.1109/TSE.2017.2782813",
      "Title": "The Good, the Bad and the Ugly: A Study of Security Decisions in a Cyber-Physical Systems Game",
      "Abstract": "Stakeholders' security decisions play a fundamental role in determining security requirements, yet, little is currently understood about how different stakeholder groups within an organisation approach security and the drivers and tacit biases underpinning their decisions. We studied and contrasted the security decisions of three demographics-security experts, computer scientists and managers-when playing a tabletop game that we designed and developed. The game tasks players with managing the security of a cyber-physical environment while facing various threats. Analysis of 12 groups of players (4 groups in each of our demographics) reveals strategies that repeat in particular demographics, e.g., managers and security experts generally favoring technological solutions over personnel training, which computer scientists preferred. Surprisingly, security experts were not ipso facto better players-in some cases, they made very questionable decisions-yet they showed a higher level of confidence in themselves. We classified players' decision-making processes, i.e., procedure-, experience-, scenario- or intuition-driven. We identified decision patterns, both good practices and typical errors and pitfalls. Our game provides a requirements sandbox in which players can experiment with security risks, learn about decision-making and its consequences, and reflect on their own perception of security.",
      "Keywords": "decision patterns | game | Security decisions | security requirements",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-05-01",
      "Publication type": "Article",
      "Authors": "Frey, Sylvain;Rashid, Awais;Anthonysamy, Pauline;Pinto-Albuquerque, Maria;Naqvi, Syed Asad",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85038846845",
      "Primary study DOI": "10.1109/TSE.2017.2782280",
      "Title": "An Empirical Study on API Usages",
      "Abstract": "API libraries provide thousands of APIs, and are essential in daily programming tasks. To understand their usages, it has long been a hot research topic to mine specifications that formally define legal usages for APIs. Furthermore, researchers are working on many other research topics on APIs. Although the research on APIs is intensively studied, many fundamental questions on APIs are still open. For example, the answers to open questions, such as which format can naturally define API usages and in which case, are still largely unknown. We notice that many such open questions are not concerned with concrete usages of specific APIs, but usages that describe how to use different types of APIs. To explore these questions, in this paper, we conduct an empirical study on API usages, with an emphasis on how different types of APIs are used. Our empirical results lead to nine findings on API usages. For example, we find that single-type usages are mostly strict orders, but multi-type usages are more complicated since they include both strict orders and partial orders. Based on these findings, for the research on APIs, we provide our suggestions on the four key aspects such as the challenges, the importance of different API elements, usage patterns, and pitfalls in designing evaluations. Furthermore, we interpret our findings, and present our insights on data sources, extraction techniques, mining techniques, and formats of specifications for the research of mining specifications.",
      "Keywords": "API usage | empirical study | mining specification",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-04-01",
      "Publication type": "Article",
      "Authors": "Zhong, Hao;Mei, Hong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85038824754",
      "Primary study DOI": "10.1109/TSE.2017.2781231",
      "Title": "Decomposition-Based Approach for Model-Based Test Generation",
      "Abstract": "Model-based test generation by model checking is a well-known testing technique that, however, suffers from the state explosion problem of model checking and it is, therefore, not always applicable. In this paper, we address this issue by decomposing a system model into suitable subsystem models separately analyzable. Our technique consists in decomposing that portion of a system model that is of interest for a given testing requirement, into a tree of subsystems by exploiting information on model variable dependency. The technique generates tests for the whole system model by merging tests built from those subsystems. We measure and report effectiveness and efficiency of the proposed decomposition-based test generation approach, both in terms of coverage and time.",
      "Keywords": "decomposition | model checking | Model-based testing | state explosion problem | test case generation",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-05-01",
      "Publication type": "Article",
      "Authors": "Arcaini, Paolo;Gargantini, Angelo;Riccobene, Elvinia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85038839109",
      "Primary study DOI": "10.1109/TSE.2017.2780222",
      "Title": "On the Multiple Sources and Privacy Preservation Issues for Heterogeneous Defect Prediction",
      "Abstract": "Heterogeneous defect prediction (HDP) refers to predicting defect-proneness of software modules in a target project using heterogeneous metric data from other projects. Existing HDP methods mainly focus on predicting target instances with single source. In practice, there exist plenty of external projects. Multiple sources can generally provide more information than a single project. Therefore, it is meaningful to investigate whether the HDP performance can be improved by employing multiple sources. However, a precondition of conducting HDP is that the external sources are available. Due to privacy concerns, most companies are not willing to share their data. To facilitate data sharing, it is essential to study how to protect the privacy of data owners before they release their data. In this paper, we study the above two issues in HDP. Specifically, to utilize multiple sources effectively, we propose a multi-source selection based manifold discriminant alignment (MSMDA) approach. To protect the privacy of data owners, a sparse representation based double obfuscation algorithm is designed and applied to HDP. Through a case study of 28 projects, our results show that MSMDA can achieve better performance than a range of baseline methods. The improvement is 3.4-15.315.3 percent in g-measure and 3.0-19.119.1 percent in AUC.",
      "Keywords": "Heterogeneous defect prediction | manifold discriminant alignment | multiple sources | privacy preservation | source selection | utility",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-04-01",
      "Publication type": "Article",
      "Authors": "Li, Zhiqiang;Jing, Xiao Yuan;Zhu, Xiaoke;Zhang, Hongyu;Xu, Baowen;Ying, Shi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85037627519",
      "Primary study DOI": "10.1109/TSE.2017.2779479",
      "Title": "Automatic Identification and Classification of Software Development Video Tutorial Fragments",
      "Abstract": "Software development video tutorials have seen a steep increase in popularity in recent years. Their main advantage is that they thoroughly illustrate how certain technologies, programming languages, etc. are to be used. However, they come with a caveat: there is currently little support for searching and browsing their content. This makes it difficult to quickly find the useful parts in a longer video, as the only options are watching the entire video, leading to wasted time, or fast-forwarding through it, leading to missed information. We present an approach to mine video tutorials found on the web and enable developers to query their contents as opposed to just their metadata. The video tutorials are processed and split into coherent fragments, such that only relevant fragments are returned in response to a query. Moreover, fragments are automatically classified according to their purpose, such as introducing theoretical concepts, explaining code implementation steps, or dealing with errors. This allows developers to set filters in their search to target a specific type of video fragment they are interested in. In addition, the video fragments in CodeTube are complemented with information from other sources, such as Stack Overflow discussions, giving more context and useful information for understanding the concepts.",
      "Keywords": "mining unstructured data | Recommender systems | video tutorials",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-05-01",
      "Publication type": "Article",
      "Authors": "Ponzanelli, Luca;Bavota, Gabriele;Mocci, Andrea;Oliveto, Rocco;Penta, Massimiliano DI;Haiduc, Sonia;Russo, Barbara;Lanza, Michele",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85037651663",
      "Primary study DOI": "10.1109/TSE.2017.2778711",
      "Title": "Automatic Generation of Tests to Exploit XML Injection Vulnerabilities in Web Applications",
      "Abstract": "Modern enterprise systems can be composed of many web services (e.g., SOAP and RESTful). Users of such systems might not have direct access to those services, and rather interact with them through a single-entry point which provides a GUI (e.g., a web page or a mobile app). Although the interactions with such entry point might be secure, a hacker could trick such systems to send malicious inputs to those internal web services. A typical example is XML injection targeting SOAP communications. Previous work has shown that it is possible to automatically generate such kind of attacks using search-based techniques. In this paper, we improve upon previous results by providing more efficient techniques to generate such attacks. In particular, we investigate four different algorithms and two different fitness functions. A large empirical study, involving also two industrial systems, shows that our technique is effective at automatically generating XML injection attacks.",
      "Keywords": "Evolutionary testing | security testing | XML injection",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-04-01",
      "Publication type": "Article",
      "Authors": "Jan, Sadeeq;Panichella, Annibale;Arcuri, Andrea;Briand, Lionel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85035745628",
      "Primary study DOI": "10.1109/TSE.2017.2777831",
      "Title": "Combining Code and Requirements Coverage with Execution Cost for Test Suite Reduction",
      "Abstract": "Test suites tend to become large and complex after software evolution iterations, thus increasing effort and cost to execute regression testing. In this context, test suite reduction approaches could be applied to identify subsets of original test suites that preserve the capability of satisfying testing requirements and revealing faults. In this paper, we propose Multi-Objective test suites REduction (named MORE+): a three-dimension approach for test suite reduction. The first dimension is the structural one and concerns the information on how test cases in a suite exercise the under-test application. The second dimension is functional and concerns how test cases exercise business application requirements. The third dimension is the cost and concerns the time to execute test cases. We define MORE+ as a multi-objective approach that reduces test suites so maximizing their capability in revealing faults according to the three considered dimensions. We have compared MORE+ with seven baseline approaches on 20 Java applications. Results showed, in particular, the effectiveness of MORE+ in reducing test suites with respect to these baselines, i.e., significantly more faults are revealed with test suites reduced by applying MORE+.",
      "Keywords": "Multi-objective approach | regression testing | test suite reduction | testing",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-04-01",
      "Publication type": "Article",
      "Authors": "Marchetto, Alessandro;Scanniello, Giuseppe;Susi, Angelo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85035786634",
      "Primary study DOI": "10.1109/TSE.2017.2777830",
      "Title": "Smart Bound Selection for the Verification of UML/OCL Class Diagrams",
      "Abstract": "Correctness of UML class diagrams annotated with OCL constraints can be checked using bounded verification techniques, e.g., SAT or constraint programming (CP) solvers. Bounded verification detects faults efficiently but, on the other hand, the absence of faults does not guarantee a correct behavior outside the bounded domain. Hence, choosing suitable bounds is a non-trivial process as there is a trade-off between the verification time (faster for smaller domains) and the confidence in the result (better for larger domains). Unfortunately, bounded verification tools provide little support in the bound selection process. In this paper, we present a technique that can be used to (i) automatically infer verification bounds whenever possible, (ii) tighten a set of bounds proposed by the user and (iii) guide the user in the bound selection process. This approach may increase the usability of UML/OCL bounded verification tools and improve the efficiency of the verification process.",
      "Keywords": "class diagram | constraint propagation | Formal verification | OCL | SAT | UML",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-04-01",
      "Publication type": "Article",
      "Authors": "Clariso, Robert;Gonzalez, Carlos A.;Cabot, Jordi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85035785999",
      "Primary study DOI": "10.1109/TSE.2017.2774297",
      "Title": "Competition-Based Crowdsourcing Software Development: A Multi-Method Study from a Customer Perspective",
      "Abstract": "Crowdsourcing is emerging as an alternative outsourcing strategy which is gaining increasing attention in the software engineering community. However, crowdsourcing software development involves complex tasks which differ significantly from the micro-tasks that can be found on crowdsourcing platforms such as Amazon Mechanical Turk which are much shorter in duration, are typically very simple, and do not involve any task interdependencies. To achieve the potential benefits of crowdsourcing in the software development context, companies need to understand how this strategy works, and what factors might affect crowd participation. We present a multi-method qualitative and quantitative theory-building research study. First, we derive a set of key concerns from the crowdsourcing literature as an initial analytical framework for an exploratory case study in a Fortune 500 company. We complement the case study findings with an analysis of 13,602 crowdsourcing competitions over a ten-year period on the very popular Topcoder crowdsourcing platform. Drawing from our empirical findings and the crowdsourcing literature, we propose a theoretical model of crowd interest and actual participation in crowdsourcing competitions. We evaluate this model using Structural Equation Modeling. Among the findings are that the level of prize and duration of competitions do not significantly increase crowd interest in competitions.",
      "Keywords": "case study | Crowdsourcing | multi-method study | sample study | software engineering",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-03-01",
      "Publication type": "Article",
      "Authors": "Stol, Klaas Jan;Caglayan, Bora;Fitzgerald, Brian",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85035779468",
      "Primary study DOI": "10.1109/TSE.2017.2776912",
      "Title": "MSeer-An Advanced Technique for Locating Multiple Bugs in Parallel",
      "Abstract": "In practice, a program may contain multiple bugs. The simultaneous presence of these bugs may deteriorate the effectiveness of existing fault-localization techniques to locate program bugs. While it is acceptable to use all failed and successful tests to identify suspicious code for programs with exactly one bug, it is not appropriate to use the same approach for programs with multiple bugs because the due-to relationship between failed tests and underlying bugs cannot be easily identified. One solution is to generate fault-focused clusters by grouping failed tests caused by the same bug into the same clusters. We propose MSeer-an advanced fault localization technique for locating multiple bugs in parallel. Our major contributions include the use of (1) a revised Kendall tau distance to measure the distance between two failed tests, (2) an innovative approach to simultaneously estimate the number of clusters and assign initial medoids to these clusters, and (3) an improved K-medoids clustering algorithm to better identify the due-to relationship between failed tests and their corresponding bugs. Case studies on 840 multiple-bug versions of seven programs suggest that MSeer performs better in terms of effectiveness and efficiency than two other techniques for locating multiple bugs in parallel.",
      "Keywords": "clustering | distance metrics | multiple bugs | parallel debugging | Software fault localization",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-03-01",
      "Publication type": "Article",
      "Authors": "Gao, Ruizhi;Eric Wong, W.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85035777556",
      "Primary study DOI": "10.1109/TSE.2017.2776152",
      "Title": "Developer Testing in The IDE: Patterns, Beliefs, And Behavior",
      "Abstract": "Software testing is one of the key activities to achieve software quality in practice. Despite its importance, however, we have a remarkable lack of knowledge on how developers test in real-world projects. In this paper, we report on a large-scale field study with 2,443 software engineers whose development activities we closely monitored over 2.5 years in four integrated development environments (IDEs). Our findings, which largely generalized across the studied IDEs and programming languages Java and C#, question several commonly shared assumptions and beliefs about developer testing: half of the developers in our study do not test; developers rarely run their tests in the IDE; most programming sessions end without any test execution; only once they start testing, do they do it extensively; a quarter of test cases is responsible for three quarters of all test failures; 12 percent of tests show flaky behavior; Test-Driven Development (TDD) is not widely practiced; and software developers only spend a quarter of their time engineering tests, whereas they think they test half of their time. We summarize these practices of loosely guiding one's development efforts with the help of testing in an initial summary on Test-Guided Development (TGD), a behavior we argue to be closer to the development reality of most developers than TDD.",
      "Keywords": "Developer testing | field study | JUnit | KaVE FeedBag++ | test-driven development (TDD) | testing effort | TestRoots WatchDog | unit tests",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-03-01",
      "Publication type": "Article",
      "Authors": "Beller, Moritz;Gousios, Georgios;Panichella, Annibale;Proksch, Sebastian;Amann, Sven;Zaidman, Andy",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85035746824",
      "Primary study DOI": "10.1109/TSE.2017.2775634",
      "Title": "Automatically Exploring Tradeoffs Between Software Output Fidelity and Energy Costs",
      "Abstract": "Data centers account for a significant fraction of global energy consumption and represent a growing business cost. Most current approaches to reducing energy use in data centers treat it as a hardware, compiler, or scheduling problem. This article focuses instead on the software level, showing how to reduce the energy used by programs when they execute. By combining insights from search-based software engineering, mutational robustness, profile-guided optimization, and approximate computing, the Producing Green Applications Using Genetic Exploration (PowerGAUGE) algorithm finds variants of individual programs that use less energy than the original. We apply hardware, software, and statistical techniques to manage the complexity of accurately assigning physical energy measurements to particular processes. In addition, our approach allows, but does not require, relaxing output quality requirements to achieve greater non-functional improvements. PowerGAUGE optimizations are validated using physical performance measurements. Experimental results on PARSEC benchmarks and two larger programs show average energy reductions of 14% when requiring the preservation of original output quality and 41% when allowing for human-acceptable levels of error.",
      "Keywords": "accurate energy measurement | genetic algorithms | optimizing noisy functions | Power optimization | profile-guided optimization | search-based software engineering",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-03-01",
      "Publication type": "Article",
      "Authors": "Dorn, Jonathan;Lacomis, Jeremy;Weimer, Westley;Forrest, Stephanie",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85035112846",
      "Primary study DOI": "10.1109/TSE.2017.2774829",
      "Title": "Automated Refactoring of OCL Constraints with Search",
      "Abstract": "Object Constraint Language (OCL) constraints are typically used to provide precise semantics to models developed with the Unified Modeling Language (UML). When OCL constraints evolve regularly, it is essential that they are easy to understand and maintain. For instance, in cancer registries, to ensure the quality of cancer data, more than one thousand medical rules are defined and evolve regularly. Such rules can be specified with OCL. It is, therefore, important to ensure the understandability and maintainability of medical rules specified with OCL. To tackle such a challenge, we propose an automated search-based OCL constraint refactoring approach (SBORA) by defining and applying four semantics-preserving refactoring operators (i.e., Context Change, Swap, Split and Merge) and three OCL quality metrics (Complexity, Coupling, and Cohesion) to measure the understandability and maintainability of OCL constraints. We evaluate SBORA along with six commonly used multi-objective search algorithms (e.g., Indicator-Based Evolutionary Algorithm (IBEA)) by employing four case studies from different domains: healthcare (i.e., cancer registry system from Cancer Registry of Norway (CRN)), Oil&Gas (i.e., subsea production systems), warehouse (i.e., handling systems), and an open source case study named SEPA. Results show: 1) IBEA achieves the best performance among all the search algorithms and 2) the refactoring approach along with IBEA can manage to reduce on average 29.25 percent Complexity and 39 percent Coupling and improve 47.75 percent Cohesion, as compared to the original OCL constraint set from CRN. To further test the performance of SBORA, we also applied it to refactor an OCL constraint set specified on the UML 2.3 metamodel and we obtained positive results. Furthermore, we conducted a controlled experiment with 96 subjects and results show that the understandability and maintainability of the original constraint set can be improved significantly from the perspectives of the 96 participants of the controlled experiment.",
      "Keywords": "CASE | Constraints | methodologies | metrics/measurement",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-02-01",
      "Publication type": "Article",
      "Authors": "Lu, Hong;Wang, Shuai;Yue, Tao;Ali, Shaukat;Nygard, Jan F.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85035102019",
      "Primary study DOI": "10.1109/TSE.2017.2774832",
      "Title": "Integrating Technical Debt Management and Software Quality Management Processes: A Normative Framework and Field Tests",
      "Abstract": "Despite the increasing awareness of the importance of managing technical debt in software product development, systematic processes for implementing technical debt management in software production have not been readily available. In this paper we report on the development and field tests of a normative process framework that systematically incorporates steps for managing technical debt in commercial software production. The framework integrates processes required for technical debt management with existing software quality management processes prescribed by the project management body of knowledge (PMBOK), and it contributes to the further development of software-specific extensions to the PMBOK. We partnered with three commercial software product development organizations to implement the framework in real-world software production settings. All three organizations, irrespective of their varying software process maturity levels, were able to adopt the proposed framework and integrate the prescribed technical debt management processes with their existing software quality management processes. Our longitudinal observations and case-study interviews indicate that the organizations were able to accrue economic benefits from the adoption and use of the integrated framework.",
      "Keywords": "case study | cost of quality | software engineering economics | software extension to PMBOK | software maintenance | software process | software product development | software quality | Technical debt",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-03-01",
      "Publication type": "Article",
      "Authors": "Ramasubbu, Narayan;Kemerer, Chris F.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85034256681",
      "Primary study DOI": "10.1109/TSE.2017.2772812",
      "Title": "Gray Computing: A Framework for Computing with Background JavaScript Tasks",
      "Abstract": "Website visitors are performing increasingly complex computational work on the websites' behalf, such as validating forms, rendering animations, and producing data visualizations. In this article, we explore the possibility of increasing the work offloaded to web visitors' browsers. The idle computing cycles of web visitors can be turned into a large-scale distributed data processing engine, which we term gray computing. Past research has looked primarily at either volunteer computing with specialized clients or browser-based volunteer computing where the visitors keep their browsers open to a single web page for a long period of time. This article provides a comprehensive analysis of the architecture, performance, security, cost effectiveness, user experience, and other issues of gray computing distributed data processing engines with heterogeneous computing power, non-uniform page view times, and high computing pool volatility. Several real-world applications are examined and gray computing is shown to be cost effective for a number of complex tasks ranging from computer vision to bioinformatics to cryptology.",
      "Keywords": "cloud computing | JavaScript | Software economics | web browser",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-02-01",
      "Publication type": "Article",
      "Authors": "Pan, Yao;White, Jules;Sun, Yu;Gray, Jeff",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85034218670",
      "Primary study DOI": "10.1109/TSE.2017.2770124",
      "Title": "A Systematic Literature Review and Meta-Analysis on Cross Project Defect Prediction",
      "Abstract": "Background: Cross project defect prediction (CPDP) recently gained considerable attention, yet there are no systematic efforts to analyse existing empirical evidence. Objective: To synthesise literature to understand the state-of-the-art in CPDP with respect to metrics, models, data approaches, datasets and associated performances. Further, we aim to assess the performance of CPDP versus within project DP models. Method: We conducted a systematic literature review. Results from primary studies are synthesised (thematic, meta-analysis) to answer research questions. Results: We identified 30 primary studies passing quality assessment. Performance measures, except precision, vary with the choice of metrics. Recall, precision, f-measure, and AUC are the most common measures. Models based on Nearest-Neighbour and Decision Tree tend to perform well in CPDP, whereas the popular naïve Bayes yields average performance. Performance of ensembles varies greatly across f-measure and AUC. Data approaches address CPDP challenges using row/column processing, which improve CPDP in terms of recall at the cost of precision. This is observed in multiple occasions including the meta-analysis of CPDP versus WPDP. NASA and Jureczko datasets seem to favour CPDP over WPDP more frequently. Conclusion: CPDP is still a challenge and requires more research before trustworthy applications can take place. We provide guidelines for further research.",
      "Keywords": "cross project | Defect prediction | fault prediction | meta-analysis | systematic literature review | within project",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-02-01",
      "Publication type": "Review",
      "Authors": "Hosseini, Seyedrebvar;Turhan, Burak;Gunarathna, Dimuthu",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040309554",
      "Primary study DOI": "10.1109/TSE.2017.2653778",
      "Title": "Language Inclusion Checking of Timed Automata with Non-Zenoness",
      "Abstract": "Given a timed automaton P modeling an implementation and a timed automaton S as a specification, the problem of language inclusion checking is to decide whether the language of P is a subset of that of S. It is known to be undecidable. The problem gets more complicated if non-Zenoness is taken into consideration. A run is Zeno if it permits infinitely many actions within finite time. Otherwise it is non-Zeno. Zeno runs might present in both P and S. It is necessary to check whether a run is Zeno or not so as to avoid presenting Zeno runs as counterexamples of language inclusion checking. In this work, we propose a zone-based semi-Algorithm for language inclusion checking with non-Zenoness. It is further improved with simulation reduction based on LU-simulation. Though our approach is not guaranteed to terminate, we show that it does in many cases through empirical study. Our approach has been incorporated into the PAT model checker, and applied to multiple systems to show its usefulness.",
      "Keywords": "language inclusion | non-Zenoness | Timed automata",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Wang, Xinyu;Sun, Jun;Wang, Ting;Qin, Shengchao",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029188984",
      "Primary study DOI": "10.1109/TSE.2017.2748129",
      "Title": "Comments on ScottKnottESD in response to an empirical comparison of model validation techniques for defect prediction models",
      "Abstract": "In this article, we discuss the ScottKnottESD test, which was proposed in a recent paper 'An Empirical Comparison of Model Validation Techniques for Defect Prediction Models' that was published in this journal. We discuss the implications and the empirical impact of the proposed normality correction of ScottKnottESD and come to the conclusion that this correction does not necessarily lead to the fulfillment of the assumptions of the original Scott-Knott test and may cause problems with the statistical analysis.",
      "Keywords": "log transformation | Scott-knott test | statistics",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Herbold, Steffen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85038569971",
      "Primary study DOI": "10.1109/TSE.2017.2652457",
      "Title": "Testing from Partial Finite State Machines without Harmonised Traces",
      "Abstract": "This paper concerns the problem of testing from a partial, possibly non-deterministic, finite state machine (FSM) S. Two notions of correctness (quasi-reduction and quasi-equivalence) have previously been defined for partial FSMs but these, and the corresponding test generation techniques, only apply to FSMs that have harmonised traces. We show how quasi-reduction and quasi-equivalence can be generalised to all partial FSMs. We also consider the problem of generating an m -complete test suite from a partial FSM S : a test suite that is guaranteed to determine correctness as long as the system under test has no more than m states. We prove that we can complete S to form a completely-specified non-deterministic FSM S such that any m -complete test suite generated from S can be converted into an m -complete test suite for S. We also show that there is a correspondence between test suites that are reduced for S and S and also that are minimal for S and S.",
      "Keywords": "checking experiment | partial finite state machine | Software engineering/software/program verification | software engineering/testing and debugging | systems and software",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Hierons, Robert Mark",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85032750697",
      "Primary study DOI": "10.1109/TSE.2017.2768368",
      "Title": "What Makes a Great Manager of Software Engineers?",
      "Abstract": "Having great managers is as critical to success as having a good team or organization. In general, a great manager is seen as fuelling the team they manage, enabling it to use its full potential. Though software engineering research studies factors that may affect the performance and productivity of software engineers and teams (like tools and skills), it has overlooked the software engineering manager. The software industry's growth and change in the last decades is creating a need for a domain-specific view of management. On the one hand, experts are questioning how the abundant work in management applies to software engineering. On the other hand, practitioners are looking to researchers for evidence-based guidance on how to manage software teams. We conducted a mixed methods empirical study of software engineering management at Microsoft to investigate what manager attributes developers and engineering managers perceive important and why. We present a conceptual framework of manager attributes, and find that technical skills are not the sign of greatness for an engineering manager. Through statistical analysis we identify how engineers and managers relate in their views, and how software engineering differs from other knowledge work groups in its perceptions about what makes great managers. We present strategies for putting the attributes to use, discuss implications for research and practice, and offer avenues for further work.",
      "Keywords": "empirical studies | software companies | software engineering management",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-01-01",
      "Publication type": "Article",
      "Authors": "Kalliamvakou, Eirini;Bird, Christian;Zimmermann, Thomas;Begel, Andrew;Deline, Robert;German, Daniel M.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85025122019",
      "Primary study DOI": "10.1109/TSE.2017.2654244",
      "Title": "Using Natural Language Processing to Automatically Detect Self-Admitted Technical Debt",
      "Abstract": "The metaphor of technical debt was introduced to express the trade off between productivity and quality, i.e., when developers take shortcuts or perform quick hacks. More recently, our work has shown that it is possible to detect technical debt using source code comments (i.e., self-Admitted technical debt), and that the most common types of self-Admitted technical debt are design and requirement debt. However, all approaches thus far heavily depend on the manual classification of source code comments. In this paper, we present an approach to automatically identify design and requirement self-Admitted technical debt using Natural Language Processing (NLP). We study 10 open source projects: Ant, ArgoUML, Columba, EMF, Hibernate, JEdit, JFreeChart, JMeter, JRuby and SQuirrel SQL and find that 1) we are able to accurately identify self-Admitted technical debt, significantly outperforming the current state-of-The-Art based on fixed keywords and phrases; 2) words related to sloppy code or mediocre source code quality are the best indicators of design debt, whereas words related to the need to complete a partially implemented requirement in the future are the best indicators of requirement debt; and 3) we can achieve 90 percent of the best classification performance, using as little as 23 percent of the comments for both design and requirement self-Admitted technical debt, and 80 percent of the best performance, using as little as 9 and 5 percent of the comments for design and requirement self-Admitted technical debt, respectively. The last finding shows that the proposed approach can achieve a good accuracy even with a relatively small training dataset.",
      "Keywords": "empirical study | natural language processing | source code comments | Technical debt",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Maldonado, Everton Da Silva;Shihab, Emad;Tsantalis, Nikolaos",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040307811",
      "Primary study DOI": "10.1109/TSE.2017.2653105",
      "Title": "When and Why Your Code Starts to Smell Bad (and Whether the Smells Go Away)",
      "Abstract": "Technical debt is a metaphor introduced by Cunningham to indicate 'not quite right code which we postpone making it right'. One noticeable symptom of technical debt is represented by code smells, defined as symptoms of poor design and implementation choices. Previous studies showed the negative impact of code smells on the comprehensibility and maintainability of code. While the repercussions of smells on code quality have been empirically assessed, there is still only anecdotal evidence on when and why bad smells are introduced, what is their survivability, and how they are removed by developers. To empirically corroborate such anecdotal evidence, we conducted a large empirical study over the change history of 200 open source projects. This study required the development of a strategy to identify smell-introducing commits, the mining of over half a million of commits, and the manual analysis and classification of over 10K of them. Our findings mostly contradict common wisdom, showing that most of the smell instances are introduced when an artifact is created and not as a result of its evolution. At the same time, 80 percent of smells survive in the system. Also, among the 20 percent of removed instances, only 9 percent are removed as a direct consequence of refactoring operations.",
      "Keywords": "Code smells | empirical study | mining software repositories",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Tufano, Michele;Palomba, Fabio;Bavota, Gabriele;Oliveto, Rocco;Penta, Massimiliano Di;De Lucia, Andrea;Poshyvanyk, Denys",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030752279",
      "Primary study DOI": "10.1109/TSE.2017.2755651",
      "Title": "Clarifications on the construction and use of the manybugs benchmark",
      "Abstract": "High-quality research requires timely dissemination and the incorporation of feedback. Since the publication of the ManyBugs benchmark and its release on http://repairbenchmarks.cs.umass.edu/, researchers have provided feedback on the benchmark's construction and use. Here, we describe that feedback and our subsequent improvements to the ManyBugs benchmark.",
      "Keywords": "",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Le Goues, Claire;Brun, Yuriy;Forrest, Stephanie;Weimer, Westley",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85032443846",
      "Primary study DOI": "10.1109/TSE.2017.2765640",
      "Title": "A Rigorous Framework for Specification, Analysis and Enforcement of Access Control Policies",
      "Abstract": "Access control systems are widely used means for the protection of computing systems. They are defined in terms of access control policies regulating the access to system resources. In this paper, we introduce a formally-defined, fully-implemented framework for specification, analysis and enforcement of attribute-based access control policies. The framework rests on FACPL, a language with a compact, yet expressive, syntax for specification of real-world access control policies and with a rigorously defined denotational semantics. The framework enables the automated verification of properties regarding both the authorisations enforced by single policies and the relationships among multiple policies. Effectiveness and performance of the analysis rely on a semantic-preserving representation of FACPL policies in terms of SMT formulae and on the use of efficient SMT solvers. Our analysis approach explicitly addresses some crucial aspects of policy evaluation, such as missing attributes, erroneous values and obligations, which are instead overlooked in other proposals. The framework is supported by Java-based tools, among which an Eclipse-based IDE offering a tailored development and analysis environment for FACPL policies and a Java library for policy enforcement. We illustrate the framework and its formal ingredients by means of an e-Health case study, while its effectiveness is assessed by means of performance stress tests and experiments on a well-established benchmark.",
      "Keywords": "Attribute-based access control | policy analysis | policy languages | SMT",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-01-01",
      "Publication type": "Article",
      "Authors": "Margheri, Andrea;Masi, Massimiliano;Pugliese, Rosario;Tiezzi, Francesco",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85032444751",
      "Primary study DOI": "10.1109/TSE.2017.2766070",
      "Title": "Entropy Based Software Reliability Analysis of Multi-Version Open Source Software",
      "Abstract": "The number of issues fixed in the current release of the software is one of the factors which decides the next release of the software. The source code files get changed during fixing of these issues. The uncertainty arises due to these changes is quantified using entropy based measures. We developed a Non-Homogeneous Poisson Process model for Open Source Software to understand the fixing of issues across releases. Based on this model, optimal release-updating using entropy and maximizing the active user's satisfaction level subject to fixing of issues up to a desired level, is investigated as well. The proposed models have been validated on five products of the Apache open source project. The optimal release time estimated from the proposed model is close to the observed release time at different active user's satisfaction levels. The proposed decision model can assist management to appropriately determine the optimal release-update time. The proposed entropy based model for issues estimation shows improvement in performance for 21 releases out of total 23 releases, when compared with well-known traditional software reliability growth models, namely GO model [1] and S-shaped model [2]. The proposed model is also found statistically significant.",
      "Keywords": "cobb-douglas | Entropy | feature improvement | new feature | release time problem | software repositories",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-12-01",
      "Publication type": "Article",
      "Authors": "Singh, V. B.;Sharma, Meera;Pham, Hoang",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85032230617",
      "Primary study DOI": "10.1109/TSE.2017.2764464",
      "Title": "Metamorphic Testing of RESTful Web APIs",
      "Abstract": "Web Application Programming Interfaces (APIs) allow systems to interact with each other over the network. Modern Web APIs often adhere to the REST architectural style, being referred to as RESTful Web APIs. RESTful Web APIs are decomposed into multiple resources (e.g., a video in the YouTube API) that clients can manipulate through HTTP interactions. Testing Web APIs is critical but challenging due to the difficulty to assess the correctness of API responses, i.e., the oracle problem. Metamorphic testing alleviates the oracle problem by exploiting relations (so-called metamorphic relations) among multiple executions of the program under test. In this paper, we present a metamorphic testing approach for the detection of faults in RESTful Web APIs. We first propose six abstract relations that capture the shape of many of the metamorphic relations found in RESTful Web APIs, we call these Metamorphic Relation Output Patterns (MROPs). Each MROP can then be instantiated into one or more concrete metamorphic relations. The approach was evaluated using both automatically seeded and real faults in six subject Web APIs. Among other results, we identified 60 metamorphic relations (instances of the proposed MROPs) in the Web APIs of Spotify and YouTube. Each metamorphic relation was implemented using both random and manual test data, running over 4.7K automated tests. As a result, 11 issues were detected (3 in Spotify and 8 in YouTube), 10 of them confirmed by the API developers or reproduced by other users, supporting the effectiveness of the approach.",
      "Keywords": "Metamorphic testing | REST | RESTful Web services | Web API",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-11-01",
      "Publication type": "Article",
      "Authors": "Segura, Sergio;Parejo, Jose A.;Troya, Javier;Ruiz-Cortes, Antonio",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85031778888",
      "Primary study DOI": "10.1109/TSE.2017.2759112",
      "Title": "Listening to the Crowd for the Release Planning of Mobile Apps",
      "Abstract": "The market for mobile apps is getting bigger and bigger, and it is expected to be worth over 100 Billion dollars in 2020. To have a chance to succeed in such a competitive environment, developers need to build and maintain high-quality apps, continuously astonishing their users with the coolest new features. Mobile app marketplaces allow users to release reviews. Despite reviews are aimed at recommending apps among users, they also contain precious information for developers, reporting bugs and suggesting new features. To exploit such a source of information, developers are supposed to manually read user reviews, something not doable when hundreds of them are collected per day. To help developers dealing with such a task, we developed CLAP (Crowd Listener for releAse Planning), a web application able to (i) categorize user reviews based on the information they carry out, (ii) cluster together related reviews, and (iii) prioritize the clusters of reviews to be implemented when planning the subsequent app release. We evaluated all the steps behind CLAP, showing its high accuracy in categorizing and clustering reviews and the meaningfulness of the recommended prioritizations. Also, given the availability of CLAP as a working tool, we assessed its applicability in industrial environments.",
      "Keywords": "mining software repositories | mobile apps | Release planning",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-01-01",
      "Publication type": "Article",
      "Authors": "Scalabrino, Simone;Bavota, Gabriele;Russo, Barbara;Penta, Massimiliano Di;Oliveto, Rocco",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85037053699",
      "Primary study DOI": "10.1109/TSE.2016.2642956",
      "Title": "Software Numerical Instability Detection and Diagnosis by Combining Stochastic and Infinite-Precision Testing",
      "Abstract": "Numerical instability is a well-known problem that may cause serious runtime failures. This paper discusses the reason of instability in software development process, and presents a toolchain that not only detects the potential instability in software, but also diagnoses the reason for such instability. We classify the reason of instability into two categories. When it is introduced by software requirements, we call the instability caused by problem. In this case, it cannot be avoided by improving software development, but requires inspecting the requirements, especially the underlying mathematical properties. Otherwise, we call the instability caused by practice. We design our toolchain as four loosely-coupled tools, which combine stochastic arithmetic with infinite-precision testing. Each tool in our toolchain can be configured with different strategies according to the properties of the analyzed software. We evaluate our toolchain on subjects from literature. The results show that it effectively detects and separates the instabilities caused by problems from others. We also conduct an evaluation on the latest version of GNU Scientific Library, and the toolchain finds a few real bugs in the well-maintained and widely deployed numerical library. With the help of our toolchain, we report the details and fixing advices to the GSL buglist.",
      "Keywords": "infinite-precision arithmetic | Numerical analysis | software testing | stochastic arithmetic",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Tang, Enyi;Zhang, Xiangyu;Muller, Norbert Th;Chen, Zhenyu;Li, Xuandong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85037050434",
      "Primary study DOI": "10.1109/TSE.2016.2645687",
      "Title": "Deriving Bisimulation Relations from Path Extension Based Equivalence Checkers",
      "Abstract": "Constructing bisimulation relations between programs as a means of translation validation has been an active field of study. The problem is in general undecidable. Currently available mechanisms suffer from drawbacks such as non-termination and significant restrictions on the structures of programs to be checked. We have developed a path extension based equivalence checking method as an alternative translation validation technique to alleviate these drawbacks. In this work, path extension based equivalence checking of programs (flowcharts) is leveraged to establish a bisimulation relation between a program and its translated version by constructing the relation from the outputs of the equivalence checker.",
      "Keywords": "bisimulation relation | equivalence checking | path extension based equivalence checker | Translation validation",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Banerjee, Kunal;Sarkar, Dipankar;Mandal, Chittaranjan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029857527",
      "Primary study DOI": "10.1109/TSE.2016.2645572",
      "Title": "Identifying Extract Method Refactoring Opportunities Based on Functional Relevance",
      "Abstract": "'Extract Method' is considered one of the most frequently applied and beneficial refactorings, since the corresponding Long Method smell is among the most common and persistent ones. Although Long Method is conceptually related to the implementation of diverse functionalities within a method, until now, this relationship has not been utilized while identifying refactoring opportunities. In this paper we introduce an approach (accompanied by a tool) that aims at identifying source code chunks that collaborate to provide a specific functionality, and propose their extraction as separate methods. The accuracy of the proposed approach has been empirically validated both in an industrial and an open-source setting. In the former case, the approach was capable of identifying functionally related statements within two industrial long methods (approx. 500 LoC each), with a recall rate of 93 percent. In the latter case, based on a comparative study on open-source data, our approach ranks better compared to two well-known techniques of the literature. To assist software engineers in the prioritization of the suggested refactoring opportunities the approach ranks them based on an estimate of their fitness for extraction. The provided ranking has been validated in both settings and proved to be strongly correlated with experts' opinion.",
      "Keywords": "Design tools and techniques | metrics/measurement | object-oriented programming",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Charalampidou, Sofia;Ampatzoglou, Apostolos;Chatzigeorgiou, Alexander;Gkortzis, Antonios;Avgeriou, Paris",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85037034683",
      "Primary study DOI": "10.1109/TSE.2016.2635134",
      "Title": "Automated Extraction and Clustering of Requirements Glossary Terms",
      "Abstract": "A glossary is an important part of any software requirements document. By making explicit the technical terms in a domain and providing definitions for them, a glossary helps mitigate imprecision and ambiguity. A key step in building a glossary is to decide upon the terms to include in the glossary and to find any related terms. Doing so manually is laborious, particularly for large requirements documents. In this article, we develop an automated approach for extracting candidate glossary terms and their related terms from natural language requirements documents. Our approach differs from existing work on term extraction mainly in that it clusters the extracted terms by relevance, instead of providing a flat list of terms. We provide an automated, mathematically-based procedure for selecting the number of clusters. This procedure makes the underlying clustering algorithm transparent to users, thus alleviating the need for any user-specified parameters. To evaluate our approach, we report on three industrial case studies, as part of which we also examine the perceptions of the involved subject matter experts about the usefulness of our approach. Our evaluation notably suggests that: (1) Over requirements documents, our approach is more accurate than major generic term extraction tools. Specifically, in our case studies, our approach leads to gains of 20 percent or more in terms of recall when compared to existing tools, while at the same time either improving precision or leaving it virtually unchanged. And, (2) the experts involved in our case studies find the clusters generated by our approach useful as an aid for glossary construction.",
      "Keywords": "case study research | clustering | natural language processing | Requirements glossaries | term extraction",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Arora, Chetan;Sabetzadeh, Mehrdad;Briand, Lionel;Zimmer, Frank",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85031317356",
      "Primary study DOI": "10.1109/TSE.2017.2650914",
      "Title": "Adaptive multi-objective evolutionary algorithms for overtime planning in software projects",
      "Abstract": "Software engineering and development is well-known to suffer from unplanned overtime, which causes stress and illness in engineers and can lead to poor quality software with higher defects. Recently, we introduced a multi-objective decision support approach to help balance project risks and duration against overtime, so that software engineers can better plan overtime. This approach was empirically evaluated on six real world software projects and compared against state-of-the-art evolutionary approaches and currently used overtime strategies. The results showed that our proposal comfortably outperformed all the benchmarks considered. This paper extends our previous work by investigating adaptive multi-objective approaches to meta-heuristic operator selection, thereby extending and (as the results show) improving algorithmic performance. We also extended our empirical study to include two new real world software projects, thereby enhancing the scientific evidence for the technical performance claims made in the paper. Our new results, over all eight projects studied, showed that our adaptive algorithm outperforms the considered state of the art multi-objective approaches in 93 percent of the experiments (with large effect size). The results also confirm that our approach significantly outperforms current overtime planning practices in 100 percent of the experiments (with large effect size).",
      "Keywords": "Hyperheuristic | Management | Multi-objective evolutionary algorithms | NSGAII | Overtime | Planning | Project scheduling | Search-based software engineering | Software engineering",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Sarro, Federica;Ferrucci, Filomena;Harman, Mark;Manna, Alessandra;Ren, Jian",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030719585",
      "Primary study DOI": "10.1109/TSE.2017.2757480",
      "Title": "On the Use of Hidden Markov Model to Predict the Time to Fix Bugs",
      "Abstract": "A significant amount of time is spent by software developers in investigating bug reports. It is useful to indicate when a bug report will be closed, since it would help software teams to prioritise their work. Several studies have been conducted to address this problem in the past decade. Most of these studies have used the frequency of occurrence of certain developer activities as input attributes in building their prediction models. However, these approaches tend to ignore the temporal nature of the occurrence of these activities. In this paper, a novel approach using Hidden Markov Models and temporal sequences of developer activities is proposed. The approach is empirically demonstrated in a case study using eight years of bug reports collected from the Firefox project. Our proposed model correctly identifies bug reports with expected bug fix times. We also compared our proposed approach with the state of the art technique in the literature in the context of our case study. Our approach results in approximately 33 percent higher F-measure than the contemporary technique based on the Firefox project data.",
      "Keywords": "Bug repositories | hidden markov model | temporal activities | time to fix a bug",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-12-01",
      "Publication type": "Article",
      "Authors": "Habayeb, Mayy;Murtaza, Syed Shariyar;Miranskyy, Andriy;Bener, Ayse Basar",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030702184",
      "Primary study DOI": "10.1109/TSE.2017.2757486",
      "Title": "EARMO: An Energy-Aware Refactoring Approach for Mobile Apps",
      "Abstract": "The energy consumption of mobile apps is a trending topic and researchers are actively investigating the role of coding practices on energy consumption. Recent studies suggest that design choices can conflict with energy consumption. Therefore, it is important to take into account energy consumption when evolving the design of a mobile app. In this paper, we analyze the impact of eight type of anti-patterns on a testbed of 20 android apps extracted from F-Droid. We propose EARMO, a novel anti-pattern correction approach that accounts for energy consumption when refactoring mobile anti-patterns. We evaluate EARMO using three multiobjective search-based algorithms. The obtained results show that EARMO can generate refactoring recommendations in less than a minute, and remove a median of 84 percent of anti-patterns. Moreover, EARMO extended the battery life of a mobile phone by up to 29 minutes when running in isolation a refactored multimedia app with default settings (no Wi-Fi, no location services, and minimum screen brightness). Finally, we conducted a qualitative study with developers of our studied apps, to assess the refactoring recommendations made by EARMO. Developers found 68 percent of refactorings suggested by EARMO to be very relevant.",
      "Keywords": "anti-patterns | energy consumption | mobile apps | refactoring | search-based software engineering | Software maintenance",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-12-01",
      "Publication type": "Article",
      "Authors": "Morales, Rodrigo;Saborido, Ruben;Khomh, Foutse;Chicano, Francisco;Antoniol, Giuliano",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030707914",
      "Primary study DOI": "10.1109/TSE.2017.2756043",
      "Title": "A Study of Social Interactions in Open Source Component Use",
      "Abstract": "All kinds of software projects, whether open or closed source, rely on open source components. Repositories that serve open source components to organizations, such as the Central Repository and npmjs.org, report billions of requests per year. Despite the widespread reliance of projects on open source components, little is known about the social interactions that occur between developers of a project using a component and developers of the component itself. In this paper, we investigate the social interactions that occur for 5,133 pairs of projects, from two different communities (Java and Ruby) representing user projects that depend on a component project. We consider such questions as how often are there social interactions when a component is used? When do the social interactions occur? And, why do social interactions occur? From our investigation, we observed that social interactions typically occur after a component has been chosen for use and relied upon. When social interactions occur, they most frequently begin with creating issues or feature requests. We also found that the more use a component receives, the less likely it is that developers of project using the component will interact with the component project, and when those interactions occur, they will be shorter in duration. Our results provide insight into how socio-technical interactions occur beyond the level of an individual or small group of projects previously studied by others and identify the need for a new model of socio-technical congruence for dependencies between, instead of within, projects.",
      "Keywords": "collaboration | OSS components | social interactions | Software reuse",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-12-01",
      "Publication type": "Article",
      "Authors": "Palyart, Marc;Murphy, Gail C.;Masrani, Vaden",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030719563",
      "Primary study DOI": "10.1109/TSE.2017.2756048",
      "Title": "Tracking Load-time Configuration Options",
      "Abstract": "Many software systems are highly configurable, despite the fact that configuration options and their interactions make those systems significantly harder to understand and maintain. In this work, we consider load-time configuration options, such as parameters from the command-line or from configuration files. They are particularly hard to reason about: tracking configuration options from the point at which they are loaded to the point at which they influence control-flow decisions is tedious and error-prone, if done manually. We design and implement Lotrack, an extended static taint analysis to track configuration options automatically. Lotrack derives a configuration map that explains for each code fragment under which configurations it may be executed. An evaluation on Android apps and Java applications from different domains shows that Lotrack yields high accuracy with reasonable performance. We use Lotrack to empirically characterize how much of the implementation of Android apps depends on the platform's configuration options or interactions of these options.",
      "Keywords": "configuration options | static analysis | Variability mining",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-12-01",
      "Publication type": "Article",
      "Authors": "Lillack, Max;Kastner, Christian;Bodden, Eric",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030640196",
      "Primary study DOI": "10.1109/TSE.2017.2755005",
      "Title": "Revisiting the Performance Evaluation of Automated Approaches for the Retrieval of Duplicate Issue Reports",
      "Abstract": "Issue tracking systems (ITSs), such as Bugzilla, are commonly used to track reported bugs, improvements and change requests for a software project. To avoid wasting developer resources on previously-reported (i.e., duplicate) issues, it is necessary to identify such duplicates as soon as they are reported. Several automated approaches have been proposed for retrieving duplicate reports, i.e., identifying the duplicate of a new issue report in a list of n candidates. These approaches rely on leveraging the textual, categorical, and contextual information in previously-reported issues to decide whether a newly-reported issue has previously been reported. In general, these approaches are evaluated using data that spans a relatively short period of time (i.e., the classical evaluation). However, in this paper, we show that the classical evaluation tends to overestimate the performance of automated approaches for retrieving duplicate issue reports. Instead, we propose a realistic evaluation using all the reports that are available in the ITS of a software project. We conduct experiments in which we evaluate two popular approaches for retrieving duplicate issues (BM25F and REP) using the classical and realistic evaluations. We find that for the issue tracking data of the Mozilla foundation, the Eclipse foundation and OpenOffice, the realistic evaluation shows that previously proposed approaches perform considerably lower than previously reported using the classical evaluation. As a result, we conclude that the reported performance of approaches for retrieving duplicate issue reports is significantly overestimated in literature. In order to improve the performance of the automated retrieval of duplicate issue reports, we propose to leverage the resolution field of issue reports. Our experiments show that a relative improvement in the performance of a median of 7-21.5 percent and a maximum of 19-60 percent can be achieved by leveraging the resolution field of issue reports for the automated retrieval of duplicates.",
      "Keywords": "performance evaluation | software engineering | Text analysis",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-12-01",
      "Publication type": "Article",
      "Authors": "Rakha, Mohamed Sami;Bezemer, Cor Paul;Hassan, Ahmed E.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030632465",
      "Primary study DOI": "10.1109/TSE.2017.2755039",
      "Title": "Collaborative Model-Driven Software Engineering: a Classification Framework and a Research Map",
      "Abstract": "Context: Collaborative Model-Driven Software Engineering (MDSE) consists of methods and techniques where multiple stakeholders manage, collaborate, and are aware of each others' work on shared models. Objective: Collaborative MDSE is attracting research efforts from different areas, resulting in a variegated scientific body of knowledge. This study aims at identifying, classifying, and understanding existing collaborative MDSE approaches. Method: We designed and conducted a systematic mapping study. Starting from over 3,000 potentially relevant studies, we applied a rigorous selection procedure resulting in 106 selected papers, further clustered into 48 primary studies along a time span of 19 years. We rigorously defined and applied a classification framework and extracted key information from each selected study for subsequent analysis. Results: Our analysis revealed the following main fidings: (i) there is a growing scientific interest on collaborative MDSE in the last years; (ii) multi-view modeling, validation support, reuse, and branching are more rarely covered with respect to other aspects about collaborative MDSE; (iii) different primary studies focus differently on individual dimensions of collaborative MDSE (i.e., model management, collaboration, and communication); (iv) most approaches are language-specific, with a prominence of UML-based approaches; (v) few approaches support the interplay between synchronous and asynchronous collaboration. Conclusion: This study gives a solid foundation for classifying existing and future approaches for collaborative MDSE. Researchers and practitioners can use our results for identifying existing research/technical gaps to attack, better scoping their own contributions, or understanding existing ones.",
      "Keywords": "C-MDSE | Collaborative MDSE | collaborative software engineering | CoMDSE | CoSE | model-driven engineering | systematic mapping study",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-12-01",
      "Publication type": "Article",
      "Authors": "Franzago, Mirco;Ruscio, Davide Di;Malavolta, Ivano;MucCini, Henry",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030636707",
      "Primary study DOI": "10.1109/TSE.2017.2754374",
      "Title": "Data Scientists in Software Teams: State of the Art and Challenges",
      "Abstract": "The demand for analyzing large scale telemetry, machine, and quality data is rapidly increasing in software industry. Data scientists are becoming popular within software teams, e.g., Facebook, LinkedIn and Microsoft are creating a new career path for data scientists. In this paper, we present a large-scale survey with 793 professional data scientists at Microsoft to understand their educational background, problem topics that they work on, tool usages, and activities. We cluster these data scientists based on the time spent for various activities and identify 9 distinct clusters of data scientists, and their corresponding characteristics. We also discuss the challenges that they face and the best practices they share with other data scientists. Our study finds several trends about data scientists in the software engineering context at Microsoft, and should inform managers on how to leverage data science capability effectively within their teams.",
      "Keywords": "Data science | Development roles | Industry | Software engineering",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-11-01",
      "Publication type": "Article",
      "Authors": "Kim, Miryung;Zimmermann, Thomas;Deline, Robert;Begel, Andrew",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030233148",
      "Primary study DOI": "10.1109/TSE.2017.2752158",
      "Title": "Using Local Clocks to Reproduce Concurrency Bugs",
      "Abstract": "Multi-threaded programs play an increasingly important role in current multi-core environments. Exposing concurrency bugs and debugging such multi-threaded programs are quite challenging due to their inherent non-determinism. In order to mitigate such non-determinism, many approaches such as record-and-replay have been proposed. However, those approaches often suffer significant performance degradation because they require a large amount of recorded information and/or long analysis and replay time. In this paper, we propose an efficient and effective approach, ReCBuLC (reproducing concurrency bugs using local clocks), to take advantage of the hardware clocks available on modern processors. The key idea is to reduce the recording overhead and the time to analyze events' global order by recording timestamps in each thread. These timestamps are used to determine the global order of shared accesses. To avoid the large overhead in accessing system-wide global clock, we opt to use local per-core clocks that incur much less access overhead.We then propose techniques to resolve skews among local clocks and obtain an accurate global event order. By using per-core clocks, state-of-the-art bug reproducing systems such as PRES and CLAP can reduce their recording overheads by up to 85 percent, and the analysis time up to 84.66% 99.99%, respectively.",
      "Keywords": "Bug reproducing | Concurrency | Local clock",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-09-13",
      "Publication type": "Article",
      "Authors": "Wang, Zhe;Wu, Chenggang;Yuan, Xiang;Wang, Zhenjiang;Li, Jianjun;Yew, Pen Chung;Huang, Jeff;Feng, Xiaobing;Lan, Yanyan;Chen, Yunji;Lai, Yuanming;Guan, Yong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030554017",
      "Primary study DOI": "10.1109/TSE.2016.2638427",
      "Title": "Reporting Usability Defects: A Systematic Literature Review",
      "Abstract": "Usability defects can be found either by formal usability evaluation methods or indirectly during system testing or usage. No matter how they are discovered, these defects must be tracked and reported. However, empirical studies indicate that usability defects are often not clearly and fully described. This study aims to identify the state of the art in reporting of usability defects in the software engineering and usability engineering literature. We conducted a systematic literature review of usability defect reporting drawing from both the usability and software engineering literature from January 2000 until March 2016. As a result, a total of 57 studies were identified, in which we classified the studies into three categories: reporting usability defect information, analysing usability defect data and key challenges. Out of these, 20 were software engineering studies and 37 were usability studies. The results of this systematic literature review show that usability defect reporting processes suffer from a number of limitations, including: mixed data, inconsistency of terms and values of usability defect data, and insufficient attributes to classify usability defects. We make a number of recommendations to improve usability defect reporting and management in software engineering.",
      "Keywords": "Systematic review | test management | usability defect reporting | usability testing | user interface",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Yusop, Nor Shahida Mohamad;Grundy, John;Vasa, Rajesh",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030529656",
      "Primary study DOI": "10.1109/TSE.2016.2635137",
      "Title": "Static Analysis of Model Transformations",
      "Abstract": "Model transformations are central to Model-Driven Engineering (MDE), where they are used to transform models between different languages; to refactor and simulate models; or to generate code from models. Thus, given their prominent role in MDE, practical methods helping in detecting errors in transformations and automate their verification are needed. In this paper, we present a method for the static analysis of ATL model transformations. The method aims at discovering typing and rule errors, like unresolved bindings, uninitialized features or rule conflicts. It relies on static analysis and type inference, and uses constraint solving to assert whether a source model triggering the execution of a given problematic statement can possibly exist. Our method is supported by a tool that integrates seamlessly with the ATL development environment. To evaluate the usefulness of our method, we have used it to analyse a public repository of ATL transformations. The high number of errors discovered shows that static analysis of ATL transformations is needed in practice. Moreover, we have measured the precision and recall of the method by considering a synthetic set of transformations obtained by mutation techniques, and comparing with random testing. The experiment shows good overall results in terms of false positives and negatives.",
      "Keywords": "ATL | model finders | model transformation | Model-driven engineering | static analysis | verification and testing",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Cuadrado, Jesus Sanchez;Guerra, Esther;De Lara, Juan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030555057",
      "Primary study DOI": "10.1109/TSE.2016.2633992",
      "Title": "A Qualitative Study of Application-Level Caching",
      "Abstract": "Latency and cost of Internet-based services are encouraging the use of application-level caching to continue satisfying users' demands, and improve the scalability and availability of origin servers. Despite its popularity, this level of caching involves the manual implementation by developers and is typically addressed in an ad-hoc way, given that it depends on specific details of the application. As a result, application-level caching is a time-consuming and error-prone task, becoming a common source of bugs. Furthermore, it forces application developers to reason about a crosscutting concern, which is unrelated to the application business logic. In this paper, we present the results of a qualitative study of how developers handle caching logic in their web applications, which involved the investigation of ten software projects with different characteristics. The study we designed is based on comparative and interactive principles of grounded theory, and the analysis of our data allowed us to extract and understand how developers address cache-related concerns to improve performance and scalability of their web applications. Based on our analysis, we derived guidelines and patterns, which guide developers while designing, implementing and maintaining application-level caching, thus supporting developers in this challenging task that is crucial for enterprise web applications.",
      "Keywords": "Application-level caching | guideline | pattern | qualitative study | web application",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Mertz, Jhonny;Nunes, Ingrid",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018434065",
      "Primary study DOI": "10.1109/TSE.2016.2630689",
      "Title": "A survey of app store analysis for software engineering",
      "Abstract": "App Store Analysis studies information about applications obtained from app stores. App stores provide a wealth of information derived from users that would not exist had the applications been distributed via previous software deployment methods. App Store Analysis combines this non-technical information with technical information to learn trends and behaviours within these forms of software repositories. Findings from App Store Analysis have a direct and actionable impact on the software teams that develop software for app stores, and have led to techniques for requirements engineering, release planning, software design, security and testing. This survey describes and compares the areas of research that have been explored thus far, drawing out common aspects, trends and directions future research should take to address open problems and challenges.",
      "Keywords": "Analysis | API | App Store | Ecosystem | Feature | Mining | Release planning | Requirements engineering | Reviews | Security",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-09-01",
      "Publication type": "Review",
      "Authors": "Martin, William;Sarro, Federica;Jia, Yue;Zhang, Yuanyuan;Harman, Mark",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029172869",
      "Primary study DOI": "10.1109/TSE.2017.2748134",
      "Title": "Predicting Future Developer Behavior in the IDE Using Topic Models",
      "Abstract": "While early software command recommender systems drew negative user reaction, recent studies show that users of unusually complex applications will accept and utilize command recommendations. Given this new interest, more than a decade after first attempts, both the recommendation generation (backend) and the user experience (frontend) should be revisited. In this work, we focus on recommendation generation. One shortcoming of existing command recommenders is that algorithms focus primarily on mirroring the short-term past,-i.e., assuming that a developer who is currently debugging will continue to debug endlessly. We propose an approach to improve on the state of the art by modeling future task context to make better recommendations to developers. That is, the approach can predict that a developer who is currently debugging may continue to debug OR may edit their program. To predict future development commands, we applied Temporal Latent Dirichlet Allocation, a topic model used primarily for natural language, to software development interaction data (i.e., command streams). We evaluated this approach on two large interaction datasets for two different IDEs, Microsoft Visual Studio and ABB Robot Studio. Our evaluation shows that this is a promising approach for both predicting future IDE commands and producing empirically-interpretable observations.",
      "Keywords": "Command recommendation systems | IDE interaction data",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-11-01",
      "Publication type": "Article",
      "Authors": "Damevski, Kostadin;Chen, Hui;Shepherd, David C.;Kraft, Nicholas A.;Pollock, Lori",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029184307",
      "Primary study DOI": "10.1109/TSE.2017.2738640",
      "Title": "Engineering Trustworthy Self-Adaptive Software with Dynamic Assurance Cases",
      "Abstract": "Building on concepts drawn from control theory, self-adaptive software handles environmental and internal uncertainties by dynamically adjusting its architecture and parameters in response to events such as workload changes and component failures. Self-adaptive software is increasingly expected to meet strict functional and non-functional requirements in applications from areas as diverse as manufacturing, healthcare and finance. To address this need, we introduce a methodology for the systematic ENgineering of TRUstworthy Self-adaptive sofTware (ENTRUST). ENTRUST uses a combination of (1) design-time and runtime modelling and verification, and (2) industry-adopted assurance processes to develop trustworthy self-adaptive software and assurance cases arguing the suitability of the software for its intended application. To evaluate the effectiveness of our methodology, we present a tool-supported instance of ENTRUST and its use to develop proof-of-concept self-adaptive software for embedded and service-based systems from the oceanic monitoring and e-finance domains, respectively. The experimental results show that ENTRUST can be used to engineer self-adaptive software systems in different application domains and to generate dynamic assurance cases for these systems.",
      "Keywords": "Assurance cases | Assurance evidence | Self-adaptive software systems | Software engineering methodology",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-11-01",
      "Publication type": "Article",
      "Authors": "Calinescu, Radu;Weyns, Danny;Gerasimou, Simos;Iftikhar, Muhammad Usman;Habli, Ibrahim;Kelly, Tim",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029605348",
      "Primary study DOI": "10.1109/TSE.2016.2632115",
      "Title": "Imprecise Matching of Requirements Specifications for Software Services Using Fuzzy Logic",
      "Abstract": "Today, software components are provided by global markets in the form of services. In order to optimally satisfy service requesters and service providers, adequate techniques for automatic service matching are needed. However, a requester's requirements may be vague and the information available about a provided service may be incomplete. As a consequence, fuzziness is induced into the matching procedure. The contribution of this paper is the development of a systematic matching procedure that leverages concepts and techniques from fuzzy logic and possibility theory based on our formal distinction between different sources and types of fuzziness in the context of service matching. In contrast to existing methods, our approach is able to deal with imprecision and incompleteness in service specifications and to inform users about the extent of induced fuzziness in order to improve the user's decision-making. We demonstrate our approach on the example of specifications for service reputation based on ratings given by previous users. Our evaluation based on real service ratings shows the utility and applicability of our approach.",
      "Keywords": "decision making | fuzzy logic | non-functional properties | requirements specifications | service matching | Service selection | uncertainty",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Platenius, Marie C.;Shaker, Ammar;Becker, Matthias;Hullermeier, Eyke;Schafer, Wilhelm",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029593487",
      "Primary study DOI": "10.1109/TSE.2016.2632122",
      "Title": "Locating Software Faults Based on Minimum Debugging Frontier Set",
      "Abstract": "In this article, we propose a novel state-based fault-localization approach. Given an observed failure that is reproducible under the same program input, this new approach uses two main techniques to reduce the state exploration cost. Firstly, the execution trace to be analyzed for the observed failure is successively narrowed by making the set of trace points in each step a cut of the dynamic dependence graph. Such a cut divides the remaining trace into two parts and, based on the sparse symbolic exploration outcome, one part is removed from further exploration. This process continues until reaching where the fault is determined to be. Second, the cut in each step is chosen such that the union of the program states from the members of the cut is of the minimum size among all candidate cuts. The set of statement instances in the chosen cut is called a minimum debugging frontier set (MDFS). To evaluate our approach, we apply it to 16 real bugs from real world programs and compare our fault reports with those generated by state-of-The-Art approaches. Results show that the MDFS approach obtains high quality fault reports for these test cases with considerably higher efficiency than previous approaches.",
      "Keywords": "dynamic dependence graph | Fault localization | minimum debugging frontier set | sparse symbolic exploration",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Li, Feng;Li, Zhiyuan;Huo, Wei;Feng, Xiaobing",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029589253",
      "Primary study DOI": "10.1109/TSE.2016.2625248",
      "Title": "Automatic Contract Insertion with CCBot",
      "Abstract": "Existing static analysis tools require significant programmer effort. On large code bases, static analysis tools produce thousands of warnings. It is unrealistic to expect users to review such a massive list and to manually make changes for each warning. To address this issue we propose CCBot (short for CodeContracts Bot), a new tool that applies the results of static analysis to existing code through automatic code transformation. Specifically, CCBot instruments the code with method preconditions, postconditions, and object invariants which detect faults at runtime or statically using a static contract checker. The only configuration the programmer needs to perform is to give CCBot the file paths to code she wants instrumented. This allows the programmer to adopt contract-based static analysis with little effort. CCBot's instrumented version of the code is guaranteed to compile if the original code did. This guarantee means the programmer can deploy or test the instrumented code immediately without additional manual effort. The inserted contracts can detect common errors such as null pointer dereferences and out-of-bounds array accesses. CCBot is a robust large-scale tool with an open-source C# implementation. We have tested it on real world projects with tens of thousands of lines of code. We discuss several projects as case studies, highlighting undiscovered bugs found by CCBot, including 22 new contracts that were accepted by the project authors.",
      "Keywords": "assertions | automated patching | class invariants | Contract-based verification",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Carr, Scott A.;Logozzo, Francesco;Payer, Mathias",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029587290",
      "Primary study DOI": "10.1109/TSE.2016.2622264",
      "Title": "Preventing defects: The impact of requirements traceability completeness on software quality",
      "Abstract": "Requirements traceability has long been recognized as an important quality of a well-engineered system. Among stakeholders, traceability is often unpopular due to the unclear benefits. In fact, little evidence exists regarding the expected traceability benefits. There is a need for empirical work that studies the effect of traceability. In this paper, we focus on the four main requirements implementation supporting activities that utilize traceability. For each activity, we propose generalized traceability completeness measures. In a defined process, we selected 24 medium to large-scale open-source projects. For each software project, we quantified the degree to which a studied development activity was enabled by existing traceability with the proposed measures. We analyzed that data in a multi-level Poisson regression analysis. We found that the degree of traceability completeness for three of the studied activities significantly affects software quality, which we quantified as defect rate. Our results provide for the first time empirical evidence that more complete traceability decreases the expected defect rate in the developed software. The strong impact of traceability completeness on the defect rate suggests that traceability is of great practical value for any kind of software development project, even if traceability is not mandated by a standard or regulation.",
      "Keywords": "bugs | change impact analysis | defects | empirical validation | error proneness | regression analysis | requirements satisfaction analysis | Requirements traceability | software quality | source code justification analysis | traceability completeness | traceability metrics",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Rempel, Patrick;Mader, Parick",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029600289",
      "Primary study DOI": "10.1109/TSE.2016.2623623",
      "Title": "GK-Tail+ An Efficient Approach to Learn Software Models",
      "Abstract": "Inferring models of program behavior from execution samples can provide useful information about a system, also in the increasingly common case of systems that evolve and adapt in their lifetime, and without requiring large developers' effort. Techniques for learning models of program behavior from execution traces shall address conflicting challenges of recall, specificity and performance: They shall generate models that comprehensively represent the system behavior (recall) while limiting the amount of illegal behaviors that may be erroneously accepted by the model (specificity), and should infer the models within a reasonable time budget to process industrial scale systems (performance). In our early work, we designed GK-Tail, an approach that can infer guarded finite state machines that model the behavior of object-oriented programs in terms of sequences of method calls and constraints on the parameter values. GK-Tail addresses well two of the three main challenges, since it infers guarded finite state machines with a high level of recall and specificity, but presents severe limitations in terms of performance that reduce its scalability. In this paper, we present GK-Tail+, a new approach to infer guarded finite state machines from execution traces of object-oriented programs. GK-Tail+ proposes a new set of inference criteria that represent the core element of the inference process: It largely reduces the inference time of GK-Tail while producing guarded finite state machines with a comparable level of recall and specificity. Thus, GK-Tail+ advances the preliminary results of GK-Tail by addressing all the three main challenges of learning models of program behavior from execution traces.",
      "Keywords": "Dynamic model learning | guarded finite state machines | software models | specification mining | state based models",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Mariani, Leonardo;Pezze, Mauro;Santoro, Mauro",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028931582",
      "Primary study DOI": "10.1109/TSE.2017.2734091",
      "Title": "Measuring Program Comprehension: A Large-Scale Field Study with Professionals",
      "Abstract": "During software development and maintenance, developers spend a considerable amount of time on program comprehension activities. Previous studies show that program comprehension takes up as much as half of a developer's time. However, most of these studies are performed in a controlled setting, or with a small number of participants, and investigate the program comprehension activities only within the IDEs. However, developers' program comprehension activities go well beyond their IDE interactions. In this paper, we extend our ActivitySpace framework to collect and analyze Human-Computer Interaction (HCI) data across many applications (not just the IDEs). We follow Minelli et al.'s approach to assign developers' activities into four categories: navigation, editing, comprehension, and other. We then measure the comprehension time by calculating the time that developers spend on program comprehension, e.g., inspecting console and breakpoints in IDE, or reading and understanding tutorials in web browsers. Using this approach, we can perform a more realistic investigation of program comprehension activities, through a field study of program comprehension in practice across a total of seven real projects, on 78 professional developers, and amounting to 3,148 working hours. Our study leverages interaction data that is collected across many applications by the developers. Our study finds that on average developers spend ∼ 58 percent of their time on program comprehension activities, and that they frequently use web browsers and document editors to perform program comprehension activities. We also investigate the impact of programming language, developers' experience, and project phase on the time that is spent on program comprehension, and we find senior developers spend significantly less percentages of time on program comprehension than junior developers. Our study also highlights the importance of several research directions needed to reduce program comprehension time, e.g., building automatic detection and improvement of low quality code and documentation, construction of software-engineering-specific search engines, designing better IDEs that help developers navigate code and browse information more efficiently, etc.",
      "Keywords": "field study | inference model | Program comprehension",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-10-01",
      "Publication type": "Article",
      "Authors": "Xia, Xin;Bao, Lingfeng;Lo, David;Xing, Zhenchang;Hassan, Ahmed E.;Li, Shanping",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028943370",
      "Primary study DOI": "10.1109/TSE.2017.2731853",
      "Title": "Two-phase Assessment Approach to Improve the Efficiency of Refactoring Identification",
      "Abstract": "To automate the refactoring identification process, a large number of candidates need to be compared. Such an overhead can make the refactoring approach impractical if the software size is large and the computational load of a fitness function is substantial. In this paper, we propose a two-phase assessment approach to improving the efficiency of the process. For each iteration of the refactoring process, refactoring candidates are preliminarily assessed using a lightweight, fast delta assessment method called the Delta Table. Using multiple Delta Tables, candidates to be evaluated with a fitness function are selected. A refactoring can be selected either interactively by the developer or automatically by choosing the best refactoring, and the refactorings are applied one after another in a stepwise fashion. The Delta Table is the key concept enabling a two-phase assessment approach because of its ability to quickly calculate the varying amounts of maintainability provided by each refactoring candidate. Our approach has been evaluated for three large-scale open-source projects. The results convincingly show that the proposed approach is efficient because it saves a considerable time while still achieving the same amount of fitness improvement as the approach examining all possible candidates.",
      "Keywords": "maintainability improvement | Refactoring assessment | refactoring identification",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-10-01",
      "Publication type": "Article",
      "Authors": "Han, Ah Rim;Cha, Sungdeok",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028936214",
      "Primary study DOI": "10.1109/TSE.2017.2731766",
      "Title": "MAHAKIL:Diversity based Oversampling Approach to Alleviate the Class Imbalance Issue in Software Defect Prediction",
      "Abstract": "Highly imbalanced data typically make accurate predictions difficult. Unfortunately, software defect datasets tend to have fewer defective modules than non-defective modules. Synthetic oversampling approaches address this concern by creating new minority defective modules to balance the class distribution before a model is trained. Notwithstanding the successes achieved by these approaches, they mostly result in over-generalization (high rates of false alarms) and generate near-duplicated data instances (less diverse data). In this study, we introduce MAHAKIL, a novel and efficient synthetic oversampling approach for software defect datasets that is based on the chromosomal theory of inheritance. Exploiting this theory, MAHAKIL interprets two distinct sub-classes as parents and generates a new instance that inherits different traits from each parent and contributes to the diversity within the data distribution. We extensively compare MAHAKIL with SMOTE, Borderline-SMOTE, ADASYN, Random Oversampling and the No sampling approach using 20 releases of defect datasets from the PROMISE repository and five prediction models. Our experiments indicate that MAHAKIL improves the prediction performance for all the models and achieves better and more significant pf values than the other oversampling approaches, based on Brunner's statistical significance test and Cliff's effect sizes. Therefore, MAHAKIL is strongly recommended as an efficient alternative for defect prediction models built on highly imbalanced datasets.",
      "Keywords": "class imbalance learning | classification problems | data sampling methods | Software defect prediction | synthetic sample generation",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-06-01",
      "Publication type": "Article",
      "Authors": "Bennin, Kwabena Ebo;Keung, Jacky;Phannachitta, Passakorn;Monden, Akito;Mensah, Solomon",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028922080",
      "Primary study DOI": "10.1109/TSE.2017.2731308",
      "Title": "Authors&#x0027; Reply to &#x201C;Comments on &#x0027;Researcher Bias: The Use of Machine Learning in Software Defect Prediction&#x0027; &#x201D;",
      "Abstract": "In 2014 we published a meta-analysis of software defect prediction studies [1] . This suggested that the most important factor in determining results was Research Group, i.e., who conducts the experiment is more important than the classifier algorithms being investigated. A recent re-analysis [2] sought to argue that the effect is less strong than originally claimed since there is a relationship between Research Group and Dataset. In this response we show (i) the re-analysis is based on a small (21 percent) subset of our original data, (ii) using the same re-analysis approach with a larger subset shows that Research Group is more important than type of Classifier and (iii) however the data are analysed there is compelling evidence that who conducts the research has an effect on the results. This means that the problem of researcher bias remains. Addressing it should be seen as a matter of priority amongst those of us who conduct and publish experiments comparing the performance of competing software defect prediction systems.",
      "Keywords": "Defect prediction | Researcher bias | Software quality assurance",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-11-01",
      "Publication type": "Article",
      "Authors": "Shepperd, Martin;Hall, Tracy;Bowes, David",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028940218",
      "Primary study DOI": "10.1109/TSE.2017.2730870",
      "Title": "Coordination Challenges in Large-Scale Software Development: A Case Study of Planning Misalignment in Hybrid Settings",
      "Abstract": "Achieving effective inter-Team coordination is one of the most pressing challenges in large-scale software development. Hybrid approaches of traditional and agile development promise combining the overview and predictability of long-Term planning on an inter-Team level with the flexibility and adaptability of agile development on a team level. It is currently unclear, however, why such hybrids often fail. Our case study within a large software development unit of 13 teams at a global enterprise software company explores how and why a combination of traditional planning on an inter-Team level and agile development on a team level can result in ineffective coordination. Based on a variety of data, including interviews with scrum masters, product owners, architects and senior management, and using Grounded Theory data analysis procedures, we identify a lack of dependency awareness across development teams as a key explanation of ineffective coordination. Our findings show how a lack of dependency awareness emerges from misaligned planning activities of specification, prioritization, estimation and allocation between agile team and traditional inter-Team levels and ultimately prevents effective coordination. Knowing about these issues, large-scale hybrid projects in similar contexts can try to better align their planning activities across levels to improve dependency awareness and in turn achieve more effective coordination.",
      "Keywords": "agile | dependency awareness | hybrid | information systems development | inter-Team coordination | Large-scale software development | planning alignment",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-10-01",
      "Publication type": "Article",
      "Authors": "Bick, Saskia;Spohrer, Kai;Hoda, Rashina;Scheerer, Alexander;Heinzl, Armin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028931155",
      "Primary study DOI": "10.1109/TSE.2017.2730198",
      "Title": "Enhancing the Description-to-Behavior Fidelity in Android Apps with Privacy Policy",
      "Abstract": "Since more than 96 percent of mobile malware targets the Android platform, various techniques based on static code analysis or dynamic behavior analysis have been proposed to detect malicious apps. As malware is becoming more complicated and stealthy, recent research proposed a promising detection approach that looks for the inconsistency between an app's permissions and its description. In this paper, we first revisit this approach and reveal that using description and permission will lead to many false positives because descriptions often fail to declare all sensitive operations. Then, we propose exploiting an app's privacy policy and its bytecode to enhance the malware detection based on description and permissions. It is non-trivial to automatically analyze privacy policy and perform the cross-verification among these four kinds of software artifacts including, privacy policy, bytecode, description, and permissions. To address these challenging issues, we first propose a novel data flow model for analyzing privacy policy, and then develop a new system, named TAPVerifier, for carrying out investigation of individual software artifacts and conducting the cross-verification. The experimental results show that TAPVerifier can analyze privacy policy with a high accuracy and recall rate. More importantly, integrating privacy policy and bytecode level information can remove up to 59.4 percent false alerts of the state-of-the-art systems, such as AutoCog, CHABADA, etc.",
      "Keywords": "Mobile applications | privacy policy",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-09-01",
      "Publication type": "Article",
      "Authors": "Yu, Le;Luo, Xiapu;Qian, Chenxiong;Wang, Shuai;Leung, Hareton K.N.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026637324",
      "Primary study DOI": "10.1145/2889160.2891041",
      "Title": "Blindness and Program Comprehension",
      "Abstract": "Blind programmers typically use a screen reader when reading code whereas sighted programmers are able to skim the code with their eyes. This difference has the potential to impact the generalizability of software engineering studies and approaches. We present a summary of a paper which will soon be under review at TSE that investigates how code comprehension of blind programmers differs from that of sighted programmers. Put briefly, we found no statistically-significant differences between the areas of code that the blind programmers found to be important and the areas of code that the sighted programmers found to be important.",
      "Keywords": "",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2016-05-14",
      "Publication type": "Conference Paper",
      "Authors": "Armaly, Ameer;McMillan, Collin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85023163956",
      "Primary study DOI": "10.1109/TSE.2017.2724538",
      "Title": "A Comparative Study to Benchmark Cross-project Defect Prediction Approaches",
      "Abstract": "Cross-Project Defect Prediction (CPDP) as a means to focus quality assurance of software projects was under heavy investigation in recent years. However, within the current state-of-the-art it is unclear which of the many proposals performs best due to a lack of replication of results and diverse experiment setups that utilize different performance metrics and are based on different underlying data. Within this article, we provide a benchmark for CPDP. We replicate 24 approaches proposed by researchers between 2008 and 2015 and evaluate their performance on software products from five different data sets. Based on our benchmark, we determined that an approach proposed by Camargo Cruz and Ochimizu (2009) based on data standardization performs best and is always ranked among the statistically significant best results for all metrics and data sets. Approaches proposed by Turhan et al. (2009), Menzies et al. (2011), and Watanabe et al. (2008) are also nearly always among the best results. Moreover, we determined that predictions only seldom achieve a high performance of 0.75 recall, precision, and accuracy. Thus, CPDP still has not reached a point where the performance of the results is sufficient for the application in practice.",
      "Keywords": "benchmark | comparison | Cross-project defect prediction | replication",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-09-01",
      "Publication type": "Article",
      "Authors": "Herbold, Steffen;Trautsch, Alexander;Grabowski, Jens",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029226586",
      "Primary study DOI": "10.1109/TSE.2016.2620145",
      "Title": "A Feature-Based Classification of Model Repair Approaches",
      "Abstract": "Consistency management, the ability to detect, diagnose and handle inconsistencies, is crucial during the development process in Model-driven Engineering (MDE). As the popularity and application scenarios of MDE expanded, a variety of different techniques were proposed to address these tasks in specific contexts. Of the various stages of consistency management, this work focuses on inconsistency handling in MDE, particularly in model repair techniques. This paper proposes a feature-based classification system for model repair techniques, based on an systematic literature review of the area. We expect this work to assist developers and researchers from different disciplines in comparing their work under a unifying framework, and aid MDE practitioners in selecting suitable model repair approaches.",
      "Keywords": "consistency management | inconsistency handling | model repair | Model-driven engineering",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "MacEdo, Nuno;Jorge, Tiago;Cunha, Alcino",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029315080",
      "Primary study DOI": "10.1109/TSE.2016.2620458",
      "Title": "Supporting Change Impact Analysis Using a Recommendation System: An Industrial Case Study in a Safety-Critical Context",
      "Abstract": "Change Impact Analysis (CIA) during software evolution of safety-critical systems is a labor-intensive task. Several authors have proposed tool support for CIA, but very few tools were evaluated in industry. We present a case study on ImpRec, a recommendation System for Software Engineering (RSSE), tailored for CIA at a process automation company. ImpRec builds on assisted tracing, using information retrieval solutions and mining software repositories to recommend development artifacts, potentially impacted when resolving incoming issue reports. In contrast to the majority of tools for automated CIA, ImpRec explicitly targets development artifacts that are not source code. We evaluate ImpRec in a two-phase study. First, we measure the correctness of ImpRec's recommendations by a simulation based on 12 years' worth of issue reports in the company. Second, we assess the utility of working with ImpRec by deploying the RSSE in two development teams on different continents. The results suggest that ImpRec presents about 40 percent of the true impact among the top-10 recommendations. Furthermore, user log analysis indicates that ImpRec can support CIA in industry, and developers acknowledge the value of ImpRec in interviews. In conclusion, our findings show the potential of reusing traceability associated with developers' past activities in an RSSE.",
      "Keywords": "Case | maintenance management | software and system safety | tracing",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Borg, Markus;Wnuk, Krzysztof;Regnell, Bjorn;Runeson, Per",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029218768",
      "Primary study DOI": "10.1109/TSE.2016.2616306",
      "Title": "A Framework for Evaluating the Results of the SZZ Approach for Identifying Bug-Introducing Changes",
      "Abstract": "The approach proposed by Śliwerski, Zimmermann, and Zeller (SZZ) for identifying bug-introducing changes is at the foundation of several research areas within the software engineering discipline. Despite the foundational role of SZZ, little effort has been made to evaluate its results. Such an evaluation is a challenging task because the ground truth is not readily available. By acknowledging such challenges, we propose a framework to evaluate the results of alternative SZZ implementations. The framework evaluates the following criteria: (1) the earliest bug appearance, (2) the future impact of changes, and (3) the realism of bug introduction. We use the proposed framework to evaluate five SZZ implementations using data from ten open source projects. We find that previously proposed improvements to SZZ tend to inflate the number of incorrectly identified bug-introducing changes. We also find that a single bug-introducing change may be blamed for introducing hundreds of future bugs. Furthermore, we find that SZZ implementations report that at least 46 percent of the bugs are caused by bug-introducing changes that are years apart from one another. Such results suggest that current SZZ implementations still lack mechanisms to accurately identify bug-introducing changes. Our proposed framework provides a systematic mean for evaluating the data that is generated by a given SZZ implementation.",
      "Keywords": "bug detection | evaluation framework | software repository mining | SZZ",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Da Costa, Daniel Alencar;McIntosh, Shane;Shang, Weiyi;Kulesza, Uira;Coelho, Roberta;Hassan, Ahmed E.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029211132",
      "Primary study DOI": "10.1109/TSE.2016.2616877",
      "Title": "A Dissection of the Test-Driven Development Process: Does It Really Matter to Test-First or to Test-Last?",
      "Abstract": "Background: Test-driven development (TDD) is a technique that repeats short coding cycles interleaved with testing. The developer first writes a unit test for the desired functionality, followed by the necessary production code, and refactors the code. Many empirical studies neglect unique process characteristics related to TDD iterative nature. Aim: We formulate four process characteristic: sequencing, granularity, uniformity, and refactoring effort. We investigate how these characteristics impact quality and productivity in TDD and related variations. Method: We analyzed 82 data points collected from 39 professionals, each capturing the process used while performing a specific development task. We built regression models to assess the impact of process characteristics on quality and productivity. Quality was measured by functional correctness. Result: Quality and productivity improvements were primarily positively associated with the granularity and uniformity. Sequencing, the order in which test and production code are written, had no important influence. Refactoring effort was negatively associated with both outcomes. We explain the unexpected negative correlation with quality by possible prevalence of mixed refactoring. Conclusion: The claimed benefits of TDD may not be due to its distinctive test-first dynamic, but rather due to the fact that TDD-like processes encourage fine-grained, steady steps that improve focus and flow.",
      "Keywords": "empirical investigation | external quality | process dimensions | productivity | Test-driven development",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Fucci, Davide;Erdogmus, Hakan;Turhan, Burak;Oivo, Markku;Juristo, Natalia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029316378",
      "Primary study DOI": "10.1109/TSE.2016.2624293",
      "Title": "Keyword Search for Building Service-Based Systems",
      "Abstract": "With the fast growth of applications of service-oriented architecture (SOA) in software engineering, there has been a rapid increase in demand for building service-based systems (SBSs) by composing existing Web services. Finding appropriate component services to compose is a key step in the SBS engineering process. Existing approaches require that system engineers have detailed knowledge of SOA techniques which is often too demanding. To address this issue, we propose Keyword Search for Service-based Systems (KS3), a novel approach that integrates and automates the system planning, service discovery and service selection operations for building SBSs based on keyword search. KS3 assists system engineers without detailed knowledge of SOA techniques in searching for component services to build SBSs by typing a few keywords that represent the tasks of the SBSs with quality constraints and optimisation goals for system quality, e.g., reliability, throughput and cost. KS3 offers a new paradigm for SBS engineering that can significantly save the time and effort during the system engineering process. We conducted large-scale experiments using two real-world Web service datasets to demonstrate the practicality, effectiveness and efficiency of KS3.",
      "Keywords": "cloud computing | keyword search | quality of service | service composition | Service-based system | web service",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "He, Qiang;Zhou, Rui;Zhang, Xuyun;Wang, Yanchun;Ye, Dayong;Chen, Feifei;Grundy, John C.;Yang, Yun",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85023741332",
      "Primary study DOI": "10.1109/TSE.2017.2702606",
      "Title": "Specialising Software for Different Downstream Applications Using Genetic Improvement and Code Transplantation",
      "Abstract": "Genetic improvement uses automated search to find improved versions of existing software. Genetic improvement has previously been concerned with improving a system with respect to all possible usage scenarios. In this paper, we show how genetic improvement can also be used to achieve specialisation to a specific set of usage scenarios. We use genetic improvement to evolve faster versions of a C++ program, a Boolean satisfiability solver called MiniSAT, specialising it for three different applications, each with their own characteristics. Our specialised solvers achieve between 4 and 36 percent execution time improvement, which is commensurate with efficiency gains achievable using human expert optimisation for the general solver. We also use genetic improvement to evolve faster versions of an image processing tool called ImageMagick, utilising code from GraphicsMagick, another image processing tool which was forked from it. We specialise the format conversion functionality to greyscale images and colour images only. Our specialised versions achieve up to 3 percent execution time improvement.",
      "Keywords": "code specialisation | code transplants | Genetic improvement | GI | GraphicsMagick | ImageMagick | SAT",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-06-01",
      "Publication type": "Article",
      "Authors": "Petke, Justyna;Harman, Mark;Langdon, William B.;Weimer, Westley",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85023757910",
      "Primary study DOI": "10.1109/TSE.2017.2720603",
      "Title": "Heterogeneous Defect Prediction",
      "Abstract": "Many recent studies have documented the success of cross-project defect prediction (CPDP) to predict defects for new projects lacking in defect data by using prediction models built by other projects. However, most studies share the same limitations: it requires homogeneous data; i.e., different projects must describe themselves using the same metrics. This paper presents methods for heterogeneous defect prediction (HDP) that matches up different metrics in different projects. Metric matching for HDP requires a 'large enough' sample of distributions in the source and target projects - which raises the question on how large is 'large enough' for effective heterogeneous defect prediction. This paper shows that empirically and theoretically, 'large enough' may be very small indeed. For example, using a mathematical model of defect prediction, we identify categories of data sets were as few as 50 instances are enough to build a defect prediction model. Our conclusion for this work is that, even when projects use different metric sets, it is possible to quickly transfer lessons learned about defect prediction.",
      "Keywords": "Defect prediction | heterogeneous metrics | quality assurance | transfer learning",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-09-01",
      "Publication type": "Article",
      "Authors": "Nam, Jaechang;Fu, Wei;Kim, Sunghun;Menzies, Tim;Tan, Lin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85023742957",
      "Primary study DOI": "10.1109/TSE.2017.2718516",
      "Title": "Implementing and Evaluating Candidate-Based Invariant Generation",
      "Abstract": "The discovery of inductive invariants lies at the heart of static program verification. Presently, many automatic solutions to inductive invariant generation are inflexible, only applicable to certain classes of programs, or unpredictable. An automatic technique that circumvents these deficiencies to some extent is candidate-based invariant generation, whereby a large number of candidate invariants are guessed and then proven to be inductive or rejected using a sound program analyzer. This paper describes our efforts to apply candidate-based invariant generation in GPUVerify, a static checker for programs that run on GPUs. We study a set of $383$ GPU programs that contain loops, drawn from a number of open source suites and vendor SDKs. Among this set, $253$ benchmarks require provision of loop invariants for verification to succeed. We describe the methodology we used to incrementally improve the invariant generation capabilities of GPUVerify to handle these benchmarks, through candidate-based invariant generation, using cheap static analysis to speculate potential program invariants. We also describe a set of experiments that we used to examine the effectiveness of our rules for candidate generation, assessing rules based on their generality (the extent to which they generate candidate invariants), hit rate (the extent to which the generated candidates hold), worth (the extent to which provable candidates actually help in allowing verification to succeed), and influence (the extent to which the success of one generation rule depends on candidates generated by another rule). We believe that our methodology may serve as a useful framework for other researchers interested in candidate-based invariant generation. The candidates produced by GPUVerify help to verify $231$ of the $253$ programs. This increase in precision, however, makes GPUVerify sluggish: the more candidates that are generated, the more time is spent determining which are inductive invariants. To speed up this process, we have investigated four under-approximating program analyses that aim to reject false candidates quickly and a framework whereby these analyses can run in sequence or in parallel. Across two platforms, running Windows and Linux, our results show that the best combination of these techniques running sequentially speeds up invariant generation across our benchmarks by 1.17× (Windows) and 1.01× (Linux), with per-benchmark best speedups of 93.58× (Windows) and 48.34× (Linux), and worst slowdowns of 10.24× (Windows) and 43.31× (Linux). We find that parallelizing the strategies marginally improves overall invariant generation speedups to 1.27× (Windows) and 1.11× (Linux), maintains good best-case speedups of 91.18× (Windows) and 44.60× (Linux), and, importantly, dramatically reduces worst-case slowdowns to 3.15× (Windows) and 3.17× (Linux).",
      "Keywords": "Formal verification | GPUs | invariant generation",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-07-01",
      "Publication type": "Article",
      "Authors": "Betts, Adam;Chong, Nathan;Deligiannis, Pantazis;Donaldson, Alastair F.;Ketema, Jeroen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021834018",
      "Primary study DOI": "10.1109/TSE.2017.2716925",
      "Title": "Global-Aware Recommendations for Repairing Violations in Exception Handling",
      "Abstract": "Empirical evidence suggests exception handling is not reliably implemented. Most faults in exception handling are related to global exceptions violating the intended exception handling design. However, repairing these violations is a cumbersome and error-prone task. It requires knowing the intended design and understanding how the source code violates it. It also requires changing the source code to make it compliant with the intended design. But changing the exception handling code is a difficult task, since changes in exception handling requires changing different parts of a program. Currently, there is still no solution to assist the repair of this type of violations. To bridge this gap, we present RAVEN, a heuristic strategy aware of the global context of exceptions that produces recommendations of how violations in exception handling may be repaired. This strategy takes advantage of explicit specifications of the intended design, although their availability is not mandatory. Our results revealed RAVEN provides recommendations able to repair violations in 69 percent of the cases when policy specifications are not available and in 97 percent of the cases when specifications are available. Thus, development teams may benefit from RAVEN, even when exception handling design decisions are not documented in their projects.",
      "Keywords": "Exception handling | recommender heuristic | software repair",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-09-01",
      "Publication type": "Article",
      "Authors": "Barbosa, Eiji Adachi;Garcia, Alessandro",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85054688960",
      "Primary study DOI": "10.1109/TSE.2017.2716950",
      "Title": "Towards Prioritizing Documentation Effort",
      "Abstract": "Programmers need documentation to comprehend software, but they often lack the time to write it. Thus, programmers must prioritize their documentation effort to ensure that sections of code important to program comprehension are thoroughly explained. In this paper, we explore the possibility of automatically prioritizing documentation effort. We performed two user studies to evaluate the effectiveness of static source code attributes and textual analysis of source code towards prioritizing documentation effort. The first study used open-source API Libraries while the second study was conducted using closed-source industrial software from ABB. Our findings suggest that static source code attributes are poor predictors of documentation effort priority, whereas textual analysis of source code consistently performed well as a predictor of documentation effort priority.",
      "Keywords": "Code documentation | Program comprehension | Software maintenance",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-06-17",
      "Publication type": "Article",
      "Authors": "McBurney, Paul W.;Jiang, Siyuan;Kessentini, Marouane;Kraft, Nicholas A.;Armaly, Ameer;Mkaouer, Mohamed Wiem;McMillan, Collin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026797899",
      "Primary study DOI": "10.1109/TSE.2016.2615615",
      "Title": "Online reliability prediction via motifs-based dynamic Bayesian networks for service-oriented systems",
      "Abstract": "A service-oriented System of Systems (SoS) considers a system as a service and constructs a robust and value-added SoS by outsourcing external component systems through service composition techniques. Online reliability prediction for the component systems for the purpose of assuring the overall Quality of Service (QoS) is often a major challenge in coping with a loosely coupled SoS operating under dynamic and uncertain running environments. It is also a prerequisite for guaranteeing runtime QoS of a SoS through optimal service selection for reliable system construction.We propose a novel online reliability time series prediction approach for the component systems in a service-oriented SoS. We utilize Probabilistic Graphical Models (PGMs) to yield near-future, time series predictions. We assess the approach via invocation records collected from widely used real Web services. Experimental results have confirmed the effectiveness of the approach.",
      "Keywords": "Online reliability prediction | Service-oriented computing | System of systems | Time series",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Wang, Hongbing;Wang, Lei;Yu, Qi;Zheng, Zibin;Bouguettaya, Athman;Lyu, Michael R.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027410597",
      "Primary study DOI": "10.1109/TSE.2016.2615100",
      "Title": "The Value of Exact Analysis in Requirements Selection",
      "Abstract": "Uncertainty is characterised by incomplete understanding. It is inevitable in the early phase of requirements engineering, and can lead to unsound requirement decisions. Inappropriate requirement choices may result in products that fail to satisfy stakeholders' needs, and might cause loss of revenue. To overcome uncertainty, requirements engineering decision support needs uncertainty management. In this research, we develop a decision support framework METRO for the Next Release Problem (NRP) to manage algorithmic uncertainty and requirements uncertainty. An exact NRP solver (NSGDP) lies at the heart of METRO. NSGDP's exactness eliminates interference caused by approximate existing NRP solvers. We apply NSGDP to three NRP instances, derived from a real world NRP instance, RALIC, and compare with NSGA-II, a widely-used approximate (inexact) technique. We find the randomness of NSGA-II results in decision makers missing up to 99.95 percent of the optimal solutions and obtaining up to 36.48 percent inexact requirement selection decisions. The chance of getting an inexact decision using existing approximate approaches is negatively correlated with the implementation cost of a requirement (Spearman ρ up to -0.72). Compared to the inexact existing approach, NSGDP saves 15.21 percent lost revenue, on average, for the RALIC dataset.",
      "Keywords": "exact multi-objective optimisation | next release problem | simulation optimisation | Software engineering",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Li, Lingbo;Harman, Mark;Wu, Fan;Zhang, Yuanyuan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027420063",
      "Primary study DOI": "10.1109/TSE.2016.2615311",
      "Title": "Automated Steering of Model-Based Test Oracles to Admit Real Program Behaviors",
      "Abstract": "The test oracle-a judge of the correctness of the system under test (SUT)-is a major component of the testing process. Specifying test oracles is challenging for some domains, such as real-time embedded systems, where small changes in timing or sensory input may cause large behavioral differences. Models of such systems, often built for analysis and simulation, are appealing for reuse as test oracles. These models, however, typically represent an idealized system, abstracting away certain issues such as non-deterministic timing behavior and sensor noise. Thus, even with the same inputs, the model's behavior may fail to match an acceptable behavior of the SUT, leading to many false positives reported by the test oracle. We propose an automated steering framework that can adjust the behavior of the model to better match the behavior of the SUT to reduce the rate of false positives. This model steering is limited by a set of constraints (defining the differences in behavior that are acceptable) and is based on a search process attempting to minimize a dissimilarity metric. This framework allows non-deterministic, but bounded, behavioral differences, while preventing future mismatches by guiding the oracle-within limits-to match the execution of the SUT. Results show that steering significantly increases SUT-oracle conformance with minimal masking of real faults and, thus, has significant potential for reducing false positives and, consequently, testing and debugging costs while improving the quality of the testing process.",
      "Keywords": "model-based development | model-based testing | Software testing | test oracles | verification",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Gay, Gregory;Rayadurgam, Sanjai;Heimdahl, Mats P.E.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027418801",
      "Primary study DOI": "10.1109/TSE.2016.2615307",
      "Title": "A Taxonomy and Qualitative Comparison of Program Analysis Techniques for Security Assessment of Android Software",
      "Abstract": "In parallel with the meteoric rise of mobile software, we are witnessing an alarming escalation in the number and sophistication of the security threats targeted at mobile platforms, particularly Android, as the dominant platform. While existing research has made significant progress towards detection and mitigation of Android security, gaps and challenges remain. This paper contributes a comprehensive taxonomy to classify and characterize the state-of-the-art research in this area. We have carefully followed the systematic literature review process, and analyzed the results of more than 300 research papers, resulting in the most comprehensive and elaborate investigation of the literature in this area of research. The systematic analysis of the research literature has revealed patterns, trends, and gaps in the existing literature, and underlined key challenges and opportunities that will shape the focus of future research efforts.",
      "Keywords": "android platform | program analysis | security assessment | Taxonomy and survey",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Sadeghi, Alireza;Bagheri, Hamid;Garcia, Joshua;Malek, Sam",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021706923",
      "Primary study DOI": "10.1109/TSE.2016.2613863",
      "Title": "Model-based self-Aware performance and resource management using the descartes modeling language",
      "Abstract": "Modern IT systems have increasingly distributed and dynamic architectures providing flexibility to adapt to changes in the environment and thus enabling higher resource efficiency. However, these benefits come at the cost of higher system complexity and dynamics. Thus, engineering systems that manage their end-To-end application performance and resource efficiency in an autonomic manner is a challenge. In this article, we present a holistic model-based approach for self-Aware performance and resource management leveraging the Descartes Modeling Language (DML), an architecture-level modeling language for online performance and resource management. We propose a novel online performance prediction process that dynamically tailors the model solving depending on the requirements regarding accuracy and overhead. Using these prediction capabilities, we implement a generic modelbased control loop for proactive system adaptation. We evaluate our model-based approach in the context of two representative case studies showing that with the proposed methods, significant resource efficiency gains can be achieved while maintaining performance requirements. These results represent the first end-To-end validation of our approach, demonstrating its potential for self-Aware performance and resource management in the context of modern IT systems and infrastructures.",
      "Keywords": "Adaptation | Autonomic | Efficiency | Model-based | Modeling language | Performance | Self-Aware",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Huber, Nikolaus;Brosig, Fabian;Spinner, Simon;Kounev, Samuel;Bähr, Manuel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021774962",
      "Primary study DOI": "10.1109/TSE.2016.2608826",
      "Title": "Self-Adaptive and online QoS modeling for cloud-based software services",
      "Abstract": "In the presence of scale, dynamism, uncertainty and elasticity, cloud software engineers faces several challenges when modeling Quality of Service (QoS) for cloud-based software services. These challenges can be best managed through self-Adaptivity because engineers' intervention is difficult, if not impossible, given the dynamic and uncertain QoS sensitivity to the environment and control knobs in the cloud. This is especially true for the shared infrastructure of cloud, where unexpected interference can be caused by co-located software services running on the same virtual machine; and co-hosted virtual machines within the same physical machine. In this paper, we describe the related challenges and present a fully dynamic, self-Adaptive and online QoS modeling approach, which grounds on sound information theory and machine learning algorithms, to create QoS model that is capable to predict the QoS value as output over time by using the information on environmental conditions, control knobs and interference as inputs. In particular, we report on in-depth analysis on the correlations of selected inputs to the accuracy of QoS model in cloud. To dynamically selects inputs to the models at runtime and tune accuracy, we design self-Adaptive hybrid dual-learners that partition the possible inputs space into two sub-spaces, each of which applies different symmetric uncertainty based selection techniques; the results of sub-spaces are then combined. Subsequently, we propose the use of adaptive multi-learners for building the model. These learners simultaneously allow several learning algorithms to model the QoS function, permitting the capability for dynamically selecting the best model for prediction on the fly. We experimentally evaluate our models in the cloud environment using RUBiS benchmark and realistic FIFA 98 workload. The results show that our approach is more accurate and effective than state-of-The-Art modelings.",
      "Keywords": "Cloud computing | Machine learning | Performance modeling | Search-based software engineering | Self-Adaptive systems | Software quality",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Chen, Tao;Bahsoon, Rami",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021770842",
      "Primary study DOI": "10.1109/TSE.2016.2610424",
      "Title": "Approaches to co-evolution of metamodels and models: A survey",
      "Abstract": "Modeling languages, just as all software artifacts, evolve. This poses the risk that legacy models of a company get lost, when they become incompatible with the new language version. To address this risk, a multitude of approaches for metamodel-model co-evolution were proposed in the last 10 years. However, the high number of solutions makes it difficult for practitioners to choose an appropriate approach. In this paper, we present a survey on 31 approaches to support metamodel-model co-evolution. We introduce a taxonomy of solution techniques and classify the existing approaches. To support researchers, we discuss the state of the art, in order to better identify open issues. Furthermore, we use the results to provide a decision support for practitioners, who aim to adopt solutions from research.",
      "Keywords": "Design notations and documentation | Metamodels | Models | Software engineering | Survey",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-05-01",
      "Publication type": "Review",
      "Authors": "Hebig, Regina;Khelladi, Djamel Eddine;Bendraou, Reda",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021694948",
      "Primary study DOI": "10.1109/TSE.2016.2599161",
      "Title": "The use of summation to aggregate software metrics hinders the performance of defect prediction models",
      "Abstract": "Defect prediction models help software organizations to anticipate where defects will appear in the future. When training a defect prediction model, historical defect data is often mined from a Version Control System (VCS, e.g., Subversion), which records software changes at the file-level. Software metrics, on the other hand, are often calculated at the class- or method-level (e.g., McCabe's Cyclomatic Complexity). To address the disagreement in granularity, the class-And method-level software metrics are aggregated to file-level, often using summation (i.e., McCabe of a file is the sum of the McCabe of all methods within the file). A recent study shows that summation significantly inflates the correlation between lines of code (SLOC) and cyclomatic complexity (CC) in Java projects. While there are many other aggregation schemes (e.g., central tendency, dispersion), they have remained unexplored in the scope of defect prediction. In this study, we set out to investigate how different aggregation schemes impact defect prediction models. Through an analysis of 11 aggregation schemes using data collected from 255 open source projects, we find that: (1) aggregation schemes can significantly alter correlations among metrics, as well as the correlations between metrics and the defect count; (2) when constructing models to predict defect proneness, applying only the summation scheme (i.e., the most commonly used aggregation scheme in the literature) only achieves the best performance (the best among the 12 studied configurations) in 11 percent of the studied projects, while applying all of the studied aggregation schemes achieves the best performance in 40 percent of the studied projects; (3) when constructing models to predict defect rank or count, either applying only the summation or applying all of the studied aggregation schemes achieves similar performance, with both achieving the closest to the best performance more often than the other studied aggregation schemes; and (4) when constructing models for effort-Aware defect prediction, the mean or median aggregation schemes yield performance values that are significantly closer to the best performance than any of the other studied aggregation schemes. Broadly speaking, the performance of defect prediction models are often underestimated due to our community's tendency to only use the summation aggregation scheme. Given the potential benefit of applying additional aggregation schemes, we advise that future defect prediction models should explore a variety of aggregation schemes.",
      "Keywords": "Aggregation scheme | Defect prediction | Software metrics",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Zhang, Feng;Hassan, Ahmed E.;Mcintosh, Shane;Zou, Ying",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021744825",
      "Primary study DOI": "10.1109/TSE.2016.2598336",
      "Title": "A system for profiling and monitoring database access patterns by application programs for anomaly detection",
      "Abstract": "Database Management Systems (DBMSs) provide access control mechanisms that allow database administrators (DBAs) to grant application programs access privileges to databases. Though such mechanisms are powerful, in practice finer-grained access control mechanism tailored to the semantics of the data stored in the DMBS is required as a first class defense mechanism against smart attackers. Hence, custom written applications which access databases implement an additional layer of access control. Therefore, securing a database alone is not enough for such applications, as attackers aiming at stealing data can take advantage of vulnerabilities in the privileged applications and make these applications to issue malicious database queries. An access control mechanism can only prevent application programs from accessing the data to which the programs are not authorized, but it is unable to prevent misuse of the data to which application programs are authorized for access. Hence, we need a mechanism able to detect malicious behavior resulting from previously authorized applications. In this paper, we present the architecture of an anomaly detection mechanism, DetAnom, that aims to solve such problem. Our approach is based the analysis and profiling of the application in order to create a succinct representation of its interaction with the database. Such a profile keeps a signature for every submitted query and also the corresponding constraints that the application program must satisfy to submit the query. Later, in the detection phase, whenever the application issues a query, a module captures the query before it reaches the database and verifies the corresponding signature and constraints against the current context of the application. If there is a mismatch, the query is marked as anomalous. The main advantage of our anomaly detection mechanism is that, in order to build the application profiles, we need neither any previous knowledge of application vulnerabilities nor any example of possible attacks. As a result, our mechanism is able to protect the data from attacks tailored to database applications such as code modification attacks, SQL injections, and also from other data-centric attacks as well. We have implemented our mechanism with a software testing technique called concolic testing and the PostgreSQL DBMS. Experimental results show that our profiling technique is close to accurate, requires acceptable amount of time, and the detection mechanism incurs low runtime overhead.",
      "Keywords": "Anomaly detection | Application profile | Database | Insider attacks | SQL injection",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Bossi, Lorenzo;Bertino, Elisa;Hussain, Syed Rafiul",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018948715",
      "Primary study DOI": "10.1109/TSE.2016.2592913",
      "Title": "Efficient Dynamic Updates of Distributed Components Through Version Consistency",
      "Abstract": "Modern component-based distributed software systems are increasingly required to offer non-stop service and thus their updates must be carried out at runtime. Different authors have already proposed solutions for the safe management of dynamic updates. Our contribution aims at improving their efficiency without compromising safety. We propose a new criterion, called version consistency, which defines when a dynamic update can be safely and efficiently applied to the components that execute distributed transactions. Version consistency ensures that distributed transactions be served as if they were operated on a single coherent version of the system despite possible concurrent updates. The paper presents a distributed algorithm for checking version consistency efficiently, formalizes the proposed approach by means of a graph transformation system, and verifies its correctness through model checking. The paper also presents ConUp, a novel prototype framework that supports the approach and offers a viable, concrete solution for the use of version consistency. Both the approach and ConUp are evaluated on a significant third-party application. Obtained results witness the benefits of the proposed solution with respect to both timeliness and disruption.",
      "Keywords": "Component-based distributed system | dynamic update | version-consistency",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Baresi, Luciano;Ghezzi, Carlo;Ma, Xiaoxing;Manna, Valerio Panzica La",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019005181",
      "Primary study DOI": "10.1109/TSE.2016.2597136",
      "Title": "Test Oracle Strategies for Model-Based Testing",
      "Abstract": "Testers use model-based testing to design abstract tests from models of the system's behavior. Testers instantiate the abstract tests into concrete tests with test input values and test oracles that check the results. Given the same test inputs, more elaborate test oracles have the potential to reveal more failures, but may also be more costly. This research investigates the ability for test oracles to reveal failures. We define ten new test oracle strategies that vary in amount and frequency of program state checked. We empirically compared them with two baseline test oracle strategies. The paper presents several main findings. (1) Test oracles must check more than runtime exceptions because checking exceptions alone is not effective at revealing failures. (2) Test oracles do not need to check the entire output state because checking partial states reveals nearly as many failures as checking entire states. (3) Test oracles do not need to check program states multiple times because checking states less frequently is as effective as checking states more frequently. In general, when state machine diagrams are used to generate tests, checking state invariants is a reasonably effective low cost approach to creating test oracles.",
      "Keywords": "Model-Based testing | RIPR model | Subsumption | Test automation | Test oracle | Test oracle strategy",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Li, Nan;Offutt, Jeff",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018977446",
      "Primary study DOI": "10.1109/TSE.2016.2592905",
      "Title": "Mining Sequences of Developer Interactions in Visual Studio for Usage Smells",
      "Abstract": "In this paper, we present a semi-automatic approach for mining a large-scale dataset of IDE interactions to extract usage smells, i.e., inefficient IDE usage patterns exhibited by developers in the field. The approach outlined in this paper first mines frequent IDE usage patterns, filtered via a set of thresholds and by the authors, that are subsequently supported (or disputed) using a developer survey, in order to form usage smells. In contrast with conventional mining of IDE usage data, our approach identifies time-ordered sequences of developer actions that are exhibited by many developers in the field. This pattern mining workflow is resilient to the ample noise present in IDE datasets due to the mix of actions and events that these datasets typically contain. We identify usage patterns and smells that contribute to the understanding of the usability of Visual Studio for debugging, code search, and active file navigation, and, more broadly, to the understanding of developer behavior during these software development activities. Among our findings is the discovery that developers are reluctant to use conditional breakpoints when debugging, due to perceived IDE performance problems as well as due to the lack of error checking in specifying the conditional.",
      "Keywords": "data mining | IDE usage data | pattern mining | usability analysis",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Damevski, Kostadin;Shepherd, David C.;Schneider, Johannes;Pollock, Lori",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018998019",
      "Primary study DOI": "10.1109/TSE.2016.2597849",
      "Title": "An Improved SDA Based Defect Prediction Framework for Both Within-Project and Cross-Project Class-Imbalance Problems",
      "Abstract": "Background. Solving the class-imbalance problem of within-project software defect prediction (SDP) is an important research topic. Although some class-imbalance learning methods have been presented, there exists room for improvement. For cross-project SDP, we found that the class-imbalanced source usually leads to misclassification of defective instances. However, only one work has paid attention to this cross-project class-imbalance problem. Objective. We aim to provide effective solutions for both within-project and cross-project class-imbalance problems. Method. Subclass discriminant analysis (SDA), an effective feature learning method, is introduced to solve the problems. It can learn features with more powerful classification ability from original metrics. For within-project prediction, we improve SDA for achieving balanced subclasses and propose the improved SDA (ISDA) approach. For cross-project prediction, we employ the semi-supervised transfer component analysis (SSTCA) method to make the distributions of source and target data consistent, and propose the SSTCA+ISDA prediction approach. Results. Extensive experiments on four widely used datasets indicate that: 1) ISDA-based solution performs better than other state-of-the-art methods for within-project class-imbalance problem; 2) SSTCA+ISDA proposed for cross-project class-imbalance problem significantly outperforms related methods. Conclusion. Within-project and cross-project class-imbalance problems greatly affect prediction performance, and we provide a unified and effective prediction framework for both problems.",
      "Keywords": "Cross-Project class-imbalance | Improved subclass discriminant analysis (ISDA) | ISDA based defect prediction framework | Software defect prediction (SDP) | Within-Project class-imbalance",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Jing, Xiao Yuan;Wu, Fei;Dong, Xiwei;Xu, Baowen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018943829",
      "Primary study DOI": "10.1109/TSE.2016.2592907",
      "Title": "An Enhanced Bailout Protocol for Mixed Criticality Embedded Software",
      "Abstract": "To move mixed criticality research into industrial practice requires models whose run-time behaviour is acceptable to systems engineers. Certain aspects of current models, such as abandoning lower criticality tasks when certain situations arise, do not give the robustness required in application domains such as the automotive and aerospace industries. In this paper a new bailout protocol is developed that still guarantees high criticality software but minimises the negative impact on lower criticality software via a timely return to normal operation. We show how the bailout protocol can be integrated with existing techniques, utilising both offline slack and online gain-time to further improve performance. Static analysis is provided for schedulability guarantees, while scenario-based evaluation via simulation is used to explore the effectiveness of the protocol.",
      "Keywords": "Fixed priority scheduling | Mixed criticality | Mode changes | Real-time systems",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Bate, Iain;Burns, Alan;Davis, Robert I.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018479817",
      "Primary study DOI": "10.1109/TSE.2016.2576454",
      "Title": "Improving Automated Bug Triaging with Specialized Topic Model",
      "Abstract": "Bug triaging refers to the process of assigning a bug to the most appropriate developer to fix. It becomes more and more difficult and complicated as the size of software and the number of developers increase. In this paper, we propose a new framework for bug triaging, which maps the words in the bug reports (i.e., the term space) to their corresponding topics (i.e., the topic space). We propose a specialized topic modeling algorithm named multi-feature topic model (MTM) which extends Latent Dirichlet Allocation (LDA) for bug triaging. MTM considers product and component information of bug reports to map the term space to the topic space. Finally, we propose an incremental learning method named TopicMiner which considers the topic distribution of a new bug report to assign an appropriate fixer based on the affinity of the fixer to the topics. We pair TopicMiner with MTM (TopicMiner^{MTM} ). We have evaluated our solution on 5 large bug report datasets including GCC, OpenOffice, Mozilla, Netbeans, and Eclipse containing a total of 227,278 bug reports. We show that TopicMiner ^{MTM} can achieve top-1 and top-5 prediction accuracies of 0.4831-0.6868, and 0.7686-0.9084, respectively. We also compare TopicMiner^{MTM} with Bugzie, LDA-KL, SVM-LDA, LDA-Activity, and Yang et al.'s approach. The results show that TopicMiner ^{MTM} on average improves top-1 and top-5 prediction accuracies of Bugzie by 128.48 and 53.22 percent, LDA-KL by 262.91 and 105.97 percent, SVM-LDA by 205.89 and 110.48 percent, LDA-Activity by 377.60 and 176.32 percent, and Yang et al.'s approach by 59.88 and 13.70 percent, respectively.",
      "Keywords": "bug triaging | Developer | feature information | topic model",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Xia, Xin;Lo, David;Ding, Ying;Al-Kofahi, Jafar M.;Nguyen, Tien N.;Wang, Xinyu",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84997314686",
      "Primary study DOI": "10.1109/TSE.2016.2584059",
      "Title": "CACheck: Detecting and Repairing Cell Arrays in Spreadsheets",
      "Abstract": "Spreadsheets are widely used by end users for numerical computation in their business. Spreadsheet cells whose computation is subject to the same semantics are often clustered in a row or column as a cell array. When a spreadsheet evolves, the cells in a cell array can degenerate due to ad hoc modifications. Such degenerated cell arrays no longer keep cells prescribing the same computational semantics, and are said to exhibit ambiguous computation smells. We propose CACheck, a novel technique that automatically detects and repairs smelly cell arrays by recovering their intended computational semantics. Our empirical study on the EUSES and Enron corpora finds that such smelly cell arrays are common. Our study also suggests that CACheck is useful for detecting and repairing real spreadsheet problems caused by smelly cell arrays. Compared with our previous work AmCheck, CACheck detects smelly cell arrays with higher precision and recall rate.",
      "Keywords": "ambiguous computation smell | cell array | Spreadsheet",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Dou, Wensheng;Xu, Chang;Cheung, S. C.;Wei, Jun",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85017682288",
      "Primary study DOI": "10.1109/TSE.2016.2584066",
      "Title": "Automating Live Update for Generic Server Programs",
      "Abstract": "The pressing demand to deploy software updates without stopping running programs has fostered much research on live update systems in the past decades. Prior solutions, however, either make strong assumptions on the nature of the update or require extensive and error-prone manual effort, factors which discourage the adoption of live update. This paper presents Mutable Checkpoint-Restart (MCR), a new live update solution for generic (multiprocess and multithreaded) server programs written in C. Compared to prior solutions, MCR can support arbitrary software updates and automate most of the common live update operations. The key idea is to allow the running version to safely reach a quiescent state and then allow the new version to restart as similarly to a fresh program initialization as possible, relying on existing code paths to automatically restore the old program threads and reinitialize a relevant portion of the program data structures. To transfer the remaining data structures, MCR relies on a combination of precise and conservative garbage collection techniques to trace all the global pointers and apply the required state transformations on the fly. Experimental results on popular server programs (Apache httpd, nginx, OpenSSH and vsftpd) confirm that our techniques can effectively automate problems previously deemed difficult at the cost of negligible performance overhead (2 percent on average) and moderate memory overhead (3.9\\times on average, without optimizations).",
      "Keywords": "checkpoint-restart | DSU | garbage collection | Live update | quiescence detection | record-replay",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Giuffrida, Cristiano;Iorgulescu, Clin;Tamburrelli, Giordano;Tanenbaum, Andrew S.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018502021",
      "Primary study DOI": "10.1109/TSE.2016.2584063",
      "Title": "Dependence Guided Symbolic Execution",
      "Abstract": "Symbolic execution is a powerful technique for systematically exploring the paths of a program and generating the corresponding test inputs. However, its practical usage is often limited by the path explosion problem, that is, the number of explored paths usually grows exponentially with the increase of program size. In this paper, we argue that for the purpose of fault detection it is not necessary to systematically explore the paths, and propose a new symbolic execution approach to mitigate the path explosion problem by predicting and eliminating the redundant paths based on symbolic value. Our approach can achieve the equivalent fault detection capability as traditional symbolic execution without exhaustive path exploration. In addition, we develop a practical implementation called Dependence Guided Symbolic Execution (DGSE) to soundly approximate our approach. Through exploiting program dependence, DGSE can predict and eliminate the redundant paths at a reasonable computational cost. Our empirical study shows that the redundant paths are abundant and widespread in a program. Compared with traditional symbolic execution, DGSE only explores 6.96 to 96.57 percent of the paths and achieves a speedup of 1.02 \\times to 49.56\\times. We have released our tool and the benchmarks used to evaluate DGSE^\\ast.",
      "Keywords": "path coverage | program dependence | Symbolic execution",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Wang, Haijun;Liu, Ting;Guan, Xiaohong;Shen, Chao;Zheng, Qinghua;Yang, Zijiang",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013096214",
      "Primary study DOI": "10.1109/TSE.2016.2586066",
      "Title": "A Study of Causes and Consequences of Client-Side JavaScript Bugs",
      "Abstract": "Client-side JavaScript is widely used in web applications to improve user-interactivity and minimize client-server communications. Unfortunately, JavaScript is known to be error-prone. While prior studies have demonstrated the prevalence of JavaScript faults, no attempts have been made to determine their causes and consequences. The goal of our study is to understand the root causes and impact of JavaScript faults and how the results can impact JavaScript programmers, testers and tool developers. We perform an empirical study of 502 bug reports from 19 bug repositories. The bug reports are thoroughly examined to classify and extract information about each bug' cause (the error) and consequence (the failure and impact). Our results show that the majority (68 percent) of JavaScript faults are DOM-related, meaning they are caused by faulty interactions of the JavaScript code with the Document Object Model (DOM). Further, 80 percent of the highest impact JavaScript faults are DOM-related. Finally, most JavaScript faults originate from programmer mistakes committed in the JavaScript code itself, as opposed to other web application components. These results indicate that JavaScript programmers and testers need tools that can help them reason about the DOM. Additionally, developers can use the error patterns we found to design more powerful static analysis tools for JavaScript.",
      "Keywords": "bug reports | Document Object Model (DOM) | empirical study | Faults | JavaScript",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Ocariza, Frolin S.;Bajaj, Kartik;Pattabiraman, Karthik;Mesbah, Ali",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013078272",
      "Primary study DOI": "10.1109/TSE.2016.2589242",
      "Title": "DECAF: A Platform-Neutral Whole-System Dynamic Binary Analysis Platform",
      "Abstract": "Dynamic binary analysis is a prevalent and indispensable technique in program analysis. While several dynamic binary analysis tools and frameworks have been proposed, all suffer from one or more of: prohibitive performance degradation, a semantic gap between the analysis code and the program being analyzed, architecture/OS specificity, being user-mode only, and lacking APIs. We present DECAF, a virtual machine based, multi-target, whole-system dynamic binary analysis framework built on top of QEMU. DECAF provides Just-In-Time Virtual Machine Introspection and a plugin architecture with a simple-to-use event-driven programming interface. DECAF implements a new instruction-level taint tracking engine at bit granularity, which exercises fine control over the QEMU Tiny Code Generator (TCG) intermediate representation to accomplish on-the-fly optimizations while ensuring that the taint propagation is sound and highly precise. We perform a formal analysis of DECAF's taint propagation rules to verify that most instructions introduce neither false positives nor false negatives. We also present three platform-neutral plugins - Instruction Tracer, Keylogger Detector, and API Tracer, to demonstrate the ease of use and effectiveness of DECAF in writing cross-platform and system-wide analysis tools. Implementation of DECAF consists of 9,550 lines of C++ code and 10,270 lines of C code and we evaluate DECAF using CPU2006 SPEC benchmarks and show average overhead of 605 percent for system wide tainting and 12 percent for VMI.",
      "Keywords": "Dynamic binary analysis | dynamic taint analysis | virtual machine introspection",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Henderson, Andrew;Yan, Lok Kwong;Hu, Xunchao;Prakash, Aravind;Yin, Heng;McCamant, Stephen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013031980",
      "Primary study DOI": "10.1109/TSE.2016.2587646",
      "Title": "Automated Synthesis and Dynamic Analysis of Tradeoff Spaces for Object-Relational Mapping",
      "Abstract": "Producing software systems that achieve acceptable tradeoffs among multiple non-functional properties remains a significant engineering problem. We propose an approach to solving this problem that combines synthesis of spaces of design alternatives from logical specifications and dynamic analysis of each point in the resulting spaces. We hypothesize that this approach has potential to help engineers understand important tradeoffs among dynamically measurable properties of system components at meaningful scales within reach of existing synthesis tools. To test this hypothesis, we developed tools to enable, and we conducted, a set of experiments in the domain of relational databases for object-oriented data models. For each of several data models, we used our approach to empirically test the accuracy of a published suite of metrics to predict tradeoffs based on the static schema structure alone. The results show that exhaustive synthesis and analysis provides a superior view of the tradeoff spaces for such designs. This work creates a path forward toward systems that achieve significantly better tradeoffs among important system properties.",
      "Keywords": "dynamic analysis | ORM | relational logic | Specification-driven synthesis | static analysis | tradespace analysis",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Bagheri, Hamid;Tang, Chong;Sullivan, Kevin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013074755",
      "Primary study DOI": "10.1109/TSE.2016.2584053",
      "Title": "How Social and Communication Channels Shape and Challenge a Participatory Culture in Software Development",
      "Abstract": "Software developers use many different communication tools and channels in their work. The diversity of these tools has dramatically increased over the past decade and developers now have access to a wide range of socially enabled communication channels and social media to support their activities. The availability of such social tools is leading to a participatory culture of software development, where developers want to engage with, learn from, and co-create software with other developers. However, the interplay of these social channels, as well as the opportunities and challenges they may create when used together within this participatory development culture are not yet well understood. In this paper, we report on a large-scale survey conducted with 1,449 GitHub users. We discuss the channels these developers find essential to their work and gain an understanding of the challenges they face using them. Our findings lay the empirical foundation for providing recommendations to developers and tool designers on how to use and improve tools for software developers.",
      "Keywords": "communication | CSCW | Participatory culture | social media | software engineering",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Storey, Margaret Anne;Zagalsky, Alexey;Filho, Fernando Figueira;Singer, Leif;German, Daniel M.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013119887",
      "Primary study DOI": "10.1109/TSE.2016.2591536",
      "Title": "ARENA: An Approach for the Automated Generation of Release Notes",
      "Abstract": "Release notes document corrections, enhancements, and, in general, changes that were implemented in a new release of a software project. They are usually created manually and may include hundreds of different items, such as descriptions of new features, bug fixes, structural changes, new or deprecated APIs, and changes to software licenses. Thus, producing them can be a time-consuming and daunting task. This paper describes ARENA (Automatic RElease Notes generAtor), an approach for the automatic generation of release notes. ARENA extracts changes from the source code, summarizes them, and integrates them with information from versioning systems and issue trackers. ARENA was designed based on the manual analysis of 990 existing release notes. In order to evaluate the quality of the release notes automatically generated by ARENA, we performed four empirical studies involving a total of 56 participants (48 professional developers and eight students). The obtained results indicate that the generated release notes are very good approximations of the ones manually produced by developers and often include important information that is missing in the manually created release notes.",
      "Keywords": "Release notes | software documentation | software evolution",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Moreno, Laura;Bavota, Gabriele;Di Penta, Massimiliano;Oliveto, Rocco;Marcus, Andrian;Canfora, Gerardo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85009874260",
      "Primary study DOI": "10.1109/TSE.2016.2584050",
      "Title": "An Empirical Comparison of Model Validation Techniques for Defect Prediction Models",
      "Abstract": "Defect prediction models help software quality assurance teams to allocate their limited resources to the most defect-prone modules. Model validation techniques, such as k-fold cross-validation, use historical data to estimate how well a model will perform in the future. However, little is known about how accurate the estimates of model validation techniques tend to be. In this paper, we investigate the bias and variance of model validation techniques in the domain of defect prediction. Analysis of 101 public defect datasets suggests that 77 percent of them are highly susceptible to producing unstable results-selecting an appropriate model validation technique is a critical experimental design choice. Based on an analysis of 256 studies in the defect prediction literature, we select the 12 most commonly adopted model validation techniques for evaluation. Through a case study of 18 systems, we find that single-repetition holdout validation tends to produce estimates with 46-229 percent more bias and 53-863 percent more variance than the top-ranked model validation techniques. On the other hand, out-of-sample bootstrap validation yields the best balance between the bias and variance of estimates in the context of our study. Therefore, we recommend that future defect prediction studies avoid single-repetition holdout validation, and instead, use out-of-sample bootstrap validation.",
      "Keywords": "bootstrap validation | cross validation | Defect prediction models | holdout validation | model validation techniques",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Tantithamthavorn, Chakkrit;McIntosh, Shane;Hassan, Ahmed E.;Matsumoto, Kenichi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85025128979",
      "Primary study DOI": "10.1109/TSE.2017.2654255",
      "Title": "Model transformation modularization as a many-objective optimization problem",
      "Abstract": "Model transformation programs are iteratively refined, restructured, and evolved due to many reasons such as fixing bugs and adapting existing transformation rules to new metamodels version. Thus, modular design is a desirable property for model transformations as it can significantly improve their evolution, comprehensibility, maintainability, reusability, and thus, their overall quality. Although language support for modularization of model transformations is emerging, model transformations are created as monolithic artifacts containing a huge number of rules. To the best of our knowledge, the problem of automatically modularizing model transformation programs was not addressed before in the current literature. These programs written in transformation languages, such as ATL, are implemented as one main module including a huge number of rules. To tackle this problem and improve the quality and maintainability of model transformation programs, we propose an automated search-based approach to modularize model transformations based on higher-order transformations. Their application and execution is guided by our search framework which combines an in-place transformation engine and a search-based algorithm framework. We demonstrate the feasibility of our approach by using ATL as concrete transformation language and NSGA-III as search algorithm to find a trade-off between different well-known conflicting design metrics for the fitness functions to evaluate the generated modularized solutions. To validate our approach, we apply it to a comprehensive dataset of model transformations. As the study shows, ATL transformations can be modularized automatically, efficiently, and effectively by our approach. We found that, on average, the majority of recommended modules, for all the ATL programs, by NSGA-III are considered correct with more than 84 percent of precision and 86 percent of recall when compared to manual solutions provided by active developers. The statistical analysis of our experiments over several runs shows that NSGA-III performed significantly better than multi-objective algorithms and random search. We were not able to compare with existing model transformations modularization approaches since our study is the first to address this problem. The software developers considered in our experiments confirm the relevance of the recommended modularization solutions for several maintenance activities based on different scenarios and interviews.",
      "Keywords": "ATL | MDE | Model transformation | Modularization | NSGA-III | SBSE",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Fleck, Martin;Troya, Javier;Kessentini, Marouane;Wimmer, Manuel;Alkhazi, Bader",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85009892148",
      "Primary study DOI": "10.1109/TSE.2016.2560811",
      "Title": "Nopol: Automatic Repair of Conditional Statement Bugs in Java Programs",
      "Abstract": "We propose Nopol, an approach to automatic repair of buggy conditional statements (i.e., if-then-else statements). This approach takes a buggy program as well as a test suite as input and generates a patch with a conditional expression as output. The test suite is required to contain passing test cases to model the expected behavior of the program and at least one failing test case that reveals the bug to be repaired. The process of Nopol consists of three major phases. First, Nopol employs angelic fix localization to identify expected values of a condition during the test execution. Second, runtime trace collection is used to collect variables and their actual values, including primitive data types and objected-oriented features (e.g., nullness checks), to serve as building blocks for patch generation. Third, Nopol encodes these collected data into an instance of a Satisfiability Modulo Theory (SMT) problem; then a feasible solution to the SMT instance is translated back into a code patch. We evaluate Nopol on 22 real-world bugs (16 bugs with buggy if conditions and six bugs with missing preconditions) on two large open-source projects, namely Apache Commons Math and Apache Commons Lang. Empirical analysis on these bugs shows that our approach can effectively fix bugs with buggy if conditions and missing preconditions. We illustrate the capabilities and limitations of Nopol using case studies of real bug fixes.",
      "Keywords": "Automatic repair | fault localization | patch generation | SMT",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Xuan, Jifeng;Martinez, Matias;DeMarco, Favio;Clement, Maxime;Marcote, Sebastian Lamelas;Durieux, Thomas;Le Berre, Daniel;Monperrus, Martin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85009872879",
      "Primary study DOI": "10.1109/TSE.2016.2560842",
      "Title": "Timed Automata Modeling and Verification for Publish-Subscribe Structures Using Distributed Resources",
      "Abstract": "In this paper we present a Timed Automata model for the Publish/Subscribe paradigm in the context of Web Service Compositions with distributed resources, on the basis of an algebraic language inspired by the WSRF standard constructions. This framework allows a set of participants in a Web Service composition to interact with one another and also to manage a collection of distributed resources. The model includes operations for clients to publish, discover and subscribe to resources, so as to be notified when the resource property values fulfill certain conditions (topic-based subscription). Simulation and model-checking techniques can therefore be applied to the obtained network of timed automata, in order to check whether certain properties of interest are satisfied. A specific case study is finally presented to illustrate the model and the verification of the relevant properties on the obtained timed automata model.",
      "Keywords": "formal modeling | model checking | Publish/subscribe | timed automata | verification",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Valero, Valentin;Diaz, Gregorio;Cambronero, Maria Emilia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85009895253",
      "Primary study DOI": "10.1109/TSE.2016.2576451",
      "Title": "Process Aspects and Social Dynamics of Contemporary Code Review: Insights from Open Source Development and Industrial Practice at Microsoft",
      "Abstract": "Many open source and commercial developers practice contemporary code review, a lightweight, informal, tool-based code review process. To better understand this process and its benefits, we gathered information about code review practices via surveys of open source software developers and developers from Microsoft. The results of our analysis suggest that developers spend approximately 10-15 percent of their time in code reviews, with the amount of effort increasing with experience. Developers consider code review important, stating that in addition to finding defects, code reviews offer other benefits, including knowledge sharing, community building, and maintaining code quality. The quality of the code submitted for review helps reviewers form impressions about their teammates, which can influence future collaborations. We found a large amount of similarity between the Microsoft and OSS respondents. One interesting difference is that while OSS respondents view code review as an important method of impression formation, Microsoft respondents found knowledge dissemination to be more important. Finally, we found little difference between distributed and co-located Microsoft teams. Our findings identify the following key areas that warrant focused research: 1) exploring the non-technical benefits of code reviews, 2) helping developers in articulating review comments, and 3) assisting reviewers' program comprehension during code reviews.",
      "Keywords": "Code review | commercial projects | open source | OSS | peer impressions | survey",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Bosu, Amiangshu;Carver, Jeffrey C.;Bird, Christian;Orbeck, Jonathan;Chockley, Christopher",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85009851940",
      "Primary study DOI": "10.1109/TSE.2016.2564959",
      "Title": "Interaction Models and Automated Control under Partial Observable Environments",
      "Abstract": "The problem of automatically constructing a software component such that when executed in a given environment satisfies a goal, is recurrent in software engineering. Controller synthesis is a field which fits into this vision. In this paper we study controller synthesis for partially observable LTS models. We exploit the link between partially observable control and non-determinism and show that, unlike fully observable LTS or Kripke structure control problems, in this setting the existence of a solution depends on the interaction model between the controller-to-be and its environment. We identify two interaction models, namely Interface Automata and Weak Interface Automata, define appropriate control problems and describe synthesis algorithms for each of them.",
      "Keywords": "controller synthesis | imperfect-information games | LTS",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Ciolek, Daniel;Braberman, Victor;Dippolito, Nicolas;Piterman, Nir;Uchitel, Sebastian",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85034272704",
      "Primary study DOI": "10.1109/TSE.2017.2770122",
      "Title": "Toward a Smell-aware Bug Prediction Model",
      "Abstract": "Code smells are symptoms of poor design and implementation choices. Previous studies empirically assessed the impact of smells on code quality and clearly indicate their negative impact on maintainability, including a higher bug-proneness of components affected by code smells. In this paper, we capture previous findings on bug-proneness to build a specialized bug prediction model for smelly classes. Specifically, we evaluate the contribution of a measure of the severity of code smells (i.e., code smell intensity) by adding it to existing bug prediction models based on both product and process metrics, and comparing the results of the new model against the baseline models. Results indicate that the accuracy of a bug prediction model increases by adding the code smell intensity as predictor. We also compare the results achieved by the proposed model with the ones of an alternative technique which considers metrics about the history of code smells in files, finding that our model works generally better. However, we observed interesting complementarities between the set of buggy and smelly classes correctly classified by the two models. By evaluating the actual information gain provided by the intensity index with respect to the other metrics in the model, we found that the intensity index is a relevant feature for both product and process metrics-based models. At the same time, the metric counting the average number of code smells in previous versions of a class considered by the alternative model is also able to reduce the entropy of the model. On the basis of this result, we devise and evaluate a smell-aware combined bug prediction model that included product, process, and smell-related features. We demonstrate how such model classifies bug-prone code components with an F-Measure at least 13 percent higher than the existing state-of-the-art models.",
      "Keywords": "bug prediction | Code smells | empirical study | mining software repositories",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-02-01",
      "Publication type": "Article",
      "Authors": "Palomba, Fabio;Zanoni, Marco;Fontana, Francesca Arcelli;De Lucia, Andrea;Oliveto, Rocco",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030217493",
      "Primary study DOI": "10.1109/TSE.2017.2752171",
      "Title": "The Scent of a Smell: An Extensive Comparison between Textual and Structural Smells",
      "Abstract": "Code smells are symptoms of poor design or implementation choices that have a negative effect on several aspects of software maintenance and evolution, such as program comprehension or change-and fault-proneness. This is why researchers have spent a lot of effort on devising methods that help developers to automatically detect them in source code. Almost all the techniques presented in literature are based on the analysis of structural properties extracted from source code, although alternative sources of information (e.g., textual analysis) for code smell detection have also been recently investigated. Nevertheless, some studies have indicated that code smells detected by existing tools based on the analysis of structural properties are generally ignored (and thus not refactored) by the developers. In this paper, we aim at understanding whether code smells detected using textual analysis are perceived and refactored by developers in the same or different way than code smells detected through structural analysis. To this aim, we set up two different experiments. We have first carried out a software repository mining study to analyze how developers act on textually or structurally detected code smells. Subsequently, we have conducted a user study with industrial developers and quality experts in order to qualitatively analyze how they perceive code smells identified using the two different sources of information. Results indicate that textually detected code smells are easier to identify and for this reason they are considered easier to refactor with respect to code smells detected using structural properties. On the other hand, the latter are often perceived as more severe, but more difficult to exactly identify and remove.",
      "Keywords": "Code smells | empirical study | mining software repositories",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2018-10-01",
      "Publication type": "Article",
      "Authors": "Palomba, Fabio;Panichella, Annibale;Zaidman, Andy;Oliveto, Rocco;De Lucia, Andrea",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028363124",
      "Primary study DOI": "10.1016/j.infsof.2017.08.006",
      "Title": "A graphical user interface for presenting integrated development environment command recommendations: Design, evaluation, and implementation",
      "Abstract": "Context A set of algorithms exist to generate integrated development environment (IDE) command recommendations. The recommendations are aimed at improving software developer's interaction with an IDE. Even though the interface is a critical element of every recommender system, we are not aware of any existing graphical user interface to present such recommendations. Objective This paper describes and evaluates a novel design of a graphical user interface to recommend commands within an IDE. The interface contains a description of the suggested command, an explanation of why the command is recommended, and a command usage example. Method The proposed design is based on the analysis of guidelines identified in the literature. Its acceptance and usability were evaluated through a user study with 36 software developers and semi-structured interviews with 11 software developers. Results The results indicate that the suggested interface is well accepted, but it can be further improved. Through the interviews and the implementation of the interface, we identified a series of requirements important for the development of future IDE command recommender systems. Conclusions This paper shows that a convenient graphical user interface is critical to achieve high acceptance of IDE command recommendations. Our work also illustrates steps useful for undertaking user studies related to IDE command recommendations in a practical setting without human intervention. A future step is to evaluate the interface within the business environment, where recommendations are generated and presented in an IDE used by practicing software developers as part of their normal workday.",
      "Keywords": "Command | Functionality | Integrated development environment | Recommender system | Software development | User interface",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Gasparic, Marko;Janes, Andrea;Ricci, Francesco;Murphy, Gail C.;Gurbanov, Tural",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028326819",
      "Primary study DOI": "10.1016/j.infsof.2017.08.001",
      "Title": "A formal approach to derive an aspect oriented programming-based implementation of a secure access control filter",
      "Abstract": "Context: Nowadays, Information Systems (IS) are at the heart of most companies and constitute then a critical element that needs an adequate attention regarding security issues of sensitive data it manages. Objective: This paper presents a formal approach for the development of a filter to secure access to sensitive resources of information systems. Method: The proposed approach consists of three complementary steps. Designers start by modeling the functionalities of the system and its security requirements using dedicated UML diagrams. These diagrams are then automatically translated into a formal B specification suitable not only for reasoning about data integrity checking but also for the derivation of a trustworthy implementation. Indeed, a formal refinement process is applied on the generated B specification to obtain a relational-like B implementation which is then translated into an AspectJ implementation, connected to a SQL Server (release 2014) relational database system. Such a generation is performed following the aspect oriented programming paradigm which permits a separation of concerns by making a clear distinction between functional and security aspects. Results: A systematic formal approach to derive a secure filter that regulates access to the sensitive data of an information system. The filter considers both static and dynamic access rules. A tool that supports the proposed approach is also provided. Conclusion: The approach has been applied on several case studies that demonstrate that the development of a tool permits to free the developers from tedious and error-prone tasks since they have just to push a button to generate the AspectJ code of an application.",
      "Keywords": "",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Mammar, Amel;Nguyen, Thi Mai;Laleau, Régine",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021967431",
      "Primary study DOI": "10.1016/j.infsof.2017.07.003",
      "Title": "Automated change-prone class prediction on unlabeled dataset using unsupervised method",
      "Abstract": "Context Software change-prone class prediction can enhance software decision making activities during software maintenance (e.g., resource allocating). Researchers have proposed many change-prone class prediction approaches and most are effective on labeled datasets (projects with historical labeled data). These approaches usually build a supervised model by learning from historical labeled data. However, a major challenge is that this typical change-prone prediction setting cannot be used for unlabeled datasets (e.g., new projects or projects with limited historical data). Although the cross-project prediction is a solution on unlabeled dataset, it needs the prior labeled data from other projects and how to select the appropriate training project is a difficult task. Objective We aim to build a change-prone class prediction model on unlabeled datasets without the need of prior labeled data. Method We propose to tackle this task by adopting a state-of-art unsupervised method, namely CLAMI. In addition, we propose a novel unsupervised approach CLAMI+ by extending CLAMI. The key idea is to enable change-prone class prediction on unlabeled dataset by learning from itself. Results The experiments among 14 open source projects show that the unsupervised methods achieve comparable results to the typical supervised within-project and cross-project prediction baselines in average and the proposed CLAMI+ slightly improves the CLAMI method in average. Conclusion Our method discovers that it is effective for building change-prone class prediction model by using unsupervised method. It is convenient for practical usage in industry, since it does not need prior labeled data.",
      "Keywords": "Change-prone prediction | Software maintenance | Unlabeled dataset | Unsupervised prediction",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Yan, Meng;Zhang, Xiaohong;Liu, Chao;Xu, Ling;Yang, Mengning;Yang, Dan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028361709",
      "Primary study DOI": "10.1016/j.infsof.2017.08.003",
      "Title": "Contextual attributes impacting the effectiveness of requirements elicitation Techniques: Mapping theoretical and empirical research",
      "Abstract": "Background: Software engineers can utilise a myriad of elicitation techniques to capture relevant information in order to specify requirements. The effectiveness of these techniques varies depending on the context in which the elicitation takes place. So, it is important to identify the attributes that represent this context. Objective: This paper aims to match theoretical to empirical research on contextual attributes that influence elicitation technique effectiveness. Method: We conduct a systematic mapping study to identify proposed attributes (by theoretical works) and attributes studied empirically. Then we map empirical results with theoretical proposals. Results: 60% of theoretically proposed attributes have been studied empirically. There seems to be some degree of coordination between theory and empiricism. However, there is empirical confirmation of the impact of only a third of the theoretically proposed attributes. Conclusions: These results call for more empirical research in order to evaluate beliefs with respect to elicitation techniques.",
      "Keywords": "Contextual attributes | Elicitation methods | Requirements elicitation | Systematic mapping study",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-12-01",
      "Publication type": "Review",
      "Authors": "Carrizo, Dante;Dieste, Oscar;Juristo, Natalia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026254905",
      "Primary study DOI": "10.1016/j.infsof.2017.07.012",
      "Title": "Experimental comparison of approaches for checking completeness of test suites from finite state machines",
      "Abstract": "Context Many approaches have been proposed for checking test suite completeness for Finite State Machines (FSMs). Some approaches provide sufficient conditions whereas others give necessary and sufficient conditions for test suite completeness. One method, called the CONF method, is based on sufficient conditions, and relies on a search for confirmed sets when checking completeness. If a confirmed set cannot be found, then the outcome is inconclusive. Another method, the SIM method, is based on the notion of simulation relations, and relies on necessary and sufficient conditions when checking test suite completeness. The SIM method always returns conclusive verdicts about suite completeness. Objective In this work, we describe experimental results comparing these two methods. We also investigate when both methods can be combined for checking completeness of test suites. Method We evaluate both strategies according to different parameters of the FSMs, such as the number of states and the number of transitions in the FSM models, the size of input and output alphabets of the FSM models, as well as the size of the test suites. We also report on the relative rates of conclusive and inconclusive verdicts when using both methods. Results We see that these methods are complementary, which allows for a combined strategy: the CONF method is the fastest in terms of processing time, while the SIM method is not as scalable in terms of the size of the specifications. Conclusion The experimental results indicated a substantial difference for the rate of positive verdicts obtained by the SIM method when compared with the number of positive answers returned by the CONF method.",
      "Keywords": "Completeness conditions | Confirmed sets | Experiments | Simulation relations | Test suite completeness",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Bonifacio, Adilson;Moura, Arnaldo;Simao, Adenilso",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85025631316",
      "Primary study DOI": "10.1016/j.infsof.2017.07.006",
      "Title": "Developing software systems to Big Data platform based on MapReduce model: An approach based on Model Driven Engineering",
      "Abstract": "Context: The need to analyze a large volume and variety of data for the purpose of extracting information has been promoting investments in Big Data, e.g., for storage, analysis and, more recently, methodologies and approaches for software system development for Big Data platforms. The application of software engineering for Big Data is recent and emerging, so in the literature we find a number of challenges and opportunities related to Big Data, but few practical approaches. Objective In this paper, we propose a practical approach based on MDE (Model Driven Engineering) to support the semi-automated development of software systems for Big Data platform that use MapReduce model. Method The proposed approach consists of framework, process, metamodels, visual Alf, transformation definitions written in ATL and Eclipse IDE plug-in. The proposed framework uses concepts of MDE, Weaving and software development based on Y. Our proposed process guides the use of our approach. A graphical notation and extended metamodel for Alf (i.e. visual Alf) assign executable behavior for UML or DSLs. An Eclipse IDE plug-in implements our approach. Results We show the applicability of the proposed approach through an illustrative example. Conclusion Our approach brings a contribution because the development of software systems is assisted by models which preserves the business logic and adds Big Data features throughout the development process.",
      "Keywords": "Big Data | Framework | Metamodels | Model Driven Engineering",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Osvaldo, S. Sousa;Lopes, Denivaldo;Silva, Aristófanes C.;Abdelouahab, Zair",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026813040",
      "Primary study DOI": "10.1016/j.infsof.2017.07.015",
      "Title": "Software effort estimation based on open source projects: Case study of Github",
      "Abstract": "Context Managers usually want to pre-estimate the effort of a new project for reasonably dividing their limited resources. In reality, it is common practice to train a prediction model based on effort datasets to predict the effort required by a project. Sufficient data is the basis for training a good estimator, yet most of the data owners are unwilling to share their closed source project (CSP) effort data due to the privacy concerns, which means that we can only obtain a small number of effort data. Effort estimator built on the limited data usually cannot satisfy the practical requirement. Objective We aim to provide a method which can be used to collect sufficient data for solving the problem of lack of training data when building an effort estimation model. Method We propose to mine GitHub to collect sufficient and diverse real-life effort data for effort estimation. Specifically, we first demonstrate the feasibility of our cost metrics (including functional point analysis and personnel factors). In particular, we design a quantitative method for evaluating the personnel metrics based on GitHub data. Then we design a samples incremental approach based on AdaBoost and Classification And Regression Tree (ABCART) to make the collected dataset owns dynamic expansion capability. Results Experimental results on the collected dataset show that: (1) the personnel factor is helpful for improving the performance of the effort estimation. (2) the proposed ABCART algorithm can increase the samples of the collected dataset online. (3) the estimators built on the collected data can achieve comparable performance with those of the estimators which built on existing effort datasets. Conclusions Effort estimation based on Open Source Project (OSP) is an effective way for getting the effort required by a new project, especially for the case of lacking training data.",
      "Keywords": "AdaBoost | Automated function point | Effort data collection | Open source project | Software effort estimation",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Qi, Fumin;Jing, Xiao Yuan;Zhu, Xiaoke;Xie, Xiaoyuan;Xu, Baowen;Ying, Shi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028762206",
      "Primary study DOI": "10.1016/j.infsof.2017.08.008",
      "Title": "How developers perceive smells in source code: A replicated study",
      "Abstract": "Context. In recent years, smells, also referred to as bad smells, have gained popularity among developers. However, it is still not clear how harmful they are perceived from the developers’ point of view. Many developers talk about them, but only few know what they really are, and even fewer really take care of them in their source code. Objective. The goal of this work is to understand the perceived criticality of code smells both in theory, when reading their description, and in practice. Method. We executed an empirical study as a differentiated external replication of two previous studies. The studies were conducted as surveys involving only highly experienced developers (63 in the first study and 41 in the second one). First the perceived criticality was analyzed by proposing the description of the smells, then different pieces of code infected by the smells were proposed, and finally their ability to identify the smells in the analyzed code was tested. Results. According to our knowledge, this is the largest study so far investigating the perception of code smells with professional software developers. The results show that developers are very concerned about code smells in theory, nearly always considering them as harmful or very harmful (17 out of 23 smells). However, when they were asked to analyze an infected piece of code, only few infected classes were considered harmful and even fewer were considered harmful because of the smell. Conclusions. The results confirm our initial hypotheses that code smells are perceived as more critical in theory but not as critical in practice.",
      "Keywords": "Antipatterns | Bad smells | Code smells | Refactoring | Software maintenance",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Taibi, Davide;Janes, Andrea;Lenarduzzi, Valentina",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026779685",
      "Primary study DOI": "10.1016/j.infsof.2017.07.013",
      "Title": "Predicting move method refactoring opportunities in object-oriented code",
      "Abstract": "Context Refactoring is the maintenance process of restructuring software source code to improve its quality without changing its external behavior. Move Method Refactoring (MMR) refers to moving a method from one class to the class in which the method is used the most often. Manually inspecting and analyzing the source code of the system under consideration to determine the methods in need of MMR is a costly and time-consuming process. Existing techniques for identifying MMR opportunities have several limitations, such as scalability problems and being inapplicable in early development stages. Most of these techniques do not consider semantic relationships. Objective We introduce a measure and a corresponding model to precisely predict whether a class includes methods in need of MMR. The measure is applicable once a class has entered the early development stages without waiting for other classes to be developed. Method The proposed measure considers both the cohesion and coupling aspects of methods. In addition, the measure uses structural and semantic data available within the class of interest. A statistical technique is applied to construct prediction models for classes that include methods in need of MMR. The models are applied on seven object-oriented systems to empirically evaluate their abilities to predict MMR opportunities. Results The results show both that the prediction models based on the proposed measure had outstanding prediction abilities and that the measure was able to correctly detect more than 90% of the methods in need of MMR within the predicted classes. Conclusions The proposed measure and corresponding prediction models are expected to greatly assist software engineers both in locating classes that include methods in need of MMR and in identifying these methods within the predicted classes.",
      "Keywords": "Class quality | Logistic regression analysis | Move method refactoring | Object-oriented design",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Al Dallal, Jehad",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85022048269",
      "Primary study DOI": "10.1016/j.infsof.2017.07.004",
      "Title": "Which type of metrics are useful to deal with class imbalance in software defect prediction?",
      "Abstract": "Context There are various ways to cope with class imbalance problem which is one of the main issues of software defect prediction. Sampling algorithms are implemented on both industrial and open-source software defect prediction data sets by practitioners to wipe out imbalanced data points. Sampling algorithms, up-to-date, have been employed either static or process code metrics. Objective In this study, sampling algorithms including Virtual, SMOTE, and HSDD (hybrid sampling for defect data sets) are explored using static code and quality metrics together. Our goal is not only to lead practitioners to decide the type of the metrics in defect prediction but also provide useful information for developers to design less defective software projects. Method We ran sampling experiments with three sampling algorithms on ten data sets (from GitHub). Feature selection is applied on large features of the data sets. Using five classifiers, the performance of the data sets after sampling is compared with initial data sets. Regression analyzes are implemented on quality metrics to find the most influential metrics for detecting defect proneness. Results Regardless of the type of the sampling, prediction performances are similar. Quality metrics surpassed static code metrics with respect to training times and prediction accuracies. Conclusion Using quality metrics yields better prediction results rather than static code metrics in imbalanced data sets. As the count of project cloning increases, the number of defects decreases. Thus, approaches, related to the class imbalance, should be evaluated not only in terms of static code metrics but also for quality metrics.",
      "Keywords": "Class imbalance | Defect prediction | Process metrics | Static code metrics",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Öztürk, Muhammed Maruf",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027512992",
      "Primary study DOI": "10.1016/j.infsof.2017.07.008",
      "Title": "Reusable and generic design decisions for developing UML-based domain-specific languages",
      "Abstract": "Context: In recent years, UML-based domain-specific model languages (DSMLs) have become a popular option in model-driven development projects. However, making informed design decisions for such DSMLs involves a large number of non-trivial and inter-related options. These options concern the language-model specification, UML extension techniques, concrete-syntax language design, and modeling-tool support. Objective: In order to make the corresponding knowledge on design decisions reusable, proven design rationale from existing DSML projects must be collected, systematized, and documented using an agreed upon documentation format. Method: We applied a sequential multi-method approach to identify and to document reusable design decisions for UML-based DSMLs. The approach included a Web-based survey with 80 participants. Moreover, 80 DSML projects1 which have been identified through a prior systematic literature review, were analyzed in detail in order to identify reusable design decisions for such DSMLs. Results: We present insights on the current state of practice in documenting UML-based DSMLs (e.g., perceived barriers, documentation techniques, reuse potential) and a publicly available collection of reusable design decisions, including 35 decision options on different DSML development concerns (especially concerning the language model, concrete-syntax language design, and modeling tools). The reusable design decisions are documented using a structured documentation format (decision record). Conclusion: Our results are both, scientifically relevant (e.g. for design-space analyses or for creating classification schemas for further research on UML-based DSML development) and important for actual software engineering projects (e.g. by providing best-practice guidelines and pointers to common pitfalls).",
      "Keywords": "Design decision | Design rationale | Domain-specific language | Model-driven software development | Survey | Unified modeling language",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Hoisl, Bernhard;Sobernig, Stefan;Strembeck, Mark",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027125804",
      "Primary study DOI": "10.1016/j.infsof.2017.07.014",
      "Title": "Formal verification of ECML hybrid models with spaceex",
      "Abstract": "Context ECML is a modeling language for hybrid systems, proposed by ETRI in Korea. ECML extended the basic formalism, DEV&DESS, with uses in modeling and simulation, whereas algorithmic verification on the ECML models continues to be an on-going research task. Objective This paper proposes a verification technique to verify ECML models with SpaceEx, a verification platform for hybrid systems. It includes translation rules from ECML into the SpaceEx model. Method As SpaceEx reads linear hybrid automata, we developed translation rules from ECML models to linear hybrid automata and implemented an automatic translator ECMLtoSpaceEx. We also developed a rule checker ECML Checker to check whether an ECML model complies with assumptions and restrictions to overcome the semantic gap between the two formal languages. We performed a case study with an extension of the widely used example ‘barrel-filler system’ to demonstrate the effectiveness of our verification technique. Results The verification result shows that our verification technique can translate ECML models into SpaceEx models, and we also perform formal verification on ECML models with SpaceEx. Conclusions The proposed technique can verify ECML with support fromSpaceEx. We expect that the proposed translation rules can be used with minor modifications to translate ECML models into different notations, and thus allow for the use of verification tools other than SpaceEx.",
      "Keywords": "Automatic translation | ECML | Formal verification | Linear hybrid automata | Spaceex",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Yoon, Sanghyun;Yoo, Junbeom",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028321091",
      "Primary study DOI": "10.1016/j.infsof.2017.08.002",
      "Title": "LoCo CoCo: Automatically constructing coordination and communication networks from model-based systems engineering data",
      "Abstract": "Context: Communication and coordination are essential ingredients to successful requirements and software engineering. However, especially in large organisations, it is difficult to establish and maintain communication channels. Objective: In order to facilitate communication, we investigate automatic construction of social network models from existing requirements and systems engineering models. Method: We conducted a design science research study in three iterative cycles at a large automotive company, and evaluated the outcome based on 15 interviews with practitioners and a survey with 12 participants. Results: The resulting approach, denoted LoCo CoCo, automatically creates and visualises social networks based on selected systems engineering components of real-life, productive systems engineering models. Our results indicate that automatic construction and visualisation of social network models could be feasible and useful to overcome existing communication challenges. Conclusion: Despite a lack of quality in existing social data at the case company, practitioners found LoCo CoCo potentially helpful to overcome existing communication challenges. Additionally, the visualisation could trigger practitioners to keep their social data up to date.",
      "Keywords": "Communication | Coordination | Empirical software engineering | Requirements clarification | Systems engineering",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Mohamad, Mazen;Liebel, Grischa;Knauss, Eric",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026297110",
      "Primary study DOI": "10.1016/j.infsof.2017.07.010",
      "Title": "Modeling and measuring attributes influencing DevOps implementation in an enterprise using structural equation modeling",
      "Abstract": "Context DevOps refer to set of principles that advocate a tight integration between development and operation to achieve higher quality with faster turnaround. It is paramount to assess and measure the DevOps attributes in an enterprise. The literature provides references to these attributes but the detail assessment of these attributes and determination of the maturity of DevOps implementation is still a challenge. Objective This paper provides important insights for practitioners to assess and measure the DevOps attributes using statistical analysis and Two-way assessment. The proposed framework facilitates the detailed assessment of eighteen attributes to identify key independent attributes and measure them to determine the maturity of DevOps implementation in an enterprise. Method The relationship between eighteen attributes was examined; a structural model was established using Exploratory and Confirmatory Factor Analysis, the model was validated using Structural Equation Modelling. Key independent attributes were identified which influences other attributes and overall DevOps implementation. Using Two-way assessment, key independent attributes were measured and the maturity of the DevOps implementation was determined in an enterprise. Results Using Exploratory and Confirmatory Factor Analysis, 18 attributes were categorized under 4 latent variables namely Automation, Source Control, Cohesive Teams and Continuous Delivery. Using Structural Equation Modelling, 10 key independent attributes were determined, that influenced other attributes and overall DevOps implementation. Two-way assessment was applied to measure the key independent attributes and it was found that 4 of these attributes were performing below threshold level. Corrective actions were taken by the management team, and the revised measurement of these attributes demonstrated 40% improvement in the maturity level of DevOps implementation. Conclusion The proposed framework contributes significantly to the field of DevOps by enabling practitioners to conduct the detailed assessment and measurement of DevOps attributes to determine the maturity of DevOps implementation to achieve higher quality.",
      "Keywords": "Continuous Delivery | DevOps | Enterprise applications | Exploratory and Confirmatory Factor Analysis | Structural Equation Modelling | Two-way assessment",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Gupta, Viral;Kapur, P. K.;Kumar, Deepak",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021905032",
      "Primary study DOI": "10.1016/j.infsof.2017.06.009",
      "Title": "Key factors that influence task allocation in global software development",
      "Abstract": "Context Planning and managing task allocation in Global Software Development (GSD) projects is both critical and challenging. To date, a number of models that support task allocation have been proposed, including cost models and risk-based multi-criteria optimization models. Objective The objective of this paper is to identify the factors that influence task allocation in the GSD project management context. Method First, we implemented a formal Systematic Literature Review (SLR) approach and identified a set of factors that influence task allocation in GSD projects. Second, a questionnaire survey was developed based on the SLR, and we collected feedback from 62 industry practitioners. Results The findings of this combined SLR and questionnaire survey indicate that site technical expertise, time zone difference, resource cost, task dependency, task size and vendor reliability are the key criteria for the distribution of work units in a GSD project. The results of the t-test show that there is no significant difference between the findings of the SLR and questionnaire survey. However, the industry study data indicates that resource cost and task dependency are more important to a centralized GSD project structure while task size is a key factor in a decentralized GSD project structure. Conclusion GSD organizations should try to consider the identified task allocation factors when managing their global software development activities to better understand, plan and manage work distribution decisions.",
      "Keywords": "Empirical study | Global Software Development | Systematic Literature Review | Task Allocation",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-11-01",
      "Publication type": "Review",
      "Authors": "Mahmood, Sajjad;Anwer, Sajid;Niazi, Mahmood;Alshayeb, Mohammad;Richardson, Ita",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021212048",
      "Primary study DOI": "10.1016/j.infsof.2017.06.003",
      "Title": "Investigating comprehension and learnability aspects of use cases for software specification problems",
      "Abstract": "Context: Availability of multiple use case templates to document software requirements inevitably requires their characterization in terms of their relevance, usefulness, and the degree of the formality of the expressions. Objective: This paper reports two experimental studies that separately investigate two usability aspects, namely the comprehension and the learnability of use case templates for software specification problems. Method: We judged the comprehension aspect by evaluating the subjects’ understanding of the requirements, specified in eight different use case templates, and the ease with which the changes were made by them in the requirement specifications. The learnability aspect was judged by assessing the completeness, the correctness, and the redundancy of the use case specifications developed by the subjects using these eight use case templates for three software specification problems. Results: Our results suggested that the Kettenis's use case template was found to be significantly more understandable, and the templates by Tiwari, Yue and Somé were found to be significantly more flexible to adapt to the changes. On the learnability aspect, the way we formulated it, we found different templates to be more complete (Kettenis), correct (Somé), and non-redundant (Tiwari). Conclusion: The specifications documented using a more detailed use case template with an intermediate degree of formality can be more comprehensible and flexible to adapt to the required changes to be made in the specification. A more formal template seems to enhance the learnability as well.",
      "Keywords": "Comprehension | Experimental study | Learnability | Software specification problem | Usability aspects | Use case templates | Use cases",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Tiwari, Saurabh;Gupta, Atul",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021803651",
      "Primary study DOI": "10.1016/j.infsof.2017.06.005",
      "Title": "Understanding inactive yet available assignees in GitHub",
      "Abstract": "Context In GitHub, an issue or a pull request can be assigned to a specific assignee who is responsible for working on this issue or pull request. Due to the principle of voluntary participation, available assignees may remain inactive in projects. If assignees ever participate in projects, they are active assignees; otherwise, they are inactive yet available assignees (inactive assignees for short). Objective Our objective in this paper is to provide a comprehensive analysis of inactive yet available assignees in GitHub. Method We collect 2,374,474 records of activities in 37 popular projects, and 797,756 records of activities in 687 projects belonging to 8 organizations. We compute the percentage of inactive assignees in projects, and compare projects with and without inactive assignees. Then we analyze datasets to explore why some assignees are inactive. Finally, we send questionnaires to understand impacts of inactive assignees. Results We find that some projects have high percentage of inactive yet available assignees. For example, 66.35% of assignees never participate in the project paperclip. The project paperclip belongs to the organization thoughtbot. In the organization thoughtbot, 84.4% of projects have more than 80% of inactive assignees. We further observe that the main reason for developers being inactive assignees is that developers work for organizations and automatically become available assignees of some projects in the organizations. However, these developers do not work on projects. 37.25% of developers that we have surveyed agree that inactive assignees affect open source software development (i.e., causing unresolved issues or pull requests, and delaying software development). Conclusion Some organizations should improve team management, and carefully select developers to become assignees in projects. Future studies about assignees should be careful to perform data cleaning, since some available assignees are added by virtue of their employment and do not really work on projects.",
      "Keywords": "Assignee | GitHub | Inactive",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Jiang, Jing;Lo, David;Ma, Xinyu;Feng, Fuli;Zhang, Li",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021928111",
      "Primary study DOI": "10.1016/j.infsof.2017.07.002",
      "Title": "A dedicated approach for model composition traceability",
      "Abstract": "Context: Software systems are often too complex to be expressed by a single model. Recognizing this, the Model Driven Engineering (MDE) proposes multi-modeling approaches to allow developers to describe a system from different perspectives. In this context, model composition has become important since the combination of those partial representations is inevitable. Nevertheless, no approach has been defined for keeping track of the composition effects, and this operation has been overshadowed by model transformations. Objective This paper presents a traceability approach dedicated to the composition of models. Two aspects of quality are considered: producing relevant traces; and dealing with scalability. Method The composition of softgoal trees has been selected to motivate the need for tracing the composition of models and to illustrate our approach. The base principle is to augment the specification of the composition with the behavior needed to generate the expected composed model accompanied with a trace model. This latter includes traces of the execution details. For that, traceability is considered as a crosscutting concern and encapsulated in an aspect. As part of the proposal, an Eclipse plug-in has been implemented as a tool support. Besides, a comparative experiment has been conducted to assess the traces relevance. We also used the regression method to validate the scalability of the tool support. Results Our experiments show that the proposed approach allows generating relevant traces. In addition, the obtained results reveal that tracing a growing number of elements causes an acceptable increase of response time. Conclusion This paper presents a traceability approach dedicated to the composition of models and its application to softgoal trees. The experiment results reveal that our proposal considers the composition specificities for producing valuable traceability information while supporting scalability.",
      "Keywords": "Aspect-oriented modeling | Graph transformations | Model composition | Model traceability | NFR framework",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Laghouaouta, Youness;Anwar, Adil;Nassar, Mahmoud;Coulette, Bernard",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85025661400",
      "Primary study DOI": "10.1016/j.infsof.2017.07.009",
      "Title": "An anomaly detection system based on variable N-gram features and one-class SVM",
      "Abstract": "Context: Run-time detection of system anomalies at the host level remains a challenging task. Existing techniques suffer from high rates of false alarms, hindering large-scale deployment of anomaly detection techniques in commercial settings.Objective: To reduce the false alarm rate, we present a new anomaly detection system based on a novel feature extraction technique, which combines the frequency with the temporal information from system call traces, and on one-class support vector machine (OC-SVM) detector.Method: The proposed feature extraction approach starts by segmenting the system call traces into multiple n-grams of variable length and mapping them to fixed-size sparse feature vectors, which are then used to train OC-SVM detectors.Results: The results achieved on a real-world system call dataset show that our feature vectors with up to 6-grams outperform the term vector models (using the most common weighting schemes) proposed in related work. More importantly, our anomaly detection system using OC-SVM with a Gaussian kernel, trained on our feature vectors, achieves a higher-level of detection accuracy (with a lower false alarm rate) than that achieved by Markovian and n-gram based models as well as by the state-of-the-art anomaly detection techniques.Conclusion: The proposed feature extraction approach from traces of events provides new and general data representations that are suitable for training standard one-class machine learning algorithms, while preserving the temporal dependencies among these events.",
      "Keywords": "Anomaly detection systems | Feature extraction | Intrusion detection and prevention | Software security | System calls | Tracing",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Khreich, Wael;Khosravifar, Babak;Hamou-Lhadj, Abdelwahab;Talhi, Chamseddine",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85025476436",
      "Primary study DOI": "10.1016/j.infsof.2017.07.007",
      "Title": "Open source software ecosystems: A Systematic mapping",
      "Abstract": "Context: Open source software (OSS) and software ecosystems (SECOs) are two consolidated research areas in software engineering. OSS influences the way organizations develop, acquire, use and commercialize software. SECOs have emerged as a paradigm to understand dynamics and heterogeneity in collaborative software development. For this reason, SECOs appear as a valid instrument to analyze OSS systems. However, there are few studies that blend both topics together. Objective: The purpose of this study is to evaluate the current state of the art in OSS ecosystems (OSSECOs) research, specifically: (a) what the most relevant definitions related to OSSECOs are; (b) what the particularities of this type of SECO are; and (c) how the knowledge about OSSECO is represented. Method: We conducted a systematic mapping following recommended practices. We applied automatic and manual searches on different sources and used a rigorous method to elicit the keywords from the research questions and selection criteria to retrieve the final papers. As a result, 82 papers were selected and evaluated. Threats to validity were identified and mitigated whenever possible. Results: The analysis allowed us to answer the research questions. Most notably, we did the following: (a) identified 64 terms related to the OSSECO and arranged them into a taxonomy; (b) built a genealogical tree to understand the genesis of the OSSECO term from related definitions; (c) analyzed the available definitions of SECO in the context of OSS; and (d) classified the existing modelling and analysis techniques of OSSECOs. Conclusion: As a summary of the systematic mapping, we conclude that existing research on several topics related to OSSECOs is still scarce (e.g., modelling and analysis techniques, quality models, standard definitions, etc.). This situation calls for further investigation efforts on how organizations and OSS communities actually understand OSSECOs.",
      "Keywords": "Literature review | Open source software | OSS | OSSECO | SECO | Software ecosystem | Systematic mapping",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Franco-Bedoya, Oscar;Ameller, David;Costal, Dolors;Franch, Xavier",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85022081801",
      "Primary study DOI": "10.1016/j.infsof.2017.06.006",
      "Title": "Analysing problem handling schemes in software projects",
      "Abstract": "Context Although many papers have been published on software development and maintenance processes, there is still a need for deeper exploration of software repositories related to real projects to evaluate these processes. Objective The aim of this study is to present and evaluate different schemes of handling problems (bugs) during software development and maintenance. It is based on exploring information comprised in various software repositories. Method Having analysed the contents of software repositories we have introduced a graph model describing problem handling processes and a scheme of analysing the effectiveness of these processes. This analysis is targeted at structural, throughput and timing aspects. The presented methodology has been verified on real data comprised in open source and commercial projects. Moreover, having identified some drawbacks and anomalies, we point out possible improvements in problem reports and their management. Results Detailed software repositories provide a wider scope of possible measures to assess the relevant problem handling processes. This is helpful in controlling single projects (local perspective) as well as in managing these processes in the whole company (global perspective). Conclusion Detailed problem handling reports extend the space and quality of statistical analysis, the presented graph model with proposed metrics provides a useful tool to evaluate and refine problem resolution schemes.",
      "Keywords": "Fault management | Software process evaluation and improvement | Software repositories | Tracking software bugs",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Sosnowski, Janusz;Dobrzyński, Bartosz;Janczarek, Paweł",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021818457",
      "Primary study DOI": "10.1016/j.infsof.2017.06.008",
      "Title": "Choreography in the embedded systems domain: A systematic literature review",
      "Abstract": "Context Software companies that develop their products on a basis of service-oriented architecture can expect various improvements as a result of choreography. Current choreography practices, however, are not yet used extensively in the embedded systems domain even though service-oriented architecture is increasingly used in this domain. Objective The objective of this study is to identify current features of the use of choreography in the embedded systems domain for practitioners and researchers by systematically analysing current developments in the scientific literature, strategies for choreography adaption, choreography specification and execution types, and implicit assumptions about choreography. Method To fulfil this objective, a systematic literature review of scientific publications that focus on the use of choreography in the embedded systems domain was carried out. After a systematic screening of 6823 publications, 48 were selected as primary studies and analysed using thematic synthesis. Results The main results of the study showed that there are differences in how choreography is used in embedded and non-embedded systems domain. In the embedded systems domain, it is used to capture the service interactions of a single organisation, while, for example, in the enterprise systems domain it captures the service interactions among multiple organisations. Additionally, the results indicate that the use of choreography can lead to improvements in system performance and that the languages that are used for choreography modelling in the embedded systems domain are insufficiently expressive to capture the complexities that are typical in this domain. Conclusion The selection of the key information resources and the identified gaps in the existing literature offer researchers a foundation for further investigations and contribute to the advancement of the use of choreography in the embedded systems domain. The study results facilitate the work of practitioners by allowing them to make informed decisions about the applicability of choreography in their organisations.",
      "Keywords": "Choreography | Embedded systems | Service-oriented architecture | Systematic literature review",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-11-01",
      "Publication type": "Review",
      "Authors": "Taušan, Nebojša;Markkula, Jouni;Kuvaja, Pasi;Oivo, Markku",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85020853907",
      "Primary study DOI": "10.1016/j.infsof.2017.06.002",
      "Title": "Research patterns and trends in software effort estimation",
      "Abstract": "Context Software effort estimation (SEE) is most crucial activity in the field of software engineering. Vast research has been conducted in SEE resulting into a tremendous increase in literature. Thus it is of utmost importance to identify the core research areas and trends in SEE which may lead the researchers to understand and discern the research patterns in large literature dataset. Objective To identify unobserved research patterns through natural language processing from a large set of research articles on SEE published during the period 1996 to 2016. Method A generative statistical method, called Latent Dirichlet Allocation (LDA), applied on a literature dataset of 1178 articles published on SEE. Results As many as twelve core research areas and sixty research trends have been revealed; and the identified research trends have been semantically mapped to associate core research areas. Conclusions This study summarises the research trends in SEE based upon a corpus of 1178 articles. The patterns and trends identified through this research can help in finding the potential research areas.",
      "Keywords": "Latent Dirichlet allocation | Research trends | Software effort estimation",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-11-01",
      "Publication type": "Review",
      "Authors": "Sehra, Sumeet Kaur;Brar, Yadwinder Singh;Kaur, Navdeep;Sehra, Sukhjit Singh",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85023638446",
      "Primary study DOI": "10.1016/j.infsof.2017.07.001",
      "Title": "A method for generation and design of business processes with business rules",
      "Abstract": "Context: Business Processes provide a universal method of describing operational aspects of business. Business Rules, in turn, support declarative specification of business domain knowledge. Although there is a difference in abstraction levels between these both modeling techniques, rules can be complementary to processes. Rules can be efficiently used to specify process low-level logic, while processes can serve as a procedural specification of the workflow, including the inference control. Objective: One of the research problems in this area is supporting business analytics in the modeling of processes integrated with rules. Such a support can take advantage of new design method for such models. Method: We describe a model of procedural Business Process as well as the model and method of creating Attribute Relationship Diagrams. Based on these two representations, we provide a formalized model combining a process model with rules. Using these models, we introduce an algorithm that generates an executable process model along with decision table schemas for rules (rule templates for rule sets grouped in decision tables). Results: The paper provides an automated approach for generation of Business Process models from Attribute Relationship Diagrams. The approach was evaluated based on the selected benchmark cases, which were deployed and tested in the provided modeling and execution environment for such integrated models. Conclusion: The paper presents an efficient and formalized method for design of processes with rules that allows for generating BPMN models integrated with the rules from the Semantic Knowledge Engineering approach. Such a model can be treated as a structured rule base that provides explicit inference flow determined by the process control flow.",
      "Keywords": "Business process modeling | Business rules | Process with rules integration",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Kluza, Krzysztof;Nalepa, Grzegorz J.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021798809",
      "Primary study DOI": "10.1016/j.infsof.2017.06.007",
      "Title": "Vision for SLR tooling infrastructure: Prioritizing value-added requirements",
      "Abstract": "Context Even with the increasing use of Systematic Literature Reviews (SLR) in software engineering (SE), there are still a number of barriers faced by SLR authors. These barriers increase the cost of conducting SLRs. Objective For many of these barriers, appropriate tool support could reduce their impact. In this paper, we use interactions with the SLR community in SE to identify and prioritize a set of requirements for SLR tooling infrastructure. Method This paper analyzes and combines the results from three studies on SLR process barriers and SLR tool requirements to produce a prioritized list of functional requirements for SLR tool support. Using this list of requirements, we perform a feature analysis of the current SLR support tools to identify requirements that are supported as well as identify the need for additional tooling infrastructure. Results The analysis resulted in a list 112 detailed requirements (consolidated into a set of composite requirements) that SE community desires in SLR support tools. The requirements span all the phases of the SLR process. The results show that, while recent tools cover more of the requirements, there are a number of high-priority requirements that are not yet fully covered by any of the existing tools. Conclusion The existing set of SLR tools do not cover all the requirements posed by the community. The list of requirements in this paper is useful for tool developers and researchers wishing to provide support to the SLR community with SE.",
      "Keywords": "Empirical software engineering | Systematic literature review | Tooling infrastructure",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Al-Zubidy, Ahmed;Carver, Jeffrey C.;Hale, David P.;Hassler, Edgar E.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019052872",
      "Primary study DOI": "10.1016/j.infsof.2017.05.001",
      "Title": "Domain-aware Mashup service clustering based on LDA topic model from multiple data sources",
      "Abstract": "Context Mashup is emerging as a promising software development method for allowing software developers to compose existing Web APIs to create new or value-added composite Web services. However, the rapid growth in the number of available Mashup services makes it difficult for software developers to select a suitable Mashup service to satisfy their requirements. Even though clustering based Mashup discovery technique shows a promise of improving the quality of Mashup service discovery, Mashup service clustering with high accuracy and good efficiency is still a challenge problem. Objective This paper proposes a novel domain-aware Mashup service clustering method with high accuracy and good efficiency by exploiting LDA topic model built from multiple data sources, to improve the quality of Mashup service discovery. Method The proposed method firstly designs a domain-aware Mashup service feature selection and reduction process by refining characterization of their domains to consolidate domain relevance. Then, it presents an extended LDA topic model built from multiple data sources (include Mashup description text, Web APIs and tags) to infer topic probability distribution of Mashup services, which serves as a basis of Mashup service similarity computation. Finally, K-means and Agnes algorithm are used to perform Mashup service clustering in terms of their similarities. Results Compared with other existing Mashup service clustering methods, experimental results show that the proposed method achieves a significant improvement in terms of precision, recall, F-measure, purity and entropy. Conclusion The results of the proposed method help software developers to improve the quality of Mashup service discovery and Mashup-based software development. In the future, there will be a need to extend the method by considering heterogeneous network information among Mashup, Web APIs, tags, users, and applying it to Mashup discovery for software developers.",
      "Keywords": "Domain feature selection and reduction | LDA | Mashup service | Multiple data sources | Service clustering",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Cao, Buqing;Frank Liu, Xiaoqing;Liu, Jianxun;Tang, Mingdong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85020375972",
      "Primary study DOI": "10.1016/j.infsof.2017.06.001",
      "Title": "A software reference architecture for semantic-aware Big Data systems",
      "Abstract": "Context: Big Data systems are a class of software systems that ingest, store, process and serve massive amounts of heterogeneous data, from multiple sources. Despite their undisputed impact in current society, their engineering is still in its infancy and companies find it difficult to adopt them due to their inherent complexity. Existing attempts to provide architectural guidelines for their engineering fail to take into account important Big Data characteristics, such as the management, evolution and quality of the data. Objective: In this paper, we follow software engineering principles to refine the λ-architecture, a reference model for Big Data systems, and use it as seed to create Bolster, a software reference architecture (SRA) for semantic-aware Big Data systems. Method: By including a new layer into the λ-architecture, the Semantic Layer, Bolster is capable of handling the most representative Big Data characteristics (i.e., Volume, Velocity, Variety, Variability and Veracity). Results: We present the successful implementation of Bolster in three industrial projects, involving five organizations. The validation results show high level of agreement among practitioners from all organizations with respect to standard quality factors. Conclusion: As an SRA, Bolster allows organizations to design concrete architectures tailored to their specific needs. A distinguishing feature is that it provides semantic-awareness in Big Data Systems. These are Big Data system implementations that have components to simplify data definition and exploitation. In particular, they leverage metadata (i.e., data describing data) to enable (partial) automation of data exploitation and to aid the user in their decision making processes. This simplification supports the differentiation of responsibilities into cohesive roles enhancing data governance.",
      "Keywords": "Big Data | Data analysis | Data management | Semantic-aware | Software reference architecture",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Nadal, Sergi;Herrero, Victor;Romero, Oscar;Abelló, Alberto;Franch, Xavier;Vansummeren, Stijn;Valerio, Danilo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019070974",
      "Primary study DOI": "10.1016/j.infsof.2017.05.003",
      "Title": "Incorporating user preferences in search-based software engineering: A systematic mapping study",
      "Abstract": "Context Search-based algorithms have been successfully applied to solve software engineering problems in the field known as Search-based Software Engineering (SBSE). However, in practice, the user may reject the obtained solutions, since many characteristics of the problem cannot be mathematically modeled. To cope with this situation, preference-based algorithms have been investigated and raised interest in the SBSE field. Objective To identify the quantity and type of research on SBSE preference-based approaches and to contribute to this new research subject, named here Preference and Search-Based Software Engineering (PSBSE), Method We conducted a systematic mapping, following a research plan to locate, assess, extract and group the outcomes from relevant studies. Results Few software engineering activities have been addressed. The most used algorithms are evolutionary and single-objective. In most studies the preferences are provided interactively and, in many cases, the user preferences are incorporated in the fitness functions. We observe a lack of evaluation measures and works comparing existing approaches. Conclusions The use of preference-based algorithms in SBSE is an underexplored subject, and many research opportunities exist.",
      "Keywords": "Preference-based algorithms | Search-based software engineering | Systematic mapping",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-10-01",
      "Publication type": "Review",
      "Authors": "Ferreira, Thiago Nascimento;Vergilio, Silvia Regina;de Souza, Jerffeson Teixeira",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018397382",
      "Primary study DOI": "10.1016/j.infsof.2017.04.007",
      "Title": "Characterizing malicious Android apps by mining topic-specific data flow signatures",
      "Abstract": "Context: State-of-the-art works on automated detection of Android malware have leveraged app descriptions to spot anomalies w.r.t the functionality implemented, or have used data flow information as a feature to discriminate malicious from benign apps. Although these works have yielded promising performance, we hypothesize that these performances can be improved by a better understanding of malicious behavior. Objective: To characterize malicious apps, we take into account both information on app descriptions, which are indicative of apps’ topics, and information on sensitive data flow, which can be relevant to discriminate malware from benign apps. Method: In this paper, we propose a topic-specific approach to malware comprehension based on app descriptions and data-flow information. First, we use an advanced topic model, adaptive LDA with GA, to cluster apps according to their descriptions. Then, we use information gain ratio of sensitive data flow information to build so-called “topic-specific data flow signatures”. Results: We conduct an empirical study on 3691 benign and 1612 malicious apps. We group them into 118 topics and generate topic-specific data flow signature. We verify the effectiveness of the topic-specific data flow signatures by comparing them with the overall data flow signature. In addition, we perform a deeper analysis on 25 representative topic-specific signatures and yield several implications. Conclusion: Topic-specific data flow signatures are efficient in highlighting the malicious behavior, and thus can help in characterizing malware.",
      "Keywords": "Data flow signature | Empirical study | Malware characterization | Topic-specific",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Yang, Xinli;Lo, David;Li, Li;Xia, Xin;Bissyandé, Tegawendé F.;Klein, Jacques",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019033585",
      "Primary study DOI": "10.1016/j.infsof.2017.03.013",
      "Title": "A systematic literature review on methods that handle multiple quality attributes in architecture-based self-adaptive systems",
      "Abstract": "Context Handling multiple quality attributes (QAs) in the domain of self-adaptive systems is an understudied research area. One well-known approach to engineer adaptive software systems and fulfill QAs of the system is architecture-based self-adaptation. In order to develop models that capture the required knowledge of the QAs of interest, and to investigate how these models can be employed at runtime to handle multiple quality attributes, we need to first examine current architecture-based self-adaptive methods. Objective In this paper we review the state-of-the-art of architecture-based methods for handling multiple QAs in self-adaptive systems. We also provide a descriptive analysis of the collected data from the literature. Method We conducted a systematic literature review by performing an automatic search on 28 selected venues and books in the domain of self-adaptive systems. As a result, we selected 54 primary studies which we used for data extraction and analysis. Results Performance and cost are the most frequently addressed set of QAs. Current self-adaptive systems dealing with multiple QAs mostly belong to the domain of robotics and web-based systems paradigm. The most widely used mechanisms/models to measure and quantify QAs sets are QA data variables. After QA data variables, utility functions and Markov chain models are the most common models which are also used for decision making process and selection of the best solution in presence of many alternatives. The most widely used tools to deal with multiple QAs are PRISM and IBM's autonomic computing toolkit. KLAPER is the only language that has been specifically developed to deal with quality properties analysis. Conclusions Our results help researchers to understand the current state of research regarding architecture-based methods for handling multiple QAs in self-adaptive systems, and to identity areas for improvement in the future. To summarize, further research is required to improve existing methods performing tradeoff analysis and preemption, and in particular, new methods may be proposed to make use of models to handle multiple QAs and to enhance and facilitate the tradeoffs analysis and decision making mechanism at runtime.",
      "Keywords": "",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Mahdavi-Hezavehi, Sara;Durelli, Vinicius H.S.;Weyns, Danny;Avgeriou, Paris",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85020023389",
      "Primary study DOI": "10.1016/j.infsof.2017.05.004",
      "Title": "The relation between technical debt and corrective maintenance in PHP web applications",
      "Abstract": "Context Technical Debt Management (TDM) refers to activities that are performed to prevent the accumulation of Technical Debt (TD) in software. The state-of-research on TDM lacks empirical evidence on the relationship between the amount of TD in a software module and the interest that it accumulates. Considering the fact that in the last years, a large portion of software applications are deployed in the web, we focus this study on PHP applications. Objective Although the relation between debt amount and interest is well-defined in traditional economics (i.e., interest is proportional to the amount of debt), this relation has not yet been explored in the context of TD. To this end, the aim of this study is to investigate the relation between the amount of TD and the interest that has to be paid during corrective maintenance. Method To explore this relation, we performed a case study on 10 open source PHP projects. The obtained data have been analyzed to assess the relation between the amount of TD and two aspects of interest: (a) corrective maintenance (i.e., bug fixing) frequency, which translates to interest probability and (b) corrective maintenance effort which is related to interest amount. Results Both interest probability and interest amount are positively related with the amount of TD accumulated in a specific module. Moreover, the amount of TD is able to discriminate modules that are in need of heavy corrective maintenance. Conclusions The results of the study confirm the cornerstone of TD research, which suggests that modules with a higher level of incurred TD, are costlier in maintenance activities. In particular, such modules prove to be more defect-prone and consequently require more (corrective) maintenance effort.",
      "Keywords": "Case study | Corrective maintenance | Empirical evidence | Interest | PHP | Technical debt",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Amanatidis, Theodoros;Chatzigeorgiou, Alexander;Ampatzoglou, Apostolos",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019035451",
      "Primary study DOI": "10.1016/j.infsof.2017.04.008",
      "Title": "The effects of perceived value and stakeholder satisfaction on software project impact",
      "Abstract": "Context In this paper we present a multiple case study on the insights of software organizations into stakeholder satisfaction and (perceived) value of their software projects. Our study is based on the notion that quantifying and qualifying project size, cost, duration, defects, and estimation accuracy needs to be done in relation with stakeholder satisfaction and perceived value. Objectives We contrast project metrics such as cost, duration, number of defects and estimation accuracy with stakeholder satisfaction and perceived value. Method In order to find out whether our approach is practically feasible in an industrial setting, we performed two case studies; one in a Belgian telecom company and the other in a Dutch software company. Results In this study we evaluate 22 software projects that were delivered during one release in the Belgian telecom company, and 4 additional large software releases (representing an extension of 174% in project size) that were delivered in a Dutch software company. Eighty-three (83) key stakeholders of two companies provide stakeholder satisfaction and perceived value measurements in 133 completed surveys. Conclusions We conclude that a focus on shortening overall project duration, and improving communication and team collaboration on intermediate progress is likely to have a positive impact on stakeholder satisfaction and perceived value. Our study does not provide any evidence that steering on costs helped to improve these. As an answer to our research question - how do stakeholder satisfaction and perceived value relate to cost, duration, defects, size and estimation accuracy of software projects? – we found five take-away-messages.",
      "Keywords": "Cost duration index | Evidence-based software engineering | Perceived value | Software economics | stakeholder satisfaction",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Huijgens, Hennie;van Deursen, Arie;van Solingen, Rini",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019191026",
      "Primary study DOI": "10.1016/j.infsof.2017.05.002",
      "Title": "Introduction to the EASE 2016 special section: Evidence-based software engineering: Past, present, and future",
      "Abstract": "The International Conference on Evaluation and Assessment in Software Engineering (EASE) had its twentieth anniversary in 2016, with that year's edition hosted in Limerick, Ireland. Founded in 1997, the EASE conference was the first event solely dedicated to encouraging empirical research in software engineering, and its founders have been longtime advocates of evidence-based software engineering (EBSE). In this editorial, we briefly look back at the history of EBSE and the EASE conference. We then introduce the four articles which are revised and extended versions of papers presented at EASE 2016. We conclude by looking at the future of EBSE, and provide some suggestions for conducting and reporting empirical research.",
      "Keywords": "Empirical research | Evidence-based software engineering",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-09-01",
      "Publication type": "Editorial",
      "Authors": "Beecham, Sarah;Bowes, David;Stol, Klaas Jan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85000799299",
      "Primary study DOI": "10.1016/j.infsof.2016.09.006",
      "Title": "Automated triaging of very large bug repositories",
      "Abstract": "Context: Bug tracking systems play an important role in software maintenance. They allow both developers and users to submit problem reports on observed failures. However, by allowing anyone to submit problem reports, it is likely that more than one reporter will report on the same issue. Research in open source repositories has focused on two broad areas: determining the original report associated with each known duplicate, and assigning a developer to fix a particular problem. Objective: Limited research has been done in developing a fully automated triager, one that can first ascertain if a problem report is original or duplicate, and then provide a list of 20 potential matches for a duplicate report. We address this limitation by developing an automated triaging system that can be used to assist human triagers in bug tracking systems. Method: Our automated triaging system automatically assigns a label of original or duplicate to each incoming problem report, and provides a list of 20 suggestions for reports classified as duplicate. The system uses 24 document similarity measures and associated summary statistics, along with a suite of document property and user metrics. We perform our research on a lifetime of problem reports from the Eclipse, Firefox and Open Office repositories. Results: Our system can be used as a filtration aide, with high original recall exceeding 95% and low duplicate recall, or as a triaging guide, with balanced recall of approximately 70% for both originals and duplicates. Furthermore, the system reduces the workload on the triager by over 90%. Conclusions: Our work represents the first full scale effort at automatically triaging problem reports in open source repositories. By utilizing multiple similarity measures, we reduce the potential of false matches caused by the diversity of human language.",
      "Keywords": "Automated triaging | Big data analytics | Bug tracking | Software problem repositories",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Banerjee, Sean;Syed, Zahid;Helmick, Jordan;Culp, Mark;Ryan, Kenneth;Cukic, Bojan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85017357880",
      "Primary study DOI": "10.1016/j.infsof.2017.03.005",
      "Title": "Risk-averse slope-based thresholds: Definition and empirical evaluation",
      "Abstract": "Background. Practical use of a measure X for an internal attribute (e.g., size, complexity, cohesion, coupling) of software modules often requires setting a threshold on X, to make decisions as to which modules may be estimated to be potentially faulty. To keep quality under control, practitioners may want to set a threshold on X to identify “early symptoms” of possible faultiness of those modules that should be closely monitored and possibly modified. Objective. We propose and evaluate a risk-averse approach to setting thresholds on X based on properties of the slope of statistically significant fault-proneness models, to identify “early symptoms” of module faultiness. Method. To this end, we introduce four ways for setting thresholds on X. First, we use the value of X where a fault-proneness model curve changes direction the most, i.e., it has maximum convexity. Then, we use the values of X where the slope has specific values: one-half of the maximum slope, and the median and mean slope in the interval between minimum and maximum slopes. Results. We provide the theoretical underpinnings for our approach and we apply our approach to data from the PROMISE repository by building Binary Logistic and Probit regression fault-proneness models. The empirical study shows that the proposed thresholds effectively detect “early symptoms” of module faultiness, while achieving a level of accuracy in classifying faulty modules close to other usual fault-proneness thresholds. Conclusions. Our method can be practically used for setting “early symptom” thresholds based on evidence captured by statistically significant models. Also, the thresholds depend on characteristics of the models alone, so project managers do not need to devise the thresholds themselves. The proposed thresholds correspond to increasing risk levels, so project managers can choose the threshold that best suits their needs in a risk-averse framework.",
      "Keywords": "Fault-proneness | Faultiness | Logistic regression | Probit regression | Risk-aversion | Software measures | Threshold",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Morasca, Sandro;Lavazza, Luigi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85016432904",
      "Primary study DOI": "10.1016/j.infsof.2017.03.010",
      "Title": "Findings from a multi-method study on test-driven development",
      "Abstract": "Context Test-driven development (TDD) is an iterative software development practice where unit tests are defined before production code. A number of quantitative empirical investigations have been conducted about this practice. The results are contrasting and inconclusive. In addition, previous studies fail to analyze the values, beliefs, and assumptions that inform and shape TDD. Objective We present a study designed, and conducted to understand the values, beliefs, and assumptions about TDD. Participants were novice and professional software developers. Method We conducted an ethnographically-informed study with 14 novice software developers, i.e., graduate students in Computer Science at the University of Basilicata, and six professional software developers (with one to 10 years work experience). The participants worked on the implementation of a new feature for an existing software written in Java. We immersed ourselves in the context of our study. We collected qualitative information by means of audio recordings, contemporaneous field notes, and other kinds of artifacts. We collected quantitative data from the integrated development environment to support or refute the ethnography results. Results The main insights of our study can be summarized as follows: (i) refactoring (one of the phases of TDD) is not performed as often as the process requires and it is considered less important than other phases, (ii) the most important phase is implementation, (iii) unit tests are almost never up-to-date, and (iv) participants first build in their mind a sort of model of the source code to be implemented and only then write test cases. The analysis of the quantitative data supported the following qualitative findings: (i), (iii), and (iv). Conclusions Developers write quick-and-dirty production code to pass the tests, do not update their tests often, and ignore refactoring.",
      "Keywords": "Ethnographically-informed study | Qualitative study | Test driven development",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Romano, Simone;Fucci, Davide;Scanniello, Giuseppe;Turhan, Burak;Juristo, Natalia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018272470",
      "Primary study DOI": "10.1016/j.infsof.2017.04.006",
      "Title": "Benefits and limitations of project-to-project job rotation in software organizations: A synthesis of evidence",
      "Abstract": "Context Job rotation has been proposed as a managerial practice to be applied in the organizational environment to reduce job monotony, boredom, and exhaustion resulting from job simplification, specialization, and repetition. The scientific literature distinguishes between job-to-job and project-to-project rotations. Despite the potential benefits and its actual use on behalf of software companies, we do not have an accumulated body of scientific knowledge about benefits and limitations of job rotation in software engineering practice. In particular, we have no concrete empirical evidence about the use of project-to-project rotations in practice. Goal We aim to identify and discuss evidence about project-to-project (P2P) job rotation, in order to understand the potential benefits and limitations of this practice in software organizations. Method We deployed a mix-method research strategy to collect and analyze empirical evidence from the scientific literature, performing a systematic literature review, on one hand and from industrial practice, performing qualitative case studies on the other. We synthesized the evidence using techniques from meta-ethnography. Results We found eight benefits, nine limitations, and two factors classified as both benefits and limitations of P2P rotations in software engineering. Different research methods yielded confirmatory and complementary evidence, emphasizing the importance of conducting mix-method research. We found no contradictory evidence and five factors were identified in more than one study using different research methods, contributing to the strength of the evidence. Conclusion We synthesized evidence from multiple sources and used different research methods concerning the benefits and limitations of P2P rotation in software engineering practice. Our findings show that rotation tends to benefit important job outcomes, such as motivation, and to decrease job monotony. The main limitations were associated with the potential increase in intra-group social conflicts, individual cognitive effort, and workload, and a temporary decrease in productivity.",
      "Keywords": "Case study | Job rotation | Replication | Software engineering | Systematic literature review | Work design",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Santos, Ronnie E.S.;da Silva, Fabio Q.B.;Baldassarre, Maria Teresa;de Magalhães, Cleyton V.C.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85016195256",
      "Primary study DOI": "10.1016/j.infsof.2017.03.008",
      "Title": "Test case design for context-aware applications: Are we there yet?",
      "Abstract": "Context Current software systems have increasingly implemented context-aware adaptations to handle the diversity of conditions of their surrounding environment. Therefore, people are becoming used to a variety of context-aware software systems (CASS). This context-awareness brings challenges to the software construction and testing because the context is unpredictable and may change at any time. Therefore, software engineers need to consider the dynamic context changes while testing CASS. Different test case design techniques (TCDT) have been proposed to support the testing of CASS. However, to the best of our knowledge, there is no analysis of these proposals on the advantages, limitations and their effective support to context variation during testing. Objective To gather empirical evidence on TCDT concerned with CASS by identifying, evaluating and synthesizing knowledge available in the literature. Method To undertake a secondary study (quasi-Systematic Literature Review) on TCDT for CASS regarding their assessed quality characteristics, used coverage criteria, test type, and test technique. Results From 833 primary studies published between 2004 and 2014, just 17 studies regard the design of test cases for CASS. Most of them focus on functional suitability. Furthermore, some of them take into account the changes in the context by providing specific test cases for each context configuration (static perspective) during the test execution. These 17 studies revealed five challenges affecting the design of test cases and 20 challenges regarding the testing of CASS. Besides, seven TCDT are not empirically evaluated. Conclusion A few TCDT partially support the testing of CASS. However, it has not been observed evidence on any TCDT supporting the truly context-aware testing, which that can adapt the expected output based on the context variation (dynamic perspective) during the test execution. It is an open issue deserving greater attention from researchers to increase the testing coverage and ensure users confidence in CASS.",
      "Keywords": "Context aware application | Software testing | Systematic review",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-08-01",
      "Publication type": "Review",
      "Authors": "Santos, Ismayle de Sousa;Andrade, Rossana Maria de Castro;Rocha, Lincoln Souza;Matalonga, Santiago;de Oliveira, Káthia Marçal;Travassos, Guilherme Horta",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85017502606",
      "Primary study DOI": "10.1016/j.infsof.2017.04.004",
      "Title": "An extension of the GQM+Strategies approach with formal causal reasoning",
      "Abstract": "Context: Successful organizations need to manage and synchronize their strategic objectives with daily operations and activities. In general, achieving that requires a continuous process of organizational alignment. GQM+Strategies is an approach that helps software organizations with documenting and aligning organizational goals and strategies, and developing measurement programs. Objective: In this paper, the GQM+Strategies approach is further evolved and extended to include capabilities to evaluate the relationships of organizational goals and strategies through causal analysis. Method: We used an analytical paradigm to develop a formal causal model over the GQM+Strategies structure. In addition, an empirical pre-experimental study was designed to test practitioners’ abilities to provide necessary input for the formal causal model. Results: A developed formal causal model over the GQM+Strategies structure allows the use of causal reasoning for the purpose of analyzing dependencies among chosen sets of goals. We illustrate this by showing how to analyze the impact of risky goals on other goals in the grid. The results of the empirical study showed that the practitioners had no difficulties providing their predictions, i.e. inputs into the causal model. Conclusion: The proposed solution extends the existing GQM+Strategies knowledge base by further elaborating and clarifying the process of creating grids by introducing causality theory. The use of causality theory allows experts to quantify their knowledge and beliefs regarding the effectiveness of organizational strategies. As a part of future work, formal causal models and causal reasoning can be implemented as a supporting software tool for the GQM+Strategies approach.",
      "Keywords": "Alignment | Causal analysis | Goal-driven approaches | GQM | GQM Strategies + | Software metrics and measurement",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Mandić, Vladimir;Gvozdenović, Nebojša",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85016497221",
      "Primary study DOI": "10.1016/j.infsof.2017.03.011",
      "Title": "Benefits and drawbacks of software reference architectures: A case study",
      "Abstract": "Context Software Reference Architectures (SRAs) play a fundamental role for organizations whose business greatly depends on the efficient development and maintenance of complex software applications. However, little is known about the real value and risks associated with SRAs in industrial practice. Objective To investigate the current industrial practice of SRAs in a single company from the perspective of different stakeholders. Method An exploratory case study that investigates the benefits and drawbacks perceived by relevant stakeholders in nine SRAs designed by a multinational software consulting company. Results The study shows the perceptions of different stakeholders regarding the benefits and drawbacks of SRAs (e.g., both SRA designers and users agree that they benefit from reduced development costs; on the contrary, only application builders strongly highlighted the extra learning curve as a drawback associated with mastering SRAs). Furthermore, some of the SRA benefits and drawbacks commonly highlighted in the literature were remarkably not mentioned as a benefit of SRAs (e.g., the use of best practices). Likewise, other aspects arose that are not usually discussed in the literature, such as higher time-to-market for applications when their dependencies on the SRA are managed inappropriately. Conclusions This study aims to help practitioners and researchers to better understand real SRAs projects and the contexts where these benefits and drawbacks appeared, as well as some SRA improvement strategies. This would contribute to strengthening the evidence regarding SRAs and support practitioners in making better informed decisions about the expected SRA benefits and drawbacks. Furthermore, we make available the instruments used in this study and the anonymized data gathered to motivate others to provide similar evidence to help mature SRA research and practice.",
      "Keywords": "Benefits | Case study | Drawbacks | Empirical software engineering | Reference architecture | Software architecture",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Martínez-Fernández, Silverio;Ayala, Claudia P.;Franch, Xavier;Marques, Helena Martins",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85017398355",
      "Primary study DOI": "10.1016/j.infsof.2017.04.002",
      "Title": "An industry experience report on managing product quality requirements in a large organization",
      "Abstract": "Context Eliciting, managing and implementing product quality requirements (in-short quality requirements) in a large organization can be challenging when many stakeholders are involved and projects run in parallel; sometimes with varying priorities with regards to quality. In this case from a public organization in Norway, the separation between business units and the IT-department and the legacy burden are additional factors that increase the complexity of requirement management. Objective This paper presents results and experiences from three years long work with quality requirements, starting from ad-hoc handling of quality requirements in separate projects to systematic work across projects with reusable sets of requirements and processes. Method We present how quality requirements are captured and classified, as well as changes to the agile software development process as a consequence of increasing focus on product quality. Results The ISO/IEC-25010:2011 standard is tailored for better context fit and is supported by concrete requirements and a methodology that covers the life cycle of software products in both greenfield and brownfield projects. In addition, the organization had to examine the current state of existing IT-capabilities in order to establish a quality baseline for future development, and develop shared vision and roadmaps for product quality. Conclusions In our experience, stakeholders prefer an iterative and lightweight approach in eliciting and refining quality requirements. The classification model and requirement lists are used as guidelines in requirement workshops. The developed terminology, updated templates and processes are reusable in projects and generalizable to different contexts, and are well adopted by the IT and business units.",
      "Keywords": "Agile | Experience report | Maintenance | Non-functional requirements | Product quality | Technical debt",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Mohagheghi, Parastoo;Aparicio, Mario Ek",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018752207",
      "Primary study DOI": "10.1016/j.infsof.2017.04.005",
      "Title": "On code reuse from StackOverflow: An exploratory study on Android apps",
      "Abstract": "Context: Source code reuse has been widely accepted as a fundamental activity in software development. Recent studies showed that StackOverflow has emerged as one of the most popular resources for code reuse. Therefore, a plethora of work proposed ways to optimally ask questions, search for answers and find relevant code on StackOverflow. However, little work studies the impact of code reuse from StackOverflow. Objective: To better understand the impact of code reuse from StackOverflow, we perform an exploratory study focusing on code reuse from StackOverflow in the context of mobile apps. Specifically, we investigate how much, why, when, and who reuses code. Moreover, to understand the potential implications of code reuse, we examine the percentage of bugs in files that reuse StackOverflow code. Method: We perform our study on 22 open source Android apps. For each project, we mine their source code and use clone detection techniques to identify code that is reused from StackOverflow. We then apply different quantitative and qualitative methods to answer our research questions. Results: Our findings indicate that 1) the amount of reused StackOverflow code varies for different mobile apps, 2) feature additions and enhancements in apps are the main reasons for code reuse from StackOverflow, 3) mid-age and older apps reuse StackOverflow code mostly later on in their project lifetime and 4) that in smaller teams/apps, more experienced developers reuse code, whereas in larger teams/apps, the less experienced developers reuse code the most. Additionally, we found that the percentage of bugs is higher in files after reusing code from StackOverflow. Conclusion: Our results provide insights on the potential impact of code reuse from StackOverflow on mobile apps. Furthermore, these results can benefit the research community in developing new techniques and tools to facilitate and improve code reuse from StackOverflow.",
      "Keywords": "Code reuse | Mobile app | StackOverflow",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Abdalkareem, Rabe;Shihab, Emad;Rilling, Juergen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018634750",
      "Primary study DOI": "10.1016/j.infsof.2017.04.003",
      "Title": "A historical, textual analysis approach to feature location",
      "Abstract": "Context Feature location is the task of finding the source code that implements specific functionality in software systems. A common approach is to leverage textual information in source code against a query, using Information Retrieval (IR) techniques. To address the paucity of meaningful terms in source code, alternative, relevant source-code descriptions, like change-sets could be leveraged for these IR techniques. However, the extent to which these descriptions are useful has not been thoroughly studied. Objective This work rigorously characterizes the efficacy of source-code lexical annotation by change-sets (ACIR), in terms of its best-performing configuration. Method A tool, implementing ACIR, was used to study different configurations of the approach and to compare them to a baseline approach (thus allowing comparison against other techniques going forward). This large-scale evaluation employs eight subject systems and 600 features. Results It was found that, for ACIR: (1) method level granularity demands less search effort; (2) using more recent change-sets improves effectiveness; (3) aggregation of recent change-sets by change request, decreases effectiveness; (4) naive, text-classification-based filtering of “management” change-sets also decreases the effectiveness. In addition, a strongly pronounced dichotomy of subject systems emerged, where one set recorded better feature location using ACIR and the other recorded better feature location using the baseline approach. Finally, merging ACIR and the baseline approach significantly improved performance over both standalone approaches for all systems. Conclusion The most fundamental finding is the importance of rigorously characterizing proposed feature location techniques, to identify their optimal configurations. The results also suggest it is important to characterize the software systems under study when selecting the appropriate feature location technique. In the past, configuration of the techniques and characterization of subject systems have not been considered first-class entities in research papers, whereas the results presented here suggests these factors can have a big impact.",
      "Keywords": "Dataset expansion | Feature location | Search effort | Software systems’ characterization | Version histories",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Chochlov, Muslim;English, Michael;Buckley, Jim",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85017357042",
      "Primary study DOI": "10.1016/j.infsof.2017.04.001",
      "Title": "Static analysis of android apps: A systematic literature review",
      "Abstract": "Context Static analysis exploits techniques that parse program source code or bytecode, often traversing program paths to check some program properties. Static analysis approaches have been proposed for different tasks, including for assessing the security of Android apps, detecting app clones, automating test cases generation, or for uncovering non-functional issues related to performance or energy. The literature thus has proposed a large body of works, each of which attempts to tackle one or more of the several challenges that program analyzers face when dealing with Android apps. Objective We aim to provide a clear view of the state-of-the-art works that statically analyze Android apps, from which we highlight the trends of static analysis approaches, pinpoint where the focus has been put, and enumerate the key aspects where future researches are still needed. Method We have performed a systematic literature review (SLR) which involves studying 124 research papers published in software engineering, programming languages and security venues in the last 5 years (January 2011–December 2015). This review is performed mainly in five dimensions: problems targeted by the approach, fundamental techniques used by authors, static analysis sensitivities considered, android characteristics taken into account and the scale of evaluation performed. Results Our in-depth examination has led to several key findings: 1) Static analysis is largely performed to uncover security and privacy issues; 2) The Soot framework and the Jimple intermediate representation are the most adopted basic support tool and format, respectively; 3) Taint analysis remains the most applied technique in research approaches; 4) Most approaches support several analysis sensitivities, but very few approaches consider path-sensitivity; 5) There is no single work that has been proposed to tackle all challenges of static analysis that are related to Android programming; and 6) Only a small portion of state-of-the-art works have made their artifacts publicly available. Conclusion The research community is still facing a number of challenges for building approaches that are aware altogether of implicit-Flows, dynamic code loading features, reflective calls, native code and multi-threading, in order to implement sound and highly precise static analyzers.",
      "Keywords": "",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-08-01",
      "Publication type": "Review",
      "Authors": "Li, Li;Bissyandé, Tegawendé F.;Papadakis, Mike;Rasthofer, Siegfried;Bartel, Alexandre;Octeau, Damien;Klein, Jacques;Traon, Le",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85016444613",
      "Primary study DOI": "10.1016/j.infsof.2017.03.012",
      "Title": "Less is more: Minimizing code reorganization using XTREE",
      "Abstract": "Context: Developers use bad code smells to guide code reorganization. Yet developers, textbooks, tools, and researchers disagree on which bad smells are important. How can we offer reliable advice to developers about which bad smells to fix? Objective: To evaluate the likelihood that a code reorganization to address bad code smells will yield improvement in the defect-proneness of the code. Method: We introduce XTREE, a framework that analyzes a historical log of defects seen previously in the code and generates a set of useful code changes. Any bad smell that requires changes outside of that set can be deprioritized (since there is no historical evidence that the bad smell causes any problems). Evaluation: We evaluate XTREE's recommendations for bad smell improvement against recommendations from previous work (Shatnawi, Alves, and Borges) using multiple data sets of code metrics and defect counts. Results: Code modules that are changed in response to XTREE's recommendations contain significantly fewer defects than recommendations from previous studies. Further, XTREE endorses changes to very few code metrics, so XTREE requires programmers to do less work. Further, XTREE's recommendations are more responsive to the particulars of different data sets. Finally XTREE's recommendations may be generalized to identify the most crucial factors affecting multiple datasets (see the last figure in paper). Conclusion: Before undertaking a code reorganization based on a bad smell report, use a framework like XTREE to check and ignore any such operations that are useless; i.e. ones which lack evidence in the historical record that it is useful to make that change. Note that this use case applies to both manual code reorganizations proposed by developers as well as those conducted by automatic methods.",
      "Keywords": "Bad smells | Decision trees | Performance prediction",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Krishna, Rahul;Menzies, Tim;Layman, Lucas",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85008166396",
      "Primary study DOI": "10.1016/j.infsof.2016.10.003",
      "Title": "Stable and predictable Voronoi treemaps for software quality monitoring",
      "Abstract": "Context: Voronoi treemaps can be used to effectively visualize software quality attributes of a given software system. Algorithms for computing Voronoi treemaps are non-deterministic making them unsuited for monitoring the development of such attributes over time. Objective: We adapt an existing sweep line algorithm to efficiently compute Voronoi treemaps and we introduce a novel algorithm that adds stability and predictability. Method: We introduce stable and predictable Voronoi treemaps based on additively weighted power Voronoi diagrams. We employ scaled Hilbert curves to place Voronoi sites in the plane, retaining the order in which sites are placed along the curve for easy comparison with revisions of the same software system. Results: Our algorithm achieves a predictable first good approximation of the final location of the sites. We show that our algorithm not only provides more stability, but also that because of better placement it needs fewer iterations to compute its result. As part of our implementation we introduce a visualization to show the difference between two versions of a software system. We also present a small case study in which we use a web based application that implements our work to investigate the usefulness of stability and predictability of visualizations. Conclusion: It is possible to achieve stable and predictable visualizations of software system attributes, while, as a pleasant side effect, decreasing the number of iterations necessary to arrive at the visualization.",
      "Keywords": "Hilbert curves | Software quality monitoring | Software visualization | Stable Voronoi treemaps | Voronoi diagrams",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "van Hees, Rinse;Hage, Jurriaan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85016836831",
      "Primary study DOI": "10.1016/j.infsof.2017.03.007",
      "Title": "TLEL: A two-layer ensemble learning approach for just-in-time defect prediction",
      "Abstract": "Context Defect prediction is a very meaningful topic, particularly at change-level. Change-level defect prediction, which is also referred as just-in-time defect prediction, could not only ensure software quality in the development process, but also make the developers check and fix the defects in time [1]. Objective Ensemble learning becomes a hot topic in recent years. There have been several studies about applying ensemble learning to defect prediction [2–5]. Traditional ensemble learning approaches only have one layer, i.e., they use ensemble learning once. There are few studies that leverages ensemble learning twice or more. To bridge this research gap, we try to hybridize various ensemble learning methods to see if it will improve the performance of just-in-time defect prediction. In particular, we focus on one way to do this by hybridizing bagging and stacking together and leave other possibly hybridization strategies for future work. Method In this paper, we propose a two-layer ensemble learning approach TLEL which leverages decision tree and ensemble learning to improve the performance of just-in-time defect prediction. In the inner layer, we combine decision tree and bagging to build a Random Forest model. In the outer layer, we use random under-sampling to train many different Random Forest models and use stacking to ensemble them once more. Results To evaluate the performance of TLEL, we use two metrics, i.e., cost effectiveness and F1-score. We perform experiments on the datasets from six large open source projects, i.e., Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL, containing a total of 137,417 changes. Also, we compare our approach with three baselines, i.e., Deeper, the approach proposed by us [6], DNC, the approach proposed by Wang et al. [2], and MKEL, the approach proposed by Wang et al. [3]. The experimental results show that on average across the six datasets, TLEL could discover over 70% of the bugs by reviewing only 20% of the lines of code, as compared with about 50% for the baselines. In addition, the F1-scores TLEL can achieve are substantially and statistically significantly higher than those of three baselines across the six datasets. Conclusion TLEL can achieve a substantial and statistically significant improvement over the state-of-the-art methods, i.e., Deeper, DNC and MKEL. Moreover, TLEL could discover over 70% of the bugs by reviewing only 20% of the lines of code.",
      "Keywords": "Cost effectiveness | Ensemble learning | Just-in-time defect prediction",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Yang, Xinli;Lo, David;Xia, Xin;Sun, Jianling",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85011277924",
      "Primary study DOI": "10.1016/j.infsof.2017.01.002",
      "Title": "Analyzing and predicting effort associated with finding and fixing software faults",
      "Abstract": "Context: Software developers spend a significant amount of time fixing faults. However, not many papers have addressed the actual effort needed to fix software faults. Objective: The objective of this paper is twofold: (1) analysis of the effort needed to fix software faults and how it was affected by several factors and (2) prediction of the level of fix implementation effort based on the information provided in software change requests. Method: The work is based on data related to 1200 failures, extracted from the change tracking system of a large NASA mission. The analysis includes descriptive and inferential statistics. Predictions are made using three supervised machine learning algorithms and three sampling techniques aimed at addressing the imbalanced data problem. Results: Our results show that (1) 83% of the total fix implementation effort was associated with only 20% of failures. (2) Both post-release failures and safety-critical failures required more effort to fix than pre-release and non-critical counterparts, respectively; median values were two or more times higher. (3) Failures with fixes spread across multiple components or across multiple types of software artifacts required more effort. The spread across artifacts was more costly than spread across components. (4) Surprisingly, some types of faults associated with later life-cycle activities did not require significant effort. (5) The level of fix implementation effort was predicted with 73% overall accuracy using the original, imbalanced data. Oversampling techniques improved the overall accuracy up to 77% and, more importantly, significantly improved the prediction of the high level effort, from 31% to 85%. Conclusions: This paper shows the importance of tying software failures to changes made to fix all associated faults, in one or more software components and/or in one or more software artifacts, and the benefit of studying how the spread of faults and other factors affect the fix implementation effort.",
      "Keywords": "Analysis | Case study | Prediction | Software faults and failures | Software fix implementation effort",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Hamill, Maggie;Goseva-Popstojanova, Katerina",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85011101156",
      "Primary study DOI": "10.1016/j.infsof.2016.12.001",
      "Title": "Visualizing and exploring software version control repositories using interactive tag clouds over formal concept lattices",
      "Abstract": "Context: version control repositories contain a wealth of implicit information that can be used to answer many questions about a project's development process. However, this information is not directly accessible in the repositories and must be extracted and visualized. Objective: the main objective of this work is to develop a flexible and generic interactive visualization engine called ConceptCloud that supports exploratory search in version control repositories. Method: ConceptCloud is a flexible, interactive browser for SVN and Git repositories. Its main novelty is the combination of an intuitive tag cloud visualization with an underlying concept lattice that provides a formal structure for navigation. ConceptCloud supports concurrent navigation in multiple linked but individually customizable tag clouds, which allows for multi-faceted repository browsing, and scriptable construction of unique visualizations. Results: we describe the mathematical foundations and implementation of our approach and use ConceptCloud to quickly gain insight into the team structure and development process of three projects. We perform a user study to determine the usability of ConceptCloud. We show that untrained participants are able to answer historical questions about a software project better using ConceptCloud than using a linear list of commits. Conclusion: ConceptCloud can be used to answer many difficult questions such as “What has happened in this project while I was away?” and “Which developers collaborate?”. Tag clouds generated from our approach provide a visualization in which version control data can be aggregated and explored interactively.",
      "Keywords": "Browsing software repositories | Formal concept analysis | Interactive tag cloud visualization | Tag clouds",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Greene, Gillian J.;Esterhuizen, Marvin;Fischer, Bernd",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85016551133",
      "Primary study DOI": "10.1016/j.infsof.2017.03.002",
      "Title": "Investigating the relationship between price, rating, and popularity in the Blackberry World App Store",
      "Abstract": "Context: App stores provide a software development space and a market place that are both different from those to which we have become accustomed for traditional software development: The granularity is finer and there is a far greater source of information available for research and analysis. Information is available on price, customer rating and, through the data mining approach presented in this paper, the features claimed by app developers. These attributes make app stores ideal for empirical software engineering analysis. Objective: This paper1 exploits App Store Analysis to understand the rich interplay between app customers and their developers. Method: We use data mining to extract app descriptions, price, rating, and popularity information from the Blackberry World App Store, and natural language processing to elicit each apps’ claimed features from its description. Results: The findings reveal that there are strong correlations between customer rating and popularity (rank of app downloads). We found evidence for a mild correlation between app price and the number of features claimed for the app and also found that higher priced features tended to be lower rated by their users. We also found that free apps have significantly (p-value < 0.001) higher ratings than non-free apps, with a moderately high effect size (A^12=0.68). All data from our experiments and analysis are made available on-line to support further investigations.",
      "Keywords": "App features | App store analysis | Data mining | Mobile apps | Natural language processing",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Finkelstein, Anthony;Harman, Mark;Jia, Yue;Martin, William;Sarro, Federica;Zhang, Yuanyuan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84978859472",
      "Primary study DOI": "10.1016/j.infsof.2016.07.004",
      "Title": "Software landscape and application visualization for system comprehension with ExplorViz",
      "Abstract": "Context: The number of software applications deployed in organizations is constantly increasing. Those applications – often several hundreds – form large software landscapes. Objective: The comprehension of such landscapes and their applications is often impeded by, for instance, architectural erosion, personnel turnover, or changing requirements. Therefore, an efficient and effective way to comprehend such software landscapes is required. Method: In our ExplorViz visualization, we introduce hierarchical abstractions aiming at solving system comprehension tasks fast and accurately for large software landscapes. Besides hierarchical visualization on the landscape level, ExplorViz provides multi-level visualization from the landscape to the level of individual applications. The 3D application-level visualization is empirically evaluated with a comparison to the Extravis approach, with physical models and in virtual reality. To evaluate ExplorViz, we conducted four controlled experiments. We provide packages containing all our experimental data to facilitate the verifiability, reproducibility, and further extensibility of our results. Results: We observed a statistical significant increase in task correctness of the hierarchical visualization compared to the flat visualization. The time spent did not show any significant differences. For the comparison with Extravis, we observed that solving program comprehension tasks using ExplorViz leads to a significant increase in correctness and in less or similar time spent. The physical models improved the team-based program comprehension process for specific tasks by initiating gesture-based interaction, but not for all tasks. The participants of our virtual reality experiment with ExplorViz rated the realized gestures for translation, rotation, and selection as highly usable. However, our zooming gesture was less favored. Conclusion: The results backup our claim that our hierarchical and multi-level approach enhances the current state of the art in landscape and application visualization for better software system comprehension, including new forms of interaction with physical models and virtual reality.",
      "Keywords": "Dynamic analysis | Software visualization | System comprehension",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Fittkau, Florian;Krause, Alexander;Hasselbring, Wilhelm",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85015634473",
      "Primary study DOI": "10.1016/j.infsof.2017.02.001",
      "Title": "Corrigendum to  Assessment of class mutation operators for C++ with the MuCPP mutation system  [Information and Software Technology (2017) 81 (169 184)] (S0950584916301161) (10.1016/j.infsof.2016.07.002)",
      "Abstract": "The authors regret that we forgot to include that our work was partially supported by the European Commission (FEDER) in the acknowledgements section. Therefore, the correct acknowledgements section should read as follows: “This paper was partially supported by the European Commission (FEDER) and the Spanish Ministry of Economy and Competitiveness (National Program for Research, Development and Innovation), through the project DArDOS (TIN2015-65845-C3-3-R) and the Excellence Network SEBASENet (TIN2015-71841-REDT), and by the research scholarship PU-EPIF-FPI-PPI-BC 2012-037 of the University of Cádiz.” The authors would like to apologise for any inconvenience caused.",
      "Keywords": "",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-07-01",
      "Publication type": "Erratum",
      "Authors": "Delgado-Pérez, Pedro;Medina-Bulo, Inmaculada;Palomo-Lozano, Francisco;García-Domínguez, Antonio;Domínguez-Jiménez, Juan José",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85015382293",
      "Primary study DOI": "10.1016/j.infsof.2017.03.003",
      "Title": "Uncertainty-wise evolution of test ready models",
      "Abstract": "Context Cyber-Physical Systems (CPSs), when deployed for operation, are inherently prone to uncertainty. Considering their applications in critical domains (e.g., healthcare), it is important that such CPSs are tested sufficiently, with the explicit consideration of uncertainty. Model-based testing (MBT) involves creating test ready models capturing the expected behavior of a CPS and its operating environment. These test ready models are then used for generating executable test cases. It is, therefore, necessary to develop methods that can continuously evolve, based on real operational data collected during the operation of CPSs, test ready models and uncertainty captured in them, all together termed as Belief Test Ready Models (BMs) Objective Our objective is to propose a model evolution framework that can interactively improve the quality of BMs, based on operational data. Such BMs are developed by one or more test modelers (belief agents) with their assumptions about the expected behavior of a CPS, its expected physical environment, and potential future deployments. Thus, these models explicitly contain subjective uncertainty of the test modelers. Method We propose a framework (named as UncerTolve) for interactively evolving BMs (specified with extended UML notations) of CPSs with subjective uncertainty developed by test modelers. The key inputs of UncerTolve include initial BMs of CPSs with known subjective uncertainty and real data collected from the operation of CPSs. UncerTolve has three key features: 1) Validating the syntactic correctness and conformance of BMs against real operational data via model execution, 2) Evolving objective uncertainty measurements of BMs via model execution, and 3) Evolving state invariants (modeling test oracles) and guards of transitions (modeling constraints for test data generation) of BMs with a machine learning technique. Results As a proof-of-concept, we evaluated UncerTolve with one industrial CPS case study, i.e., GeoSports from the healthcare domain. Using UncerTolve, we managed to evolve 51% of belief elements, 18% of states, and 21% of transitions as compared to the initial BM developed in an industrial setting. Conclusion UncerTolve can successfully evolve model elements of the initial BM, in addition to objective uncertainty measurements using real operational data. The evolved model can be used to generate additional test cases covering evolved model elements and objective uncertainty. These additional test cases can be used to test the current and future deployments of a CPS to ensure that it will handle uncertainty gracefully during its operations.",
      "Keywords": "Belief model | Belief test ready model | Model evolution | Model-based testing | Uncertainty",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Zhang, Man;Ali, Shaukat;Yue, Tao;Norgre, Roland",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85016804749",
      "Primary study DOI": "10.1016/j.infsof.2017.03.004",
      "Title": "REASSURE: Requirements elicitation for adaptive socio-technical systems using repertory grid",
      "Abstract": "Context Socio-technical systems are expected to understand the dynamics of the execution environment and behave accordingly. Significant work has been done on formalizing and modeling requirements of such adaptive systems. However, not enough attention is paid on eliciting requirements from users and introducing flexibility in the system behavior at an early phase of requirements engineering. Most of the work is based on an assumption that general users’ cognitive level would be able to support the inherent complexity of variability acquisition. Objective Our main focus is on providing help to the users with ordinary cognitive level to express their expectations from the complex system considering various contexts. This work also helps the designers to explore the design variability based on the general users’ preferences. Method We explore the idea of using a cognitive technique Repertory Grid (RG) to acquire knowledge from users and experts along multiple dimensions of problem and design space. We propose REASSURE methodology which guides requirements engineers to explore the intentional and design variability in an organized way. We also provide a tool support to analyze the knowledge captured in multiple repertory grid files and detect potential conflicts in the intentional variability. Finally, we evaluate the proposed idea by performing an empirical study using smart home system domain. Results The result of our study shows that a greater number of requirements can be elicited after applying our approach. With the help of the provided tool support, it is even possible to detect a greater number of conflicts in user's requirements than the traditional practices. Conclusion We envision RG as a technique to filter design options based on the intentional variability in various contexts. The promising results of empirical study open up new research questions: “how to elicit requirements from multiple stakeholders and reach consensus for multi-dimensional problem domain”.",
      "Keywords": "Adaptive systems | Repertory grid | Requirements elicitation | Socio-technical systems",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Dey, Sangeeta;Lee, Seok Won",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85012888858",
      "Primary study DOI": "10.1016/j.infsof.2017.01.008",
      "Title": "A case study of TTCN-3 test scripts clone analysis in an industrial telecommunication setting",
      "Abstract": "Context: This paper presents a novel experiment focused on detecting and analyzing clones in test suites written in TTCN-3, a standard telecommunication test script language, for different industrial projects. Objective: This paper investigates frequencies, types, and similarity distributions of TTCN-3 clones in test scripts from three industrial projects in telecommunication. We also compare the distribution of clones in TTCN-3 test scripts with the distribution of clones in C/C++ and Java projects from the telecommunication domain. We then perform a statistical analysis to validate the significance of differences between these distributions. Method: Similarity is computed using CLAN, which compares metrics syntactically derived from script fragments. Metrics are computed from the Abstract Syntax Trees produced by a TTCN-3 parser called Titan developed by Ericsson as an Eclipse plugin. Finally, clone classification of similar script pairs is computed using the Longest Common Subsequence algorithm on token types and token images. Results: This paper presents figures and diagrams reporting TTCN-3 clone frequencies, types, and similarity distributions. We show that the differences between the distribution of clones in test scripts and the distribution of clones in applications are statistically significant. We also present and discuss some lessons that can be learned about the transferability of technology from this study. Conclusion: About 24% of fragments in the test suites are cloned, which is a very high proportion of clones compared to what is generally found in source code. The difference in proportion of Type-1 and Type-2 clones is statistically significant and remarkably higher in TTCN-3 than in source code. Type-1 and Type-2 clones represent 82.9% and 15.3% of clone fragments for a total of 98.2%. Within the projects this study investigated, this represents more and easier potential re-factoring opportunities for test scripts than for code.",
      "Keywords": "Clone detection | Telecommunications software | Test",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Lavoie, Thierry;Mérineau, Mathieu;Merlo, Ettore;Potvin, Pascal",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85011824686",
      "Primary study DOI": "10.1016/j.infsof.2017.01.012",
      "Title": "Investigating styles in variability modeling: Hierarchical vs. constrained styles",
      "Abstract": "Context A common way to represent product lines is with variability modeling. Yet, there are different ways to extract and organize relevant characteristics of variability. Comprehensibility of these models and the ease of creating models are important for the efficiency of any variability management approach. Objective The goal of this paper is to investigate the comprehensibility of two common styles to organize variability into models – hierarchical and constrained – where the dependencies between choices are specified either through the hierarchy of the model or as cross-cutting constraints, respectively. Method We conducted a controlled experiment with a sample of 90 participants who were students with prior training in modeling. Each participant was provided with two variability models specified in Common Variability Language (CVL) and was asked to answer questions requiring interpretation of provided models. The models included 9–20 nodes and 8–19 edges and used the main variability elements. After answering the questions, the participants were asked to create a model based on a textual description. Results The results indicate that the hierarchical modeling style was easier to comprehend from a subjective point of view, but there was also a significant interaction effect with the degree of dependency in the models, that influenced objective comprehension. With respect to model creation, we found that the use of a constrained modeling style resulted in higher correctness of variability models. Conclusions Prior exposure to modeling style and the degree of dependency among elements in the model determine what modeling style a participant chose when creating the model from natural language descriptions. Participants tended to choose a hierarchical style for modeling situations with high dependency and a constrained style for situations with low dependency. Furthermore, the degree of dependency also influences the comprehension of the variability model.",
      "Keywords": "Cognitive aspects | Comprehensibility | Empirical research | Feature modeling | Hierarchical modeling | Product line engineering | Textual constraints | Variability modeling",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Reinhartz-Berger, Iris;Figl, Kathrin;Haugen, Øystein",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85011975963",
      "Primary study DOI": "10.1016/j.infsof.2017.01.004",
      "Title": "Applying process mining techniques in software process appraisals",
      "Abstract": "Context Process assessments are performed to identify the current maturity of organizations in relation to best practices. Existing process assessment methods, although widely used, have limitations such as: dependence on the competencies of appraisers; high amount of effort and resources required; subjectivity to analyze data and to judge on the implementation of practices; low confidence in sampling and its representativeness. Currently, due to the increasing use of information systems to support process execution, detailed information on the implementation of processes are recorded as event logs, transaction logs, etc. This fact enables the usage of process mining techniques as a powerful tool for process analysis. It allows using a significant amount of data with agility and reliability for process assessments. Objective The objective of this paper is to present the development and application of a feasible, usable and useful method, which reduces the limitations of current SCAMPI method and defines how to apply process mining techniques in SCAMPI-based process appraisals. Method Research method comprises nine steps that were performed in a manner that raised questions in the first four steps were answered by the last four steps of the research design. Results The method “Process Mining Extension to SCAMPI” was designed, developed, applied in two cases and submitted for review by experts who judged it viable, usable, and useful. Conclusions As per this research, process mining techniques are suitable to be applied in software process assessments since they are aligned with the purposes of data collection and analysis tasks. In addition to that, the resulting method was judged by experts as something that reduces identified limitations of one of the most used process assessment method.",
      "Keywords": "Data collection and analysis | Process mining | Process Mining Extension to SCAMPI | SCAMPI | Software process assessment",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "M. Valle, Arthur;A.P. Santos, Eduardo;R. Loures, Eduardo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85017125309",
      "Primary study DOI": "10.1016/j.infsof.2017.03.006",
      "Title": "Systematic literature review and empirical investigation of barriers to process improvement in global software development: Client vendor perspective",
      "Abstract": "Context Increasingly, software development organizations are adopting global software development (GSD) strategies, mainly because of the significant return on investment they produce. However, there are many challenges associated with GSD, particularly with regards to software process improvement (SPI). SPI can play a significant role in the successful execution of GSD projects. Objective The aim of the present study was to identify barriers that can negatively affect SPI initiatives in GSD organizations from both client and vendor perspectives. Method A systematic literature review (SLR) and survey questionnaire were used to identify and validate the barriers. Results Twenty-two barriers to successful SPI programs were identified. Results illustrate that the barriers identified using SLR and survey approaches have more similarities However, there were significant differences between the ranking of these barriers in the SLR and survey approaches, as indicated by the results of t-tests (for instance, t = 2.28, p = 0.011 < 0.05). Our findings demonstrate that there is a moderate positive correlation between the ranks obtained from the SLR and the empirical study (rs (22) = 0.567, p = 0.006). Conclusions The identified barriers can assist both client and vendor GSD organizations during initiation of an SPI program. Client-vendor classification was used to provide a broad picture of SPI programs, and their respective barriers. The top-ranked barriers can be used as a guide for GSD organizations prior to the initiation of an SPI program. We believe that the results of this study can be useful in tackling the problems associated with the implementation of SPI, which is vital to the success and progression of GSD organizations.",
      "Keywords": "Barriers | Client | Global software development | Software process improvement | Systematic literature review | Vendor",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Khan, Arif Ali;Keung, Jacky;Niazi, Mahmood;Hussain, Shahid;Ahmad, Awais",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85015091212",
      "Primary study DOI": "10.1016/j.infsof.2017.03.001",
      "Title": "Minimizing the stakeholder dissatisfaction risk in requirement selection for next release planning",
      "Abstract": "Context The requirements to be delivered in the next software release are selected according to the stakeholders’ perceived value, expected implementation cost, budget availability, and precedence and technical dependency constraints. Existing approaches to the requirement selection problem do not take into account the risk of stakeholders’ dissatisfaction possibly resulting from divergence in the stakeholders’ estimates of the requirement value. Objective We present a novel risk-aware, multi-objective approach to the next release problem that aims at reducing the stakeholder dissatisfaction risk in a given cost/value region of interest provided by stakeholders. Method We have devised an exact algorithm to address the risk-aware formulation of the next release problem and implemented the algorithm using two well-known SMT solvers, Yices and Z3. To allow the application of the proposed formulation to large size problems, we have also implemented an approximate algorithm based on the NSGA-II metaheuristic. Results Results show that (1) the stakeholder dissatisfaction risk can be minimised with minimum impact on cost/value, and (2) our approach is scalable when NSGA-II is used. SMT solvers scale up to problems that are not overly large in terms of the number of requirements and/or are not too sparse in terms of dependencies, but the metaheuristic can quickly find good solutions even for large size problems. Conclusion We recommend the users of our approach to apply an SMT solver and to resort to a metaheuristic algorithm only if the SMT solver does not terminate within reasonable time, due to the actual combination of number of requirements and dependency density.",
      "Keywords": "Multi-stakeholder | Next release problem | Risk-aware decision making",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Pitangueira, A. M.;Tonella, P.;Susi, A.;Maciel, R. S.P.;Barros, M.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85014777653",
      "Primary study DOI": "10.1016/j.infsof.2017.01.010",
      "Title": "A design theory for software engineering",
      "Abstract": "Context: Software Engineering is a discipline that has been shaped by over 50 years of practice. Many have argued that its theoretical basis has been slow to develop and that, in fact, a substantial theory of Software Engineering is still lacking. Objective: We propose a design theory for Software Engineering as a contribution to the debate. Having done this, we extend it to a design theory for socio-technical systems. Method: We elaborate our theory based on Gregor's influential ‘meta-theoretical’ exploration of the structural nature of a theory in the discipline of Information Systems, with particular attention to ontological and epistemological arguments. Results: We argue how, from an ontological perspective, our theory embodies a view of Software Engineering as the practice of framing, representing and transforming Software Engineering problems. As such, theory statements concern the characterisation of individual problems and how problems relate and transform to other problems as part of an iterative, potentially backtracking, problem solving process, accounting for the way Software Engineering transforms the physical world to meet a recognised need. From an epistemological perspective, we argue how the theory has developed through research cycles including both theory-then-(empirical-)research and (empirical-)research-then-theory strategies spanning over a decade; both theoretical statements and related empirical evidence are included. Conclusion: The resulting theory provides descriptions and explanations for many phenomena observed in Software Engineering and in the combination of software with other technologies, and embodies analytic, explanatory and predictive properties. There are however acknowledged limitations and current research to overcome them is outlined.",
      "Keywords": "Design theory | General engineering | Problem orientation | Problem solving | Software engineering",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Hall, Jon G.;Rapanotti, Lucia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85011582035",
      "Primary study DOI": "10.1016/j.infsof.2017.01.011",
      "Title": "Using argumentation theory to analyse software practitioners  defeasible evidence, inference and belief",
      "Abstract": "Context Software practitioners are often the primary source of information for software engineering research. They naturally produce information about their experiences of software practice, and the beliefs they infer from their experiences. Researchers must evaluate the quality and quantity of this information for their research. Objective To examine how concepts and methods from argumentation research can be used to study practitioners’ evidence, inference and beliefs so as to better understand and improve software practice. Method We develop a preliminary framework and preliminary methodology, and use those to identify, extract and structure practitioners’ evidence, inference and beliefs. We illustrate the application of the framework and methodology with examples from a practitioner's blog post. Result The practitioner uses (factual) stories, analogies, examples and popular opinion as evidence, and uses that evidence in defeasible reasoning to justify his beliefs and to rebut the beliefs of other practitioners. Conclusion The framework, methodology and examples could provide a foundation for software engineering researchers to develop a more sophisticated understanding of, and appreciation for, practitioners’ defeasible evidence, inference and belief. Further work needs to automate (parts of) the methodology to support larger-scale application of the methodology.",
      "Keywords": "Analogy | Argumentation | Behavioural software engineering | Evidence | Evidence based software engineering | Experience | Explanation | Qualitative analysis | Software practice | Story",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Rainer, Austen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028270241",
      "Primary study DOI": "10.1016/j.infsof.2017.02.005",
      "Title": "A study on the statistical convertibility of IFPUG Function Point, COSMIC Function Point and Simple Function Point",
      "Abstract": "Background Several functional size measurement methods have been proposed. A few ones –like IFPUG and COSMIC methods– are widely used, while others –like Simple Function Points method– are interesting new proposals, which promise to deliver functional size measures via a faster and cheaper measurement process. Objectives Since all functional size measurement methods address the measurement of the same property of software (namely, the size of functional specifications), it is expected that measures provided in a given measurement unit can be converted into a different measurement unit. In this paper, convertibility of IFPUG Function Points, COSMIC Function Points, and Simple Function Points is studied. Method Convertibility is analyzed statistically via regression techniques. Seven datasets, each one containing measures of a set of software applications expressed in IFPUG Function Points, COSMIC Function Points and Simple Function Points, were analyzed. The components of functional size measures (usually known as Base Functional Components) were also involved in the analysis. Results All the analyzed measures appear well correlated to each other. Statistically significant quantitative models were found for all the combinations of measures, for all the analyzed datasets. Several models involving Base Functional Components were found as well. Conclusions From a practical point of view, the paper shows that converting measures from a given functional size unit into another one is viable. The magnitude of the conversion errors is reported, so that practitioners can evaluate if the expected conversion error is acceptable for their specific purposes. From a conceptual point of view, the paper shows that Base Functional Components of a given method can be used to estimate measures expressed in a different measurement unit: this seems to imply that different functional size measurement methods are ‘structurally’ strongly correlated.",
      "Keywords": "Base Functional Components (BFC) | Convertibility | COSMIC Function Points | Functional size measurement | IFPUG Function Points | Simple Function Points",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Abualkishik, Abedallah Zaid;Ferrucci, Filomena;Gravino, Carmine;Lavazza, Luigi;Liu, Geng;Meli, Roberto;Robiolo, Gabriela",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028249467",
      "Primary study DOI": "10.1016/j.infsof.2017.02.002",
      "Title": "Building reliable and maintainable Dynamic Software Product Lines: An investigation in the Body Sensor Network domain",
      "Abstract": "Context: Dependability is a key requirement, especially in safety-critical applications. Many of these applications have changing context and configurations at runtime to achieve functional and quality goals and can be realized as Dynamic Software Product Lines (DSPLs). DSPL constitutes an emerging but promising research area. Nevertheless, ensuring dependability in DSPLs remains insufficiently explored, especially in terms of reliability and maintainability. This compromises quality assurance and applicability of DSPLs in safety-critical domains, such as Body Sensor Network (BSN). Objective: To address this issue, we propose an approach to developing reliable and maintainable DSPLs in the context of the BSN domain. Method: Adaptation plans are instances of a Domain Specific Language (DSL) describing reliability goals and adaptability at runtime. These instances are automatically checked for reliability goal satisfiability before being deployed and interpreted at runtime to provide more suitable adaptation goals complying with evolving needs perceived by a domain specialist. Results: The approach is evaluated in the BSN domain. Results show that reliability and maintainability could be provided with execution and reconfiguration times of around 30 ms, notification and adaptation plan update time over the network around 5 s, and space consumption around 5 MB. Conclusion: The method is feasible at reasonable cost. The incurred benefits are reliable vital signal monitoring for the patient—thus providing early detection of serious health issues and the possibility of proactive treatment—and a maintainable infrastructure allowing medical DSL instance update to suit the needs of the domain specialist and ultimately of the patient.",
      "Keywords": "Adaptiveness | Body Sensor Network | Context-awareness | Dynamic Software Product Lines | Maintainability | Reliability",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Pessoa, Leonardo;Fernandes, Paula;Castro, Thiago;Alves, Vander;Rodrigues, Genaína N.;Carvalho, Hervaldo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028271394",
      "Primary study DOI": "10.1016/j.infsof.2017.01.005",
      "Title": "An examination of personality traits and how they impact on software development teams",
      "Abstract": "Context: Research has shown that a significant number of software projects fail due to social issues such as team or personality conflicts. However, only a limited number of empirical studies have been undertaken to understand the impact of individuals’ personalities on software team configurations. These studies suffer from an important limitation as they lack a systematic and rigorous method to relate personality traits of software practitioners and software team structures. Objective: Based on an interactive personality profiling approach, the goal of this study is to reveal the personality traits of software practitioners with an aim to explore effective software team structures. Method: To explore the importance of individuals’ personalities on software teams, we employed a two-step empirical approach. Firstly, to assess the personality traits of software practitioners, we developed a context-specific survey instrument, which was conducted on 216 participants from a middle-sized software company. Secondly, we propose a novel team personality illustration method to visualize team structures. Results: Study results indicated that effective team structures support teams with higher emotional stability, agreeableness, extroversion, and conscientiousness personality traits. Conclusion: Furthermore, empirical results of the current study show that extroversion trait was more predominant than previously suggested in the literature, which was especially more observable among agile software development teams.",
      "Keywords": "Interactive personality assessment | Social aspects of software development | Software developer personality traits | Software team visualization",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Yilmaz, Murat;O'Connor, Rory V.;Colomo-Palacios, Ricardo;Clarke, Paul",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028272067",
      "Primary study DOI": "10.1016/j.infsof.2017.01.009",
      "Title": "Systematic literature review on the impacts of agile release engineering practices",
      "Abstract": "Context Agile release engineering (ARE) practices are designed to deliver software faster and cheaper to end users; hence, claims of such impacts should be validated by rigorous and relevant empirical studies. Objective The study objective was to analyze both direct and indirect impacts of ARE practices as well as to determine how they have been empirically studied. Method The study applied the systematic literature review research method. ARE practices were identified in empirical studies by searching articles for “rapid release,” “continuous integration,” “continuous delivery,” and “continuous deployment.” We systematically analyzed 619 articles and selected 71 primary studies for deeper investigation. The impacts of ARE practices were analyzed from three viewpoints: impacts associated with adoption of the practice, prevalence of the practice, and success of software development. Results The results indicated that ARE practices can create shorter lead times and better communication within and between development teams. However, challenges and drawbacks were also found in change management, software quality assurance, and stakeholder acceptance. The analysis revealed that 33 out of 71 primary studies were casual experience reports that had neither an explicit research method nor a data collection approach specified, and 23 out of 38 empirical studies applied qualitative methods, such as interviews, among practitioners. Additionally, 12 studies applied quantitative methods, such as mining of software repositories. Only three empirical studies combined these research approaches. Conclusion ARE practices can contribute to improved efficiency of the development process. Moreover, release stakeholders can develop a better understanding of the software project's status. Future empirical studies should consider the comprehensive reporting of the context and how the practice is implemented instead of merely referring to usage of the practice. In addition, different stakeholder points of view, such as customer perceptions regarding ARE practices, still clearly require further research.",
      "Keywords": "Agile | Continuous delivery | Continuous deployment | Continuous integration | Rapid release | Release engineering",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-06-01",
      "Publication type": "Review",
      "Authors": "Karvonen, Teemu;Behutiye, Woubshet;Oivo, Markku;Kuvaja, Pasi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028245417",
      "Primary study DOI": "10.1016/j.infsof.2017.02.003",
      "Title": "Towards an understanding of change types in bug fixing code",
      "Abstract": "Context As developing high quality software becomes increasingly challenging because of the explosive growth of scale and complexity, bugs become inevitable in software systems. The knowledge of bugs will naturally guide software development and hence improve software quality. As changes in bug fixing code provide essential insights into the original bugs, analyzing change types is an intuitive and effective way to understand the characteristics of bugs. Objective In this work, we conduct a thorough empirical study to investigate the characteristics of change types in bug fixing code. Method We first propose a new change classification scheme with 5 change types and 9 change subtypes. We then develop an automatic classification tool CTforC to categorize changes. To gain deeper insights into change types, we perform our empirical study based on three questions from three perspectives, i.e. across project, across domain and across version. Results Based on 17 versions of 11 systems with thousands of faulty functions, we find that: (1) across project: the frequencies of change subtypes are significantly similar across most studied projects; interface related code changes are the most frequent bug-fixing changes (74.6% on average); most of faulty functions (65.2% on average) in studied projects are finally fixed by only one or two change subtypes; function call statements are likely to be changed together with assignment statements or branch statements; (2) across domain: the frequencies of change subtypes share similar trends across studied domains; changes on function call, assignment, and branch statements are often the three most frequent changes in studied domains; and (3) across version: change subtypes occur with similar frequencies across studied versions, and the most common subtype pairs tend to be same. Conclusion Our experimental results improve the understanding of changes in bug fixing code and hence the understanding of the characteristics of bugs.",
      "Keywords": "Bug fixing code | Change | Empirical study | Software quality | Understanding",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Zhao, Yangyang;Leung, Hareton;Yang, Yibiao;Zhou, Yuming;Xu, Baowen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027938830",
      "Primary study DOI": "10.1016/j.infsof.2017.01.003",
      "Title": "Software teams and their knowledge networks in large-scale software development",
      "Abstract": "Context Large software development projects involve multiple interconnected teams, often spread around the world, developing complex products for a growing number of customers and users. Succeeding with large-scale software development requires access to an enormous amount of knowledge and skills. Since neither individuals nor teams can possibly possess all the needed expertise, the resource availability in a team's knowledge network, also known as social capital, and effective knowledge coordination become paramount. Objective In this paper, we explore the role of social capital in terms of knowledge networks and networking behavior in large-scale software development projects. Method We conducted a multi-case study in two organizations, Ericsson and ABB, with software development teams as embedded units of analysis. We organized focus groups with ten software teams and surveyed 61 members from these teams to characterize and visualize the teams’ knowledge networks. To complement the team perspective, we conducted individual interviews with representatives of supporting and coordination roles. Based on survey data, data obtained from focus groups, and individual interviews, we compared the different network characteristics and mechanisms that support knowledge networks. We used social network analysis to construct the team networks, thematic coding to identify network characteristics and context factors, and tabular summaries to identify the trends. Results Our findings indicate that social capital and networking are essential for both novice and mature teams when solving complex, unfamiliar, or interdependent tasks. Network size and networking behavior depend on company experience, employee turnover, team culture, need for networking, and organizational support. A number of mechanisms can support the development of knowledge networks and social capital, for example, introduction of formal technical experts, facilitation of communities of practice and adequate communication infrastructure. Conclusions Our study emphasizes the importance of social capital and knowledge networks. Therefore, we suggest that, along with investments into training programs, software companies should also cultivate a networking culture to strengthen their social capital, a known driver of better performance.",
      "Keywords": "Agile | Case study | Cross-functional | Empirical | Feature teams | Intellectual capital | Knowledge networks | Large-scale software development | Social capital | Teams",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Šmite, Darja;Moe, Nils Brede;Šāblis, Aivars;Wohlin, Claes",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028241787",
      "Primary study DOI": "10.1016/j.infsof.2017.02.004",
      "Title": "Handling constraints in combinatorial interaction testing in the presence of multi objective particle swarm and multithreading",
      "Abstract": "Context Combinatorial testing strategies have lately received a lot of attention as a result of their diverse applications. In its simple form, a combinatorial strategy can reduce several input parameters (configurations) of a system into a small set based on their interaction (or combination). In practice, the input configurations of software systems are subjected to constraints, especially in case of highly configurable systems. To implement this feature within a strategy, many difficulties arise for construction. While there are many combinatorial interaction testing strategies nowadays, few of them support constraints. Objective This paper presents a new strategy, to construct combinatorial interaction test suites in the presence of constraints. Method The design and algorithms are provided in detail. To overcome the multi-judgement criteria for an optimal solution, the multi-objective particle swarm optimisation and multithreading are used. The strategy and its associated algorithms are evaluated extensively using different benchmarks and comparisons. Results Our results are promising as the evaluation results showed the efficiency and performance of each algorithm in the strategy. The benchmarking results also showed that the strategy can generate constrained test suites efficiently as compared to state-of-the-art strategies. Conclusion The proposed strategy can form a new way for constructing of constrained combinatorial interaction test suites. The strategy can form a new and effective base for future implementations.",
      "Keywords": "Constrained combinatorial interaction | Multi-objective particle swarm optimisation | Search-based software engineering | Test case design techniques | Test generation tools",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Ahmed, Bestoun S.;Gambardella, Luca M.;Afzal, Wasif;Zamli, Kamal Z.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85009204967",
      "Primary study DOI": "10.1016/j.infsof.2016.12.005",
      "Title": "A risk management framework for distributed agile projects",
      "Abstract": "Context Distributed agile development (DAD) approach has been adopted by the software companies for cost and time benefits. However, it causes significant challenges considering the contradicting nature of the agile and distributed development. Objective The objective of this study is to develop a risk management framework that comprises the perceived risks in DAD projects, their causes and the methods used in industry for managing those risks. Method This work is an extension of an exploratory study, wherein, DAD practitioners reported the risks they face in projects and the methods they use for managing those risks. The identified risks were further categorized based on their relevance to different aspects of DAD projects. In this extension, industry practitioners ranked the risks for their impact on DAD projects and rated the methods for the frequency of their use in projects. As the number of risks under each category was large for ranking, they were grouped under the risk areas within each category. The ranking of risk categories, risk areas and risk factors for their impact on DAD projects manifests their importance. The framework includes ranked risks, their causes and the risk management approaches. It was partially implemented in live projects in three different companies and was found to be beneficial. Results The perceived impact of the risk categories, ‘Group Awareness’, ‘External Stakeholder Collaboration’ and ‘Software Development Life Cycle’ on DAD projects has been found to be high and caused by the properties of Distributed Software Development (DSD). The partial validation of the framework in three companies reported the elimination of majority of risk factors and/or reduction in their impact. Conclusion DAD projects provide significant benefits but hold substantial risks due to the contradiction between distributed development and agile practices. The reported framework could effectively minimize the DAD risks in practice.",
      "Keywords": "Agile Software Development | Distributed Agile Development (DAD) | Distributed Software Development (DSD) | Risk factor classification | Risk management framework in distributed agile development",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Shrivastava, Suprika Vasudeva;Rathod, Urvashi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85009961943",
      "Primary study DOI": "10.1016/j.infsof.2017.01.007",
      "Title": "Systematic literature reviews in agile software development: A tertiary study",
      "Abstract": "Context A number of systematic literature reviews and mapping studies (SLRs) covering numerous primary research studies on various aspects of agile software development (ASD) exist. Objective The aim of this paper is to provide an overview of the SLRs on ASD research topics for software engineering researchers and practitioners. Method We followed the tertiary study guidelines by Kitchenham et al. to find SLRs published between late 1990s to December 2015. Results We found 28 SLRs focusing on ten different ASD research areas: adoption, methods, practices, human and social aspects, CMMI, usability, global software engineering (GSE), organizational agility, embedded systems, and software product line engineering. The number of SLRs on ASD topics, similar to those on software engineering (SE) topics in general, is on the rise. A majority of the SLRs applied standardized guidelines and the quality of these SLRs on ASD topics was found to be slightly higher for journal publications than for conferences. While some individuals and institutions seem to lead this area, the spread of authors and institutions is wide. With respect to prior review recommendations, significant progress was noticed in the area of connecting agile to established domains such as usability, CMMI, and GSE; and considerable progress was observed in focusing on management-oriented approaches as Scrum and sustaining ASD in different contexts such as embedded systems. Conclusion SLRs of ASD studies are on the rise and cover a variety of ASD aspects, ranging from early adoption issues to newer applications of ASD such as in product line engineering. ASD research can benefit from further primary and secondary studies on evaluating benefits and challenges of ASD methods, agile hybrids in large-scale setups, sustainability, motivation, teamwork, and project management; as well as a fresh review of empirical studies in ASD to cover the period post 2008.",
      "Keywords": "Agile software development | Mapping study | Systematic literature reviews | Tertiary study",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Hoda, Rashina;Salleh, Norsaremah;Grundy, John;Tee, Hui Mien",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85009743225",
      "Primary study DOI": "10.1016/j.infsof.2017.01.001",
      "Title": "Software test maturity assessment and test process improvement: A multivocal literature review",
      "Abstract": "Context Software testing practices and processes in many companies are far from being mature and are usually conducted in ad-hoc fashions. Such immature practices lead to various negative outcomes, e.g., ineffectiveness of testing practices in detecting all the defects, and cost and schedule overruns of testing activities. To conduct test maturity assessment (TMA) and test process improvement (TPI) in a systematic manner, various TMA/TPI models and approaches have been proposed. Objective It is important to identify the state-of-the-art and the –practice in this area to consolidate the list of all various test maturity models proposed by practitioners and researchers, the drivers of TMA/TPI, the associated challenges and the benefits and results of TMA/TPI. Our article aims to benefit the readers (both practitioners and researchers) by providing the most comprehensive survey of the area, to this date, in assessing and improving the maturity of test processes. Method To achieve the above objective, we have performed a Multivocal Literature Review (MLR) study to find out what we know about TMA/TPI. A MLR is a form of a Systematic Literature Review (SLR) which includes the grey literature (e.g., blog posts and white papers) in addition to the published (formal) literature (e.g., journal and conference papers). We searched the academic literature using the Google Scholar and the grey literature using the regular Google search engine. Results Our MLR and its results are based on 181 sources, 51 (29%) of which were grey literature and 130 (71%) were formally published sources. By summarizing what we know about TMA/TPI, our review identified 58 different test maturity models and a large number of sources with varying degrees of empirical evidence on this topic. We also conducted qualitative analysis (coding) to synthesize the drivers, challenges and benefits of TMA/TPI from the primary sources. Conclusion We show that current maturity models and techniques in TMA/TPI provides reasonable advice for industry and the research community. We suggest directions for follow-up work, e.g., using the findings of this MLR in industry-academia collaborative projects and empirical evaluation of models and techniques in the area of TMA/TPI as reported in this article.",
      "Keywords": "Multivocal literature review | Software testing | Systematic literature review | Test management | Test maturity | Test process | Test process assessment | Test process improvement",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-05-01",
      "Publication type": "Review",
      "Authors": "Garousi, Vahid;Felderer, Michael;Hacaloğlu, Tuna",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85009789130",
      "Primary study DOI": "10.1016/j.infsof.2017.01.006",
      "Title": "Taxonomies in software engineering: A Systematic mapping study and a revised taxonomy development method",
      "Abstract": "Context: Software Engineering (SE) is an evolving discipline with new subareas being continuously developed and added. To structure and better understand the SE body of knowledge, taxonomies have been proposed in all SE knowledge areas. Objective: The objective of this paper is to characterize the state-of-the-art research on SE taxonomies. Method: A systematic mapping study was conducted, based on 270 primary studies. Results: An increasing number of SE taxonomies have been published since 2000 in a broad range of venues, including the top SE journals and conferences. The majority of taxonomies can be grouped into the following SWEBOK knowledge areas: construction (19.55%), design (19.55%), requirements (15.50%) and maintenance (11.81%). Illustration (45.76%) is the most frequently used approach for taxonomy validation. Hierarchy (53.14%) and faceted analysis (39.48%) are the most frequently used classification structures. Most taxonomies rely on qualitative procedures to classify subject matter instances, but in most cases (86.53%) these procedures are not described in sufficient detail. The majority of the taxonomies (97%) target unique subject matters and many taxonomy-papers are cited frequently. Most SE taxonomies are designed in an ad-hoc way. To address this issue, we have revised an existing method for developing taxonomies in a more systematic way. Conclusion: There is a strong interest in taxonomies in SE, but few taxonomies are extended or revised. Taxonomy design decisions regarding the used classification structures, procedures and descriptive bases are usually not well described and motivated.",
      "Keywords": "Classification | Software engineering | Systematic mapping study | Taxonomy",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-05-01",
      "Publication type": "Review",
      "Authors": "Usman, Muhammad;Britto, Ricardo;Börstler, Jürgen;Mendes, Emilia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85008601796",
      "Primary study DOI": "10.1016/j.infsof.2016.10.006",
      "Title": "Who should comment on this pull request? Analyzing attributes for more accurate commenter recommendation in pull-based development",
      "Abstract": "Context: The pull-based software development helps developers make contributions flexibly and efficiently. Commenters freely discuss code changes and provide suggestions. Core members make decision of pull requests. Both commenters and core members are reviewers in the evaluation of pull requests. Since some popular projects receive many pull requests, commenters may not notice new pull requests in time, and even ignore appropriate pull requests. Objective: Our objective in this paper is to analyze attributes that affect the precision and recall of commenter prediction, and choose appropriate attributes to build commenter recommendation approach. Method: We collect 19,543 pull requests, 206,664 comments and 4817 commenters from 8 popular projects in GitHub. We build approaches based on different attributes, including activeness, text similarity, file similarity and social relation. We also build composite approaches, including time-based text similarity, time-based file similarity and time-based social relation. The time-based social relation approach is the state-of-the-art approach proposed by Yu et al. Then we compare precision and recall of different approaches. Results: We find that for 8 projects, the activeness based approach achieves the top-3 precision of 0.276, 0.386, 0.389, 0.516, 0.322, 0.572, 0.428, 0.402, and achieves the top-3 recall of 0.475, 0.593, 0.613, 0.66, 0.644, 0.791, 0.714, 0.65, which outperforms approaches based on text similarity, file similarity or social relation by a substantial margin. Moreover, the activeness based approach achieves better precision and recall than composite approaches. In comparison with the state-of-the-art approach, the activeness based approach improves the top-3 precision by 178.788%, 30.41%, 25.08%, 41.76%, 49.07%, 32.71%, 25.15%, 78.67%, and improves the top-3 recall by 196.875%, 36.32%, 29.05%, 46.02%, 43.43%, 27.79%, 25.483%, 79.06% for 8 projects. Conclusion: The activeness is the most important attribute in the commenter prediction. The activeness based approach can be used to improve the commenter recommendation in code review.",
      "Keywords": "Attribute selection | Commenter recommendation | Pull-based software development | Reviewer recommendation",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Jiang, Jing;Yang, Yun;He, Jiahuan;Blanc, Xavier;Zhang, Li",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85008193696",
      "Primary study DOI": "10.1016/j.infsof.2016.11.010",
      "Title": "FRLink: Improving the recovery of missing issue-commit links by revisiting file relevance",
      "Abstract": "Context: Though linking issues and commits plays an important role in software verification and maintenance, such link information is not always explicitly provided during software development or maintenance activities. Current practices in recovering such links highly depend on tedious manual examination. To automatically recover missing links, several approaches have been proposed to compare issue reports with log messages and source code files in commits. However, none of such approaches looked at the role of non-source code complementary documents in commits; nor did they consider the distinct roles each piece of the source code played in the same commit. Objective: We propose to revisit the definition of relevant files contributing to missing link recovery. More specifically, our work extends existing approaches from two perspectives: (1) Inclusion extension: incorporating complementary documents (i.e., non-source documents) to learn from more relevant data; (2) Exclusion extension: analyzing and filtering out irrelevant source code files to reduce data noise. Method: We propose a File Relevance-based approach (FRLink), to implement the above two considerations. FRLink utilizes non-source documents in commits, since they typically clarify code changes details, with similar textual information from corresponding issues. Moreover, FRLink differentiates the roles of different source code files in a single commit and discards files containing no similar code terms as those in issues based on similarity analysis. Results: FRLink is evaluated on 6 projects and compared with RCLinker, which is the latest state-of-the-art approach in missing link recovery. The result shows that FRLink outperforms RCLinker in F-Measure by 40.75% when achieving the highest recalls. Conclusion: FRLink can significantly improve the performance of missing link recovery compared with existing approaches. This indicates that in missing link recovery studies, sophisticated data selection and processing techniques necessitate more discussions due to the increasing variety and volume of information associated with issues and commits.",
      "Keywords": "Commit analysis | Information retrieval (IR) | Issue reports | Mining software repositories | Missing links",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Sun, Yan;Wang, Qing;Yang, Ye",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85007518175",
      "Primary study DOI": "10.1016/j.infsof.2016.12.004",
      "Title": "Software product lines traceability: A systematic mapping study",
      "Abstract": "Context: Traceability in Software Product Lines (SPL) is the ability to interrelate software engineering artifacts through required links to answer specific questions related to the families of products and underlying development processes. Despite the existence of studies to map out available evidence on traceability for single systems development, there is a lack of understanding on common strategies, activities, artifacts, and research gaps for SPL traceability. Objective: This paper analyzes 62 studies dating from 2001 to 2015 and discusses seven aspects of SPL traceability: main goals, strategies, application domains, research intensity, research challenges, rigor, and industrial relevance. In addition to the analysis, this paper also synthesizes the available evidence, identifies open issues and points out areas calling for further research. Method: To gather evidence, we defined a mapping study process adapted from existing guidelines. Driven by a set of research questions, this process comprises three major phases: planning, conducting, and documenting the review. Results: This work provides a structured understanding of SPL traceability, indicating areas for further research. The lack of evidence regarding the application of research methods indicates the need for more rigorous SPL traceability studies with better description of context, study design, and limitations. For practitioners, although most identified studies have low industrial relevance, a few of them have high relevance and thus could provide some decision making support for application of SPL traceability in practice. Conclusions: This work concludes that SPL traceability is maturing and pinpoints areas where further investigation should be performed. As future work, we intend to improve the comparison between traceability proposals for SPL and single-system development.",
      "Keywords": "Software and systems traceability | Software product lines | Software reuse | Systematic mapping study",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Vale, Tassio;de Almeida, Eduardo Santana;Alves, Vander;Kulesza, Uirá;Niu, Nan;de Lima, Ricardo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85008516012",
      "Primary study DOI": "10.1016/j.infsof.2016.12.003",
      "Title": "Towards comprehending the non-functional requirements through Developers  eyes: An exploration of Stack Overflow using topic analysis",
      "Abstract": "Context As a vital role for the quality of software, non-functional requirements (NFRs) are attracting greater attention from developers. The programming question and answer (Q&A) websites like Stack Overflow gathered the knowledge and expertise of developers worldwide which reflects some insight into the development activities (e.g., NFRs), but the NFRs in the Q&A site are rarely investigated. Objective Our research aims to aid comprehension on the actual thoughts and needs of the developers by analyzing the NFRs on Stack Overflow. Method We extracted the textual content of Stack Overflow discussions, and then we applied the topic modeling technique called latent Dirichlet allocation (LDA) helping us to discover the main topics of the corpus. Next, we labelled the topics with NFRs by the wordlists to analyze the hot, unresolved, difficult NFRs, and the evolutionary trends which involves the trends of the NFRs focus and NFRs difficulty. Results Our findings show that (1) The developers mostly discuss usability and reliability while discussing less on maintainability and efficiency. (2) The most unresolved problems also occurred in usability and reliability. (3) The visualization of the NFR evolutions over time shows the functionality and reliability attract more and more attention from developers and usability remains hot. (4) The NFRs investigation in specific technologies indicates the quality is a similar concern among different technologies and some NFRs are of more interest as time progresses. (5) The research on NFRs difficulty in specific technologies shows the maintainability is the most difficult NFR. In addition, the trends of the NFRs difficulty over time in the seven categories signal that we should focus more on usability to address them. Conclusion We present an empirical study on 21.7 million posts and 32.5 million comments of Stack Overflow, and our research provides some guide to understand the NFRs through developers’ eyes.",
      "Keywords": "Latent Dirichlet allocation (LDA) | Non-functional requirements (NFRs) | Stack Overflow | Topic model",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Zou, Jie;Xu, Ling;Yang, Mengning;Zhang, Xiaohong;Yang, Dan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85006307087",
      "Primary study DOI": "10.1016/j.infsof.2016.11.005",
      "Title": "An experiment on an ontology-based support approach for process modeling",
      "Abstract": "Context: Recent research discusses the use of ontologies, dictionaries and thesaurus as a means to improve activity labels of process models. However, the trade-off between quality improvement and extra effort is still an open question. It is suspected that ontology-based support could require additional effort for the modeler. Objective: In this paper, we investigate to which degree ontology-based support potentially increases the effort of modeling. We develop a theoretical perspective grounded in cognitive psychology, which leads us to the definition of three design principles for appropriate ontology-based support. The objective is to evaluate the design principles through empirical experimentation. Method: We tested the effect of presenting relevant content from the ontology to the modeler by means of a quantitative analysis. We performed controlled experiments using a prototype, which generates a simplified and context-aware visual representation of the ontology. It logs every action of the process modeler for analysis. The experiment refers to novice modelers and was performed as between-subject design with vs. without ontology-based support. It was carried out with two different samples. Results: Part of the effort-related variables we measured showed significant statistical difference between the group with and without ontology-based support. Overall, for the collected data, the ontology support achieved good results. Conclusion: We conclude that it is feasible to provide ontology-based support to the modeler in order to improve process modeling without strongly compromising time consumption and cognitive effort.",
      "Keywords": "Activity labels | Business process modeling | Cognitive load | Ontologies | Process of process modeling",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Bulegon Gassen, Jonas;Mendling, Jan;Bouzeghoub, Amel;Thom, Lucinéia Heloisa;Palazzo M. de Oliveira, José",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85006293265",
      "Primary study DOI": "10.1016/j.infsof.2016.11.006",
      "Title": "Cinders: The continuous integration and delivery architecture framework",
      "Abstract": "Context: The popular agile practices of continuous integration and delivery have become an essential part of the software development process in many companies, yet effective methods and tools to support design, description and communication of continuous integration and delivery systems are lacking. Objective: The work reported on in this paper addresses that lack by presenting Cinders — an architecture framework designed specifically to meet the needs of such systems, influenced both by prominent enterprise and software architecture frameworks as well as experiences from continuous integration and delivery modeling in industry. Method: The state of the art for systematic design and description of continuous integration and delivery systems is established through review of literature, whereupon a proposal for an architecture framework addressing requirements derived from continuous integration and delivery modeling experiences is proposed. This framework is subsequently evaluated through interviews and workshops with engineers in varying roles in three independent companies. Results: Cinders, an architecture framework designed specifically for the purpose of describing continuous integration and delivery systems is proposed and confirmed to constitute an improvement over previous methods. This work presents software professionals with a demonstrably effective method for describing their continuous integration and delivery systems from multiple points of view and supporting multiple use-cases, including system design, communication and documentation. Conclusion: It is concluded that an architecture framework for the continuous integration and delivery domain has value; at the same time potential for further improvement is identified, particularly in the area of tool support for data collection as well as for manual modeling.",
      "Keywords": "Architecture framework | Cinders | Continuous delivery | Continuous integration | Software integration | Software testing",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Ståhl, Daniel;Bosch, Jan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85006313110",
      "Primary study DOI": "10.1016/j.infsof.2016.11.011",
      "Title": "Goal-based testing of semantic web services",
      "Abstract": "Context: Recent years have witnessed growing interests in semantic web and its related technologies. While various frameworks have been proposed for designing semantic web services (SWS), few of them aim at testing. Objective: This paper investigates into the technologies for automatically deriving test cases from semantic web service descriptions based on the Web Service Modeling Ontology (WSMO) framework. Method: WSMO goal specifications were translated into B abstract machines. Test cases were generated via model checking with calculated trap properties from coverage criteria. Furthermore, we employed mutation analysis to evaluate the test suite. In this approach, the model-based test case generation and code-based evaluation techniques are independent of each other, which provides much more accurate measures of the testing results. Results: We applied our approach to a real-world case study of the Amazon E-Commerce Service (ECS). The experimental results have validated the effectiveness of the proposed solution. Conclusion: It is concluded that our approach is capable of automatically generating an effective set of test cases from the WSMO goal descriptions for SWS testing. The quality of test cases was measured in terms of their abilities to discover the injected faults at the code level. We implemented a tool to automate the steps for the mutation-based evaluation.",
      "Keywords": "Formal methods | Model checking | Mutation analysis | Semantic web services | Web Service Modeling Ontology",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Jokhio, M. Shaban;Sun, Jing;Dobbie, Gillian;Hu, Tianming",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85006282719",
      "Primary study DOI": "10.1016/j.infsof.2016.11.007",
      "Title": "Search-based software library recommendation using multi-objective optimization",
      "Abstract": "Context: Software library reuse has significantly increased the productivity of software developers, reduced time-to-market and improved software quality and reusability. However, with the growing number of reusable software libraries in code repositories, finding and adopting a relevant software library becomes a fastidious and complex task for developers. Objective: In this paper, we propose a novel approach called LibFinder to prevent missed reuse opportunities during software maintenance and evolution. The goal is to provide a decision support for developers to easily find “useful” third-party libraries to the implementation of their software systems. Method: To this end, we used the non-dominated sorting genetic algorithm (NSGA-II), a multi-objective search-based algorithm, to find a trade-off between three objectives: 1) maximizing co-usage between a candidate library and the actual libraries used by a given system, 2) maximizing the semantic similarity between a candidate library and the source code of the system, and 3) minimizing the number of recommended libraries. Results: We evaluated our approach on 6083 different libraries from Maven Central super repository that were used by 32,760 client systems obtained from Github super repository. Our results show that our approach outperforms three other existing search techniques and a state-of-the art approach, not based on heuristic search, and succeeds in recommending useful libraries at an accuracy score of 92%, precision of 51% and recall of 68%, while finding the best trade-off between the three considered objectives. Furthermore, we evaluate the usefulness of our approach in practice through an empirical study on two industrial Java systems with developers. Results show that the top 10 recommended libraries was rated by the original developers with an average of 3.25 out of 5. Conclusion: This study suggests that (1) library usage history collected from different client systems and (2) library semantics/content embodied in library identifiers should be balanced together for an efficient library recommendation technique.",
      "Keywords": "Multi-objective optimization | Search-based software engineering | Software library | Software reuse",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Ouni, Ali;Kula, Raula Gaikovina;Kessentini, Marouane;Ishio, Takashi;German, Daniel M.;Inoue, Katsuro",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85006307160",
      "Primary study DOI": "10.1016/j.infsof.2016.11.004",
      "Title": "Model-based security engineering for cyber-physical systems: A systematic mapping study",
      "Abstract": "Context Cyber-physical systems (CPSs) have emerged to be the next generation of engineered systems driving the so-called fourth industrial revolution. CPSs are becoming more complex, open and more prone to security threats, which urges security to be engineered systematically into CPSs. Model-Based Security Engineering (MBSE) could be a key means to tackle this challenge via security by design, abstraction, and automation. Objective We aim at providing an initial assessment of the state of the art in MBSE for CPSs (MBSE4CPS). Specifically, this work focuses on finding out 1) the publication statistics of MBSE4CPS studies; 2) the characteristics of MBSE4CPS studies; and 3) the open issues of MBSE4CPS research. Method We conducted a systematic mapping study (SMS) following a rigorous protocol that was developed based on the state-of-the-art SMS and systematic review guidelines. From thousands of relevant publications, we systematically identified 48 primary MBSE4CPS studies for data extraction and synthesis to answer predefined research questions. Results SMS results show that for three recent years (2014–2016) the number of primary MBSE4CPS studies has increased significantly. Within the primary studies, the popularity of using Domain-Specific Languages (DSLs) is comparable with the use of the standardised UML modelling notation. Most primary studies do not explicitly address specific security concerns (e.g., confidentiality, integrity) but rather focus on security analyses in general on threats, attacks or vulnerabilities. Few primary studies propose to engineer security solutions for CPSs. Many focus on the early stages of development lifecycle such as security requirement engineering or analysis. Conclusion The SMS does not only provide the state of the art in MBSE4CPS, but also points out several open issues that would deserve more investigation, e.g., the lack of engineering security solutions for CPSs, limited tool support, too few industrial case studies, and the challenge of bridging DSLs in engineering secure CPSs.",
      "Keywords": "Cyber-physical systems | Model-based engineering | Security | Security engineering | Snowballing | Survey | Systematic mapping",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-03-01",
      "Publication type": "Review",
      "Authors": "Nguyen, Phu H.;Ali, Shaukat;Yue, Tao",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84993990146",
      "Primary study DOI": "10.1016/j.infsof.2016.10.004",
      "Title": "Analyzing the concept of technical debt in the context of agile software development: A systematic literature review",
      "Abstract": "Context Technical debt (TD) is a metaphor that is used to communicate the consequences of poor software development practices to non-technical stakeholders. In recent years, it has gained significant attention in agile software development (ASD). Objective The purpose of this study is to analyze and synthesize the state of the art of TD, and its causes, consequences, and management strategies in the context of ASD. Research Method Using a systematic literature review (SLR), 38 primary studies, out of 346 studies, were identified and analyzed. Results We found five research areas of interest related to the literature of TD in ASD. Among those areas, “managing TD in ASD” received the highest attention, followed by “architecture in ASD and its relationship with TD”. In addition, eight categories regarding the causes and five categories regarding the consequences of incurring TD in ASD were identified. “Focus on quick delivery” and “architectural and design issues” were the most popular causes of incurring TD in ASD. “Reduced productivity”, “system degradation” and “increased maintenance cost” were identified as significant consequences of incurring TD in ASD. Additionally, we found 12 strategies for managing TD in the context of ASD, out of which “refactoring” and “enhancing the visibility of TD” were the most significant. Conclusion The results of this study provide a structured synthesis of TD and its management in the context of ASD as well as potential research areas for further investigation.",
      "Keywords": "Agile software development | Systematic literature review | Technical debt | Technical debt management",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-02-01",
      "Publication type": "Review",
      "Authors": "Behutiye, Woubshet Nema;Rodríguez, Pilar;Oivo, Markku;Tosun, Ayşe",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84994350835",
      "Primary study DOI": "10.1016/j.infsof.2016.10.005",
      "Title": "Quality attributes and quality models for ambient assisted living software systems: A systematic mapping",
      "Abstract": "Context: Ambient Assisted Living (AAL) has become an essential, multidisciplinary research topic, aiming at providing software systems and services that assist people in their everyday life activities. Considering the critical nature of AAL systems, several initiatives have already contributed to the improvement of their quality, by mainly focusing on their non-functional requirements. Despite the importance of quality assurance in AAL systems, there is a lack of a comprehensive analysis on how quality assurance is performed in such systems. This fact might in turn lead to an absence of standardization with regard to the quality assurance process of these systems. Objective: We provide a broad, detailed panorama about the state of the art on quality models (QMs) and quality attributes (QAs) that are important for the AAL domain. Method: We performed a Systematic Mapping (SM). We used six publication databases to cover all published material pertinent for our SM. We initially obtained 287 studies that were filtered based on a set of well-defined inclusion/exclusion criteria, resulting into a set of 27 studies that were used for exploring QAs for AAL systems. Results: The most common QAs used in the development of AAL systems were identified and defined. We also characterized important critical attributes for software systems in the AAL domain. Additionally, QAs for some AAL sub-domains were defined. Furthermore, we investigated how QM&QA have been defined, evaluated, and used in that domain. Finally, we offered an analysis of the maturity of the studies identified in our SM. Conclusion: It is necessary to develop a complete QM that: (i) defines all common QAs for AAL systems; (ii) considers variability of QAs among AAL sub-domains; (iii) analyses dependences among QAs; (iv) offers indicators or metrics to measure QAs; and (v) offers means to assess and predict quality of AAL systems.",
      "Keywords": "Ambient assisted living | ISO/IEC 25010 | Quality attribute | Quality model | Systematic mapping",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Garcés, Lina;Ampatzoglou, Apostolos;Avgeriou, Paris;Nakagawa, Elisa Yumi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85010202046",
      "Primary study DOI": "10.1016/j.infsof.2016.11.003",
      "Title": "An idiom to represent data types in Alloy",
      "Abstract": "Context: It is common to consider Alloy signatures or UML classes as data types that have a canonical fixed interpretation: the elements of the type correspond to terms recursively generated by the type constructors. However, these language constructs resemble data types but, strictly, they are not. Objective: In this article, we propose an idiom to specify data types in Alloy. Method: We compare our approach to others in the context of checking data refinement using the Alloy Analyzer tool. Results: Some previous studies do not include the generation axiom and may perform unsound analysis. Other studies recommend some optimizations to overcome a limitation in the Alloy Analyzer tool. Conclusion: The problem is not related to the tool but the way data types must be represented in Alloy. This study shows the importance of using automated analyses to test translation between different language constructs.",
      "Keywords": "Alloy | Data types",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Gheyi, Rohit;Borba, Paulo;Sampaio, Augusto;Ribeiro, Márcio",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84992523467",
      "Primary study DOI": "10.1016/j.infsof.2016.09.011",
      "Title": "Improving modular structure of software system using structural and lexical dependency",
      "Abstract": "Context A software system's structure often degrades due to repetitive maintenance. To make a sustainable evolution of such systems, it becomes mandatory to improve their modular structure after a certain time. Many remodularization approaches were proposed to improve the modular structure of software systems. Most of the existing approaches rely on structural or lexical dependencies. However, there is a lack of research that distinguishes different types of structural (e.g., inheritance, method calls, references, etc.) or lexical (Name of classes, methods, variables, etc.) dependencies, but assumes that they are equivalent, which is illogical from a software developer's point of view. Objective In this paper, we propose an approach that considers various types of structural as well as lexical dependencies along with their relative importance to remodularize the Object-Oriented (OO) systems. The main goal of the paper is to generate remodularization solutions that can reflect the developers' perspective (as visible in the well-modularized software system) of remodularization, which is highly desirable in software evolution. Method The paper computes coupling strength among classes using different weights (computed on basis of well-modularized software system) in terms of various mechanisms of structural and lexical dependencies. Software remodularization problem is formulated as a single and multi-objective optimization problem and solved using Genetic Algorithms (GA). Based on the different types of structural and lexical dependencies and as per their un-weighted/weighted variants, we have designed following 24 coupling schemes: structural-based (i.e., SBUW, SBW, SAUW, SAW, STFUW, STFW, STFIDFUW, and STFIDFW), lexical-based (i.e., LBUW, LBW, LAUW, LAW, LTFUW, LTFW, LTFIDFUW, and LTFIDFW), and combined structural-lexical based (i.e., SLBUW, SLBW, SLAUW, SLAW, SLTFUW, SLTFW, SLTFIDFUW, and SLTFIDFW). Values obtained through these coupling schemes are used in coupling and cohesion objective function of the GA. Along with this objective, some supportive objective functions such as MCI and MSI have been used to drive the optimization process towards a good quality modularization solution. Results We assess the effectiveness of our proposed remodularization approach over eight real-world object-oriented software systems in terms of original design of the experimented software systems and modularization decisions provided by the developers. Results indicate that TFIDF based weighted variants (i.e. STFIDFW, LTFIDFW, and SLTFIDFW) of each broad three categories outperformed rest of variants within each category. However, TFIDF weighted variant in the third broad category (i.e., SLTFIDFW) outperformed all others. Conclusion Our combined lexical-structural approach (SLTFIDFW) considering various types of dependencies along with their relative weights performs well and results into better remodularization compared to rest of considered alternates. It also shows significant improvement over techniques based on only lexical or structural information. Thus this approach can be very useful to improve the quality of the software whose remodularization quality deteriorates beyond accepted level.",
      "Keywords": "Multi-objective optimization | Remodularization | Search-based software engineering",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Amarjeet, ;Chhabra, Jitender Kumar",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84995673130",
      "Primary study DOI": "10.1016/j.infsof.2016.11.002",
      "Title": "Improved bug localization based on code change histories and bug reports",
      "Abstract": "Context Several issues or defects in released software during the maintenance phase are reported to the development team. It is costly and time-consuming for developers to precisely localize bugs. Bug reports and the code change history are frequently used and provide information for identifying fault locations during the software maintenance phase. Objective It is difficult to standardize the style of bug reports written in natural languages to improve the accuracy of bug localization. The objective of this paper is to propose an effective information retrieval-based bug localization method to find suspicious files and methods for resolving bugs. Method In this paper, we propose a novel information retrieval-based bug localization approach, termed Bug Localization using Integrated Analysis (BLIA). Our proposed BLIA integrates analyzed data by utilizing texts, stack traces and comments in bug reports, structured information of source files, and the source code change history. We improved the granularity of bug localization from the file level to the method level by extending previous bug repository data. Results We evaluated the effectiveness of our approach based on experiments using three open-source projects, namely AspectJ, SWT, and ZXing. In terms of the mean average precision, on average our approach improves the metric of BugLocator, BLUiR, BRTracer, AmaLgam and the preliminary version of BLIA by 54%, 42%, 30%, 25% and 15%, respectively, at the file level of bug localization. Conclusion Compared with prior tools, the results showed that BLIA outperforms these other methods. We analyzed the influence of each score of BLIA from various combinations based on the analyzed information. Our proposed enhancement significantly improved the accuracy. To improve the granularity level of bug localization, a new approach at the method level is proposed and its potential is evaluated.",
      "Keywords": "Bug localization | Bug reports | Code change history | Information retrieval | Method analysis | Stack traces",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Youm, Klaus Changsun;Ahn, June;Lee, Eunseok",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85006870209",
      "Primary study DOI": "10.1016/j.infsof.2016.10.001",
      "Title": "Problems, causes and solutions when adopting continuous delivery A systematic literature review",
      "Abstract": "Context: Continuous delivery is a software development discipline in which software is always kept releasable. The literature contains instructions on how to adopt continuous delivery, but the adoption has been challenging in practice. Objective: In this study, a systematic literature review is conducted to survey the faced problems when adopting continuous delivery. In addition, we identify causes for and solutions to the problems. Method: By searching five major bibliographic databases, we identified 293 articles related to continuous delivery. We selected 30 of them for further analysis based on them containing empirical evidence of adoption of continuous delivery, and focus on practice instead of only tooling. We analyzed the selected articles qualitatively and extracted problems, causes and solutions. The problems and solutions were thematically synthesized into seven themes: build design, system design, integration, testing, release, human and organizational and resource. Results: We identified a total of 40 problems, 28 causal relationships and 29 solutions related to adoption of continuous delivery. Testing and integration problems were reported most often, while the most critical reported problems were related to testing and system design. Causally, system design and testing were most connected to other themes. Solutions in the system design, resource and human and organizational themes had the most significant impact on the other themes. The system design and build design themes had the least reported solutions. Conclusions: When adopting continuous delivery, problems related to system design are common, critical and little studied. The found problems, causes and solutions can be used to solve problems when adopting continuous delivery in practice.",
      "Keywords": "Continuous delivery | Continuous deployment | Continuous integration | Systematic literature review",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-02-01",
      "Publication type": "Review",
      "Authors": "Laukkanen, Eero;Itkonen, Juha;Lassenius, Casper",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84991790848",
      "Primary study DOI": "10.1016/j.infsof.2016.09.009",
      "Title": "Twenty years of object-relational mapping: A survey on patterns, solutions, and their implications on application design",
      "Abstract": "Context Almost twenty years after the first release of TopLink for Java, Object-Relational Mapping Solutions (ORMSs) are available at every popular development platform, providing useful tools for developers to deal with the impedance mismatch problem. However, no matter how ubiquitous these solutions are, this essential problem remains as challenging as ever. Different solutions, each with a particular vocabulary, are difficult to learn, and make the impedance problem looks deceptively simpler than it really is. Objective The objective of this paper is to identify, discuss, and organize the knowledge concerning ORMSs, helping designers towards making better informed decisions about designing and implementing their models, focusing at the static view of persistence mapping. Method This paper presents a survey with nine ORMSs, selected from the top ten development platforms in popularity. Each ORMS was assessed, by documentation review and experience, in relation to architectural and structural patterns, selected from literature, and its characteristics and implementation options, including platform specific particularities. Results We found out that all studied ORMSs followed architectural and structural patterns in the literature, but often with distinct nomenclature, and some singularities. Many decisions, depending on how patterns are implemented and configured, affect how class models should be adapted, in order to create practical mappings to the database. Conclusion This survey identified what structural patterns each ORMS followed, highlighting major structural decisions a designer must take, and its consequences, in order to turn analysis models into object oriented systems. It also offers a pattern based set of characteristics that developers can use as a baseline to make their own assessments of ORMSs.",
      "Keywords": "Class models | Design patterns | Enterprise patterns | Impedance mismatch problem | Object-relational mapping",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Torres, Alexandre;Galante, Renata;Pimenta, Marcelo S.;Martins, Alexandre Jonatan B.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84991641336",
      "Primary study DOI": "10.1016/j.infsof.2016.09.010",
      "Title": "Reliability-redundancy-location allocation with maximum reliability and minimum cost using search techniques",
      "Abstract": "Context A safety critical system requires an automated and optimal allocation of redundant component instances to its existing components, including: 1) the selection of components (locations) on which the redundancy must be applied, 2) how many redundant component instances of varying reliability and cost should be allocated to each selected location. Objective Our work aims to searching for the near optimal allocation solutions achieving the higher reliability of the system within the allowed cost. Such allocation must be made earlier, for example, while designing the architecture of the system to avoid unnecessary complexity of addressing unsafe situations discovered in the system development and deployment phases. Method With the above objective in mind, we propose a search-based allocation approach based on the overall objectives of maximizing the overall system reliability and minimizing the cost of introducing and allocating redundancy structures to the system. The architecture of a system modeled using the Unified Modeling Language (UML) along with redundancy structures is encoded as an optimization problem. To guide a search algorithm to solve the problem, we propose a fitness function based on the two optimization objectives: high reliability and low cost. Results We empirically evaluated the performance of four search algorithms (Genetic Algorithm, (1 + 1) Evolutionary Algorithm, Alternating Variable Method (AVM) and Random Search) together with the proposed fitness function on 10 real-world Subsea Oil&Gas Production Systems of varying complexity. Results show that the AVM algorithm significantly outperforms the rest. Conclusion Based on the results of empirical evaluation, we found that AVM can provide the best allocation of redundancy structures as compared to the rest of the algorithms. On average, AVM provided 0.008% of more reliability while saving 26.78% on allocation cost as compared to RS. Our novel solution based on the results of empirical evaluation is implemented as a software tool.",
      "Keywords": "Empirical studies | Optimization | Redundancy structure | Reliability block diagram | Search algorithms | UML",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Qiu, Xiang;Ali, Shaukat;Yue, Tao;Zhang, Li",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84992146806",
      "Primary study DOI": "10.1016/j.infsof.2016.10.002",
      "Title": "MUSEUM: Debugging real-world multilingual programs using mutation analysis",
      "Abstract": "Context: The programming language ecosystem has diversified over the last few decades. Non-trivial programs are likely to be written in more than a single language to take advantage of various control/data abstractions and legacy libraries. Objective: Debugging multilingual bugs is challenging because language interfaces are difficult to use correctly and the scope of fault localization goes beyond language boundaries. To locate the causes of real-world multilingual bugs, this article proposes a mutation-based fault localization technique (MUSEUM). Method: MUSEUM modifies a buggy program systematically with our new mutation operators as well as conventional mutation operators, observes the dynamic behavioral changes in a test suite, and reports suspicious statements. To reduce the analysis cost, MUSEUM selects a subset of mutated programs and test cases. Results: Our empirical evaluation shows that MUSEUM is (i) effective: it identifies the buggy statements as the most suspicious statements for both resolved and unresolved non-trivial bugs in real-world multilingual programming projects; and (ii) efficient: it locates the buggy statements in modest amount of time using multiple machines in parallel. Also, by applying selective mutation analysis (i.e., selecting subsets of mutants and test cases to use), MUSEUM achieves significant speedup with marginal accuracy loss compared to the full mutation analysis. Conclusion: It is concluded that MUSEUM locates real-world multilingual bugs accurately. This result shows that mutation analysis can provide an effective, efficient, and language semantics agnostic analysis on multilingual code. Our light-weight analysis approach would play important roles as programmers write and debug large and complex programs in diverse programming languages.",
      "Keywords": "Debugging | Foreign function interface | Language interoperability | mutation analysis",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Hong, Shin;Kwak, Taehoon;Lee, Byeongcheol;Jeon, Yiru;Ko, Bongseok;Kim, Yunho;Kim, Moonzoo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84994832137",
      "Primary study DOI": "10.1016/j.infsof.2016.11.001",
      "Title": "Assessing the influence of feedback-inclusive rapid prototyping on understanding the semantics of parallel UML statecharts by novice modellers",
      "Abstract": "Context UML diagrams are the de facto standard for analysing, communicating and designing software systems, as well as automated code generation. However there is a certain degree of difficulty in understanding a system represented by means of UML diagrams. Object Our previous research demonstrates a significant improvement in understanding the structural aspects of a system represented as a UML class diagram when using a feedback-inclusive prototype of a model. This paper extends our previous work with an empirical validation study for the effectiveness of the feedback-inclusive rapid prototyping (FIRP) method, on the comprehension of system dynamics represented as multiple interacting UML statecharts. Because models often combine structural and behavioural views that are highly intertwined, we additionally evaluate the effectiveness of the proposed method with respect to comprehension of the between-view consistency. Method The FIRP environment was built following the principles of Design Science Research in Information Systems. This study targets the empirical validation of the effectiveness of the proposed technique using an experimental study method. Two experiments were conducted with the participation of 65 final-year master students in the context of different modelling courses from different study programs at KU Leuven using two two-group factorial experimental designs. The effectiveness of the FIRP method was measured by comparing students’ performance between the cycles with and without the use of the method, using the understandability (comprehension test results) as the dependent variable and the use of FIRP as the independent variable. Effects from unknown variables were neutralized by means of randomized group compositions. The effectiveness of FIRP was additionally assessed with respect to personal characteristics (age, gender, previous knowledge, self-efficacy) and user acceptance (perceived ease of use, perceived utility, preference, satisfaction). Results The findings reveal a significant positive impact of the use of the prototyping technique on students’ comprehension of system dynamics represented as multiple interacting statecharts. Conclusions The findings provide empirical support for the advantage of the use of FIRP over manual inspection of interacting statecharts. The findings also suggest that the method is suitable for training system's analysis and modelling skills when UML statecharts are involved.",
      "Keywords": "Automated feedback | Conceptual modelling | Model pragmatics | Prototyping/simulation | Statechart | Testability of requirements",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Sedrakyan, Gayane;Poelmans, Stephan;Snoeck, Monique",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84990940968",
      "Primary study DOI": "10.1016/j.infsof.2016.09.008",
      "Title": "Automated refactoring of super-class method invocations to the Template Method design pattern",
      "Abstract": "Context: Implementation inheritance, i.e., overriding of concrete method implementations through subtyping, is prone to potential class contract violations. Call Super is a code pattern that employs implementation inheritance for extending a method's behaviour. In Call Super the overriding method includes in its body an invocation to the overridden method. TEMPLATE METHOD is a design pattern that enables extensions to a multi-step procedure without overriding its concrete implementation. Instead, subclasses provide different variants of the template method's behaviour through implementation of abstract method definitions (interface inheritance). Objective: This work studies the automated refactoring of Call Super to TEMPLATE METHOD, contributing, thus, to replacement of implementation inheritance with interface inheritance. Method: We introduce an algorithm for the discovery of refactoring candidates that is based on an extensive set of refactoring preconditions. Moreover, we specify the source code transformation for refactoring a Call Super instance to TEMPLATE METHOD. An implementation of the proposed approach is evaluated on a set of open source Java projects. Results: The evaluation results highlight (a) the frequent occurrence of the Call Super pattern among method overridings, (b) the potential provided by our method for discovery and elimination of several non-trivial Call Super instances and (c) the resulting code improvement, as reflected by the Specialization Index metric and the alignment of refactored code with the programmer's intent. The application of all refactorings identified on a set of benchmark projects and the successful execution of their test suites provide empirical evidence on the soundness of the refactoring procedure. Runtime performance results support the scalability of the proposed method. Conclusion: The proposed method automates the replacement of implementation inheritance with interface inheritance through refactoring Call Super instances to TEMPLATE METHOD. The empirical evaluation of the method supports its applicability, soundness and runtime efficiency.",
      "Keywords": "Call Super | Implementation inheritance | Interface inheritance | Refactoring | Template Method design pattern",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Zafeiris, Vassilis E.;Poulias, Sotiris H.;Diamantidis, N. A.;Giakoumakis, E. A.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84963813590",
      "Primary study DOI": "10.1016/j.infsof.2016.04.007",
      "Title": "Using mutation to design tests for aspect-oriented models",
      "Abstract": "Context: Testing for properties such as robustness or security is complicated because their concerns are often repeated in many locations and muddled with the normal code. Such “cross-cutting concerns” include things like interrupt events, exception handling, and security protocols. Aspect-oriented (AO) modeling allows developers to model the cross-cutting behavior independently of the normal behavior, thus supporting model-based testing of cross-cutting concerns. However, mutation operators defined for AO programs (source code) are usually not applicable to AO models (AOMs) and operators defined for models do not target the AO features. Objective: We present a method to design abstract tests at the aspect-oriented model level. We define mutation operators for aspect-oriented models and evaluate the generated mutants for an example system. Method: AOMs are mutated with novel operators that specifically target the AO modeling features. Test traces killing these mutant models are then generated. The generated and selected traces are abstract tests that can be transformed to concrete black-box tests and run on the implementation level, to evaluate the behavior of the woven cross-cutting concerns (combined aspect and base models). Results: This paper is a significant extension of our paper at Mutation 2015. We present a complete fault model, additional mutation operators, and a thorough analysis of the mutants generated for an example system. Conclusions: The analysis shows that some mutants are stillborn (syntactically illegal) but none is equivalent (exhibiting the same behavior as the original model). Additionally, our AOM-specific mutation operators can be combined with pre-existing operators to mutate code or models without any overlap.",
      "Keywords": "Aspect-oriented model | Model-based testing | Mutation testing",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Lindström, Birgitta;Offutt, Jeff;Sundmark, Daniel;Andler, Sten F.;Pettersson, Paul",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84958191450",
      "Primary study DOI": "10.1016/j.infsof.2016.01.017",
      "Title": "A systematic review on search based mutation testing",
      "Abstract": "Context Search Based Software Testing refers to the use of meta-heuristics for the optimization of a task in the context of software testing. Meta-heuristics can solve complex problems in which an optimum solution must be found among a large amount of possibilities. The use of meta-heuristics in testing activities is promising because of the high number of inputs that should be tested. Previous studies on search based software testing have focused on the application of meta-heuristics for the optimization of structural and functional criteria. Recently, some researchers have proposed the use of SBST for mutation testing and explored solutions for the cost of application of this testing criterion. Objective The objective is to identify how SBST has been explored in the context of mutation testing, how fitness functions are defined and the challenges and research opportunities in the application of meta-heuristic search techniques. Method A systematic review involving 263 papers published between 1996 and 2014 examined the studies on the use of meta-heuristic search techniques for the optimization of mutation testing. Results The results show meta-heuristic search techniques have been applied for the optimization of test data generation, mutant generation and selection of effective mutation operators. Five meta-heuristic techniques, namely Genetic Algorithm, Ant Colony, Bacteriological Algorithm, Hill Climbing and Simulated Annealing have been used in search based mutation testing. The review addressed different fitness functions used to guide the search. Conclusion Search based mutation testing is a field of interest, however, some issues remain unexplored. For instance, the use of meta-heuristics for the selection of effective mutation operators was identified in only one study. The results have pointed a range of possibilities for new studies to be developed, i.e., identification of equivalent mutants, experimental studies and application to different domains, such as concurrent programs.",
      "Keywords": "Meta-heuristic | Mutation testing | Search based software testing",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Silva, Rodolfo Adamshuk;Senger de Souza, Simone do Rocio;Lopes de Souza, Paulo Sérgio",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85032975393",
      "Primary study DOI": "10.1016/j.infsof.2017.10.010",
      "Title": "Calculating completeness of software project scope definition",
      "Abstract": "Context Software project plan is the basis of the project execution, and its quality depends on completeness of software scope definition. A method is required that should gauge the completeness of different aspects of scope definition, thus, providing guidance to practitioners to reconsider areas that have not been defined well. Objective This paper aims to evaluate completeness of software project scope definition. It identifies a detailed list of different aspects of software project scope definition, and builds a method where numerical score can be assigned to a scope definition. Method A detailed list of different elements of software project scope definition is identified through literature. These elements are then empirically evaluated through an electronic survey, conducted with the industrial experts. Once finalized, these elements are ranked and assigned weights to systematically build a scorecard that is used to calculate scope definition score. Evaluation of the proposed method is done through a series of formal experiments. Results Evaluation results suggest that the proposed method is useful not only in calculating completeness of software projects scope definition, but it also serves as a guide for practitioners to determine specific aspects that require further consideration.",
      "Keywords": "Project management | Project planning | Scope definition | Scope management | Scope statement",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2018-02-01",
      "Publication type": "Article",
      "Authors": "Hassan, Isma ul;Ahmad, Naveed;Zuhaira, Behjat",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84975702607",
      "Primary study DOI": "10.1016/j.infsof.2016.04.012",
      "Title": "Mutation operators for testing Android apps",
      "Abstract": "Context: Due to the widespread use of Android devices, Android applications (apps) have more releases, purchases, and downloads than apps for any other mobile devices. The sheer volume of code in these apps creates significant concerns about the quality of the software. However, testing Android apps is different from testing traditional Java programs due to the unique program structure and new features of apps. Simple testing coverage criteria such as statement coverage are insufficient to assure high quality of Android apps. While researchers show significant interest in finding better Android testing approaches, there is still a lack of effective and usable techniques to evaluate their proposed test selection strategies, and to ensure a reasonable number of effective tests. Objective: As mutation analysis has been found to be an effective way to design tests in other software domains, we hypothesize that it is also a viable solution for Android apps. Method: This paper proposes an innovative mutation analysis approach that is specific for Android apps. We define mutation operators specific to the characteristics of Android apps, such as the extensive use of XML files to specify layout and behavior, the inherent event-driven nature, and the unique Activity lifecycle structure. We also report on an empirical study to evaluate these mutation operators. Results: We have built a tool that uses the novel Android mutation operators to mutate the source code of Android apps, then generates mutants that can be installed and run on Android devices. We evaluated the effectiveness of Android mutation testing through an empirical study on real-world apps. This paper introduces several novel mutation operators based on a fault study of Android apps, presents a significant empirical study with real-world apps, and provides conclusions based on an analysis of the results. Conclusion: The results show that the novel Android mutation operators provide comprehensive testing for Android apps. Additionally, as applying mutation testing to Android apps is still at a preliminary stage, we identify challenges, possibilities, and future research directions to make mutation analysis for mobile apps more effective and efficient.",
      "Keywords": "Android | Mutation testing | Software testing",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Deng, Lin;Offutt, Jeff;Ammann, Paul;Mirzaei, Nariman",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84970015746",
      "Primary study DOI": "10.1016/j.infsof.2016.05.001",
      "Title": "Mutant reduction based on dominance relation for weak mutation testing",
      "Abstract": "Context: As a fault-based testing technique, mutation testing is effective at evaluating the quality of existing test suites. However, a large number of mutants result in the high computational cost in mutation testing. As a result, mutant reduction is of great importance to improve the efficiency of mutation testing. Objective: We aim to reduce mutants for weak mutation testing based on the dominance relation between mutant branches. Method: In our method, a new program is formed by inserting mutant branches into the original program. By analyzing the dominance relation between mutant branches in the new program, the non-dominated one is obtained, and the mutant corresponding to the non-dominated mutant branch is the mutant after reduction. Results: The proposed method is applied to test ten benchmark programs and six classes from open-source projects. The experimental results show that our method reduces over 80% mutants on average, which greatly improves the efficiency of mutation testing. Conclusion: We conclude that dominance relation between mutant branches is very important and useful in reducing mutants for mutation testing.",
      "Keywords": "Dominance relation | Mutant | Reduction | Software testing | Weak mutation testing",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Gong, Dunwei;Zhang, Gongjie;Yao, Xiangjuan;Meng, Fanlin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84958582031",
      "Primary study DOI": "10.1016/j.infsof.2016.01.016",
      "Title": "Subtle higher order mutants",
      "Abstract": "Context: Research has shown the majority of real faults are complex and cannot be simulated with traditional First Order Mutants (FOMs). Higher Order Mutants (HOMs), which are created by making multiple syntactic changes to the source code (i.e, combining multiple FOMs), can potentially be used to simulate realistic faults. Objective: Our goal is to find subtle HOMs, which we define to be HOMs that are not killed by a reference test suite that kills all the FOMs. Subtle HOMs represent cases where single faults interact by masking each other to produce complex faulty behavior. The fault-detection effectiveness of a test suite can be improved by adding test cases that detect subtle HOMs. Method: We developed six search-based techniques to find subtle HOMs: Genetic Algorithm, Local Search, Data-Interaction Guided Local Search, Test-Case Guided Local Search, Restricted Random Search and Restricted Enumeration. We then performed empirical studies to evaluate the ability of the search techniques to find subtle HOMs and investigated factors that impact the characteristics of the subtle HOMs. Results: Local Search and both the Guided Local Search techniques were more effective than the other techniques at finding subtle HOMs. The majority of subtle HOMs that were found resulted from combining mutated Java primitive operators. Conclusions: The proposed techniques are able to find subtle HOMs. Composing subtle HOMs of lower degrees is an effective way for finding new subtle HOMs of higher degrees.",
      "Keywords": "Genetic algorithm | Higher order mutation testing | Local search | Search-based software engineering | Software testing",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Omar, Elmahdi;Ghosh, Sudipto;Whitley, Darrell",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84994257635",
      "Primary study DOI": "10.1016/j.infsof.2016.07.002",
      "Title": "Assessment of class mutation operators for C++ with the MuCPP mutation system",
      "Abstract": "Context: Mutation testing has been mainly analyzed regarding traditional mutation operators involving structured programming constructs common in mainstream languages, but mutations at the class level have not been assessed to the same extent. This fact is noteworthy in the case of C++, despite being one of the most relevant languages including object-oriented features. Objective: This paper provides a complete evaluation of class operators for the C++ programming language. MuCPP, a new system devoted to the application of mutation testing to this language, was developed to this end. This mutation system implements class mutation operators in a robust way, dealing with the inherent complexity of the language. Method: MuCPP generates the mutants by traversing the abstract syntax tree of each translation unit with the Clang API, and stores mutants as branches in the Git version control system. The tool is able to detect duplicate mutants, avoid system headers, and drive the compilation process. Then, MuCPP is used to conduct experiments with several open-source C++ programs. Results: The improvement rules listed in this paper to reduce unproductive class mutants have a significant impact in the computational cost of the technique. We also calculate the quantity and distribution of mutants generated with class operators, which generate far fewer mutants than their traditional counterparts. Conclusions: We show that the tests accompanying these programs cannot detect faults related to particular object-oriented features of C++. In order to increase the mutation score, we create new test scenarios to kill the surviving class mutants for all the applications. The results confirm that, while traditional mutation operators are still needed, class operators can complement them and help testers further improve the test suite.",
      "Keywords": "C++ | Class mutation operators | Mutation system | Mutation testing | Object-oriented programming",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Delgado-Pérez, Pedro;Medina-Bulo, Inmaculada;Palomo-Lozano, Francisco;García-Domínguez, Antonio;Domínguez-Jiménez, Juan José",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84958213157",
      "Primary study DOI": "10.1016/j.infsof.2016.01.019",
      "Title": "A novel use of equivalent mutants for static anomaly detection in software artifacts",
      "Abstract": "Context: In mutation analysis, a mutant of a software artifact, either a program or a model, is said equivalent if it leaves the artifact meaning unchanged. Equivalent mutants are usually seen as an inconvenience and they reduce the applicability of mutation analysis. Objective: Instead, we here claim that equivalent mutants can be useful to define, detect, and remove static anomalies, i.e., deficiencies of given qualities: If an equivalent mutant has a better quality value than the original artifact, then an anomaly has been found and removed. Method: We present a process for detecting static anomalies based on mutation, equivalence checking, and quality measurement. Results: Our proposal and the originating technique are applicable to different kinds of software artifacts. We present anomalies and conduct several experiments in different contexts, at specification, design, and implementation level. Conclusion: We claim that in mutation analysis a new research direction should be followed, in which equivalent mutants and operators generating them are welcome.",
      "Keywords": "Equivalent mutant | Quality measure | Static anomaly",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Arcaini, Paolo;Gargantini, Angelo;Riccobene, Elvinia;Vavassori, Paolo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84964802330",
      "Primary study DOI": "10.1016/j.infsof.2016.04.009",
      "Title": "KD-ART: Should we intensify or diversify tests to kill mutants?",
      "Abstract": "Context: Adaptive Random Testing (ART) spreads test cases evenly over the input domain. Yet once a fault is found, decisions must be made to diversify or intensify subsequent inputs. Diversification employs a wide range of tests to increase the chances of finding new faults. Intensification selects test inputs similar to those previously shown to be successful. Objective: Explore the trade-off between diversification and intensification to kill mutants. Method: We augment Adaptive Random Testing (ART) to estimate the Kernel Density (KD–ART) of input values found to kill mutants. KD–ART was first proposed at the 10th International Workshop on Mutation Analysis. We now extend this work to handle real world non numeric applications. Specifically we incorporate a technique to support programs with input parameters that have composite data types (such as arrays and structs). Results: Intensification is the most effective strategy for the numerical programs (it achieves 8.5% higher mutation score than ART). By contrast, diversification seems more effective for programs with composite inputs. KD–ART kills mutants 15.4 times faster than ART. Conclusion: Intensify tests for numerical types, but diversify them for composite types.",
      "Keywords": "Adaptive random testing | Intensification and diversification | Mutation analysis",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Patrick, Matthew;Jia, Yue",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84959911592",
      "Primary study DOI": "10.1016/j.infsof.2016.02.006",
      "Title": "A path-aware approach to mutant reduction in mutation testing",
      "Abstract": "Context: Mutation testing, which systematically generates a set of mutants by seeding various faults into the base program under test, is a popular technique for evaluating the effectiveness of a testing method. However, it normally requires the execution of a large amount of mutants and thus incurs a high cost. Objective: A common way to decrease the cost of mutation testing is mutant reduction, which selects a subset of representative mutants. In this paper, we propose a new mutant reduction approach from the perspective of program structure. Method: Our approach attempts to explore path information of the program under test, and select mutants that are as diverse as possible with respect to the paths they cover. We define two path-aware heuristic rules, namely module-depth and loop-depth rules, and combine them with statement- and operator-based mutation selection to develop four mutant reduction strategies. Results: We evaluated the cost-effectiveness of our mutant reduction strategies against random mutant selection on 11 real-life C programs with varying sizes and sampling ratios. Our empirical studies show that two of our mutant reduction strategies, which primarily rely on the path-aware heuristic rules, are more effective and systematic than pure random mutant selection strategy in terms of selecting more representative mutants. In addition, among all four strategies, the one giving loop-depth the highest priority has the highest effectiveness. Conclusion: In general, our path-aware approach can reduce the number of mutants without jeopardizing its effectiveness, and thus significantly enhance the overall cost-effectiveness of mutation testing. Our approach is particularly useful for the mutation testing on large-scale complex programs that normally involve a huge amount of mutants with diverse fault characteristics.",
      "Keywords": "Control flow | Mutation testing | Path depth | Selective mutation testing",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Sun, Chang ai;Xue, Feifei;Liu, Huai;Zhang, Xiangyu",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84963861053",
      "Primary study DOI": "10.1016/j.infsof.2016.03.002",
      "Title": "Memory mutation testing",
      "Abstract": "Context Three decades of mutation testing development have given software testers a rich set of mutation operators, yet relatively few operators can target memory faults (as we demonstrate in this paper). Objective To address this shortcoming, we introduce Memory Mutation Testing, proposing 9 Memory Mutation Operators each of which targets common forms of memory fault. We compare Memory Mutation Operators with traditional Mutation Operators, while handling equivalent and duplicate mutants. Method We extend our previous workshop paper, which introduced Memory Mutation Testing, with a more extensive and precise analysis of 18 open source programs, including 2 large real-world programs, all of which come with well-designed unit test suites. Specifically, our empirical study makes use of recent results on Trivial Compiler Equivalence (TCE) to identify both equivalent and duplicate mutants. Though the literature on mutation testing has previously deployed various techniques to cater for equivalent mutants, no previous study has catered for duplicate mutants. Results Catering for such extraneous mutants improves the precision with which claims about mutation scores can be interpreted. We also report the results of a new empirical study that compares Memory Mutation Testing with traditional Mutation Testing, providing evidence to support the claim that traditional mutation testing inadequately captures memory faults; 2% of the memory mutants are TCE-duplicates of traditional mutants and average test suite effectiveness drops by 44% when the target shifts from traditional mutants to memory mutants. Conclusions Introducing Memory Mutation Operators will cost only a small portion of the overall testing effort, yet generate higher quality mutants compared with traditional operators. Moreover, TCE technique does not only help with reducing testing effort, but also improves the precision of assessment on test quality, therefore should be considered in other Mutation Testing studies.",
      "Keywords": "Memory mutation | Mutation testing",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Wu, Fan;Nanavati, Jay;Harman, Mark;Jia, Yue;Krinke, Jens",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84964914212",
      "Primary study DOI": "10.1016/j.infsof.2016.04.003",
      "Title": "Formal mutation testing for Circus",
      "Abstract": "Context: The demand from industry for more dependable and scalable test-development mechanisms has fostered the use of formal models to guide the generation of tests. Despite many advancements having been obtained with state-based models, such as Finite State Machines (FSMs) and Input/Output Transition Systems (IOTSs), more advanced formalisms are required to specify large, state-rich, concurrent systems. Circus, a state-rich process algebra combining Z, CSP and a refinement calculus, is suitable for this; however, deriving tests from such models is accordingly more challenging. Recently, a testing theory has been stated for Circus, allowing the verification of process refinement based on exhaustive test sets. Objective: We investigate fault-based testing for refinement from Circus specifications using mutation. We seek the benefits of such techniques in test-set quality assertion and fault-based test-case selection. We target results relevant not only for Circus, but to any process algebra for refinement that combines CSP with a data language. Method: We present a formal definition for fault-based test sets, extending the Circus testing theory, and an extensive study of mutation operators for Circus. Using these results, we propose an approach to generate tests to kill mutants. Finally, we explain how prototype tool support can be obtained with the implementation of a mutant generator, a translator from Circus to CSP, and a refinement checker for CSP, and with a more sophisticated chain of tools that support the use of symbolic tests. Results: We formally characterise mutation testing for Circus, defining the exhaustive test sets that can kill a given mutant. We also provide a technique to select tests from these sets based on specification traces of the mutants. Finally, we present mutation operators that consider faults related to both reactive and data manipulation behaviour. Altogether, we define a new fault-based test-generation technique for Circus. Conclusion: We conclude that mutation testing for Circus can truly aid making test generation from state-rich model more tractable, by focussing on particular faults.",
      "Keywords": "Circus | Formal specification | Mutation | Testing",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Alberto, Alex;Cavalcanti, Ana;Gaudel, Marie Claude;Simão, Adenilso",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85015986827",
      "Primary study DOI": "10.1016/j.infsof.2017.03.009",
      "Title": "A genetic algorithm-based approach for automated refactoring of component-based software",
      "Abstract": "Context: During its lifecycle, a software system undergoes repeated modifications to quickly fulfill new requirements, but its underlying design is not properly adjusted after each update. This leads to the emergence of bad smells. Refactoring provides a de facto behavior-preserving approach to eliminate these anomalies. However, manually determining and performing useful refactorings is a formidable challenge, as stated in the literature. Therefore, framing object-oriented automated refactoring as a search-based technique has been proposed. However, the literature shows that search-based refactoring of component-based software has not yet received proper attention. Objective: This paper presents a genetic algorithm-based approach for the automated refactoring of component-based software. This approach consists of detecting component-relevant bad smells and eliminating these bad smells by searching for the best sequence of refactorings using a genetic algorithm. Method: Our approach consists of four steps. The first step includes studying the literature related to component-relevant bad smells and formulating bad smell detection rules. The second step involves proposing a catalog of component-relevant refactorings. The third step consists of constructing a source code model by extracting facts from the source code of a component-based software. The final step seeks to identify the best sequence of refactorings to apply to reduce the presence of bad smells in the source code model using a genetic algorithm. The latter uses bad smell detection rules as a fitness function and the catalog of refactorings as a means to explore the search space. Results: As a case study, we conducted experiments on an unbiased set of four real-world component-based applications. The results indicate that our approach is able to efficiently reduce the total number of bad smells by more than one half, which is an acceptable value compared to the recent literature. Moreover, we determined that our approach is also accurate in refactoring only components suffering from bad smells while leaving the remaining components untouched whenever possible. Furthermore, a statistical analysis shows that our genetic algorithm outperforms random search and local search in terms of efficiency and accuracy on almost all the systems investigated in this work. Conclusion: This paper presents a search-based approach for the automated refactoring of component-based software. To the best of our knowledge, our approach is the first to focus on component-based refactoring, whereas the state-of-the-art approaches focus only on object-oriented refactoring.",
      "Keywords": "Bad smells | Component-based software engineering | Genetic algorithm | Refactoring",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Kebir, Salim;Borne, Isabelle;Meslati, Djamel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85006269717",
      "Primary study DOI": "10.1016/j.infsof.2016.11.008",
      "Title": "A model-driven approach to catch performance antipatterns in ADL specifications",
      "Abstract": "Context: While the performance analysis of a software architecture is a quite well-assessed task nowadays, the issue of interpreting the performance results for providing feedback to software architects is still very critical. Performance antipatterns represent effective instruments to tackle this issue, because they document common mistakes leading to performance problems as well as their solutions. Objective: Up today performance antipatterns have been only studied in the context of software modeling languages like UML, whereas in this manuscript our objective is to catch them in the context of ADL-based software architectures to investigate their effectiveness. Method: We have implemented a model-driven approach that allows the automatic detection of four performance antipatterns in Æmilia, that is a stochastic process algebraic ADL for performance-aware component-oriented modeling of software systems. Results: We evaluate the approach by applying it to three case studies in different application domains. Experimental results demonstrate the effectiveness of our approach to support the performance improvement of ADL-based software architectures. Conclusion: We can conclude that the detection of performance antipatterns, from the earliest stages of software development, represents an effective instrument to tackle the issue of identifying flaws and improving system performance.",
      "Keywords": "Architecture description languages | Model-driven engineering | Performance antipatterns | Software performance analysis | Æmilia ADL",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "De Sanctis, Martina;Trubiani, Catia;Cortellessa, Vittorio;Di Marco, Antinisca;Flamminj, Mirko",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85006276139",
      "Primary study DOI": "10.1016/j.infsof.2016.11.009",
      "Title": "A systematic review on search-based refactoring",
      "Abstract": "Context: To find the best sequence of refactorings to be applied in a software artifact is an optimization problem that can be solved using search techniques, in the field called Search-Based Refactoring (SBR). Over the last years, the field has gained importance, and many SBR approaches have appeared, arousing research interest. Objective: The objective of this paper is to provide an overview of existing SBR approaches, by presenting their common characteristics, and to identify trends and research opportunities. Method: A systematic review was conducted following a plan that includes the definition of research questions, selection criteria, a search string, and selection of search engines. 71 primary studies were selected, published in the last sixteen years. They were classified considering dimensions related to the main SBR elements, such as addressed artifacts, encoding, search technique, used metrics, available tools, and conducted evaluation. Results: Some results show that code is the most addressed artifact, and evolutionary algorithms are the most employed search technique. Furthermore, most times, the generated solution is a sequence of refactorings. In this respect, the refactorings considered are usually the ones of the Fowler's Catalog. Some trends and opportunities for future research include the use of models as artifacts, the use of many objectives, the study of the bad smells effect, and the use of hyper-heuristics. Conclusions: We have found many SBR approaches, most of them published recently. The approaches are presented, analyzed, and grouped following a classification scheme. The paper contributes to the SBR field as we identify a range of possibilities that serve as a basis to motivate future researches.",
      "Keywords": "Evolutionary algorithms | Refactoring | Search-based software engineering",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2017-03-01",
      "Publication type": "Review",
      "Authors": "Mariani, Thainá;Vergilio, Silvia Regina",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85032974011",
      "Primary study DOI": "10.1145/3132731",
      "Title": "Early evaluation of implementation alternatives of composite data structures toward maintainability",
      "Abstract": "Selecting between different design options is a crucial decision for object-oriented software developers that affects code quality characteristics. Conventionally developers use their experience to make such decisions, which leads to suboptimal results regarding code quality. In this article, a formal model for providing early estimates of quality metrics of object-oriented software implementation alternatives is proposed. The model supports software developers in making fast decisions in a systematic way early during the design phase to achieve improved code characteristics. The approach employs a comparison model related to the application of the Visitor design pattern and inheritance-based implementation on structures following the Composite design pattern. The model captures maintainability as a metric of software quality and provides precise assessments of the quality of each implementation alternative. Furthermore, the model introduces the structural maintenance cost metric based on which the progressive analysis of the maintenance process is introduced. The proposed approach has been applied to several test cases for different relevant quality metrics. The results prove that the proposed model delivers accurate estimations. Thus, the proposed methodology can be used for comparing different implementation alternatives against various measures and quality factors before code development, leading to reduced effort and cost for software maintenance. c 2017 ACM.",
      "Keywords": "Composition | Visitor",
      "Publication venue": "ACM Transactions on Software Engineering and Methodology",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Karanikolas, Chris;Dimitroulakos, Grigoris;Masselos, Konstantinos",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85032993901",
      "Primary study DOI": "10.1145/3106411",
      "Title": "A logic-based approach for the verification of UML timed models",
      "Abstract": "This article presents a novel technique to formally verify models of real-time systems captured through a set of heterogeneous UML diagrams. The technique is based on the following key elements: (i) a subset of Unified Modeling Language (UML) diagrams, called Coretto UML (C-UML), which allows designers to describe the components of the system and their behavior through several kinds of diagrams (e.g., state machine diagrams, sequence diagrams, activity diagrams, interaction overview diagrams), and stereotypes taken from the UML Profile for Modeling and Analysis of Real-Time and Embedded Systems; (ii) a formal semantics of C-UML diagrams, defined through formulae of the metric temporal logic Tempo Reale ImplicitO (TRIO); and (iii) a tool, called Corretto, which implements the aforementioned semantics and allows users to carry out formal verification tasks on modeled systems. We validate the feasibility of our approach through a set of different case studies, taken from both the academic and the industrial domain.",
      "Keywords": "Formal semantics | Formal verification | Metric temporal logic | Timed systems",
      "Publication venue": "ACM Transactions on Software Engineering and Methodology",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Baresi, Luciano;Morzenti, Angelo;Motta, Alfredo;Pourhashem, Mohammad Mehdi;Rossi, Matteo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027103491",
      "Primary study DOI": "10.1145/3104029",
      "Title": "Fixing faults in C and Java Source Code: Abbreviated vs. full-word identifier names",
      "Abstract": "We carried out a family of controlled experiments to investigate whether the use of abbreviated identifier names, with respect to full-word identifier names, affects fault fixing in C and Java source code. This family consists of an original (or baseline) controlled experiment and three replications.We involved 100 participants with different backgrounds and experiences in total. Overall results suggested that there is no difference in terms of effort, effectiveness, and efficiency to fix faults, when source code contains either only abbreviated or only full-word identifier names. We also conducted a qualitative study to understand the values, beliefs, and assumptions that inform and shape fault fixing when identifier names are either abbreviated or full-word.We involved in this qualitative study six professional developers with 1-3 years of work experience. A number of insights emerged from this qualitative study and can be considered a useful complement to the quantitative results from our family of experiments. One of the most interesting insights is that developers, when working on source code with abbreviated identifier names, adopt a more methodical approach to identify and fix faults by extending their focus point and only in a few cases do they expand abbreviated identifiers.",
      "Keywords": "",
      "Publication venue": "ACM Transactions on Software Engineering and Methodology",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Scanniello, Giuseppe;Risi, Michele;Tramontana, Porfirio;Romano, Simone",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027014180",
      "Primary study DOI": "10.1145/3078840",
      "Title": "Human competitiveness of Genetic Programming in spectrum-based Fault Localisation: Theoretical and empirical analysis",
      "Abstract": "We report on the application of Genetic Programming to Software Fault Localisation, a problem in the area of Search-Based Software Engineering (SBSE). We give both empirical and theoretical evidence for the human competitiveness of the evolved fault localisation formulæ under the single fault scenario, compared to those generated by human ingenuity and reported in many papers, published over more than a decade. Though there have been previous human competitive results claimed for SBSE problems, this is the first time that evolved solutions have been formally proved to be human competitive. We further prove that no future human investigation could outperform the evolved solutions. We complement these proofs with an empirical analysis of both human and evolved solutions, which indicates that the evolved solutions are not only theoretically human competitive, but also convey similar practical benefits to human-evolved counterparts.",
      "Keywords": "Genetic programming | Search-based software engineering | Spectrum-based fault localisation",
      "Publication venue": "ACM Transactions on Software Engineering and Methodology",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Yoo, Shin;Xie, Xiaoyuan;Kuo, Fei Ching;Chen, Tsong Yueh;Harman, Mark",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019876383",
      "Primary study DOI": "10.1145/3063384",
      "Title": "A posteriori typing for model-driven engineering: Concepts, analysis, and applications",
      "Abstract": "Model-Driven Engineering (MDE) is founded on the ability to create and process models conforming to a meta-model. In this context, classes in a meta-model are used in two ways: as templates to create objects and as (static) classifiers for them. These two aspects are inherently tied in most meta-modelling approaches, which results in unnecessarily rigid systems and hinders reusability of MDE artefacts. In this work, we discuss the benefits of decoupling object creation from typing in MDE. Thus, we rely on standard mechanisms for object creation, and propose a posteriori typing as a means to retype objects and enable multiple, partial, dynamic typings. This approach enhances flexibility; permits unanticipated reuse, as model management operations defined for a meta-model can be reused with other models once they get reclassified; and enables bidirectional model transformation by reclassification. In particular, we propose two mechanisms to realise model retyping and show their underlying theory and analysis methods.We show the feasibility of the approach by an implementation atop our meta-modelling tool METADEPTH and present several applications of retypings (transformations, reuse, and dynamicity).",
      "Keywords": "A-posteriori model typing | Bidirectionality | Dynamic typing | METADEPTH | Model transformations | Model-driven engineering | Partial typing | Reusability",
      "Publication venue": "ACM Transactions on Software Engineering and Methodology",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "De Lara, Juan;Guerra, Esther",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019854327",
      "Primary study DOI": "10.1145/3053430",
      "Title": "Augmenting field data for testing systems subject to incremental requirements changes",
      "Abstract": "When testing data processing systems, software engineers often use real-world data to perform system-level testing. However, in the presence of new data requirements, software engineers may no longer benefit from having real-world data with which to perform testing. Typically, new test inputs complying with the new requirements have to be manually written. We propose an automated model-based approach that combines data modelling and constraint solving to modify existing field data to generate test inputs for testing new data requirements. The approach scales in the presence of complex and structured data, thanks to both the reuse of existing field data and the adoption of an innovative input generation algorithm based on slicing the model into parts. We validated the scalability and effectiveness of the proposed approach using an industrial case study. The empirical study shows that the approach scales in the presence of large amounts of structured and complex data. The approach can produce, within a reasonable time, test input data that is over ten times larger in size than the data generated with constraint solving only. We also demonstrate that the generated test inputs achieve more code coverage than the test cases implemented by experienced software engineers.",
      "Keywords": "Alloy | Data Processing Systems | Model Slicing | System Testing",
      "Publication venue": "ACM Transactions on Software Engineering and Methodology",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Di Nardo, Daniel;Pastore, Fabrizio;Briand, Lionel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019884971",
      "Primary study DOI": "10.1145/3063385",
      "Title": "Ensuring the consistency of adaptation through interand intra-component dependency analysis",
      "Abstract": "Dynamic adaptation should not leave a software system in an inconsistent state, as it could lead to failure. Prior research has used inter-component dependency models of a system to determine a safe interval for the adaptation of its components, where the most important tradeoff is between disruption in the operations of the system and reachability of safe intervals. This article presents Savasana, which automatically analyzes a software system's code to extract both inter- and intra-component dependencies. In this way, Savasana is able to obtain more fine-grained models compared to previous approaches. Savasana then uses the detailed models to find safe adaptation intervals that cannot be determined using techniques from prior research. This allows Savasana to achieve a better tradeoff between disruption and reachability. The article demonstrates how Savasana infers safe adaptation intervals for components of a software system under various use cases and conditions.",
      "Keywords": "Adaptive software | Component-based software | Update criteria",
      "Publication venue": "ACM Transactions on Software Engineering and Methodology",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Sadeghi, Alireza;Esfahani, Naeem;Malek, Sam",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026487161",
      "Primary study DOI": "10.1145/3051121",
      "Title": "Parallel algorithms for generating distinguishing sequences for observable non-deterministic FSMs",
      "Abstract": "A distinguishing sequence (DS) for a finite-state machine (FSM) is an input sequence that distinguishes every pair of states of the FSM. There are techniques that generate a test sequence with guaranteed fault detection power, and it has been found that shorter test sequences can be produced if DSs are used. Despite these benefits, however, until recently the only published DS generation algorithms have been for deterministic FSMs. This article develops a massively parallel algorithm, which can be used in Graphics Processing Units (GPUs) Computing, to generate DSs from partial observable non-deterministic FSMs. We also present the results of experiments using randomly generated FSMs and some benchmark FSMs. The results are promising and indicate that the proposed algorithm can derive DSs from partial observable non-deterministic FSMs with 32,000 states in an acceptable amount of time.",
      "Keywords": "Distinguishing sequences | Finite state machine",
      "Publication venue": "ACM Transactions on Software Engineering and Methodology",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Hierons, Robert M.;Türker, Uraz Cengiz",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027020471",
      "Primary study DOI": "10.1145/3078841",
      "Title": "Predicting query quality for applications of text retrieval to software engineering tasks",
      "Abstract": "Context: Since the mid-2000s, numerous recommendation systems based on text retrieval (TR) have been proposed to support software engineering (SE) tasks such as concept location, traceability link recovery, code reuse, impact analysis, and so on. The success of TR-based solutions highly depends on the query submitted, which is either formulated by the developer or automatically extracted from software artifacts. Aim: We aim at predicting the quality of queries submitted to TR-based approaches in SE. This can lead to benefits for developers and for the quality of software systems alike. For example, knowing when a query is poorly formulated can save developers the time and frustration of analyzing irrelevant search results. Instead, they could focus on reformulating the query. Also, knowing if an artifact used as a query leads to irrelevant search results may uncover underlying problems in the query artifact itself. Method: We introduce an automatic query quality prediction approach for software artifact retrieval by adapting NL-inspired solutions to their use on software data. We present two applications and evaluations of the approach in the context of concept location and traceability link recovery, where TR has been applied most often in SE. For concept location, we use the approach to determine if the list of retrieved code elements is likely to contain code relevant to a particular change request or not, in which case, the queries are good candidates for reformulation. For traceability link recovery, the queries represent software artifacts. In this case, we use the query quality prediction approach to identify artifacts that are hard to trace to other artifacts and may therefore have a low intrinsic quality for TR-based traceability link recovery. Results: For concept location, the evaluation shows that our approach is able to correctly predict the quality of queries in 82% of the cases, on average, using very little training data. In the case of traceability recovery, the proposed approach is able to detect hard to trace artifacts in 74% of the cases, on average. Conclusions: The results of our evaluation on applications for concept location and traceability link recovery indicate that our approach can be used to predict the results of a TR-based approach by assessing the quality of the text query. This can lead to saved effort and time, as well as the identification of software artifacts that may be difficult to trace using TR.",
      "Keywords": "Artifact traceability | Concept location | Text retrieval",
      "Publication venue": "ACM Transactions on Software Engineering and Methodology",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Mills, Chris;Bavota, Gabriele;Haiduc, Sonia;Oliveto, Rocco;Marcus, Andrian;De Lucia, Andrea",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019853701",
      "Primary study DOI": "10.1145/2990497",
      "Title": "Generating API call rules from version history and stack overflow posts",
      "Abstract": "Researchers have shown that related functions can be mined from groupings of functions found in the version history of a system. Our first contribution is to expand this approach to a community of applications and set of similar applications. Android developers use a set of application programming interface (API) calls when creating apps. These API calls are used in similar ways across multiple applications. By clustering co-changing API calls used by 230 Android apps across 12k versions, we are able to predict the API calls that individual app developers will use with an average precision of 75% and recall of 22%. When we make predictions from the same category of app, such as Finance, we attain precision and recall of 81% and 28%, respectively. Our second contribution can be characterized as programmers who discussed these functions were also interested in these functions. Informal discussions on Stack Overflow provide a rich source of information about related API calls as developers provide solutions to common problems. By grouping API calls contained in each positively voted answer posts, we are able to create rules that predict the calls that app developers will use in their own apps with an average precision of 66% and recall of 13%. For comparison purposes, we developed a baseline by clustering co-changing API calls for each individual app and generated association rules from them. The baseline predicts API calls used by app developers with a precision and recall of 36% and 23%, respectively.",
      "Keywords": "API method calls | Association rule mining | Community of applications | Informal documentation | Stack Overflow | Version history",
      "Publication venue": "ACM Transactions on Software Engineering and Methodology",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Azad, Shams;Rigby, Peter C.;Guerrouj, Latifa",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85011325157",
      "Primary study DOI": "10.1145/2968444",
      "Title": "Dynamic dependence summaries",
      "Abstract": "Software engineers construct modern-day software applications by building on existing software libraries and components that they necessarily do not author themselves. Thus, contemporary software applications rely heavily on existing standard and third-party libraries for their execution and behavior. As such, effective runtime analysis of such a software application's behavior is met with new challenges. To perform dynamic analysis of a software application, all transitively dependent external libraries must also be monitored and analyzed at each layer of the software application's call stack. However, monitoring and analyzing large and often numerous external libraries may prove to be prohibitively expensive. Moreover, an overabundance of library-level analyses may obfuscate the details of the actual software application's dynamic behavior. In other words, the extensive use of existing libraries by a software application renders the results of its dynamic analysis both expensive to compute and difficult to understand. We model software component behavior as dynamically observed data- and control dependencies between inputs and outputs of a software component. Such data- and control dependencies are monitored at a fine-grain instruction-level and are collected as dynamic execution traces for software runs. As an approach to address the complexities and expenses associated with analyzing dynamically observable behaviorofsoftware components, we summarize and reuse the data- and control dependencies between the inputs and outputs of software components. Dynamically monitored data- and control dependencies, between the inputs and outputs of software components, upon summarization are called dynamic dependence summaries. Software components, equipped with dynamic dependence summaries, afford the omission of their exhaustive runtime analysis. Nonetheless, the reuse of dependence summaries would necessitate the abstraction of any concrete runtime information enclosed within the summary, thus potentially causing a loss in the information modeled by the dependence summary. Therefore, benefits to the efficiency of dynamic analyses that use such summarization may be afforded with losses of accuracy. As such, we evaluate the potential accuracy loss and the potential performance gain with the use of dynamic dependence summaries. Our results show, on average, a 13× speedup with the use of dynamic dependence summaries, with an accuracy of 90% in a real-world software engineering task.",
      "Keywords": "Dependence analysis | Dynamic analysis | Dynamic slicing | Summaries",
      "Publication venue": "ACM Transactions on Software Engineering and Methodology",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Palepu, Vijay Krishna;Xu, Guoqing;Jones, James A.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85039838834",
      "Primary study DOI": "10.1007/s10664-017-9585-2",
      "Title": "Fusing multi-abstraction vector space models for concern localization",
      "Abstract": "Concern localization refers to the process of locating code units that match a particular textual description. It takes as input textual documents such as bug reports and feature requests and outputs a list of candidate code units that are relevant to the bug reports or feature requests. Many information retrieval (IR) based concern localization techniques have been proposed in the literature. These techniques typically represent code units and textual descriptions as a bag of tokens at one level of abstraction, e.g., each token is a word, or each token is a topic. In this work, we propose a multi-abstraction concern localization technique named M ULAB. M ULAB represents a code unit and a textual description at multiple abstraction levels. Similarity of a textual description and a code unit is now made by considering all these abstraction levels. We combine a vector space model (VSM) and multiple topic models to compute the similarity and apply a genetic algorithm to infer semi-optimal topic model configurations. We also propose 12 variants of M ULAB by using different data fusion methods. We have evaluated our solution on 175 concerns from 9 open source Java software systems. The experimental results show that variant CombMNZ-Def performs better than other variants, and also outperforms the state-of-art baseline called P R (PageRank based algorithm), which is proposed by Scanniello et al. (Empir Softw Eng 20(6):1666–1720 2015) in terms of effectiveness and rank.",
      "Keywords": "Concern localization | Data fusion | Multi-Abstraction | Text retrieval | Topic modeling",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-08-01",
      "Publication type": "Article",
      "Authors": "Zhang, Yun;Lo, David;Xia, Xin;Scanniello, Giuseppe;Le, Tien Duy B.;Sun, Jianling",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85038808079",
      "Primary study DOI": "10.1007/s10664-017-9584-3",
      "Title": "Towards reusing hints from past fixes: An exploratory study on thousands of real samples",
      "Abstract": "With the usage of version control systems, many bug fixes have accumulated over the years. Researchers have proposed various automatic program repair (APR) approaches that reuse past fixes to fix new bugs. However, some fundamental questions, such as how new fixes overlap with old fixes, have not been investigated. Intuitively, the overlap between old and new fixes decides how APR approaches can construct new fixes with old ones. Based on this intuition, we systematically designed six overlap metrics, and performed an empirical study on 5,735 bug fixes to investigate the usefulness of past fixes when composing new fixes. For each bug fix, we created delta graphs (i.e., program dependency graphs for code changes), and identified how bug fixes overlap with each other in terms of the content, code structures, and identifier names of fixes. Our results show that if an APR approach knows all code name changes and composes new fixes by fully or partially reusing the content of past fixes, only 2.1% and 3.2% new fixes can be created from single or multiple past fixes in the same project, compared with 0.9% and 1.2% fixes created from past fixes across projects. However, if an APR approach knows all code name changes and composes new fixes by fully or partially reusing the code structures of past fixes, up to 41.3% and 29.7% new fixes can be created. By making the above observations and revealing other ten findings, we investigated the upper bound of reusable past fixes and composable new fixes, exploring the potential of existing and future APR approaches.",
      "Keywords": "",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-10-01",
      "Publication type": "Article",
      "Authors": "Zhong, Hao;Meng, Na",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85038623794",
      "Primary study DOI": "10.1007/s10664-017-9571-8",
      "Title": "Correctness attraction: a study of stability of software behavior under runtime perturbation",
      "Abstract": "Can the execution of software be perturbed without breaking the correctness of the output? In this paper, we devise a protocol to answer this question from a novel perspective. In an experimental study, we observe that many perturbations do not break the correctness in ten subject programs. We call this phenomenon “correctness attraction”. The uniqueness of this protocol is that it considers a systematic exploration of the perturbation space as well as perfect oracles to determine the correctness of the output. To this extent, our findings on the stability of software under execution perturbations have a level of validity that has never been reported before in the scarce related work. A qualitative manual analysis enables us to set up the first taxonomy ever of the reasons behind correctness attraction.",
      "Keywords": "Empirical study | Perturbation analysis | Software correctness",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-08-01",
      "Publication type": "Article",
      "Authors": "Danglot, Benjamin;Preux, Philippe;Baudry, Benoit;Monperrus, Martin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85038628515",
      "Primary study DOI": "10.1007/s10664-017-9586-1",
      "Title": "Understanding semi-structured merge conflict characteristics in open-source Java projects",
      "Abstract": "Empirical studies show that merge conflicts frequently occur, impairing developers’ productivity, since merging conflicting contributions might be a demanding and tedious task. However, the structure of changes that lead to conflicts has not been studied yet. Understanding the underlying structure of conflicts, and the involved syntactic language elements might shed light on how to better avoid merge conflicts. To this end, in this paper we derive a catalog of conflict patterns expressed in terms of the structure of code changes that lead to merge conflicts. We focus on conflicts reported by a semistructured merge tool that exploits knowledge about the underlying syntax of the artifacts. This way, we avoid analyzing a large number of spurious conflicts often reported by typical line based merge tools. To assess the occurrence of such patterns in different systems, we conduct an empirical study reproducing 70,047 merges from 123 GitHub Java projects. Our results show that most semistructured merge conflicts in our sample happen because developers independently edit the same or consecutive lines of the same method. However, the probability of creating a merge conflict is approximately the same when editing methods, class fields, and modifier lists. Furthermore, we noticed that most part of conflicting merge scenarios, and merge conflicts, involve more than two developers. Also, that copying and pasting pieces of code, or even entire files, across different repositories is a common practice and cause of conflicts. Finally, we discuss how our results reveal the need for new research studies and suggest potential improvements to tools supporting collaborative software development.",
      "Keywords": "Awareness tools | Collaborative software development | Empirical software engineering | Merge conflicts",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-08-01",
      "Publication type": "Article",
      "Authors": "Accioly, Paola;Borba, Paulo;Cavalcanti, Guilherme",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85038623621",
      "Primary study DOI": "10.1007/s10664-017-9582-5",
      "Title": "How effective are mutation testing tools? An empirical analysis of Java mutation testing tools with manual analysis and real faults",
      "Abstract": "Mutation analysis is a well-studied, fault-based testing technique. It requires testers to design tests based on a set of artificial defects. The defects help in performing testing activities by measuring the ratio that is revealed by the candidate tests. Unfortunately, applying mutation to real-world programs requires automated tools due to the vast number of defects involved. In such a case, the effectiveness of the method strongly depends on the peculiarities of the employed tools. Thus, when using automated tools, their implementation inadequacies can lead to inaccurate results. To deal with this issue, we cross-evaluate four mutation testing tools for Java, namely PIT, muJava, Major and the research version of PIT, PITRV, with respect to their fault-detection capabilities. We investigate the strengths of the tools based on: a) a set of real faults and b) manual analysis of the mutants they introduce. We find that there are large differences between the tools’ effectiveness and demonstrate that no tool is able to subsume the others. We also provide results indicating the application cost of the method. Overall, we find that PITRV achieves the best results. In particular, PITRV outperforms the other tools by finding 6% more faults than the other tools combined.",
      "Keywords": "Fault detection | Human study | Mutation testing | Real faults | Tool comparison",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-08-01",
      "Publication type": "Article",
      "Authors": "Kintis, Marinos;Papadakis, Mike;Papadopoulos, Andreas;Valvis, Evangelos;Malevris, Nicos;Le Traon, Yves",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85038089408",
      "Primary study DOI": "10.1007/s10664-017-9572-7",
      "Title": "Cloned and non-cloned Java methods: a comparative study",
      "Abstract": "Reusing code via copy-and-paste, with or without modification is a common behavior observed in software engineering. Traditionally, cloning has been considered as a bad smell suggesting flaws in design decisions. Many studies exist targeting clone discovery, removal, and refactoring. However there are not many studies which empirically investigate and compare the quality of cloned code to that of the code which has not been cloned. To this end, we present a statistical study that shows whether qualitative differences exist between cloned methods and non-cloned methods in Java projects. The dataset consists of 3562 open source Java projects containing 412,705 cloned and 616,604 non-cloned methods. The study uses 27 software metrics as a proxy for quality, spanning across complexity, modularity, and documentation (code-comments) categories. When controlling for size, no statistically significant differences were found between cloned and non-cloned methods for most of the metrics, except for three of them. The main statistically significant difference found was that cloned methods are on an average 18% smaller than non-cloned methods. After doing a mixed method analysis, we provide some insight for why cloned methods are smaller.",
      "Keywords": "Code clones | Open source software | Quality metrics",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-08-01",
      "Publication type": "Article",
      "Authors": "Saini, Vaibhav;Sajnani, Hitesh;Lopes, Cristina",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85037164853",
      "Primary study DOI": "10.1007/s10664-017-9574-5",
      "Title": "Effect sizes and their variance for AB/BA crossover design studies",
      "Abstract": "Vegas et al. IEEE Trans Softw Eng 42(2):120:135 (2016) raised concerns about the use of AB/BA crossover designs in empirical software engineering studies. This paper addresses issues related to calculating standardized effect sizes and their variances that were not addressed by the Vegas et al.’s paper. In a repeated measures design such as an AB/BA crossover design each participant uses each method. There are two major implication of this that have not been discussed in the software engineering literature. Firstly, there are potentially two different standardized mean difference effect sizes that can be calculated, depending on whether the mean difference is standardized by the pooled within groups variance or the within-participants variance. Secondly, as for any estimated parameters and also for the purposes of undertaking meta-analysis, it is necessary to calculate the variance of the standardized mean difference effect sizes (which is not the same as the variance of the study). We present the model underlying the AB/BA crossover design and provide two examples to demonstrate how to construct the two standardized mean difference effect sizes and their variances, both from standard descriptive statistics and from the outputs of statistical software. Finally, we discuss the implication of these issues for reporting and planning software engineering experiments. In particular we consider how researchers should choose between a crossover design or a between groups design.",
      "Keywords": "Crossover designs | Effect size estimation | Effect size variance estimation | Empirical software engineering | Meta-analysis",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-08-01",
      "Publication type": "Article",
      "Authors": "Madeyski, Lech;Kitchenham, Barbara",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85037161785",
      "Primary study DOI": "10.1007/s10664-017-9580-7",
      "Title": "An empirical study on the impact of AspectJ on software evolvability",
      "Abstract": "Since its inception in 1996, aspect-oriented programming (AOP) has been believed to reduce the effort required to maintain software systems by replacing cross-cutting code with aspects. However, little convincing empirical evidence exists to support this claim, while several studies suggest that AOP brings new obstacles to maintainability. This paper discusses two controlled experiments conducted to evaluate the impact of AspectJ (the most mature and popular aspect-oriented programming language) versus Java on software evolvability. We consider evolvability as the ease with which a software system can be updated to fulfill new requirements. Since a minor language was compared to the mainstream, the experiments were designed so as to anticipate that the participants were much more experienced in one of the treatments. The first experiment was performed on 35 student subjects who were asked to comprehend either Java or AspectJ implementation of the same system, and perform the corresponding comprehension tasks. Participants of both groups achieved a high rate of correct answers without a statistically significant difference between the groups. Nevertheless, the Java group significantly outperformed the AspectJ group with respect to the average completion time. In the second experiment, 24 student subjects were asked to implement (in a non-invasive way) two extension scenarios to the system that they had already known. Each subject evolved either the Java version using Java or the AspectJ version using AspectJ. We found out that a typical AspectJ programmer needs significantly fewer atomic changes to implement the change scenarios than a typical Java programmer, but we did not observe a significant difference in completion time. The overall result indicates that AspectJ has a different effect on two sub-characteristics of the evolvability: understandability and changeability. While AspectJ decreases the former, it improves one aspect of the latter.",
      "Keywords": "AOP | Aspect-oriented programming | Controlled experiment | Maintainability | Separation of concerns | Understandability",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-08-01",
      "Publication type": "Article",
      "Authors": "Przybyłek, Adam",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85010807231",
      "Primary study DOI": "10.1007/s10664-016-9497-6",
      "Title": "On the long-term use of visual gui testing in industrial practice: a case study",
      "Abstract": "Visual GUI Testing (VGT) is a tool-driven technique for automated GUI-based testing that uses image recognition to interact with and assert the correctness of the behavior of a system through its GUI as it is shown to the user. The technique’s applicability, e.g. defect-finding ability, and feasibility, e.g. time to positive return on investment, have been shown through empirical studies in industrial practice. However, there is a lack of studies that evaluate the usefulness and challenges associated with VGT when used long-term (years) in industrial practice. This paper evaluates how VGT was adopted, applied and why it was abandoned at the music streaming application development company, Spotify, after several years of use. A qualitative study with two workshops and five well chosen employees is performed at the company, supported by a survey, which is analyzed with a grounded theory approach to answer the study’s three research questions. The interviews provide insights into the challenges, problems and limitations, but also benefits, that Spotify experienced during the adoption and use of VGT. However, due to the technique’s drawbacks, VGT has been abandoned for a new technique/framework, simply called the Test interface. The Test interface is considered more robust and flexible for Spotify’s needs but has several drawbacks, including that it does not test the actual GUI as shown to the user like VGT does. From the study’s results it is concluded that VGT can be used long-term in industrial practice but it requires organizational change as well as engineering best practices to be beneficial. Through synthesis of the study’s results, and results from previous work, a set of guidelines are presented that aim to aid practitioners to adopt and use VGT in industrial practice. However, due to the abandonment of the technique, future research is required to analyze in what types of projects the technique is, and is not, long-term viable. To this end, we also present Spotify’s Test interface solution for automated GUI-based testing and conclude that it has its own benefits and drawbacks.",
      "Keywords": "Automated testing | Case study | Guidelines | Industrial | Visual GUI testing",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Alégroth, Emil;Feldt, Robert",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013679299",
      "Primary study DOI": "10.1007/s10664-017-9502-8",
      "Title": "Model comprehension for security risk assessment: an empirical comparison of tabular vs. graphical representations",
      "Abstract": "Tabular and graphical representations are used to communicate security risk assessments for IT systems. However, there is no consensus on which type of representation better supports the comprehension of risks (such as the relationships between threats, vulnerabilities and security controls). Cognitive fit theory predicts that spatial relationships should be better captured by graphs. In this paper we report the results of two studies performed in two countries with 69 and 83 participants respectively, in which we assessed the effectiveness of tabular and graphical representations with respect to extraction correct information about security risks. The experimental results show that tabular risk models are more effective than the graphical ones with respect to simple comprehension tasks and in some cases are more effective for complex comprehension tasks. We explain our findings by proposing a simple extension of Vessey’s cognitive fit theory as some linear spatial relationships could be also captured by tabular models.",
      "Keywords": "Cognitive fit | Comprehensibility | Empirical study | Risk modeling | Security risk assessment",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Labunets, Katsiaryna;Massacci, Fabio;Paci, Federica;Marczak, Sabrina;de Oliveira, Flávio Moreira",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85017654597",
      "Primary study DOI": "10.1007/s10664-017-9512-6",
      "Title": "Curating GitHub for engineered software projects",
      "Abstract": "Software forges like GitHub host millions of repositories. Software engineering researchers have been able to take advantage of such a large corpora of potential study subjects with the help of tools like GHTorrent and Boa. However, the simplicity in querying comes with a caveat: there are limited means of separating the signal (e.g. repositories containing engineered software projects) from the noise (e.g. repositories containing home work assignments). The proportion of noise in a random sample of repositories could skew the study and may lead to researchers reaching unrealistic, potentially inaccurate, conclusions. We argue that it is imperative to have the ability to sieve out the noise in such large repository forges. We propose a framework, and present a reference implementation of the framework as a tool called reaper, to enable researchers to select GitHub repositories that contain evidence of an engineered software project. We identify software engineering practices (called dimensions) and propose means for validating their existence in a GitHub repository. We used reaper to measure the dimensions of 1,857,423 GitHub repositories. We then used manually classified data sets of repositories to train classifiers capable of predicting if a given GitHub repository contains an engineered software project. The performance of the classifiers was evaluated using a set of 200 repositories with known ground truth classification. We also compared the performance of the classifiers to other approaches to classification (e.g. number of GitHub Stargazers) and found our classifiers to outperform existing approaches. We found stargazers-based classifier (with 10 as the threshold for number of stargazers) to exhibit high precision (97%) but an inversely proportional recall (32%). On the other hand, our best classifier exhibited a high precision (82%) and a high recall (86%). The stargazer-based criteria offers precision but fails to recall a significant portion of the population.",
      "Keywords": "Curation tools | Data curation | GitHub | Mining software repositories",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Munaiah, Nuthan;Kroh, Steven;Cabrey, Craig;Nagappan, Meiyappan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85008429791",
      "Primary study DOI": "10.1007/s10664-016-9492-y",
      "Title": "On the pragmatic design of literature studies in software engineering: an experience-based guideline",
      "Abstract": "Systematic literature studies have received much attention in empirical software engineering in recent years. They have become a powerful tool to collect and structure reported knowledge in a systematic and reproducible way. We distinguish systematic literature reviews to systematically analyze reported evidence in depth, and systematic mapping studies to structure a field of interest in a broader, usually quantified manner. Due to the rapidly increasing body of knowledge in software engineering, researchers who want to capture the published work in a domain often face an extensive amount of publications, which need to be screened, rated for relevance, classified, and eventually analyzed. Although there are several guidelines to conduct literature studies, they do not yet help researchers coping with the specific difficulties encountered in the practical application of these guidelines. In this article, we present an experience-based guideline to aid researchers in designing systematic literature studies with special emphasis on the data collection and selection procedures. Our guideline aims at providing a blueprint for a practical and pragmatic path through the plethora of currently available practices and deliverables capturing the dependencies among the single steps. The guideline emerges from various mapping studies and literature reviews conducted by the authors and provides recommendations for the general study design, data collection, and study selection procedures. Finally, we share our experiences and lessons learned in applying the different practices of the proposed guideline.",
      "Keywords": "Empirical software engineering | Guideline proposal | Lessons learned | Systematic literature review | Systematic mapping study",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Kuhrmann, Marco;Fernández, Daniel Méndez;Daneva, Maya",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85014955102",
      "Primary study DOI": "10.1007/s10664-017-9507-3",
      "Title": "A study of the relation of mobile device attributes with the user-perceived quality of Android apps",
      "Abstract": "The number of mobile applications (apps) and mobile devices has increased considerably over the past few years. Online app markets, such as the Google Play Store, use a star-rating mechanism to quantify the user-perceived quality of mobile apps. Users may rate apps on a five point (star) scale where a five star-rating is the highest rating. Having considered the importance of a high star-rating to the success of an app, recent studies continue to explore the relationship between the app attributes, such as User Interface (UI) complexity, and the user-perceived quality. However, the user-perceived quality reflects the users’ experience using an app on a particular mobile device. Hence, the user-perceived quality of an app is not solely determined by app attributes. In this paper, we study the relation of both device attributes and app attributes with the user-perceived quality of Android apps from the Google Play Store. We study 20 device attributes, such as the CPU and the display size, and 13 app attributes, such as code size and UI complexity. Our study is based on data from 30 types of Android mobile devices and 280 Android apps. We use linear mixed effect models to identify the device attributes and app attributes with the strongest relationship with the user-perceived quality. We find that the code size has the strongest relationship with the user-perceived quality. However, some device attributes, such as the CPU, have stronger relationships with the user-perceived quality than some app attributes, such as the number of UI inputs and outputs of an app. Our work helps both device manufacturers and app developers. Manufacturers can focus on the attributes that have significant relationships with the user-perceived quality. Moreover, app developers should be careful about the devices for which they make their apps available because the device attributes have a strong relationship with the ratings that users give to apps.",
      "Keywords": "Device attributes | Mobile applications | Software attributes | User-perceived quality",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Noei, Ehsan;Syer, Mark D.;Zou, Ying;Hassan, Ahmed E.;Keivanloo, Iman",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85017427285",
      "Primary study DOI": "10.1007/s10664-017-9516-2",
      "Title": "Data Transformation in Cross-project Defect Prediction",
      "Abstract": "Software metrics rarely follow a normal distribution. Therefore, software metrics are usually transformed prior to building a defect prediction model. To the best of our knowledge, the impact that the transformation has on cross-project defect prediction models has not been thoroughly explored. A cross-project model is built from one project and applied on another project. In this study, we investigate if cross-project defect prediction is affected by applying different transformations (i.e., log and rank transformations, as well as the Box-Cox transformation). The Box-Cox transformation subsumes log and other power transformations (e.g., square root), but has not been studied in the defect prediction literature. We propose an approach, namely Multiple Transformations (MT), to utilize multiple transformations for cross-project defect prediction. We further propose an enhanced approach MT+ to use the parameter of the Box-Cox transformation to determine the most appropriate training project for each target project. Our experiments are conducted upon three publicly available data sets (i.e., AEEEM, ReLink, and PROMISE). Comparing to the random forest model built solely using the log transformation, our MT+ approach improves the F-measure by 7, 59 and 43% for the three data sets, respectively. As a summary, our major contributions are three-fold: 1) conduct an empirical study on the impact that data transformation has on cross-project defect prediction models; 2) propose an approach to utilize the various information retained by applying different transformation methods; and 3) propose an unsupervised approach to select the most appropriate training project for each target project.",
      "Keywords": "Box-cox | Data transformation | Defect prediction | Software metrics",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Zhang, Feng;Keivanloo, Iman;Zou, Ying",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85006476262",
      "Primary study DOI": "10.1007/s10664-016-9485-x",
      "Title": "Requirements reuse and requirement patterns: a state of the practice survey",
      "Abstract": "Requirements engineering is a discipline with numerous challenges to overcome. One of these challenges is the implementation of requirements reuse approaches. Although several theoretical proposals exist, little is known about the practices that are currently adopted in industry. Our goal is to contribute to the investigation of the state of the practice in the reuse of requirements, eliciting current practices from practitioners, and their opinions whenever appropriate. Besides reuse in general, we focus on requirement patterns as a particular strategy to reuse. We conducted an exploratory survey based on an online questionnaire. We received 71 responses from requirements engineers with industrial experience in the field, which were analyzed in order to derive observations. Although we found that a high majority of respondents declared some level of reuse in their projects (in particular, non-functional requirements were identified as the most similar and recurrent among projects), it is true that only a minority of them declared such reuse as a regular practice. Larger IT organizations and IT organizations with well-established software processes and methods present higher levels of reuse. Ignorance of reuse techniques and processes is the main reason preventing wider adoption. From the different existing reuse techniques, the simplest ones based on textual copy and subsequent tailoring of former requirements are the most adopted techniques. However, participants who apply reuse more often tend to use more elaborate techniques. Opinions of respondents about the use of requirement patterns show that they can be expected to mitigate problems related to the quality of the resulting requirements, such as lack of uniformity, inconsistency, or ambiguity. The main reasons behind the lack of adoption of requirement patterns by practitioners (in spite of the increasing research approaches proposed in the community) are related to the lack of a well-defined reuse method and involvement of requirement engineers. The results of our paper are interesting for practitioners since we highlight relevant observations from the survey participants’ experiences when deciding to implement requirements reuse practices. We also suggest future lines of research based on the needs pointed out in the results.",
      "Keywords": "Empirical studies | Online questionnaire | Requirement patterns | Requirements engineering | Requirements reuse | Surveys",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Palomares, Cristina;Quer, Carme;Franch, Xavier",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85009188417",
      "Primary study DOI": "10.1007/s10664-016-9491-z",
      "Title": "Managing the requirements flow from strategy to release in large-scale agile development: a case study at Ericsson",
      "Abstract": "In a large organization, informal communication and simple backlogs are not sufficient for the management of requirements and development work. Many large organizations are struggling to successfully adopt agile methods, but there is still little scientific knowledge on requirements management in large-scale agile development organizations. We present an in-depth study of an Ericsson telecommunications node development organization which employs a large scale agile method to develop telecommunications system software. We describe how the requirements flow from strategy to release, and related benefits and problems. Data was collected by 43 interviews, which were analyzed qualitatively. The requirements management was done in three different processes, each of which had a different process model, purpose and planning horizon. The release project management process was plan-driven, feature development process was continuous and implementation management process was agile. The perceived benefits included reduced development lead time, increased flexibility, increased planning efficiency, increased developer motivation and improved communication effectiveness. The recognized problems included difficulties in balancing planning effort, overcommitment, insufficient understanding of the development team autonomy, defining the product owner role, balancing team specialization, organizing system-level work and growing technical debt. The study indicates that agile development methods can be successfully employed in organizations where the higher level planning processes are not agile. Combining agile methods with a flexible feature development process can bring many benefits, but large-scale software development seems to require specialist roles and significant coordination effort.",
      "Keywords": "Large projects | Requirements management | Scaling agile software development | Scrum | Telecommunications software",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Heikkilä, Ville T.;Paasivaara, Maria;Lasssenius, Casper;Damian, Daniela;Engblom, Christian",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85008474002",
      "Primary study DOI": "10.1007/s10664-016-9488-7",
      "Title": "An empirical study for software change prediction using imbalanced data",
      "Abstract": "Software change prediction is crucial in order to efficiently plan resource allocation during testing and maintenance phases of a software. Moreover, correct identification of change-prone classes in the early phases of software development life cycle helps in developing cost-effective, good quality and maintainable software. An effective software change prediction model should equally recognize change-prone and not change-prone classes with high accuracy. However, this is not the case as software practitioners often have to deal with imbalanced data sets where instances of one type of class is much higher than the other type. In such a scenario, the minority classes are not predicted with much accuracy leading to strategic losses. This study evaluates a number of techniques for handling imbalanced data sets using various data sampling methods and MetaCost learners on six open-source data sets. The results of the study advocate the use of resample with replacement sampling method for effective imbalanced learning.",
      "Keywords": "Change proneness | Data sampling | Empirical validation | Imbalanced learning | MetaCost learners | Object-oriented metrics",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Malhotra, Ruchika;Khanna, Megha",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85011923741",
      "Primary study DOI": "10.1007/s10664-017-9499-z",
      "Title": "Reengineering legacy applications into software product lines: a systematic mapping",
      "Abstract": "Software Product Lines (SPLs) are families of systems that share common assets allowing a disciplined reuse. Rarely SPLs start from scratch, instead they usually start from a set of existing systems that undergo a reengineering process. Many approaches to conduct the reengineering process have been proposed and documented in research literature. This scenario is a clear testament to the interest in this research area. We conducted a systematic mapping study to provide an overview of the current research on reengineering of existing systems to SPLs, identify the community activity in regarding of venues and frequency of publications in this field, and point out trends and open issues that could serve as references for future research. This study identified 119 relevant publications. These primary sources were classified in six different dimensions related to reengineering phases, strategies applied, types of systems used in the evaluation, input artefacts, output artefacts, and tool support. The analysis of the results points out the existence of a consolidate community on this topic and a wide range of strategies to deal with different phases and tasks of the reengineering process, besides the availability of some tools. We identify some open issues and areas for future research such as the implementation of automation and tool support, the use of different sources of information, need for improvements in the feature management, the definition of ways to combine different strategies and methods, lack of sophisticated refactoring, need for new metrics and measures and more robust empirical evaluation. Reengineering of existing systems into SPLs is an active research topic with real benefits in practice. This mapping study motivates new research in this field as well as the adoption of systematic reuse in software companies.",
      "Keywords": "Evolution | Legacy systems | Product family | Reengineering | Systematic reuse",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Assunção, Wesley K.G.;Lopez-Herrejon, Roberto E.;Linsbauer, Lukas;Vergilio, Silvia R.;Egyed, Alexander",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85017162548",
      "Primary study DOI": "10.1007/s10664-017-9514-4",
      "Title": "What do developers search for on the web?",
      "Abstract": "Developers commonly make use of a web search engine such as Google to locate online resources to improve their productivity. A better understanding of what developers search for could help us understand their behaviors and the problems that they meet during the software development process. Unfortunately, we have a limited understanding of what developers frequently search for and of the search tasks that they often find challenging. To address this gap, we collected search queries from 60 developers, surveyed 235 software engineers from more than 21 countries across five continents. In particular, we asked our survey participants to rate the frequency and difficulty of 34 search tasks which are grouped along the following seven dimensions: general search, debugging and bug fixing, programming, third party code reuse, tools, database, and testing. We find that searching for explanations for unknown terminologies, explanations for exceptions/error messages (e.g., HTTP 404), reusable code snippets, solutions to common programming bugs, and suitable third-party libraries/services are the most frequent search tasks that developers perform, while searching for solutions to performance bugs, solutions to multi-threading bugs, public datasets to test newly developed algorithms or systems, reusable code snippets, best industrial practices, database optimization solutions, solutions to security bugs, and solutions to software configuration bugs are the most difficult search tasks that developers consider. Our study sheds light as to why practitioners often perform some of these tasks and why they find some of them to be challenging. We also discuss the implications of our findings to future research in several research areas, e.g., code search engines, domain-specific search engines, and automated generation and refinement of search queries.",
      "Keywords": "Empirical study | Search task | Survey | Understanding",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Xia, Xin;Bao, Lingfeng;Lo, David;Kochhar, Pavneet Singh;Hassan, Ahmed E.;Xing, Zhenchang",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85007480202",
      "Primary study DOI": "10.1007/s10664-016-9490-0",
      "Title": "An industry experiment on the effects of test-driven development on external quality and productivity",
      "Abstract": "Existing empirical studies on test-driven development (TDD) report different conclusions about its effects on quality and productivity. Very few of those studies are experiments conducted with software professionals in industry. We aim to analyse the effects of TDD on the external quality of the work done and the productivity of developers in an industrial setting. We conducted an experiment with 24 professionals from three different sites of a software organization. We chose a repeated-measures design, and asked subjects to implement TDD and incremental test last development (ITLD) in two simple tasks and a realistic application close to real-life complexity. To analyse our findings, we applied a repeated-measures general linear model procedure and a linear mixed effects procedure. We did not observe a statistical difference between the quality of the work done by subjects in both treatments. We observed that the subjects are more productive when they implement TDD on a simple task compared to ITLD, but the productivity drops significantly when applying TDD to a complex brownfield task. So, the task complexity significantly obscured the effect of TDD. Further evidence is necessary to conclude whether TDD is better or worse than ITLD in terms of external quality and productivity in an industrial setting. We found that experimental factors such as selection of tasks could dominate the findings in TDD studies.",
      "Keywords": "External quality | Industry experiment | Productivity | Test-driven development",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Tosun, Ayse;Dieste, Oscar;Fucci, Davide;Vegas, Sira;Turhan, Burak;Erdogmus, Hakan;Santos, Adrian;Oivo, Markku;Toro, Kimmo;Jarvinen, Janne;Juristo, Natalia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85014583704",
      "Primary study DOI": "10.1007/s10664-017-9508-2",
      "Title": "Evaluating code complexity triggers, use of complexity measures and the influence of code complexity on maintenance time",
      "Abstract": "Code complexity has been studied intensively over the past decades because it is a quintessential characterizer of code’s internal quality. Previously, much emphasis has been put on creating code complexity measures and applying these measures in practical contexts. To date, most measures are created based on theoretical frameworks, which determine the expected properties that a code complexity measure should fulfil. Fulfilling the necessary properties, however, does not guarantee that the measure characterizes the code complexity that is experienced by software engineers. Subsequently, code complexity measures often turn out to provide rather superficial insights into code complexity. This paper supports the discipline of code complexity measurement by providing empirical insights into the code characteristics that trigger complexity, the use of code complexity measures in industry, and the influence of code complexity on maintenance time. Results of an online survey, conducted in seven companies and two universities with a total of 100 respondents, show that among several code characteristics, two substantially increase code complexity, which subsequently have a major influence on the maintenance time of code. Notably, existing code complexity measures are poorly used in industry.",
      "Keywords": "Complexity | Internal quality | Maintainability | Measure | Survey",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Antinyan, Vard;Staron, Miroslaw;Sandberg, Anna",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85017151715",
      "Primary study DOI": "10.1007/s10664-017-9510-8",
      "Title": "An empirical study of unspecified dependencies in make-based build systems",
      "Abstract": "Software developers rely on a build system to compile their source code changes and produce deliverables for testing and deployment. Since the full build of large software systems can take hours, the incremental build is a cornerstone of modern build systems. Incremental builds should only recompile deliverables whose dependencies have been changed by a developer. However, in many organizations, such dependencies still are identified by build rules that are specified and maintained (mostly) manually, typically using technologies like make. Incomplete rules lead to unspecified dependencies that can prevent certain deliverables from being rebuilt, yielding incomplete results, which leave sources and deliverables out-of-sync. In this paper, we present a case study on unspecified dependencies in the make-based build systems of the glib, openldap, linux and qt open source projects. To uncover unspecified dependencies in make-based build systems, we use an approach that combines a conceptual model of the dependencies specified in the build system with a concrete model of the files and processes that are actually exercised during the build. Our approach provides an overview of the dependencies that are used throughout the build system and reveals unspecified dependencies that are not yet expressed in the build system rules. During our analysis, we find that unspecified dependencies are common. We identify 6 common causes in more than 1.2 million unspecified dependencies.",
      "Keywords": "Build systems | Unspecified dependencies",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Bezemer, Cor Paul;McIntosh, Shane;Adams, Bram;German, Daniel M.;Hassan, Ahmed E.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85035102140",
      "Primary study DOI": "10.1007/s10664-017-9578-1",
      "Title": "Pinpointing and repairing performance bottlenecks in concurrent programs",
      "Abstract": "Developing concurrent software that is both correct and efficient is challenging. Past research has proposed various techniques that support developers in finding, understanding, and repairing concurrency-related correctness problems, such as missing or incorrect synchronization. In contrast, existing work provides little support for dealing with concurrency-related performance problems, such as unnecessary or inefficient synchronization. This paper presents SyncProf, a profiling approach that helps in identifying, localizing, and repairing performance bottlenecks in concurrent programs. The approach consists of a sequence of dynamic analyses that reason about relevant code locations with increasing precision while narrowing down performance problems and gathering data for avoiding them. A key novelty is a graph-based representation of relations between critical sections, which is the basis for computing the performance impact of a critical section and for identifying the root cause of a bottleneck. Once a bottleneck is identified, SyncProf searches for a suitable optimization strategy to avoid the problem, increasing the level of automation when repairing performance bottlenecks over a traditional, manual approach. We evaluate SyncProf on 25 versions of eleven C/C++ projects with both known and previously unknown synchronization bottlenecks. The results show that SyncProf effectively localizes the root causes of these bottlenecks with higher precision than a state of the art lock contention profiler, and that it suggests valuable strategies to repair the bottlenecks.",
      "Keywords": "Concurrency | Performance bottlenecks | Testing",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-10-01",
      "Publication type": "Article",
      "Authors": "Yu, Tingting;Pradel, Michael",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85035083036",
      "Primary study DOI": "10.1007/s10664-017-9570-9",
      "Title": "An experience report on applying software testing academic results in industry: we need usable automated test generation",
      "Abstract": "What is the impact of software engineering research on current practices in industry? In this paper, I report on my direct experience as a PhD/post-doc working in software engineering research projects, and then spending the following five years as an engineer in two different companies (the first one being the same I worked in collaboration with during my post-doc). Given a background in software engineering research, what cutting-edge techniques and tools from academia did I use in my daily work when developing and testing the systems of these companies? Regarding validation and verification (my main area of research), the answer is rather short: as far as I can tell, only FindBugs. In this paper, I report on why this was the case, and discuss all the challenging, complex open problems we face in industry and which somehow are “neglected” in the academic circles. In particular, I will first discuss what actual tools I could use in my daily work, such as JaCoCo and Selenium. Then, I will discuss the main open problems I faced, particularly related to environment simulators, unit and web testing. After that, popular topics in academia are presented, such as UML, regression and mutation testing. Their lack of impact on the type of projects I worked on in industry is then discussed. Finally, from this industrial experience, I provide my opinions about how this situation can be improved, in particular related to how academics are evaluated, and advocate for a greater involvement into open-source projects.",
      "Keywords": "Applied research | Impact | Industry | Practice | Technology transfer",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-08-01",
      "Publication type": "Article",
      "Authors": "Arcuri, Andrea",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85034841124",
      "Primary study DOI": "10.1007/s10664-017-9566-5",
      "Title": "The need for software specific natural language techniques",
      "Abstract": "For over two decades, software engineering (SE) researchers have been importing tools and techniques from information retrieval (IR). Initial results have been quite positive. For example, when applied to problems such as feature location or re-establishing traceability links, IR techniques work well on their own, and often even better in combination with more traditional source code analysis techniques such as static and dynamic analysis. However, recently there has been growing awareness among SE researchers that IR tools and techniques are designed to work under different assumptions than those that hold for a software system. Thus it may be beneficial to consider IR-inspired tools and techniques that are specifically designed to work with software. One aim of this work is to provide quantitative empirical evidence in support of this observation. To do so a new technique is introduced that captures the level of difficulty found in an information need, the true, often latent, information that a searcher desires to know. The new technique is used to compare two domains: Natural Language (NL) and SE. Analysis of the data leads to three significant findings. First, the variation in the distribution of difficulty of the SE information needs differs from that of the NL information needs; second, collection age plays a role in the differences between the NL collections; and finally, the retrieval model used has little impact on the results.",
      "Keywords": "Feature location | Information need | Information retrieval | Query quality | Test collection challenge",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-08-01",
      "Publication type": "Article",
      "Authors": "Binkley, Dave;Lawrie, Dawn;Morrell, Christopher",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85034827767",
      "Primary study DOI": "10.1007/s10664-017-9576-3",
      "Title": "Analyzing the effects of test driven development in GitHub",
      "Abstract": "Testing is an integral part of the software development lifecycle, approached with varying degrees of rigor by different process models. Agile process models recommend Test Driven Development (TDD) as a key practice for reducing costs and improving code quality. The objective of this work is to perform a cost-benefit analysis of this practice. To that end, we have conducted a comparative analysis of GitHub repositories that adopts TDD to a lesser or greater extent, in order to determine how TDD affects software development productivity and software quality. We classified GitHub repositories archived in 2015 in terms of how rigorously they practiced TDD, thus creating a TDD spectrum. We then matched and compared various subsets of these repositories on this TDD spectrum with control sets of equal size. The control sets were samples from all GitHub repositories that matched certain characteristics, and that contained at least one test file. We compared how the TDD sets differed from the control sets on the following characteristics: number of test files, average commit velocity, number of bug-referencing commits, number of issues recorded, usage of continuous integration, number of pull requests, and distribution of commits per author. We found that Java TDD projects were relatively rare. In addition, there were very few significant differences in any of the metrics we used to compare TDD-like and non-TDD projects; therefore, our results do not reveal any observable benefits from using TDD.",
      "Keywords": "Continuous integration | GitHub repositories | Human factors in software development | Test driven development",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-08-01",
      "Publication type": "Article",
      "Authors": "Borle, Neil C.;Feghhi, Meysam;Stroulia, Eleni;Greiner, Russell;Hindle, Abram",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85034764580",
      "Primary study DOI": "10.1007/s10664-017-9565-6",
      "Title": "Older adults and hackathons: a qualitative study",
      "Abstract": "Globally observed trends in aging indicate that older adults constitute a growing share of the population and an increasing demographic in the modern technologies marketplace. Therefore, it has become important to address the issue of participation of older adults in the process of developing solutions suitable for their group. In this study, we approached this topic by organizing a hackathon involving teams of young programmers and older adults participants. Below we describe a case study of that hackathon, in which our objective was to motivate older adults to participate in software engineering processes. Based on our results from an array of qualitative methods, we propose a set of good practices that may lead to improved older adult participation in similar events and an improved process of developing apps that target older adults.",
      "Keywords": "Co-design | Elderly | Hackathons | Intergenerational cooperation | Intergenerational interaction | Older adults | Participatory design | Qualitative methods | User experience | User-centered design",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-08-01",
      "Publication type": "Article",
      "Authors": "Kopeć, Wiesław;Balcerzak, Bartłomiej;Nielek, Radosław;Kowalik, Grzegorz;Wierzbicki, Adam;Casati, Fabio",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85034615909",
      "Primary study DOI": "10.1007/s10664-017-9575-4",
      "Title": "Persistent code contribution: a ranking algorithm for code contribution in crowdsourced software",
      "Abstract": "Measuring code contribution in crowdsourced software is essential for ranking contributors to a project or distributing revenue. Past studies have demonstrated that there is variation between different code contribution measures and their ability for ranking users accurately. This study proposes a new code contribution ranking algorithm, Persistent Code Contribution (PCC), that aims to be language independent, quality aware and provide a ranking balance between new and senior users. PCC tracks the number of characters contributed by a user and ranks each character based on the number of subsequent revisions that each character survived for. It also tracks lines that may have been moved between revisions in the code and attributes character changes to the appropriate user that committed them to a repository. A ranking comparison between existing code contribution measures is performed to determine the similarities and differences, and, quantitative as well as qualitative evidence is presented as a means to validate the algorithm.",
      "Keywords": "Code | Contribution | Crowdsource | Measure | Open-source | Ranking",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-08-01",
      "Publication type": "Article",
      "Authors": "Tsikerdekis, Michail",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85034603171",
      "Primary study DOI": "10.1007/s10664-017-9573-6",
      "Title": "Data-efficient performance learning for configurable systems",
      "Abstract": "Many software systems today are configurable, offering customization of functionality by feature selection. Understanding how performance varies in terms of feature selection is key for selecting appropriate configurations that meet a set of given requirements. Due to a huge configuration space and the possibly high cost of performance measurement, it is usually not feasible to explore the entire configuration space of a configurable system exhaustively. It is thus a major challenge to accurately predict performance based on a small sample of measured system variants. To address this challenge, we propose a data-efficient learning approach, called DECART, that combines several techniques of machine learning and statistics for performance prediction of configurable systems. DECART builds, validates, and determines a prediction model based on an available sample of measured system variants. Empirical results on 10 real-world configurable systems demonstrate the effectiveness and practicality of DECART. In particular, DECART achieves a prediction accuracy of 90% or higher based on a small sample, whose size is linear in the number of features. In addition, we propose a sample quality metric and introduce a quantitative analysis of the quality of a sample for performance prediction.",
      "Keywords": "Configurable systems | Model selection | Parameter tuning | Performance prediction | Regression",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-06-01",
      "Publication type": "Article",
      "Authors": "Guo, Jianmei;Yang, Dingyu;Siegmund, Norbert;Apel, Sven;Sarkar, Atrisha;Valov, Pavel;Czarnecki, Krzysztof;Wasowski, Andrzej;Yu, Huiqun",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85034604579",
      "Primary study DOI": "10.1007/s10664-017-9569-2",
      "Title": "An empirical study on the interplay between semantic coupling and co-change of software classes",
      "Abstract": "Software systems continuously evolve to accommodate new features and interoperability relationships between artifacts point to increasingly relevant software change impacts. During maintenance, developers must ensure that related entities are updated to be consistent with these changes. Studies in the static change impact analysis domain have identified that a combination of source code and lexical information outperforms using each one when adopted independently. However, the extraction of lexical information and the measure of how loosely or closely related two software artifacts are, considering the semantic information embedded in their comments and identifiers has been carried out using somewhat complex information retrieval (IR) techniques. The interplay between software semantic and change relationship strengths has also not been extensively studied. This work aims to fill both gaps by comparing the effectiveness of measuring semantic coupling of OO software classes using (i) simple identifier based techniques and (ii) the word corpora of the entire classes in a software system. Afterwards, we empirically investigate the interplay between semantic and change coupling. The empirical results show that: (1) identifier based methods have more computational efficiency but cannot always be used interchangeably with corpora-based methods of computing semantic coupling of classes and (2) there is no correlation between semantic and change coupling. Furthermore we found that (3) there is a directional relationship between the two, as over 70% of the semantic dependencies are also linked by change coupling but not vice versa.",
      "Keywords": "Change impact analysis (CIA) | Clustering | Co-change | Co-evolution | Coupling | Hidden dependencies (HD) | Information retrieval (IR) | Object-oriented (OO) | Open-source software | Software components",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-06-01",
      "Publication type": "Article",
      "Authors": "Ajienka, Nemitari;Capiluppi, Andrea;Counsell, Steve",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85034242316",
      "Primary study DOI": "10.1007/s10664-017-9550-0",
      "Title": "Do automated program repair techniques repair hard and important bugs?",
      "Abstract": "Existing evaluations of automated repair techniques focus on the fraction of the defects for which the technique can produce a patch, the time needed to produce patches, and how well patches generalize to the intended specification. However, these evaluations have not focused on the applicability of repair techniques and the characteristics of the defects that these techniques can repair. Questions such as “Can automated repair techniques repair defects that are hard for developers to repair?” and “Are automated repair techniques less likely to repair defects that involve loops?” have not, as of yet, been answered. To address such questions, we annotate two large benchmarks totaling 409 C and Java defects in real-world software, ranging from 22K to 2.8M lines of code, with measures of the defect’s importance, the developer-written patch’s complexity, and the quality of the test suite. We then analyze relationships between these measures and the ability to produce patches for the defects of seven automated repair techniques —AE, GenProg, Kali, Nopol, Prophet, SPR, and TrpAutoRepair. We find that automated repair techniques are less likely to produce patches for defects that required developers to write a lot of code or edit many files, or that have many tests relevant to the defect. Java techniques are more likely to produce patches for high-priority defects. Neither the time it took developers to fix a defect nor the test suite’s coverage correlate with the automated repair techniques’ ability to produce patches. Finally, automated repair techniques are less capable of fixing defects that require developers to add loops and new function calls, or to change method signatures. These findings identify strengths and shortcomings of the state-of-the-art of automated program repair along new dimensions. The presented methodology can drive research toward improving the applicability of automated repair techniques to hard and important bugs.",
      "Keywords": "Automated program repair | Repairability",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-10-01",
      "Publication type": "Article",
      "Authors": "Motwani, Manish;Sankaranarayanan, Sandhya;Just, René;Brun, Yuriy",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85034069842",
      "Primary study DOI": "10.1007/s10664-017-9563-8",
      "Title": "On the impact of state-based model-driven development on maintainability: a family of experiments using UniMod",
      "Abstract": "Context: Model-driven approaches are well-known in the academia but one possible showstopper to a wider adoption in the industry is the limited empirical evidence for their claimed advantages and benefits, that could convince the decision makers. Objective: The aim of this work is gauging one of the claimed benefits of model-driven approaches, namely improvement in maintainability, with respect to a code-centric approach. Method: We conducted a family of five experiments involving 100 students that possessed different levels of education (64 Bachelor, 25 Master, and 11 PhD students; in groups sized 11 to 26 per individual experiment). In these experiments, UniMod – a State-based tool for Model-Driven Development using the Unified Modeling Language – is compared with Java-based code-centric programming, in a software maintenance scenario, with the goal of analyzing the effect on the time to perform the maintenance tasks, the correctness of the modified artifacts, and the efficiency. Results: The results show a reduction in time to accomplish the tasks and no impact on correctness. The adoption of the UniMod-MDD approach almost doubles the developers’efficiency, and in presence of a higher software engineering experience the efficiency is even three times higher. Conclusions: We found that the usage of the UniMod-MDD approach in a software maintenance scenario provides benefits over a pure code-centric approach. The benefits deriving from the UniMod-MDD approach are appreciable for all the categories of students, although with differences.",
      "Keywords": "Family of experiments | Maintainability | MDD | Model-driven development | Replication and maintenance task | UML | UniMod",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-06-01",
      "Publication type": "Article",
      "Authors": "Ricca, Filippo;Torchiano, Marco;Leotta, Maurizio;Tiso, Alessandro;Guerrini, Giovanna;Reggio, Gianna",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85033478695",
      "Primary study DOI": "10.1007/s10664-017-9567-4",
      "Title": "ChangeLocator: locate crash-inducing changes based on crash reports",
      "Abstract": "Software crashes are severe manifestations of software bugs. Debugging crashing bugs is tedious and time-consuming. Understanding software changes that induce a crashing bug can provide useful contextual information for bug fixing and is highly demanded by developers. Locating the bug inducing changes is also useful for automatic program repair, since it narrows down the root causes and reduces the search space of bug fix location. However, currently there are no systematic studies on locating the software changes to a source code repository that induce a crashing bug reflected by a bucket of crash reports. To tackle this problem, we first conducted an empirical study on characterizing the bug inducing changes for crashing bugs (denoted as crash-inducing changes). We also propose ChangeLocator, a method to automatically locate crash-inducing changes for a given bucket of crash reports. We base our approach on a learning model that uses features originated from our empirical study and train the model using the data from the historical fixed crashes. We evaluated ChangeLocator with six release versions of Netbeans project. The results show that it can locate the crash-inducing changes for 44.7%, 68.5%, and 74.5% of the bugs by examining only top 1, 5 and 10 changes in the recommended list, respectively. It significantly outperforms the existing state-of-the-art approach.",
      "Keywords": "Bug localization | Crash stack | Crash-inducing change | Software crash",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-10-01",
      "Publication type": "Article",
      "Authors": "Wu, Rongxin;Wen, Ming;Cheung, Shing Chi;Zhang, Hongyu",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85033397462",
      "Primary study DOI": "10.1007/s10664-017-9559-4",
      "Title": "Are tweets useful in the bug fixing process? An empirical study on Firefox and Chrome",
      "Abstract": "When encountering an issue, technical users (e.g., developers) usually file the issue report to the issue tracking systems. But non-technical end-users are more likely to express their opinions on social network platforms, such as Twitter. For software systems (e.g., Firefox and Chrome) that have a high exposure to millions of non-technical end-users, it is important to monitor and solve issues observed by a large user base. The widely used micro-blogging site (i.e., Twitter) has millions of active users. Therefore, it can provide instant feedback on products to the developers. In this paper, we investigate whether social networks (i.e., Twitter) can improve the bug fixing process by analyzing the short messages posted by end-users on Twitter (i.e., tweets). We propose an approach to remove noisy tweets, and map the remaining tweets to bug reports. We conduct an empirical study to investigate the usefulness of Twitter in the bug fixing process. We choose two widely adopted browsers (i.e., Firefox and Chrome) that are also large and rapidly released software systems. We find that issue reports are not treated differently regardless whether users tweet about the issue or not, except that Firefox developers tend to label an issue as more severe if users tweet about it. The feedback from Firefox contributors confirms that the tweets are not currently leveraged in the bug fixing process, due to the challenges associated to discovering bugs through Twitter. Moreover, we observe that many issues are posted on Twitter earlier than on issue tracking systems. More specifically, at least one third of issues could have been reported to developers 8.2 days and 7.6 days earlier in Firefox and Chrome, respectively. In conclusion, tweets are useful in providing earlier acknowledgment of issues, which developers can potentially use to focus their efforts on the issues impacting a large user-base.",
      "Keywords": "Bug fixing | Bug report | Social network | Twitter",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-06-01",
      "Publication type": "Article",
      "Authors": "Mezouar, Mariam El;Zhang, Feng;Zou, Ying",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85032801677",
      "Primary study DOI": "10.1007/s10664-017-9561-x",
      "Title": "Evolving software trace links between requirements and source code",
      "Abstract": "Traceability provides support for diverse software engineering activities including safety analysis, compliance verification, test-case selection, and impact prediction. However, in practice, there is a tendency for trace links to degrade over time as the system continually evolves. This is especially true for links between source-code and upstream artifacts such as requirements – because developers frequently refactor and change code without updating the links. In this paper we present TLE (Trace Link Evolver), a solution for automating the evolution of bidirectional trace links between source code classes or methods and requirements. TLE depends on a set of heuristics coupled with refactoring detection tools and informational retrieval algorithms to detect predefined change scenarios that occur across contiguous versions of a software system. We first evaluate TLE at the class level in a controlled experiment to evolve trace links for revisions of two Java applications. Second, we comparatively evaluate several variants of TLE across six releases of our in-house Dronology project. We study the results of integrating human analyst feed back in the evolution cycle of this emerging project. Additionally, in this system, we compare the efficacy of class-level versus method-level evolution of trace links. Finally, we evaluate TLE in a larger scale across 27 releases of the Cassandra Database System and show that the evolved trace links are significantly more accurate than those generated using only information retrieval techniques.",
      "Keywords": "Evolution | Maintenance | Traceability",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-08-01",
      "Publication type": "Article",
      "Authors": "Rahimi, Mona;Cleland-Huang, Jane",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85032791476",
      "Primary study DOI": "10.1007/s10664-017-9532-2",
      "Title": "Reducing user input requests to improve IT support ticket resolution process",
      "Abstract": "Management and maintenance of IT infrastructure resources such as hardware, software and network is an integral part of software development and maintenance projects. Service management ensures that the tickets submitted by users, i.e. software developers, are serviced within the agreed resolution times. Failure to meet those times induces penalty on the service provider. To prevent a spurious penalty on the service provider, non-working hours such as waiting for user inputs are not included in the measured resolution time, that is, a service level clock pauses its timing. Nevertheless, the user interactions slow down the resolution process, that is, add to user experienced resolution time and degrade user experience. Therefore, this work is motivated by the need to analyze and reduce user input requests in tickets’ life cycle. To address this problem, we analyze user input requests and investigate their impact on user experienced resolution time. We distinguish between input requests of two types: real, seeking information from the user to process the ticket and tactical, when no information is asked but the user input request is raised merely to pause the service level clock. Next, we propose a system that preempts a user at the time of ticket submission to provide additional information that the analyst, a person responsible for servicing the ticket, is likely to ask, thus reducing real user input requests. Further, we propose a detection system to identify tactical user input requests. To evaluate the approach, we conducted a case study in a large global IT company. We observed that around 57% of the tickets have user input requests in the life cycle, causing user experienced resolution time to be almost twice as long as the measured service resolution time. The proposed preemptive system preempts the information needs with an average accuracy of 94– 99% across five cross validations while traditional approaches such as logistic regression and naive Bayes have accuracy in the range of 50– 60%. The detection system identifies around 15% of the total user input requests as tactical. Therefore, the proposed solution can efficiently bring down the number of user input requests and, hence, improve the user-experienced resolution time.",
      "Keywords": "Machine learning | Process mining | Service level agreement | Software process | Ticket resolution time",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-06-01",
      "Publication type": "Article",
      "Authors": "Gupta, Monika;Asadullah, Allahbaksh;Padmanabhuni, Srinivas;Serebrenik, Alexander",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85032506315",
      "Primary study DOI": "10.1007/s10664-017-9556-7",
      "Title": "Challenges and pitfalls on surveying evidence in the software engineering technical literature: an exploratory study with novices",
      "Abstract": "The evidence-based software engineering approach advocates the use of evidence from empirical studies to support the decisions on the adoption of software technologies by practitioners in the software industry. To this end, many guidelines have been proposed to contribute to the execution and repeatability of literature reviews, and to the confidence of their results, especially regarding systematic literature reviews (SLR). To investigate similarities and differences, and to characterize the challenges and pitfalls of the planning and generated results of SLR research protocols dealing with the same research question and performed by similar teams of novice researchers in the context of the software engineering field. We qualitatively compared (using Jaccard and Kappa coefficients) and evaluated (using DARE) same goal SLR research protocols and outcomes undertaken by similar research teams. Seven similar SLR protocols regarding quality attributes for use cases executed in 2010 and 2012 enabled us to observe unexpected differences in their planning and execution. Even when the participants reached some agreement in the planning, the outcomes were different. The research protocols and reports allowed us to observe six challenges contributing to the divergences in the results: researchers’ inexperience in the topic, researchers’ inexperience in the method, lack of clearness and completeness of the papers, lack of a common terminology regarding the problem domain, lack of research verification procedures, and lack of commitment to the SLR. According to our findings, it is not possible to rely on results of SLRs performed by novices. Also, similarities at a starting or intermediate step during different SLR executions may not directly translate to the next steps, since non-explicit information might entail differences in the outcomes, hampering the repeatability and confidence of the SLR process and results. Although we do have expectations that the presence and follow-up of a senior researcher can contribute to increasing SLRs’ repeatability, this conclusion can only be drawn upon the existence of additional studies on this topic. Yet, systematic planning, transparency of decisions and verification procedures are key factors to guarantee the reliability of SLRs.",
      "Keywords": "Evidence-based software engineering | Exploratory study | Novice researchers | Systematic literature review",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-06-01",
      "Publication type": "Article",
      "Authors": "Ribeiro, Talita Vieira;Massollar, Jobson;Travassos, Guilherme Horta",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85032177107",
      "Primary study DOI": "10.1007/s10664-017-9564-7",
      "Title": "A comparison of code similarity analysers",
      "Abstract": "Copying and pasting of source code is a common activity in software engineering. Often, the code is not copied as it is and it may be modified for various purposes; e.g. refactoring, bug fixing, or even software plagiarism. These code modifications could affect the performance of code similarity analysers including code clone and plagiarism detectors to some certain degree. We are interested in two types of code modification in this study: pervasive modifications, i.e. transformations that may have a global effect, and local modifications, i.e. code changes that are contained in a single method or code block. We evaluate 30 code similarity detection techniques and tools using five experimental scenarios for Java source code. These are (1) pervasively modified code, created with tools for source code and bytecode obfuscation, and boiler-plate code, (2) source code normalisation through compilation and decompilation using different decompilers, (3) reuse of optimal configurations over different data sets, (4) tool evaluation using ranked-based measures, and (5) local + global code modifications. Our experimental results show that in the presence of pervasive modifications, some of the general textual similarity measures can offer similar performance to specialised code similarity tools, whilst in the presence of boiler-plate code, highly specialised source code similarity detection techniques and tools outperform textual similarity measures. Our study strongly validates the use of compilation/decompilation as a normalisation technique. Its use reduced false classifications to zero for three of the tools. Moreover, we demonstrate that optimal configurations are very sensitive to a specific data set. After directly applying optimal configurations derived from one data set to another, the tools perform poorly on the new data set. The code similarity analysers are thoroughly evaluated not only based on several well-known pair-based and query-based error measures but also on each specific type of pervasive code modification. This broad, thorough study is the largest in existence and potentially an invaluable guide for future users of similarity detection in source code.",
      "Keywords": "Clone detection | Code similarity measurement | Empirical study | Parameter optimisation | Plagiarism detection",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-08-01",
      "Publication type": "Article",
      "Authors": "Ragkhitwetsagul, Chaiyong;Krinke, Jens;Clark, David",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85031499980",
      "Primary study DOI": "10.1007/s10664-017-9558-5",
      "Title": "Understanding the factors for fast answers in technical Q&A websites: An empirical study of four stack exchange websites",
      "Abstract": "Technical questions and answers (Q&A) websites accumulate a significant amount of knowledge from users. Developers are especially active on these Q&A websites, since developers are constantly facing new development challenges that require help from other experts. Over the years, Q&A website designers have derived several incentive systems (e.g., gamification) to encourage users to answer questions that are posted by others. However, the current incentive systems primarily focus on the quantity and quality of the answers instead of encouraging the rapid answering of questions. Improving the speed of getting an answer can significantly improve the user experience and increase user engagement on such Q&A websites. In this paper, we explore how one may improve the current incentive systems to motivate fast answering of questions. We use a logistic regression model to analyze 46 factors along four dimensions (i.e., question, asker, answer, and answerer dimension) in order to understand the relationship between the studied factors and the needed time to get an accepted answer. We conduct our study on the four most popular (i.e., with the most questions) Q&A Stack Exchange websites: Stack Overflow, Mathematics, Ask Ubuntu, and Superuser. We find that i) factors in the answerer dimension have the strongest effect on the needed time to get an accepted answer, after controlling for other factors; ii) the current incentive system does not recognize non-frequent answerers who often answer questions which frequent answerers are not able to answer. Such questions that are answered by non-frequent answerers are as important (i.e., have similar range of scores) as those that are answered by frequent answerers; iii) the current incentive system motivates frequent answerers well, but such frequent answerers tend to answer short questions. Our findings suggest that Q&A website designers should improve their incentive systems to motivate non-frequent answerers to be more active and to answer questions fast, in order to shorten the waiting time to receive an answer (especially for questions that require specific knowledge that frequent answerers might not possess). In addition, the question answering incentive system needs to factor in the value and difficulty of answering the questions (e.g., providing more rewards to harder questions or questions that remain unanswered for a long period of time).",
      "Keywords": "Factor importance analysis | Logistic regression modeling | Q&A websites | Response time",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-06-01",
      "Publication type": "Article",
      "Authors": "Wang, Shaowei;Chen, Tse Hsun;Hassan, Ahmed E.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85031433548",
      "Primary study DOI": "10.1007/s10664-017-9551-z",
      "Title": "Analyzing a decade of Linux system calls",
      "Abstract": "Over the past 25 years, thousands of developers have contributed more than 18 million lines of code (LOC) to the Linux kernel. As the Linux kernel forms the central part of various operating systems that are used by millions of users, the kernel must be continuously adapted to the changing demands and expectations of these users. The Linux kernel provides its services to an application through system calls. The combined set of all system calls forms the essential Application Programming Interface (API) through which an application interacts with the kernel. In this paper, we conduct an empirical study of 8,770 changes that were made to Linux system calls during the last decade (i.e., from April 2005 to December 2014). In particular, we study the size of the changes, and we manually identify the type of changes and bug fixes that were made. Our analysis provides an overview of the evolution of the Linux system calls over the last decade. We find that there was a considerable amount of technical debt in the kernel, that was addressed by adding a number of sibling calls (i.e., 26% of all system calls). In addition, we find that by far, the ptrace() and signal handling system calls are the most challenging to maintain. Our study can be used by developers who want to improve the design and ensure the successful evolution of their own kernel APIs.",
      "Keywords": "API evolution | Empirical software engineering | Linux kernel | Software evolution | System calls",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-06-01",
      "Publication type": "Article",
      "Authors": "Bagherzadeh, Mojtaba;Kahani, Nafiseh;Bezemer, Cor Paul;Hassan, Ahmed E.;Dingel, Juergen;Cordy, James R.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85031416657",
      "Primary study DOI": "10.1007/s10664-017-9554-9",
      "Title": "On the reaction to deprecation of clients of 4 + 1 popular Java APIs and the JDK",
      "Abstract": "Application Programming Interfaces (APIs) are a tremendous resource—that is, when they are stable. Several studies have shown that this is unfortunately not the case. Of those, a large-scale study of API changes in the Pharo Smalltalk ecosystem documented several findings about API deprecations and their impact on API clients. We extend this study, by analyzing clients of both popular third-party Java APIs and the JDK API. This results in a dataset consisting of more than 25,000 clients of five popular Java APIs on GitHub, and 60 clients of the JDK API from Maven Central. This work addresses several shortcomings of the previous study, namely: a study of several distinct API clients in a popular, statically-typed language, with more accurate version information. We compare and contrast our findings with the previous study and highlight new ones, particularly on the API client update practices and the startling similarities between reaction behavior in Smalltalk and Java. We make a comparison between reaction behavior for third-party APIs and JDK APIs, given that language APIs are a peculiar case in terms of wide-spread usage, documentation, and support from IDEs. Furthermore, we investigate the connection between reaction patterns of a client and the deprecation policy adopted by the API used.",
      "Keywords": "API popularity | API usage | Application programming interface | Dataset",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-08-01",
      "Publication type": "Article",
      "Authors": "Sawant, Anand Ashok;Robbes, Romain;Bacchelli, Alberto",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030319643",
      "Primary study DOI": "10.1007/s10664-017-9553-x",
      "Title": "Empirical study on the discrepancy between performance testing results from virtual and physical environments",
      "Abstract": "Large software systems often undergo performance tests to ensure their capability to handle expected loads. These performance tests often consume large amounts of computing resources and time since heavy loads need to be generated. Making it worse, the ever evolving field requires frequent updates to the performance testing environment. In practice, virtual machines (VMs) are widely exploited to provide flexible and less costly environments for performance tests. However, the use of VMs may introduce confounding overhead (e.g., a higher than expected memory utilization with unstable I/O traffic) to the testing environment and lead to unrealistic performance testing results. Yet, little research has studied the impact on test results of using VMs in performance testing activities. To evaluate the discrepancy between the performance testing results from virtual and physical environments, we perform a case study on two open source systems – namely Dell DVD Store (DS2) and CloudStore. We conduct the same performance tests in both virtual and physical environments and compare the performance testing results based on the three aspects that are typically examined for performance testing results: 1) single performance metric (e.g. CPU Time from virtual environment vs. CPU Time from physical environment), 2) the relationship among performance metrics (e.g. correlation between CPU and I/O) and 3) performance models that are built to predict system performance. Our results show that 1) A single metric from virtual and physical environments do not follow the same distribution, hence practitioners cannot simply use a scaling factor to compare the performance between environments, 2) correlations among performance metrics in virtual environments are different from those in physical environments 3) statistical models built based on the performance metrics from virtual environments are different from the models built from physical environments suggesting that practitioners cannot use the performance testing results across virtual and physical environments. In order to assist the practitioners leverage performance testing results in both environments, we investigate ways to reduce the discrepancy. We find that such discrepancy can be reduced by normalizing performance metrics based on deviance. Overall, we suggest that practitioners should not use the performance testing results from virtual environment with the simple assumption of straightforward performance overhead. Instead, practitioners should consider leveraging normalization techniques to reduce the discrepancy before examining performance testing results from virtual and physical environments.",
      "Keywords": "Software performance analysis and testing on virtual environments | Software performance engineering",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-06-01",
      "Publication type": "Article",
      "Authors": "Arif, Muhammad Moiz;Shang, Weiyi;Shihab, Emad",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85009224950",
      "Primary study DOI": "10.1007/s10664-016-9493-x",
      "Title": "On negative results when using sentiment analysis tools for software engineering research",
      "Abstract": "Recent years have seen an increasing attention to social aspects of software engineering, including studies of emotions and sentiments experienced and expressed by the software developers. Most of these studies reuse existing sentiment analysis tools such as SentiStrength and NLTK. However, these tools have been trained on product reviews and movie reviews and, therefore, their results might not be applicable in the software engineering domain. In this paper we study whether the sentiment analysis tools agree with the sentiment recognized by human evaluators (as reported in an earlier study) as well as with each other. Furthermore, we evaluate the impact of the choice of a sentiment analysis tool on software engineering studies by conducting a simple study of differences in issue resolution times for positive, negative and neutral texts. We repeat the study for seven datasets (issue trackers and Stack Overflow questions) and different sentiment analysis tools and observe that the disagreement between the tools can lead to diverging conclusions. Finally, we perform two replications of previously published studies and observe that the results of those studies cannot be confirmed when a different sentiment analysis tool is used.",
      "Keywords": "Negative results | Replication study | Sentiment analysis tools",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Jongeling, Robbert;Sarkar, Proshanta;Datta, Subhajit;Serebrenik, Alexander",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84992709488",
      "Primary study DOI": "10.1007/s10664-016-9458-0",
      "Title": " Failures  to be celebrated: an analysis of major pivots of software startups",
      "Abstract": "In the context of software startups, project failure is embraced actively and considered crucial to obtain validated learning that can lead to pivots. A pivot is the strategic change of a business concept, product or the different elements of a business model. A better understanding is needed on different types of pivots and different factors that lead to failures and trigger pivots, for software entrepreneurial teams to make better decisions under chaotic and unpredictable environment. Due to the nascent nature of the topic, the existing research and knowledge on the pivots of software startups are very limited. In this study, we aimed at identifying the major types of pivots that software startups make during their startup processes, and highlighting the factors that fail software projects and trigger pivots. To achieve this, we conducted a case survey study based on the secondary data of the major pivots happened in 49 software startups. 10 pivot types and 14 triggering factors were identified. The findings show that customer need pivot is the most common among all pivot types. Together with customer segment pivot, they are common market related pivots. The major product related pivots are zoom-in and technology pivots. Several new pivot types were identified, including market zoom-in, complete and side project pivots. Our study also demonstrates that negative customer reaction and flawed business model are the most common factors that trigger pivots in software startups. Our study extends the research knowledge on software startup pivot types and pivot triggering factors. Meanwhile it provides practical knowledge to software startups, which they can utilize to guide their effective decisions on pivoting.",
      "Keywords": "Lean startup | Pivot | Pivoting factor | Side project | Software startups | Validated learning",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Bajwa, Sohaib Shahid;Wang, Xiaofeng;Nguyen Duc, Anh;Abrahamsson, Pekka",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84983464748",
      "Primary study DOI": "10.1007/s10664-016-9449-1",
      "Title": "To log, or not to log: using heuristics to identify mandatory log events   a controlled experiment",
      "Abstract": "Context: User activity logs should capture evidence to help answer who, what, when, where, why, and how a security or privacy breach occurred. However, software engineers often implement logging mechanisms that inadequately record mandatory log events (MLEs), user activities that must be logged to enable forensics. Goal: The objective of this study is to support security analysts in performing forensic analysis by evaluating the use of a heuristics-driven method for identifying mandatory log events. Method: We conducted a controlled experiment with 103 computer science students enrolled in a graduate-level software security course. All subjects were first asked to identify MLEs described in a set of requirements statements during the pre-period task. In the post-period task, subjects were randomly assigned statements from one type of software artifact (traditional requirements, use-case-based requirements, or user manual), one readability score (simple or complex), and one method (standards-, resource-, or heuristics-driven). We evaluated subject performance using three metrics: statement classification correctness (values from 0 to 1), MLE identification correctness (values from 0 to 1), and response time (seconds). We test the effect of the three factors on the three metrics using generalized linear models. Results: Classification correctness for statements that did not contain MLEs increased 0.31 from pre- to post-period task. MLE identification correctness was inconsistent across treatment groups. For simple user manual statements, MLE identification correctness decreased 0.17 and 0.12 for the standards- and heuristics-driven methods, respectively. For simple traditional requirements statements, MLE identification correctness increased 0.16 and 0.17 for the standards- and heuristics-driven methods, respectively. Average response time decreased 41.7 s from the pre- to post-period task. Conclusion: We expected the performance of subjects using the heuristics-driven method to improve from pre- to post-task and to consistently demonstrate higher MLE identification correctness than the standards-driven and resource-driven methods across domains and readability levels. However, neither method consistently helped subjects more correctly identify MLEs at a statistically significant level. Our results indicate additional training and enforcement may be necessary to ensure subjects understand and consistently apply the assigned methods for identifying MLEs.",
      "Keywords": "Controlled experiment | Logging | Mandatory log events | Security | User activity logs | User study",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "King, Jason;Stallings, Jon;Riaz, Maria;Williams, Laurie",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84994372818",
      "Primary study DOI": "10.1007/s10664-016-9464-2",
      "Title": "Recurring opinions or productive improvements what agile teams actually discuss in retrospectives",
      "Abstract": "Team-level retrospectives are widely used in agile and lean software development, yet little is known about what is actually discussed during retrospectives or their outcomes. In this paper, we synthesise the outcomes of sprint retrospectives in a large, distributed, agile software development organisation. This longitudinal case study analyses data from 37 team-level retrospectives for almost 3 years. We report the outcomes of the retrospectives, their perceived importance for process improvement and relatVed action proposals. Most discussions were related to topics close to and controllable by the team. However, the discussions might suffer from participant bias, and in cases where they are not supported by hard evidence, they might not reflect reality, but rather the sometimes strong opinions of the participants. Some discussions were related to topics that could not be resolved at the team level due to their complexity. Certain topics recurred over a long period of time, either reflecting issues that can and have been solved previously, but that recur naturally as development proceeds, or reflecting waste since they cannot be resolved or improved on by the team due to a lack of controllability or their complexity. For example, the discussion on estimation accuracy did not reflect the true situation and improving the estimates was complicated. On the other hand, discussions on the high number of known bugs recurred despite effective improvements as development proceeded.",
      "Keywords": "Agile | Continuous improvement | Longitudinal case study | Retrospective | Scrum | Software engineering",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Lehtinen, Timo O.A.;Itkonen, Juha;Lassenius, Casper",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018263164",
      "Primary study DOI": "10.1007/s10664-017-9505-5",
      "Title": "Guest editorial for special section on success and failure in software engineering",
      "Abstract": "Many papers investigate success and failure of software projects from diverse perspectives, leading to a myriad of antecedents, causes, correlates, factors and predictors of success and failure. This body of research has not yet produced a solid, empirically grounded body of evidence enabling actionable practices for increasing success and avoiding failure in software projects. The need for more evidence motivates this special issue, which includes four articles that contribute to our understanding of how software project success and failure relate to topics such as: requirements engineering, user satisfaction, start-up pivots and retrospective discussions. We moreover present a brief systematic review to both situate the accepted articles in existing literature and to explore enduring methodological and conceptual challenges in this area, including developing sound instruments for measuring success, representative sampling without population lists and creating both empirically sound and practically actionable taxonomies of success antecedents.",
      "Keywords": "Failure | Failure factors | Project management | Software engineering | Success | Success factors | Systematic review",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-10-01",
      "Publication type": "Editorial",
      "Authors": "Mäntylä, Mika V.;Jørgensen, Magne;Ralph, Paul;Erdogmus, Hakan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85011654161",
      "Primary study DOI": "10.1007/s10664-016-9471-3",
      "Title": "Empirical evaluation of the effects of experience on code quality and programmer productivity: an exploratory study",
      "Abstract": "Context: There is a widespread belief in both SE and other branches of science that experience helps professionals to improve their performance. However, cases have been reported where experience not only does not have a positive influence but sometimes even degrades the performance of professionals. Aim: Determine whether years of experience influence programmer performance. Method: We have analysed 10 quasi-experiments executed both in academia with graduate and postgraduate students and in industry with professionals. The experimental task was to apply ITLD on two experimental problems and then measure external code quality and programmer productivity. Results: Programming experience gained in industry does not appear to have any effect whatsoever on quality and productivity. Overall programming experience gained in academia does tend to have a positive influence on programmer performance. These two findings may be related to the fact that, as opposed to deliberate practice, routine practice does not appear to lead to improved performance. Experience in the use of productivity tools, such as testing frameworks and IDE also has positive effects. Conclusion: Years of experience are a poor predictor of programmer performance. Academic background and specialized knowledge of task-related aspects appear to be rather good predictors.",
      "Keywords": "Academy | Experience | External quality | Industry | Iterative test-last development | Performance | Productivity | Programming",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Dieste, Oscar;Aranda, Alejandrina M.;Uyaguari, Fernando;Turhan, Burak;Tosun, Ayse;Fucci, Davide;Oivo, Markku;Juristo, Natalia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84994335106",
      "Primary study DOI": "10.1007/s10664-016-9465-1",
      "Title": "User satisfaction and system success: an empirical exploration of user involvement in software development",
      "Abstract": "For over four decades user involvement has been considered intuitively to lead to user satisfaction, which plays a pivotal role in successful outcome of a software project. The objective of this paper is to explore the notion of user satisfaction within the context of the user involvement and system success relationship. We have conducted a longitudinal case study of a software development project and collected qualitative data by means of interviews, observations and document analysis over a period of 3 years. The analysis of our case study data revealed that user satisfaction significantly contributes to the system success even when schedule and budget goals are not met. The case study data analysis also presented additional factors that contribute to the evolution of user satisfaction throughout the project. Users’ satisfaction with their involvement and the resulting system are mutually constituted while the level of user satisfaction evolves throughout the stages of software development process. Effective management strategies and user representation are essential elements of maintaining an acceptable level of user satisfaction throughout software development process.",
      "Keywords": "Software Development | System Success | User Involvement | User Satisfaction",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Bano, Muneera;Zowghi, Didar;da Rimini, Francesca",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84981169487",
      "Primary study DOI": "10.1007/s10664-016-9448-2",
      "Title": "Fragile base-class problem, problem?",
      "Abstract": "The fragile base-class problem (FBCP) has been described in the literature as a consequence of “misusing” inheritance and composition in object-oriented programming when (re)using frameworks. Many research works have focused on preventing the FBCP by proposing alternative mechanisms for reuse, but, to the best of our knowledge, there is no previous research work studying the prevalence and impact of the FBCP in real-world software systems. The goal of our work is thus twofold: (1) assess, in different systems, the prevalence of micro-architectures, called FBCS, that could lead to two aspects of the FBCP, (2) investigate the relation between the detected occurrences and the quality of the systems in terms of change and fault proneness, and (3) assess whether there exist bugs in these systems that are related to the FBCP. We therefore perform a quantitative and a qualitative study. Quantitatively, we analyse multiple versions of seven different open-source systems that use 58 different frameworks, resulting in 301 configurations. We detect in these systems 112,263 FBCS occurrences and we analyse whether classes playing the role of sub-classes in FBCS occurrences are more change and–or fault prone than other classes. Results show that classes participating in the analysed FBCS are neither more likely to change nor more likely to have faults. Qualitatively, we conduct a survey to confirm/infirm that some bugs are related to the FBCP. The survey involves 41 participants that analyse a total of 104 bugs of three open-source systems. Results indicate that none of the analysed bugs is related to the FBCP. Thus, despite large, rigorous quantitative and qualitative studies, we must conclude that the two aspects of the FBCP that we analyse may not be as problematic in terms of change and fault-proneness as previously thought in the literature. We propose reasons why the FBCP may not be so prevalent in the analysed systems and in other systems in general.",
      "Keywords": "Change proneness | Composition | Empirical study | Fault proneness | Fragile base-class | Inheritance | Overriding",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Sabané, Aminata;Guéhéneuc, Yann Gaël;Arnaoudova, Venera;Antoniol, Giuliano",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84996477418",
      "Primary study DOI": "10.1007/s10664-016-9472-2",
      "Title": "Negative results for software effort estimation",
      "Abstract": "More than half the literature on software effort estimation (SEE) focuses on comparisons of new estimation methods. Surprisingly, there are no studies comparing state of the art latest methods with decades-old approaches. Accordingly, this paper takes five steps to check if new SEE methods generated better estimates than older methods. Firstly, collect effort estimation methods ranging from “classical” COCOMO (parametric estimation over a pre-determined set of attributes) to “modern” (reasoning via analogy using spectral-based clustering plus instance and feature selection, and a recent “baseline method” proposed in ACM Transactions on Software Engineering). Secondly, catalog the list of objections that lead to the development of post-COCOMO estimation methods. Thirdly, characterize each of those objections as a comparison between newer and older estimation methods. Fourthly, using four COCOMO-style data sets (from 1991, 2000, 2005, 2010) and run those comparisons experiments. Fifthly, compare the performance of the different estimators using a Scott-Knott procedure using (i) the A12 effect size to rule out “small” differences and (ii) a 99 % confident bootstrap procedure to check for statistically different groupings of treatments. The major negative result of this paper is that for the COCOMO data sets, nothing we studied did any better than Boehms original procedure. Hence, we conclude that when COCOMO-style attributes are available, we strongly recommend (i) using that data and (ii) use COCOMO to generate predictions. We say this since the experiments of this paper show that, at least for effort estimation, how data is collected is more important than what learner is applied to that data.",
      "Keywords": "A12 | Bootstrap sampling | CART | Clustering | COCOMO | Effect size | Effort estimation | Feature selection | Nearest neighbor | Prototype generation",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Menzies, Tim;Yang, Ye;Mathew, George;Boehm, Barry;Hihn, Jairus",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84992179970",
      "Primary study DOI": "10.1007/s10664-016-9451-7",
      "Title": "Naming the pain in requirements engineering: Contemporary problems, causes, and effects in practice",
      "Abstract": "Requirements Engineering (RE) has received much attention in research and practice due to its importance to software project success. Its interdisciplinary nature, the dependency to the customer, and its inherent uncertainty still render the discipline difficult to investigate. This results in a lack of empirical data. These are necessary, however, to demonstrate which practically relevant RE problems exist and to what extent they matter. Motivated by this situation, we initiated the Naming the Pain in Requirements Engineering (NaPiRE) initiative which constitutes a globally distributed, bi-yearly replicated family of surveys on the status quo and problems in practical RE. In this article, we report on the qualitative analysis of data obtained from 228 companies working in 10 countries in various domains and we reveal which contemporary problems practitioners encounter. To this end, we analyse 21 problems derived from the literature with respect to their relevance and criticality in dependency to their context, and we complement this picture with a cause-effect analysis showing the causes and effects surrounding the most critical problems. Our results give us a better understanding of which problems exist and how they manifest themselves in practical environments. Thus, we provide a first step to ground contributions to RE on empirical observations which, until now, were dominated by conventional wisdom only.",
      "Keywords": "Requirements engineering | Survey research",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Fernández, D. Méndez;Wagner, S.;Kalinowski, M.;Felderer, M.;Mafra, P.;Vetrò, A.;Conte, T.;Christiansson, M. T.;Greer, D.;Lassenius, C.;Männistö, T.;Nayabi, M.;Oivo, M.;Penzenstadler, B.;Pfahl, D.;Prikladnicki, R.;Ruhe, G.;Schekelmann, A.;Sen, S.;Spinola, R.;Tuzcu, A.;de la Vara, J. L.;Wieringa, R.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85020540868",
      "Primary study DOI": "10.1007/s10664-017-9513-5",
      "Title": "On the correlation between size and metric validity",
      "Abstract": "Empirical validation of code metrics has a long history of success. Many metrics have been shown to be good predictors of external features, such as correlation to bugs. Our study provides an alternative explanation to such validation, attributing it to the confounding effect of size. In contradiction to received wisdom, we argue that the validity of a metric can be explained by its correlation to the size of the code artifact. In fact, this work came about in view of our failure in the quest of finding a metric that is both valid and free of this confounding effect. Our main discovery is that, with the appropriate (non-parametric) transformations, the validity of a metric can be accurately (with R-squared values being at times as high as 0.97) predicted from its correlation with size. The reported results are with respect to a suite of 26 metrics, that includes the famous Chidamber and Kemerer metrics. Concretely, it is shown that the more a metric is correlated with size, the more able it is to predict external features values, and vice-versa. We consider two methods for controlling for size, by linear transformations. As it turns out, metrics controlled for size, tend to eliminate their predictive capabilities. We also show that the famous Chidamber and Kemerer metrics are no better than other metrics in our suite. Overall, our results suggest code size is the only “unique” valid metric.",
      "Keywords": "Object-oriented programming | Software engineering",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Gil, Yossi;Lalouche, Gal",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030169360",
      "Primary study DOI": "10.1007/s10664-017-9552-y",
      "Title": "A correlation study between automated program repair and test-suite metrics",
      "Abstract": "Automated program repair is increasingly gaining traction, due to its potential to reduce debugging cost greatly. The feasibility of automated program repair has been shown in a number of works, and the research focus is gradually shifting toward the quality of generated patches. One promising direction is to control the quality of generated patches by controlling the quality of test-suites used for automated program repair. In this paper, we ask the following research question: “Can traditional test-suite metrics proposed for the purpose of software testing also be used for the purpose of automated program repair?” We empirically investigate whether traditional test-suite metrics such as statement/branch coverage and mutation score are effective in controlling the reliability of generated repairs (the likelihood that repairs cause regression errors). We conduct the largest-scale experiments of this kind to date with real-world software, and for the first time perform a correlation study between various test-suite metrics and the reliability of generated repairs. Our results show that in general, with the increase of traditional test suite metrics, the reliability of repairs tend to increase. In particular, such a trend is most strongly observed in statement coverage. Our results imply that the traditional test suite metrics proposed for software testing can also be used for automated program repair to improve the reliability of repairs.",
      "Keywords": "Automated program repair | Correlation | Empirical evaluation | Test suite",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-10-01",
      "Publication type": "Article",
      "Authors": "Yi, Jooyong;Tan, Shin Hwei;Mechtaev, Sergey;Böhme, Marcel;Roychoudhury, Abhik",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029748378",
      "Primary study DOI": "10.1007/s10664-017-9543-z",
      "Title": "General methods for software architecture recovery: a potential approach and its evaluation",
      "Abstract": "Software architecture is a critical artefact in the software lifecycle. It is a system blueprint for construction, it aids in planning teaming and division of work, and it aids in reasoning about system properties. But architecture documentation is seldom created and, even when it is initially created, it is seldom maintained. For these reasons organisations often feel the need to recover legacy architectures, for example, as part of planning for evolution or cloud migration. But there is no existing general architecture recovery approach nor tool that can be applied to any type of system, under any condition. We will show that one way of achieving such generality is to apply systematic code inspection following a Grounded Theory (GT) approach. Though relatively costly and human-intensive, a GT-based approach has several merits, for example: (a) it is general by design; (b) it can be partially automated; (c) it yields evidence-based results rooted of the system being examined. This article presents one theoretical formulation of a general architecture recovery method–called REM–and reports on the evaluation of REM in the context of a large architecture recovery campaign performed for the European Space Agency. Our results illustrate some intriguing properties and opportunities of GT-based architecture recovery approaches and point out lessons learned and venues for further research.",
      "Keywords": "European space agency | Ground-truth software architectures | Grounded theory | Industrial experience report | Model-driven architecture | Model-driven engineering | OCL | QVT | Software architecture recovery | UML profiles",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-06-01",
      "Publication type": "Article",
      "Authors": "Tamburri, Damian A.;Kazman, Rick",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029726992",
      "Primary study DOI": "10.1007/s10664-017-9545-x",
      "Title": "An exploratory study on assessing the energy impact of logging on Android applications",
      "Abstract": "Execution logs are debug statements that developers insert into their code. Execution logs are used widely to monitor and diagnose the health of software applications. However, logging comes with costs, as it uses computing resources and can have an impact on an application’s performance. Compared with desktop applications, one additional critical computing resource for mobile applications is battery power. Mobile application developers want to deploy energy efficient applications to end users while still maintaining the ability to monitor. Unfortunately, there is no previous work that study the energy impact of logging within mobile applications. This exploratory study investigates the energy cost of logging in Android applications using GreenMiner, an automated energy test-bed for mobile applications. Around 1000 versions from 24 Android applications (e.g., Calculator, FeedEx, Firefox, and VLC) were tested with logging enabled and disabled. To further investigate the energy impacting factors for logging, controlled experiments on a synthetic application were performed. Each test was conducted multiple times to ensure rigorous measurement. Our study found that although there is little to no energy impact when logging is enabled for most versions of the studied applications, about 79% (19/24) of the studied applications have at least one version that exhibit medium to large effect sizes in energy consumption when enabling and disabling logging. To further assess the energy impact of logging, we have conducted a controlled experiment with a synthetic application. We found that the rate of logging and the number of disk flushes are significant factors of energy consumption attributable to logging. Finally, we have examined the relation between the generated OS level execution logs and mobile energy consumption. In addition to the common cross-application log events relevant to garbage collection and graphics systems, some mobile applications also have workload-specific log events that are highly correlated with energy consumption. The regression models built with common log events show mixed performance. Mobile application developers do not need to worry about conservative logging (e.g., logs generated at rates of ≤ 1 message per second), as they are not likely to impact energy consumption. Logging has a negligible effect on energy consumption for most of the mobile applications tested. Although logs have been used effectively to diagnose and debug functional problems, it is still an open problem on how to leverage software instrumentation to debug energy problems.",
      "Keywords": "Android | Energy consumption | Logging",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-06-01",
      "Publication type": "Article",
      "Authors": "Chowdhury, Shaiful;Di Nardo, Silvia;Hindle, Abram;Jiang, Zhen Ming (Jack)",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029590820",
      "Primary study DOI": "10.1007/s10664-017-9541-1",
      "Title": "Are vulnerabilities discovered and resolved like other defects?",
      "Abstract": "Software defect data has long been used to drive software development process improvement. If security defects (vulnerabilities) are discovered and resolved by different software development practices than non-security defects, the knowledge of that distinction could be applied to drive process improvement. The goal of this research is to support technical leaders in making security-specific software development process improvements by analyzing the differences between the discovery and resolution of defects versus that of vulnerabilities. We extend Orthogonal Defect Classification (ODC), a scheme for classifying software defects to support software development process improvement, to study process-related differences between vulnerabilities and defects, creating ODC + Vulnerabilities (ODC + V). We applied ODC + V to classify 583 vulnerabilities and 583 defects across 133 releases of three open-source projects (Firefox, phpMyAdmin, and Chrome). Compared with defects, vulnerabilities are found later in the development cycle and are more likely to be resolved through changes to conditional logic. In Firefox, vulnerabilities are resolved 33% more quickly than defects. From a process improvement perspective, these results indicate opportunities may exist for more efficient vulnerability detection and resolution.",
      "Keywords": "Measurement | Orthogonal Defect Classification (ODC) | Process improvement | Security | Software development",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-06-01",
      "Publication type": "Article",
      "Authors": "Morrison, Patrick J.;Pandita, Rahul;Xiao, Xusheng;Chillarege, Ram;Williams, Laurie",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029604461",
      "Primary study DOI": "10.1007/s10664-017-9546-9",
      "Title": "Sentiment Polarity Detection for Software Development",
      "Abstract": "The role of sentiment analysis is increasingly emerging to study software developers’ emotions by mining crowd-generated content within social software engineering tools. However, off-the-shelf sentiment analysis tools have been trained on non-technical domains and general-purpose social media, thus resulting in misclassifications of technical jargon and problem reports. Here, we present Senti4SD, a classifier specifically trained to support sentiment analysis in developers’ communication channels. Senti4SD is trained and validated using a gold standard of Stack Overflow questions, answers, and comments manually annotated for sentiment polarity. It exploits a suite of both lexicon- and keyword-based features, as well as semantic features based on word embedding. With respect to a mainstream off-the-shelf tool, which we use as a baseline, Senti4SD reduces the misclassifications of neutral and positive posts as emotionally negative. To encourage replications, we release a lab package including the classifier, the word embedding space, and the gold standard with annotation guidelines.",
      "Keywords": "Communication Channels | Sentiment Analysis | Social Software Engineering | Stack Overflow | Word Embedding",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-06-01",
      "Publication type": "Article",
      "Authors": "Calefato, Fabio;Lanubile, Filippo;Maiorano, Federico;Novielli, Nicole",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029475777",
      "Primary study DOI": "10.1007/s10664-017-9547-8",
      "Title": "Inference of development activities from interaction with uninstrumented applications",
      "Abstract": "Studying developers’ behavior in software development tasks is crucial for designing effective techniques and tools to support developers’ daily work. In modern software development, developers frequently use different applications including IDEs, Web Browsers, documentation software (such as Office Word, Excel, and PDF applications), and other tools to complete their tasks. This creates significant challenges in collecting and analyzing developers’ behavior data. Researchers usually instrument the software tools to log developers’ behavior for further studies. This is feasible for studies on development activities using specific software tools. However, instrumenting all software tools commonly used in real work settings is difficult and requires significant human effort. Furthermore, the collected behavior data consist of low-level and fine-grained event sequences, which must be abstracted into high-level development activities for further analysis. This abstraction is often performed manually or based on simple heuristics. In this paper, we propose an approach to address the above two challenges in collecting and analyzing developers’ behavior data. First, we use our ActivitySpace framework to improve the generalizability of the data collection. ActivitySpace uses operating-system level instrumentation to track developer interactions with a wide range of applications in real work settings. Secondly, we use a machine learning approach to reduce the human effort to abstract low-level behavior data. Specifically, considering the sequential nature of the interaction data, we propose a Condition Random Field (CRF) based approach to segment and label the developers’ low-level actions into a set of basic, yet meaningful development activities. To validate the generalizability of the proposed data collection approach, we deploy the ActivitySpace framework in an industry partner’s company and collect the real working data from ten professional developers’ one-week work in three actual software projects. The experiment with the collected data confirms that with initial human-labeled training data, the CRF model can be trained to infer development activities from low-level actions with reasonable accuracy within and across developers and software projects. This suggests that the machine learning approach is promising in reducing the human efforts required for behavior data analysis.",
      "Keywords": "Condition Random Field | Developers’ interaction data | Software development",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-06-01",
      "Publication type": "Article",
      "Authors": "Bao, Lingfeng;Xing, Zhenchang;Xia, Xin;Lo, David;Hassan, Ahmed E.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029471684",
      "Primary study DOI": "10.1007/s10664-017-9542-0",
      "Title": "Erratum to: Architecture consistency: State of the practice, challenges and requirements",
      "Abstract": "The article Architecture consistency: State of the practice, challenges and requirements, written by Nour Ali et al. was originally published electronically on the publisher’s internet portal on May 15, 2017 without open access. The original article has been corrected.",
      "Keywords": "",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-06-01",
      "Publication type": "Erratum",
      "Authors": "Ali, Nour;Baker, Sean;O’Crowley, Ross;Herold, Sebastian;Buckley, Jim",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029144713",
      "Primary study DOI": "10.1007/s10664-017-9540-2",
      "Title": "Code smells for Model-View-Controller architectures",
      "Abstract": "Previous studies have shown the negative effects that low-quality code can have on maintainability proxies, such as code change- and defect-proneness. One of the symptoms of low-quality code are code smells, defined as sub-optimal implementation choices. While this definition is quite general and seems to suggest a wide spectrum of smells that can affect software systems, the research literature mostly focuses on the set of smells defined in the catalog by Fowler and Beck, reporting design issues that can potentially affect any kind of system, regardless of their architecture (e.g., Complex Class). However, systems adopting a specific architecture (e.g., the Model-View-Controller pattern) can be affected by other types of poor practices that only manifest themselves in the chosen architecture. We present a catalog of six smells tailored to MVC applications and defined by surveying/interviewing 53 MVC developers. We validate our catalog from different perspectives. First, we assess the relationship between the defined smells and the code change- and defect-proneness. Second, we investigate when these smells are introduced and how long they survive. Third, we survey 21 developers to verify their perception of the defined smells. Fourth, since our catalog has been mainly defined together with developers adopting a specific Java framework in their MVC applications (e.g., Spring), we interview four expert developers working with different technologies for the implementation of their MVC applications to check the generalizability of our catalog. The achieved results show that the defined Web MVC smells (i) more often than not, have more chances of being subject to changes and defects, (ii) are mostly introduced when the affected file (i.e., the file containing the smell) is committed for the first time in the repository and survive for long time in the system, (iii) are perceived by developers as severe problems, and (iv) generalize to other languages/frameworks.",
      "Keywords": "Code anomalies | Code anti-patterns | Code quality | Code smells | Software maintenance",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-08-01",
      "Publication type": "Article",
      "Authors": "Aniche, Maurício;Bavota, Gabriele;Treude, Christoph;Gerosa, Marco Aurélio;van Deursen, Arie",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028978549",
      "Primary study DOI": "10.1007/s10664-017-9538-9",
      "Title": "Studying the dialogue between users and developers of free apps in the Google Play Store",
      "Abstract": "The popularity of mobile apps continues to grow over the past few years. Mobile app stores, such as the Google Play Store and Apple’s App Store provide a unique user feedback mechanism to app developers through the possibility of posting app reviews. In the Google Play Store (and soon in the Apple App Store), developers are able to respond to such user feedback. Over the past years, mobile app reviews have been studied excessively by researchers. However, much of prior work (including our own prior work) incorrectly assumes that reviews are static in nature and that users never update their reviews. In a recent study, we started analyzing the dynamic nature of the review-response mechanism. Our previous study showed that responding to a review often has a positive effect on the rating that is given by the user to an app. In this paper, we revisit our prior finding in more depth by studying 4.5 million reviews with 126,686 responses for 2,328 top free-to-download apps in the Google Play Store. One of the major findings of our paper is that the assumption that reviews are static is incorrect. In particular, we find that developers and users in some cases use this response mechanism as a rudimentary user support tool, where dialogues emerge between users and developers through updated reviews and responses. Even though the messages are often simple, we find instances of as many as ten user-developer back-and-forth messages that occur via the response mechanism. Using a mixed-effect model, we identify that the likelihood of a developer responding to a review increases as the review rating gets lower or as the review content gets longer. In addition, we identify four patterns of developers: 1) developers who primarily respond to only negative reviews, 2) developers who primarily respond to negative reviews or to reviews based on their contents, 3) developers who primarily respond to reviews which are posted shortly after the latest release of their app, and 4) developers who primarily respond to reviews which are posted long after the latest release of their app. We perform a qualitative analysis of developer responses to understand what drives developers to respond to a review. We manually analyzed a statistically representative random sample of 347 reviews with responses for the top ten apps with the highest number of developer responses. We identify seven drivers that make a developer respond to a review, of which the most important ones are to thank the users for using the app and to ask the user for more details about the reported issue. Our findings show that it can be worthwhile for app owners to respond to reviews, as responding may lead to an increase in the given rating. In addition, our findings show that studying the dialogue between user and developer can provide valuable insights that can lead to improvements in the app store and user support process.",
      "Keywords": "Android mobile apps | Developer reply | Developer response | Empirical study | Google play store | Mixed-effect model | Software engineering | User-developer dialogue",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-06-01",
      "Publication type": "Article",
      "Authors": "Hassan, Safwat;Tantithamthavorn, Chakkrit;Bezemer, Cor Paul;Hassan, Ahmed E.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028588565",
      "Primary study DOI": "10.1007/s10664-017-9539-8",
      "Title": "A multi-view context-aware approach to Android malware detection and malicious code localization",
      "Abstract": "Many existing Machine Learning (ML) based Android malware detection approaches use a variety of features such as security-sensitive APIs, system calls, control-flow structures and information flows in conjunction with ML classifiers to achieve accurate detection. Each of these feature sets provides a unique semantic perspective (or view) of apps’ behaviors with inherent strengths and limitations. Meaning, some views are more amenable to detect certain attacks but may not be suitable to characterize several other attacks. Most of the existing malware detection approaches use only one (or a selected few) of the aforementioned feature sets which prevents them from detecting a vast majority of attacks. Addressing this limitation, we propose MKLDroid, a unified framework that systematically integrates multiple views of apps for performing comprehensive malware detection and malicious code localization. The rationale is that, while a malware app can disguise itself in some views, disguising in every view while maintaining malicious intent will be much harder. MKLDroid uses a graph kernel to capture structural and contextual information from apps’ dependency graphs and identify malice code patterns in each view. Subsequently, it employs Multiple Kernel Learning (MKL) to find a weighted combination of the views which yields the best detection accuracy. Besides multi-view learning, MKLDroid’s unique and salient trait is its ability to locate fine-grained malice code portions in dependency graphs (e.g., methods/classes). Malicious code localization caters several important applications such as supporting human analysts studying malware behaviors, engineering malware signatures, and other counter-measures. Through our large-scale experiments on several datasets (incl. wild apps), we demonstrate that MKLDroid outperforms three state-of-the-art techniques consistently, in terms of accuracy while maintaining comparable efficiency. In our malicious code localization experiments on a dataset of repackaged malware, MKLDroid was able to identify all the malice classes with 94% average recall. Our work opens up two new avenues in malware research: (i) enables the research community to elegantly look at Android malware behaviors in multiple perspectives simultaneously, and (ii) performing precise and scalable malicious code localization.",
      "Keywords": "Android malware detection | Graph kernels | Malicious code localization | Multiple kernel learning",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-06-01",
      "Publication type": "Article",
      "Authors": "Narayanan, Annamalai;Chandramohan, Mahinthan;Chen, Lihui;Liu, Yang",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026909182",
      "Primary study DOI": "10.1007/s10664-017-9535-z",
      "Title": "On the diffuseness and the impact on maintainability of code smells: a large scale empirical investigation",
      "Abstract": "Code smells are symptoms of poor design and implementation choices that may hinder code comprehensibility and maintainability. Despite the effort devoted by the research community in studying code smells, the extent to which code smells in software systems affect software maintainability remains still unclear. In this paper we present a large scale empirical investigation on the diffuseness of code smells and their impact on code change- and fault-proneness. The study was conducted across a total of 395 releases of 30 open source projects and considering 17,350 manually validated instances of 13 different code smell kinds. The results show that smells characterized by long and/or complex code (e.g., Complex Class) are highly diffused, and that smelly classes have a higher change- and fault-proneness than smell-free classes.",
      "Keywords": "Code smells | Empirical studies | Mining software repositories",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-06-01",
      "Publication type": "Article",
      "Authors": "Palomba, Fabio;Bavota, Gabriele;Penta, Massimiliano Di;Fasano, Fausto;Oliveto, Rocco;Lucia, Andrea De",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026823677",
      "Primary study DOI": "10.1007/s10664-017-9534-0",
      "Title": "How does developer interaction relate to software quality? an examination of product development data",
      "Abstract": "Industrial software systems are being increasingly developed by large and distributed teams. Tools like collaborative development environments (CDE) are used to facilitate interaction between members of such teams, with the expectation that social factors around the interaction would facilitate team functioning. In this paper, we first identify typically social characteristics of interaction in a software development team: reachability, connection, association, and clustering. We then examine how these factors relate to the quality of software produced by a team, in terms of the number of defects, through an empirical study of 70+ teams, involving 900+ developers in total, spread across 30+ locations and 19 time-zones, working on 40,000+ units of work in the multi-version development of a major industrial product, spreading across more than five years. After controlling for known factors affecting large scale distributed development such as dependency, system age, developer expertise and experience, geographic dispersion, socio-technical congruence, and the number of files changed, we find statistically significant effects of connection and clustering on software quality. Higher levels of intra-team connection are found to relate to higher defect count, whereas more clustering relates to fewer defects. We examine the implications of these results for individual developers, project managers, and organizations.",
      "Keywords": "Clustering | Connection | Defect count | Interaction | Jazz | Software quality | Software teams",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2018-06-01",
      "Publication type": "Article",
      "Authors": "Datta, Subhajit",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85001575065",
      "Primary study DOI": "10.1007/s10664-016-9484-y",
      "Title": "Will this localization tool be effective for this bug? Mitigating the impact of unreliability of information retrieval based bug localization tools",
      "Abstract": "Information retrieval (IR) based bug localization approaches process a textual bug report and a collection of source code files to find buggy files. They output a ranked list of files sorted by their likelihood to contain the bug. Recently, several IR-based bug localization tools have been proposed. However, there are no perfect tools that can successfully localize faults within a few number of most suspicious program elements for every single input bug report. Therefore, it is difficult for developers to decide which tool would be effective for a given bug report. Furthermore, for some bug reports, no bug localization tools would be useful. Even a state-of-the-art bug localization tool outputs many ranked lists where buggy files appear very low in the lists. This potentially causes developers to distrust bug localization tools. In this work, we build an oracle that can automatically predict whether a ranked list produced by an IR-based bug localization tool is likely to be effective or not. We consider a ranked list to be effective if a buggy file appears in the top-N position of the list. If a ranked list is unlikely to be effective, developers do not need to waste time in checking the recommended files one by one. In such cases, it is better for developers to use traditional debugging methods or request for further information to localize bugs. To build this oracle, our approach extracts features that can be divided into four categories: score features, textual features, topic model features, and metadata features. We build a separate prediction model for each category, and combine them to create a composite prediction model which is used as the oracle. We name this solution APRILE, which stands for Automated PRediction of IR-based Bug Localization’s Effectiveness. We further integrate APRILE with two other components that are learned using our bagging-based ensemble classification (BEC) method. We refer to the extension of APRILE as APRILE +. We have evaluated APRILE + to predict the effectiveness of three state-of-the-art IR-based bug localization tools on more than three thousands bug reports from AspectJ, Eclipse, SWT, and Tomcat. APRILE + can achieve an average precision, recall, and F-measure of 77.61 %, 88.94 %, and 82.09 %, respectively. Furthermore, APRILE + outperforms a baseline approach by Le and Lo and APRILE by up to a 17.43 % and 10.51 % increase in F-measure respectively.",
      "Keywords": "Bug localization | Bug reports | Effectiveness prediction | Information retrieval | Text classification",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Le, Tien Duy B.;Thung, Ferdian;Lo, David",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84995503398",
      "Primary study DOI": "10.1007/s10664-016-9463-3",
      "Title": "Tracing distributed collaborative development in apache software foundation projects",
      "Abstract": "Developing and maintaining large software systems typically requires that developers collaborate on many tasks. During such collaborations, when multiple people work on the same chunk of code at the same time, they communicate with each other and employ safeguards in various ways. Recent studies have considered group co-development in OSS projects and found that it is an essential part of many projects. However, those studies were limited to groups of size two, i.e., pairs of developers. Here we go further and characterize co-development in larger groups. We develop an effective methodology for capturing distributed collaboration beyond groups of size two, based on synchronized commit activities among multiple developers, and apply it to data from 26 OSS projects from the Apache Software Foundation. We find that distributed collaborations is prevalent, but not as frequent as expected. We also find that while in distributed collaborative groups, developers’ behavior is different than when programming alone, e.g., high developer focus on specific code packages associates with lower team participation, while packages with higher ownership get less attention from groups than from individuals. Finally, we show that productivity effort during co-development is more often lower for developers while they co-develop in groups. To verify our results we use both quantitative and qualitative methods, including a developer survey. We conclude that these methods and results can be used to understand the effects of the collaborative dynamic in OSS teams on the software engineering process. Our code, along with our datasets and survey is available at http://www.gharehyazie.com/supplementary/teamwork/.",
      "Keywords": "Distributed collaborative development | OSS | Teamwork",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Gharehyazie, Mohammad;Filkov, Vladimir",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84996560163",
      "Primary study DOI": "10.1007/s10664-016-9483-z",
      "Title": "Effectiveness and efficiency of a domain-specific language for high-performance marine ecosystem simulation: a controlled experiment",
      "Abstract": "It is a long-standing hypothesis that the concise and customized notation of a DSL improves the performance of developers when compared with a GPL. For non-technical domains—e.g., science—, this hypothesis lacks empirical evidence. Given this lack of empirical evidence, we evaluate a DSL for ecological modeling designed and implemented by us with regard to performance improvements of developers as compared to a GPL. We conduct an online survey with embedded controlled experiments among ecologists to assess the correctness and time spent of the participants when using a DSL for ecosystem simulation specifications compared with a GPL-based solution. We observe that (1) solving tasks with the DSL, the participants’ correctness point score was —depending on the task— on average 61 % up to 63 % higher than with the GPL-based solution and their average time spent per task was reduced by 31 % up to 56 %; (2) the participants subjectively find it easier to work with the DSL, and (3) more than 90 % of the subjects are able to carry out basic maintenance tasks concerning the infrastructure of the DSL used in our treatment, which is based on another internal DSL embedded into Java. The tasks of our experiments are simplified and our web-based editor components do not offer full IDE-support. Our findings indicate that the development of further DSL for the specific needs of the ecological modeling community should be a worthwhile investment to increase its members’ productivity and to enhance the reliability of their scientific results.",
      "Keywords": "Computational science | Domain-specific languages (DSLs) | Program comprehension | Scientific software development",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Johanson, Arne N.;Hasselbring, Wilhelm",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84958756250",
      "Primary study DOI": "10.1007/s10664-016-9425-9",
      "Title": "Metric-based software reliability prediction approach and its application",
      "Abstract": "This paper proposes a software reliability prediction approach based on software metrics. Metrics measurement results are connected to quantitative reliability predictions through defect information and consideration of the operational environments. An application of the proposed approach to a safety critical software deployed in a nuclear power plant is discussed. Results show that the proposed prediction approach could be applied using a variety of software metrics at different stages of the software development life cycle and could be used as an indicator of software quality. Therefore the approach could also guide the development process and help make design decisions. Experiences and lessons learned from the application are also discussed.",
      "Keywords": "Safety-critical software | Software metrics | Software reliability prediction",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Shi, Ying;Li, Ming;Arndt, Steven;Smidts, Carol",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84996550774",
      "Primary study DOI": "10.1007/s10664-016-9473-1",
      "Title": "Semantic topic models for source code analysis",
      "Abstract": "Topic modeling techniques have been recently applied to analyze and model source code. Such techniques exploit the textual content of source code to provide automated support for several basic software engineering activities. Despite these advances, applications of topic modeling in software engineering are frequently suboptimal. This can be attributed to the fact that current state-of-the-art topic modeling techniques tend to be data intensive. However, the textual content of source code, embedded in its identifiers, comments, and string literals, tends to be sparse in nature. This prevents classical topic modeling techniques, typically used to model natural language texts, to generate proper models when applied to source code. Furthermore, the operational complexity and multi-parameter calibration often associated with conventional topic modeling techniques raise important concerns about their feasibility as data analysis models in software engineering. Motivated by these observations, in this paper we propose a novel approach for topic modeling designed for source code. The proposed approach exploits the basic assumptions of the cluster hypothesis and information theory to discover semantically coherent topics in software systems. Ten software systems from different application domains are used to empirically calibrate and configure the proposed approach. The usefulness of generated topics is empirically validated using human judgment. Furthermore, a case study that demonstrates thet operation of the proposed approach in analyzing code evolution is reported. The results show that our approach produces stable, more interpretable, and more expressive topics than classical topic modeling techniques without the necessity for extensive parameter calibration.",
      "Keywords": "Clustering | Information theory | Topic modeling",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Mahmoud, Anas;Bradshaw, Gary",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84991821880",
      "Primary study DOI": "10.1007/s10664-016-9462-4",
      "Title": "Multi-objective reverse engineering of variability-safe feature models based on code dependencies of system variants",
      "Abstract": "Maintenance of many variants of a software system, developed to supply a wide range of customer-specific demands, is a complex endeavour. The consolidation of such variants into a Software Product Line is a way to effectively cope with this problem. A crucial step for this consolidation is to reverse engineer feature models that represent the desired combinations of features of all the available variants. Many approaches have been proposed for this reverse engineering task but they present two shortcomings. First, they use a single-objective perspective that does not allow software engineers to consider design trade-offs. Second, they do not exploit knowledge from implementation artifacts. To address these limitations, our work takes a multi-objective perspective and uses knowledge from source code dependencies to obtain feature models that not only represent the desired feature combinations but that also check that those combinations are indeed well-formed, i.e. variability safe. We performed an evaluation of our approach with twelve case studies using NSGA-II and SPEA2, and a single-objective algorithm. Our results indicate that the performance of the multi-objective algorithms is similar in most cases and that both clearly outperform the single-objective algorithm. Our work also unveils several avenues for further research.",
      "Keywords": "Empirical evaluation | Feature models | Multi-objective evolutionary algorithms | Reverse engineering",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Assunção, Wesley K.G.;Lopez-Herrejon, Roberto E.;Linsbauer, Lukas;Vergilio, Silvia R.;Egyed, Alexander",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84992371694",
      "Primary study DOI": "10.1007/s10664-016-9470-4",
      "Title": "Automatic repair of real bugs in java: a large-scale experiment on the defects4j dataset",
      "Abstract": "Defects4J is a large, peer-reviewed, structured dataset of real-world Java bugs. Each bug in Defects4J comes with a test suite and at least one failing test case that triggers the bug. In this paper, we report on an experiment to explore the effectiveness of automatic test-suite based repair on Defects4J. The result of our experiment shows that the considered state-of-the-art repair methods can generate patches for 47 out of 224 bugs. However, those patches are only test-suite adequate, which means that they pass the test suite and may potentially be incorrect beyond the test-suite satisfaction correctness criterion. We have manually analyzed 84 different patches to assess their real correctness. In total, 9 real Java bugs can be correctly repaired with test-suite based repair. This analysis shows that test-suite based repair suffers from under-specified bugs, for which trivial or incorrect patches still pass the test suite. With respect to practical applicability, it takes on average 14.8 minutes to find a patch. The experiment was done on a scientific grid, totaling 17.6 days of computation time. All the repair systems and experimental results are publicly available on Github in order to facilitate future research on automatic repair.",
      "Keywords": "Bugs | Defects | Fixes | Patches | Software repair",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Martinez, Matias;Durieux, Thomas;Sommerard, Romain;Xuan, Jifeng;Monperrus, Martin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85004115698",
      "Primary study DOI": "10.1007/s10664-016-9480-2",
      "Title": "Studying the urgent updates of popular games on the Steam platform",
      "Abstract": "The steadily increasing popularity of computer games has led to the rise of a multi-billion dollar industry. This increasing popularity is partly enabled by online digital distribution platforms for games, such as Steam. These platforms offer an insight into the development and test processes of game developers. In particular, we can extract the update cycle of a game and study what makes developers deviate from that cycle by releasing so-called urgent updates. An urgent update is a software update that fixes problems that are deemed critical enough to not be left unfixed until a regular-cycle update. Urgent updates are made in a state of emergency and outside the regular development and test timelines which causes unnecessary stress on the development team. Hence, avoiding the need for an urgent update is important for game developers. We define urgent updates as 0-day updates (updates that are released on the same day), updates that are released faster than the regular cycle, or self-admitted hotfixes. We conduct an empirical study of the urgent updates of the 50 most popular games from Steam, the dominant digital game delivery platform. As urgent updates are reflections of mistakes in the development and test processes, a better understanding of urgent updates can in turn stimulate the improvement of these processes, and eventually save resources for game developers. In this paper, we argue that the update strategy that is chosen by a game developer affects the number of urgent updates that are released. Although the choice of update strategy does not appear to have an impact on the percentage of updates that are released faster than the regular cycle or self-admitted hotfixes, games that use a frequent update strategy tend to have a higher proportion of 0-day updates than games that use a traditional update strategy.",
      "Keywords": "Computer games | Steam | Update cycle | Update strategy | Urgent update",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Lin, Dayi;Bezemer, Cor Paul;Hassan, Ahmed E.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84992223488",
      "Primary study DOI": "10.1007/s10664-016-9468-y",
      "Title": "Global vs. local models for cross-project defect prediction: A replication study",
      "Abstract": "Although researchers invested significant effort, the performance of defect prediction in a cross-project setting, i.e., with data that does not come from the same project, is still unsatisfactory. A recent proposal for the improvement of defect prediction is using local models. With local models, the available data is first clustered into homogeneous regions and afterwards separate classifiers are trained for each homogeneous region. Since the main problem of cross-project defect prediction is data heterogeneity, the idea of local models is promising. Therefore, we perform a conceptual replication of the previous studies on local models with a focus on cross-project defect prediction. In a large case study, we evaluate the performance of local models and investigate their advantages and drawbacks for cross-project predictions. To this aim, we also compare the performance with a global model and a transfer learning technique designed for cross-project defect predictions. Our findings show that local models make only a minor difference in comparison to global models and transfer learning for cross-project defect prediction. While these results are negative, they provide valuable knowledge about the limitations of local models and increase the validity of previously gained research results.",
      "Keywords": "Cross-project | Defect prediction | Local models",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Herbold, Steffen;Trautsch, Alexander;Grabowski, Jens",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85008465262",
      "Primary study DOI": "10.1007/s10664-016-9475-z",
      "Title": "Group versus individual use of power-only EPMcreate as a creativity enhancement technique for requirements elicitation",
      "Abstract": "Creativity is often needed in requirements elicitation, i.e., generating ideas for requirements, and therefore, techniques to enhance creativity are believed to be useful. How does the size of a group using the Power-Only EPMcreate (POEPMcreate) creativity enhancement technique affect the group’s and each member of the group’s effectiveness in generating requirement ideas? This paper describes an experiment in which individuals and two-person and four-person groups used POEPMcreate to generate ideas for requirements for enhancing a high school’s public Web site. The data of this experiment combined with the data of two previous experiments involving two-person and four-person groups using POEPMcreate show that, similar to what has been observed for brainstorming, the size of a group using POEPMcreate does affect the number of raw and new requirement ideas generated by the group and by the average member of the group. The data allow concluding that a two-person group using POEPMcreate generates more raw and new requirement ideas, both per group and per group member or individual, than does a four-person group and than does an individual. This conclusion is partially corroborated by qualitative data gathered from a survey of professional business or requirements analysts about group sizes and creativity enhancement techniques.",
      "Keywords": "",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Sakhnini, Victoria;Mich, Luisa;Berry, Daniel M.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84994709940",
      "Primary study DOI": "10.1007/s10664-016-9469-x",
      "Title": "Are delayed issues harder to resolve? Revisiting cost-to-fix of defects throughout the lifecycle",
      "Abstract": "Many practitioners and academics believe in a delayed issue effect (DIE); i.e. the longer an issue lingers in the system, the more effort it requires to resolve. This belief is often used to justify major investments in new development processes that promise to retire more issues sooner. This paper tests for the delayed issue effect in 171 software projects conducted around the world in the period from 2006–2014. To the best of our knowledge, this is the largest study yet published on this effect. We found no evidence for the delayed issue effect; i.e. the effort to resolve issues in a later phase was not consistently or substantially greater than when issues were resolved soon after their introduction. This paper documents the above study and explores reasons for this mismatch between this common rule of thumb and empirical data. In summary, DIE is not some constant across all projects. Rather, DIE might be an historical relic that occurs intermittently only in certain kinds of projects. This is a significant result since it predicts that new development processes that promise to faster retire more issues will not have a guaranteed return on investment (depending on the context where applied), and that a long-held truth in software engineering should not be considered a global truism.",
      "Keywords": "Cost to fix | Phase delay | Software economics",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Menzies, Tim;Nichols, William;Shull, Forrest;Layman, Lucas",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84976479544",
      "Primary study DOI": "10.1007/s10664-016-9440-x",
      "Title": "Assessing the quality of industrial avionics software: an extensive empirical evaluation",
      "Abstract": "A real-time operating system for avionics (RTOS4A) provides an operating environment for avionics application software. Since an RTOS4A has safety-critical applications, demonstrating a satisfactory level of its quality to its stakeholders is very important. By assessing the variation in quality across consecutive releases of an industrial RTOS4A based on test data collected over 17 months, we aim to provide a set of guidelines to 1) improve the test effectiveness and thus the quality of subsequent RTOS4A releases and 2) similarly assess the quality of other systems from test data. We carefully defined a set of research questions, for which we defined a number of variables (based on available test data), including release and measures of test effort, test effectiveness, complexity, test efficiency, test strength, and failure density. With these variables, to assess the quality in terms of number of failures found in tests, we applied a combination of analyses, including trend analysis using two-dimensional graphs, correlation analysis using Spearman’s test, and difference analysis using the Wilcoxon rank test. Key results include the following: 1) The number of failures and failure density decreased in the latest releases and the test coverage was either high or did not decrease with each release; 2) increased test effort was spent on modules of greater complexity and the number of failures was not high in these modules; and 3) the test coverage for modules without failures was not lower than the test coverage for modules with failures uncovered in all the releases. The overall assessment, based on the evidences, suggests that the quality of the latest RTOS4A release has improved. We conclude that the quality of the RTOS4A studied was improved in the latest release. In addition, our industrial partner found our guidelines useful and we believe that these guidelines can be used to assess the quality of other applications in the future.",
      "Keywords": "Avionics software | Industrial case study | Real time operating system | Software Quality assessment",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Wu, Ji;Ali, Shaukat;Yue, Tao;Tian, Jie;Liu, Chao",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85006172824",
      "Primary study DOI": "10.1007/s10664-016-9482-0",
      "Title": "An initial analysis of software engineers  attitudes towards organizational change",
      "Abstract": "Employees’ attitudes towards organizational change are a critical determinant in the change process. Researchers have therefore tried to determine what underlying concepts that affect them. These extensive efforts have resulted in the identification of several antecedents. However, no studies have been conducted in a software engineering context and the research has provided little information on the relative impact and importance of the identified concepts. In this study, we have combined results from previous social science research with results from software engineering research, and thereby identified three underlying concepts with an expected significant impact on software engineers’ attitudes towards organizational change, i.e. their knowledge about the intended change outcome, their understanding of the need for change, and their feelings of participation in the change process. The result of two separate multiple regression analysis, where we used industrial questionnaire data (N=56), showed that the attitude concept openness to change is predicted by all three concepts, while the attitude concept readiness for change is predicted by need for change and participation. Our research provides an empirical baseline to an important area of software engineering and the result can be a starting-point for future organizational change research. In addition, the proposed model prescribes practical directions for software engineering organizations to adopt in improving employees’ responses to change and, thus, increase the probability of a successful change.",
      "Keywords": "Attitudes | Behavioral software engineering | Human aspects | Openness to change | Organizational change | Readiness for change | Social psychology | Software engineering | Systematic literature review",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Lenberg, Per;Wallgren Tengberg, Lars Göran;Feldt, Robert",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84992220706",
      "Primary study DOI": "10.1007/s10664-016-9467-z",
      "Title": "Towards just-in-time suggestions for log changes",
      "Abstract": "Software developers typically insert logging statements in their source code to record runtime information. However, providing proper logging statements remains a challenging task. Prior approaches automatically enhance logging statements, as a post-implementation process. Such automatic approaches do not take into account developers’ domain knowledge; nevertheless, developers usually need to carefully design the logging statements since logs are a rich source about the field operation of a software system. The goals of this paper include: i) understanding the reasons for log changes; and ii) proposing an approach that can provide developers with log change suggestions as soon as they commit a code change, which we refer to as “just-in-time” suggestions for log changes. In particular, we derive a set of measures based on manually examining the reasons for log changes and our experiences. We use these measures as explanatory variables in random forest classifiers to model whether a code commit requires log changes. These classifiers can provide just-in-time suggestions for log changes. We perform a case study on four open source projects: Hadoop, Directory Server, Commons HttpClient, and Qpid. We find that: (i) The reasons for log changes can be grouped along four categories: block change, log improvement, dependence-driven change, and logging issue; (ii) our random forest classifiers can effectively suggest whether a log change is needed: the classifiers that are trained from within-project data achieve a balanced accuracy of 0.76 to 0.82, and the classifiers that are trained from cross-project data achieve a balanced accuracy of 0.76 to 0.80; (iii) the characteristics of code changes in a particular commit and the current snapshot of the source code are the most influential factors for determining the likelihood of a log change in a commit.",
      "Keywords": "Log improvement | Mining software repositories | Software logs",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Li, Heng;Shang, Weiyi;Zou, Ying;E. Hassan, Ahmed",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85000406013",
      "Primary study DOI": "10.1007/s10664-016-9478-9",
      "Title": "Evolutionary trends of developer coordination: a network approach",
      "Abstract": "Software evolution is a fundamental process that transcends the realm of technical artifacts and permeates the entire organizational structure of a software project. By means of a longitudinal empirical study of 18 large open-source projects, we examine and discuss the evolutionary principles that govern the coordination of developers. By applying a network-analytic approach, we found that the implicit and self-organizing structure of developer coordination is ubiquitously described by non-random organizational principles that defy conventional software-engineering wisdom. In particular, we found that: (a) developers form scale-free networks, in which the majority of coordination requirements arise among an extremely small number of developers, (b) developers tend to accumulate coordination requirements with more and more developers over time, presumably limited by an upper bound, and (c) initially developers are hierarchically arranged, but over time, form a hybrid structure, in which core developers are hierarchically arranged and peripheral developers are not. Our results suggest that the organizational structure of large projects is constrained to evolve towards a state that balances the costs and benefits of developer coordination, and the mechanisms used to achieve this state depend on the project’s scale.",
      "Keywords": "Developer coordination | Developer networks | Software evolution",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Joblin, Mitchell;Apel, Sven;Mauerer, Wolfgang",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85006428056",
      "Primary study DOI": "10.1007/s10664-016-9481-1",
      "Title": "Identifying the implied: Findings from three differentiated replications on the use of security requirements templates",
      "Abstract": "Identifying security requirements early on can lay the foundation for secure software development. Security requirements are often implied by existing functional requirements but are mostly left unspecified. The Security Discoverer (SD) process automatically identifies security implications of individual requirements sentences and suggests applicable security requirements templates. The objective of this research is to support requirements analysts in identifying security requirements by automating the suggestion of security requirements templates that are implied by existing functional requirements. We conducted a controlled experiment in a graduate-level security class at North Carolina State University (NCSU) to evaluate the SD process in eliciting implied security requirements in 2014. We have subsequently conducted three differentiated replications to evaluate the generalizability and applicability of the initial findings. The replications were conducted across three countries at the University of Trento, NCSU, and the University of Costa Rica. We evaluated the responses of the 205 total participants in terms of quality, coverage, relevance and efficiency. We also develop shared insights regarding the impact of context factors such as time, motivation and support, on the study outcomes and provide lessons learned in conducting the replications. Treatment group, using the SD process, performed significantly better than the control group (at p-value <0.05) in terms of the coverage of the identified security requirements and efficiency of the requirements elicitation process in two of the three replications, supporting the findings of the original study. Participants in the treatment group identified 84 % more security requirements in the oracle as compared to the control group on average. Overall, 80 % of the 111 participants in the treatment group were favorable towards the use of templates in identifying security requirements. Our qualitative findings indicate that participants may be able to differentiate between relevant and extraneous templates suggestions and be more inclined to fill in the templates with additional support. Security requirements templates capture the security knowledge of multiple experts and can support the security requirements elicitation process when automatically suggested, making the implied security requirements more evident. However, individual participants may still miss out on identifying a number of security requirements due to empirical constraints as well as potential limitations on knowledge and security expertise.",
      "Keywords": "Automation | Controlled experiment | Patterns | Replication | Requirements engineering | Security requirements | Templates",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Riaz, Maria;King, Jason;Slankas, John;Williams, Laurie;Massacci, Fabio;Quesada-López, Christian;Jenkins, Marcelo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84991112488",
      "Primary study DOI": "10.1007/s10664-016-9456-2",
      "Title": "Which log level should developers choose for a new logging statement?",
      "Abstract": "Logging statements are used to record valuable runtime information about applications. Each logging statement is assigned a log level such that users can disable some verbose log messages while allowing the printing of other important ones. However, prior research finds that developers often have difficulties when determining the appropriate level for their logging statements. In this paper, we propose an approach to help developers determine the appropriate log level when they add a new logging statement. We analyze the development history of four open source projects (Hadoop, Directory Server, Hama, and Qpid), and leverage ordinal regression models to automatically suggest the most appropriate level for each newly-added logging statement. First, we find that our ordinal regression model can accurately suggest the levels of logging statements with an AUC (area under the curve; the higher the better) of 0.75 to 0.81 and a Brier score (the lower the better) of 0.44 to 0.66, which is better than randomly guessing the appropriate log level (with an AUC of 0.50 and a Brier score of 0.80 to 0.83) or naively guessing the log level based on the proportional distribution of each log level (with an AUC of 0.50 and a Brier score of 0.65 to 0.76). Second, we find that the characteristics of the containing block of a newly-added logging statement, the existing logging statements in the containing source code file, and the content of the newly-added logging statement play important roles in determining the appropriate log level for that logging statement.",
      "Keywords": "Log level | Logging statement | Ordinal regression model",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Li, Heng;Shang, Weiyi;Hassan, Ahmed E.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85010789952",
      "Primary study DOI": "10.1007/s10664-016-9459-z",
      "Title": "Case study on which relations to use for clustering-based software architecture recovery",
      "Abstract": "Clustering-based software architecture recovery is an area that has received significant attention in the software engineering community over the years. Its key concept is the compilation and clustering of a system-wide graph that consists of source code entities as nodes, and source code relations as edges. However, the related research has mostly focused on investigating different clustering methods and techniques, and consequently there is limited work on addressing the question of what is a minimal set of relations that can be easily extracted from the system’s source code, and yet can be accurately used for extracting its architecture. In this paper, we report on results obtained from an architecture recovery case study we have conducted, by considering all possible combinations which can be generated from thirteen commonly used source code relations. We have examined the similarity of the extracted architectures obtained by using each different relation combination for different systems, against the corresponding architecture which is obtained by applying all thirteen relations and whch we consider as the ground truth architecture. For this purpose, we have also examined whether the use of all these thirteen relations is indeed adequate to yield a ground truth architecture, by applying this architecture extraction process on five large sofware systems for which their ground truth architecture has been independently established. The overall results of our study indicate that there is small set of relations for procedural systems, and another similar set for object oriented systems, that can be easily extracted from the source code and yet used to yield an architecture that is close to the ground truth architecture.",
      "Keywords": "Architecture recovery | Case study | Clustering | Reverse engineering | Source code relations",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Stavropoulou, Ioanna;Grigoriou, Marios;Kontogiannis, Kostas",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84981513514",
      "Primary study DOI": "10.1007/s10664-016-9445-5",
      "Title": "A repository of Unix history and evolution",
      "Abstract": "The history and evolution of the Unix operating system is made available as a revision management repository, covering the period from its inception in 1972 as a five thousand line kernel, to 2016 as a widely-used 27 million line system. The 1.1gb repository contains 496 thousand commits and 2,523 branch merges. The repository employs the commonly used Git version control system for its storage, and is hosted on the popular GitHub archive. It has been created by synthesizing with custom software 24 snapshots of systems developed at Bell Labs, the University of California at Berkeley, and the 386bsd team, two legacy repositories, and the modern repository of the open source Freebsd system. In total, 973 individual contributors are identified, the early ones through primary research. The data set can be used for empirical research in software engineering, information systems, and software archaeology.",
      "Keywords": "Configuration management | Git | Software archeology | Unix",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Spinellis, Diomidis",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85016546260",
      "Primary study DOI": "10.1007/s10664-017-9501-9",
      "Title": "Documenting and sharing software knowledge using screencasts",
      "Abstract": "Screencasts are used to capture a developer’s screen while they narrate how a piece of software works or how the software can be extended. They have recently become a popular alternative to traditional text-based documentation. This paper describes our investigation into how developers produce and share developer-focused screencasts. In this study, we identified and analyzed a set of development screencasts from YouTube to explore what kinds of software knowledge are shared in video walkthroughs of code and what techniques are used for sharing software knowledge. We also interviewed YouTube screencast producers to understand their motivations for creating screencasts as well as to discover the challenges they face while producing code-focused videos. Finally, we compared YouTube screencasts to videos hosted on the professional RailsCasts website to better understand the differences and practices of this more curated ecosystem with the YouTube platform. Our three-phase study showed that video is a useful medium for communicating program knowledge between developers and that developers build their online persona and reputation by sharing videos through social channels. These findings led to a number of best practices for future screencast creators.",
      "Keywords": "Screencasting | Social media | Software engineering",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "MacLeod, Laura;Bergen, Andreas;Storey, Margaret Anne",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85009932087",
      "Primary study DOI": "10.1007/s10664-016-9496-7",
      "Title": "Predicting the delay of issues with due dates in software projects",
      "Abstract": "Issue-tracking systems (e.g. JIRA) have increasingly been used in many software projects. An issue could represent a software bug, a new requirement or a user story, or even a project task. A deadline can be imposed on an issue by either explicitly assigning a due date to it, or implicitly assigning it to a release and having it inherit the release’s deadline. This paper presents a novel approach to providing automated support for project managers and other decision makers in predicting whether an issue is at risk of being delayed against its deadline. A set of features (hereafter called risk factors) characterizing delayed issues were extracted from eight open source projects: Apache, Duraspace, Java.net, JBoss, JIRA, Moodle, Mulesoft, and WSO2. Risk factors with good discriminative power were selected to build predictive models to predict if the resolution of an issue will be at risk of being delayed. Our predictive models are able to predict both the the extend of the delay and the likelihood of the delay occurrence. The evaluation results demonstrate the effectiveness of our predictive models, achieving on average 79 % precision, 61 % recall, 68 % F-measure, and 83 % Area Under the ROC Curve. Our predictive models also have low error rates: on average 0.66 for Macro-averaged Mean Cost-Error and 0.72 Macro-averaged Mean Absolute Error.",
      "Keywords": "Empirical software engineering | Mining software engineering repositories | Project management",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Choetkiertikul, Morakot;Dam, Hoa Khanh;Tran, Truyen;Ghose, Aditya",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85001114526",
      "Primary study DOI": "10.1007/s10664-016-9487-8",
      "Title": "Analysis of license inconsistency in large collections of open source projects",
      "Abstract": "Free and open source software (FOSS) plays an important role in source code reuse practice. They usually come with one or more software licenses written in the header part of source files, stating the requirements and conditions which should be followed when been reused. Removing or modifying the license statement by re-distributors will result in the inconsistency of license with its ancestor, and may potentially cause license infringement. In this paper, we describe and categorize different types of license inconsistencies and propose a method to detect them. Then we applied this method to Debian 7.5 and a collection of 10,514 Java projects on GitHub and present the license inconsistency cases found in these systems. With a manual analysis, we summarized various reasons behind these license inconsistency cases, some of which imply potential license infringement and require attention from the developers. This analysis also exposes the difficulty to discover license infringements, highlighting the usefulness of finding and maintaining source code provenance.",
      "Keywords": "Code clone | License inconsistency | Software license",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Wu, Yuhao;Manabe, Yuki;Kanda, Tetsuya;German, Daniel M.;Inoue, Katsuro",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84982295390",
      "Primary study DOI": "10.1007/s10664-016-9447-3",
      "Title": "Do bugs foreshadow vulnerabilities? An in-depth study of the chromium project",
      "Abstract": "As developers face an ever-increasing pressure to engineer secure software, researchers are building an understanding of security-sensitive bugs (i.e. vulnerabilities). Research into mining software repositories has greatly increased our understanding of software quality via empirical study of bugs. Conceptually, however, vulnerabilities differ from bugs: they represent an abuse of functionality as opposed to insufficient functionality commonly associated with traditional, non-security bugs. We performed an in-depth analysis of the Chromium project to empirically examine the relationship between bugs and vulnerabilities. We mined 374,686 bugs and 703 post-release vulnerabilities over five Chromium releases that span six years of development. We used logistic regression analysis, ranking analysis, bug type classifications, developer experience, and vulnerability severity metrics to examine the overarching question: are bugs and vulnerabilities in the same files? While we found statistically significant correlations between pre-release bugs and post-release vulnerabilities, we found the association to be weak. Number of features, source lines of code, and pre-release security bugs are, in general, more closely associated with post-release vulnerabilities than any of our non-security bug categories. In further analysis, we examined sub-types of bugs, such as stability-related bugs, and the associations did not improve. Even the files with the most severe vulnerabilities (by measure of CVSS or bounty payouts) did not show strong correlations with number of bugs. These results indicate that bugs and vulnerabilities are empirically dissimilar groups, motivating the need for security engineering research to target vulnerabilities specifically.",
      "Keywords": "Bugs | Code review | Security | Vulnerability",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Munaiah, Nuthan;Camilo, Felivel;Wigham, Wesley;Meneely, Andrew;Nagappan, Meiyappan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84992189467",
      "Primary study DOI": "10.1007/s10664-016-9457-1",
      "Title": "Achieving traceability in large scale continuous integration and delivery deployment, usage and validation of the eiffel framework",
      "Abstract": "The importance of traceability in software development has long been recognized, not only for reasons of legality and certification, but also to enable the development itself. At the same time, organizations are known to struggle to live up to traceability requirements, and there is an identified lack of studies on traceability practices in the industry, not least in the area of tooling and infrastructure. This paper presents, investigates and discusses Eiffel, an industry developed solution designed to provide real time traceability in continuous integration and delivery. The traceability needs of industry professionals are also investigated through interviews, providing context to that solution. It is then validated through further interviews, a comparison with previous traceability methods and a review of literature. It is found to address the identified traceability needs and found in some cases to reduce traceability data acquisition times from days to minutes, while at the same time alternatives offering comparable functionality are lacking. In this work, traceability is shown not only to be an important concern to engineers, but also regarded as a prerequisite to successful large scale continuous integration and delivery. At the same time, promising developments in technical infrastructure are documented and clear differences in traceability mindset between separate industry projects is revealed.",
      "Keywords": "Continuous delivery | Continuous integration | Traceability | Very-large-scale software systems",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Ståhl, Daniel;Hallén, Kristofer;Bosch, Jan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84994344880",
      "Primary study DOI": "10.1007/s10664-016-9466-0",
      "Title": "A large-scale study of architectural evolution in open-source software systems",
      "Abstract": "From its very inception, the study of software architecture has recognized architectural decay as a regularly occurring phenomenon in long-lived systems. Architectural decay is caused by repeated, sometimes careless changes to a system during its lifespan. Despite decay’s prevalence, there is a relative dearth of empirical data regarding the nature of architectural changes that may lead to decay, and of developers’ understanding of those changes. In this paper, we take a step toward addressing that scarcity by introducing an architecture recovery framework, ARCADE, for conducting large-scale replicable empirical studies of architectural change across different versions of a software system. ARCADE includes two novel architectural change metrics, which are the key to enabling large-scale empirical studies of architectural change. We utilize ARCADE to conduct an empirical study of changes found in software architectures spanning several hundred versions of 23 open-source systems. Our study reveals several new findings regarding the frequency of architectural changes in software systems, the common points of departure in a system’s architecture during the system’s maintenance and evolution, the difference between system-level and component-level architectural change, and the suitability of a system’s implementation-level structure as a proxy for its architecture.",
      "Keywords": "Architectural change | Architecture recovery | Open-source software | Software architecture | Software evolution",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Behnamghader, Pooyan;Le, Duc Minh;Garcia, Joshua;Link, Daniel;Shahbazian, Arman;Medvidovic, Nenad",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84982085657",
      "Primary study DOI": "10.1007/s10664-016-9443-7",
      "Title": "Exception handling bug hazards in Android: Results from a mining study and an exploratory survey",
      "Abstract": "Adequate handling of exceptions has proven difficult for many software engineers. Mobile app developers in particular, have to cope with compatibility, middleware, memory constraints, and battery restrictions. The goal of this paper is to obtain a thorough understanding of common exception handling bug hazards that app developers face. To that end, we first provide a detailed empirical study of over 6,000 Java exception stack traces we extracted from over 600 open source Android projects. Key insights from this study include common causes for system crashes, and common chains of wrappings between checked and unchecked exceptions. Furthermore, we provide a survey with 71 developers involved in at least one of the projects analyzed. The results corroborate the stack trace findings, and indicate that developers are unaware of frequently occurring undocumented exception handling behavior. Overall, the findings of our study call for tool support to help developers understand their own and third party exception handling and wrapping logic.",
      "Keywords": "Android development | Exception handling | Exploratory survey | Repository mining",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Coelho, Roberta;Almeida, Lucas;Gousios, Georgios;van Deursen, Arie;Treude, Christoph",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84991829839",
      "Primary study DOI": "10.1007/s10664-016-9460-6",
      "Title": "Estimating the number of remaining links in traceability recovery",
      "Abstract": "Although very important in software engineering, establishing traceability links between software artifacts is extremely tedious, error-prone, and it requires significant effort. Even when approaches for automated traceability recovery exist, these provide the requirements analyst with a, usually very long, ranked list of candidate links that needs to be manually inspected. In this paper we introduce an approach called Estimation of the Number of Remaining Links (ENRL) which aims at estimating, via Machine Learning (ML) classifiers, the number of remaining positive links in a ranked list of candidate traceability links produced by a Natural Language Processing techniques-based recovery approach. We have evaluated the accuracy of the ENRL approach by considering several ML classifiers and NLP techniques on three datasets from industry and academia, and concerning traceability links among different kinds of software artifacts including requirements, use cases, design documents, source code, and test cases. Results from our study indicate that: (i) specific estimation models are able to provide accurate estimates of the number of remaining positive links; (ii) the estimation accuracy depends on the choice of the NLP technique, and (iii) univariate estimation models outperform multivariate ones.",
      "Keywords": "Information retrieval | Metrics and measurement | Traceability link recovery",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Falessi, Davide;Di Penta, Massimiliano;Canfora, Gerardo;Cantone, Giovanni",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85000925641",
      "Primary study DOI": "10.1007/s10664-016-9486-9",
      "Title": "Eye movements in software traceability link recovery",
      "Abstract": "Information Retrieval (IR) approaches, such as Latent Semantic Indexing (LSI) and Vector Space Model (VSM), are commonly applied to recover software traceability links. Recently, an approach based on developers’ eye gazes was proposed to retrieve traceability links. This paper presents a comparative study on IR and eye-gaze based approaches. In addition, it reports on the possibility of using eye gaze links as an alternative benchmark in comparison to commits. The study conducted asked developers to perform bug-localization tasks on the open source subject system JabRef. The iTrace environment, which is an eye tracking enabled Eclipse plugin, was used to collect eye gaze data. During the data collection phase, an eye tracker was used to gather the source code entities (SCE’s), developers looked at while solving these tasks. We present an algorithm that uses the collected gaze dataset to produce candidate traceability links related to the tasks. In the evaluation phase, we compared the results of our algorithm with the results of an IR technique, in two different contexts. In the first context, precision and recall metric values are reported for both IR and eye gaze approaches based on commits. In the second context, another set of developers were asked to rate the candidate links from each of the two techniques in terms of how useful they were in fixing the bugs. The eye gaze approach outperforms standard LSI and VSM approaches and reports a 55 % precision and 67 % recall on average for all tasks when compared to how the developers actually fixed the bug. In the second context, the usefulness results show that links generated by our algorithm were considered to be significantly more useful (to fix the bug) than those of the IR technique in a majority of tasks. We discuss the implications of this radically different method of deriving traceability links. Techniques for feature location/bug localization are commonly evaluated on benchmarks formed from commits as is done in the evaluation phase of this study. Although, commits are a reasonable source, they only capture entities that were eventually changed to fix a bug or resolve a feature. We investigate another type of benchmark based on eye tracking data, namely links generated from the bug-localization tasks given to the developers in the data collection phase. The source code entities relevant to subjected bugs recommended from IR methods are evaluated on both commits and links generated from eye gaze. The results of the benchmarking phase show that the use of eye tracking could form an effective (complementary) benchmark and add another interesting perspective in the evaluation of bug-localization techniques.",
      "Keywords": "Continuous traceability | Eye-gaze benchmark | Eye-tracking | Software traceability link recovery",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Sharif, Bonita;Meinken, John;Shaffer, Timothy;Kagdi, Huzefa",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84994779547",
      "Primary study DOI": "10.1007/s10664-016-9476-y",
      "Title": "Automated training-set creation for software architecture traceability problem",
      "Abstract": "Automated trace retrieval methods based on machine-learning algorithms can significantly reduce the cost and effort needed to create and maintain traceability links between requirements, architecture and source code. However, there is always an upfront cost to train such algorithms to detect relevant architectural information for each quality attribute in the code. In practice, training supervised or semi-supervised algorithms requires the expert to collect several files of architectural tactics that implement a quality requirement and train a learning method. Establishing such a training set can take weeks to months to complete. Furthermore, the effectiveness of this approach is largely dependent upon the knowledge of the expert. In this paper, we present three baseline approaches for the creation of training data. These approaches are (i) Manual Expert-Based, (ii) Automated Web-Mining, which generates training sets by automatically mining tactic’s APIs from technical programming websites, and lastly (iii) Automated Big-Data Analysis, which mines ultra-large scale code repositories to generate training sets. We compare the trace-link creation accuracy achieved using each of these three baseline approaches and discuss the costs and benefits associated with them. Additionally, in a separate study, we investigate the impact of training set size on the accuracy of recovering trace links. The results indicate that automated techniques can create a reliable training set for the problem of tracing architectural tactics.",
      "Keywords": "Architecturally significant requirements | Architecture traceability | Automation | Dataset generation",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Zogaan, Waleed;Mujhid, Ibrahim;Joanna, Joanna C.;Gonzalez, Danielle;Mirakhorli, Mehdi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84996538088",
      "Primary study DOI": "10.1007/s10664-016-9477-x",
      "Title": "How programmers read regular code: a controlled experiment using eye tracking",
      "Abstract": "Regular code, which includes repetitions of the same basic pattern, has been shown to have an effect on code comprehension: a regular function can be just as easy to comprehend as a non-regular one with the same functionality, despite being significantly longer and including more control constructs. It has been speculated that this effect is due to leveraging the understanding of the first instances to ease the understanding of repeated instances of the pattern. To verify and quantify this effect, we use eye tracking to measure the time and effort spent reading and understanding regular code. The experimental subjects were 18 students and 2 faculty members. The results are that time and effort invested in the initial code segments are indeed much larger than those spent on the later ones, and the decay in effort can be modeled by an exponential model. This shows that syntactic code complexity metrics (such as LOC and MCC) need to be made context-sensitive, e.g. by giving reduced weight to repeated segments according to their place in the sequence. However, it is not the case that repeated code segments are actually read more and more quickly. Rather, initial code segments receive more focus and are looked at more times, while later ones may be only skimmed. Further, a few recurring reading patterns have been identified, which together indicate that in general code reading is far from being purely linear, and exhibits significant variability across experimental subjects.",
      "Keywords": "Code regularity | Controlled experiment | Eye-tracking | Software complexity",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Jbara, Ahmad;Feitelson, Dror G.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85007482467",
      "Primary study DOI": "10.1007/s10664-016-9489-6",
      "Title": "The last line effect explained",
      "Abstract": "Micro-clones are tiny duplicated pieces of code; they typically comprise only few statements or lines. In this paper, we study the “Last Line Effect,” the phenomenon that the last line or statement in a micro-clone is much more likely to contain an error than the previous lines or statements. We do this by analyzing 219 open source projects and reporting on 263 faulty micro-clones and interviewing six authors of real-world faulty micro-clones. In an interdisciplinary collaboration, we examine the underlying psychological mechanisms for the presence of these relatively trivial errors. Based on the interviews and further technical analyses, we suggest that so-called “action slips” play a pivotal role for the existence of the last line effect: Developers’ attention shifts away at the end of a micro-clone creation task due to noise and the routine nature of the task. Moreover, all micro-clones whose origin we could determine were introduced in unusually large commits. Practitioners benefit from this knowledge twofold: 1) They can spot situations in which they are likely to introduce a faulty micro-clone and 2) they can use PVS-Studio, our automated micro-clone detector, to help find erroneous micro-clones.",
      "Keywords": "Clone detection | Code clones | Interdisciplinary work | Last line effect | Micro-clones | Psychology",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Beller, Moritz;Zaidman, Andy;Karpov, Andrey;Zwaan, Rolf A.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84990963244",
      "Primary study DOI": "10.1007/s10664-016-9461-5",
      "Title": "The Debsources Dataset: two decades of free and open source software",
      "Abstract": "We present the Debsources Dataset: source code and related metadata spanning two decades of Free and Open Source Software (FOSS) history, seen through the lens of the Debian distribution. The dataset spans more than 3 billion lines of source code as well as metadata about them such as: size metrics (lines of code, disk usage), developer-defined symbols (ctags), file-level checksums (SHA1, SHA256, TLSH), file media types (MIME), release information (which version of which package containing which source code files has been released when), and license information (GPL, BSD, etc). The Debsources Dataset comes as a set of tarballs containing deduplicated unique source code files organized by their SHA1 checksums (the source code), plus a portable PostgreSQL database dump (the metadata). A case study is run to show how the Debsources Dataset can be used to easily and efficiently instrument very long-term analyses of the evolution of Debian from various angles (size, granularity, licensing, etc.), getting a grasp of major FOSS trends of the past two decades. The Debsources Dataset is Open Data, released under the terms of the CC BY-SA 4.0 license, and available for download from Zenodo with DOI reference 10.5281/zenodo.61089.",
      "Keywords": "Dataset | Debian | Free software | Open source | Software evolution | Source code",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Caneill, Matthieu;Germán, Daniel M.;Zacchiroli, Stefano",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84982947678",
      "Primary study DOI": "10.1007/s10664-016-9444-6",
      "Title": "fine-GRAPE: fine-grained APi usage extractor   an approach and dataset to investigate API usage",
      "Abstract": "An Application Programming Interface (API) provides a set of functionalities to a developer with the aim of enabling reuse. APIs have been investigated from different angles such as popularity usage and evolution to get a better understanding of their various characteristics. For such studies, software repositories are mined for API usage examples. However, many of the mining algorithms used for such purposes do not take type information into account. Thus making the results unreliable. In this paper, we aim to rectify this by introducing fine-GRAPE, an approach that produces fine-grained API usage information by taking advantage of type information while mining API method invocations and annotation. By means of fine-GRAPE, we investigate API usages from Java projects hosted on GitHub. We select five of the most popular APIs across GitHub Java projects and collect historical API usage information by mining both the release history of these APIs and the code history of every project that uses them. We perform two case studies on the resulting dataset. The first measures the lag time of each client. The second investigates the percentage of used API features. In the first case we find that for APIs that release more frequently clients are far less likely to upgrade to a more recent version of the API as opposed to clients of APIs that release infrequently. The second case study shows us that for most APIs there is a small number of features that is actually used and most of these features relate to those that have been introduced early in the APIs lifecycle.",
      "Keywords": "API popularity | API usage | Application programming interface | Dataset",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Sawant, Anand Ashok;Bacchelli, Alberto",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85020033576",
      "Primary study DOI": "10.1007/s10664-016-9438-4",
      "Title": "License usage and changes: a large-scale study on gitHub",
      "Abstract": "Open source software licenses determine, from a legal point of view, under which conditions software can be integrated and redistributed. The reason why developers of a project adopt (or change) a license may depend on various factors, e.g., the need for ensuring compatibility with certain third-party components, the perspective towards redistribution or commercialization of the software, or the need for protecting against somebody else’s commercial usage of the software. This paper reports a large empirical study aimed at quantitatively and qualitatively investigating when and why developers adopt or change software licenses. Specifically, we first identify license changes in 1,731,828 commits, representing the entire history of 16,221 Java projects hosted on GitHub. Then, to understand the rationale of license changes, we perform a qualitative analysis on 1,160 projects written in seven different programming languages, namely C, C++, C#, Java, Javascript, Python, and Ruby—following an open coding approach inspired by grounded theory—on commit messages and issue tracker discussions concerning licensing topics, and whenever possible, try to build traceability links between discussions and changes. On one hand, our results highlight how, in different contexts, license adoption or changes can be triggered by various reasons. On the other hand, the results also highlight a lack of traceability of when and why licensing changes are made. This can be a major concern, because a change in the license of a system can negatively impact those that reuse it. In conclusion, results of the study trigger the need for better tool support in guiding developers in choosing/changing licenses and in keeping track of the rationale of license changes.",
      "Keywords": "Empirical studies | Mining software repositories | Software licenses",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Vendome, Christopher;Bavota, Gabriele;Penta, Massimiliano Di;Linares-Vásquez, Mario;German, Daniel;Poshyvanyk, Denys",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84989910841",
      "Primary study DOI": "10.1007/s10664-016-9453-5",
      "Title": "Productivity paradoxes revisited: Assessing the relationship between quality maturity levels and labor productivity in brazilian software companies",
      "Abstract": "The adoption of quality assurance methods based on software process improvement models has been regarded as an important source of variability in software productivity. Some companies perceive that their implementation has prohibitive costs, whereas some authors identify in their use a way to comply with software development patterns and standards, produce economic value and lead to corporate performance improvement. In this paper, we investigate the relationship between quality maturity levels and labor productivity, using a data set containing 687 Brazilian software firms. We study here the relationship between labor productivity, as measured through the annual gross revenue per worker ratio, and quality levels, which were appraised from 2006 to 2012 according to two distinct software process improvement models: MPS.BR and CMMI. We perform independent statistical tests using appraisals carried out according to each of these models, consequently obtaining a data set with as many observations as possible, in order to seek strong support for our research. We first show that MPS.BR and CMMI appraised quality maturity levels are correlated, but we find no statistical evidence that they are related to higher labor productivity or productivity growth. On the contrary, we present evidence suggesting that average labor productivity is higher in software companies without appraised quality levels. Moreover, our analyses suggest that companies with appraised quality maturity levels are more or less productive depending on factors such as their business nature, main origin of capital and maintained quality level.",
      "Keywords": "Productivity | Software engineering economics | Software process models | Software quality assurance",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Carlos, Carlos Henrique",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84984788366",
      "Primary study DOI": "10.1007/s10664-016-9455-3",
      "Title": "Erratum to: Studying high impact fix-inducing changes (Empirical Software Engineering, (2016), 21, 2, (605-641), 10.1007/s10664-015-9370-z)",
      "Abstract": "The original version of this article unfortunately contained a mistake. The name of the third author was incorrectly displayed as BYasukata Kamei^. The correct information is as shown above.",
      "Keywords": "",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-04-01",
      "Publication type": "Erratum",
      "Authors": "Tosun, Ayse;Shihab, Emad;Kamei, Yasutaka",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84961217781",
      "Primary study DOI": "10.1007/s10664-015-9424-2",
      "Title": "A detailed investigation of the effectiveness of whole test suite generation",
      "Abstract": "A common application of search-based software testing is to generate test cases for all goals defined by a coverage criterion (e.g., lines, branches, mutants). Rather than generating one test case at a time for each of these goals individually, whole test suite generation optimizes entire test suites towards satisfying all goals at the same time. There is evidence that the overall coverage achieved with this approach is superior to that of targeting individual coverage goals. Nevertheless, there remains some uncertainty on (a) whether the results generalize beyond branch coverage, (b) whether the whole test suite approach might be inferior to a more focused search for some particular coverage goals, and (c) whether generating whole test suites could be optimized by only targeting coverage goals not already covered. In this paper, we perform an in-depth analysis to study these questions. An empirical study on 100 Java classes using three different coverage criteria reveals that indeed there are some testing goals that are only covered by the traditional approach, although their number is only very small in comparison with those which are exclusively covered by the whole test suite approach. We find that keeping an archive of already covered goals along with the tests covering them and focusing the search on uncovered goals overcomes this small drawback on larger classes, leading to an improved overall effectiveness of whole test suite generation.",
      "Keywords": "Automated test generation | EvoSuite | Search-based testing | Unit testing",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Rojas, José Miguel;Vivanti, Mattia;Arcuri, Andrea;Fraser, Gordon",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84991786769",
      "Primary study DOI": "10.1007/s10664-016-9452-6",
      "Title": "Review participation in modern code review: An empirical study of the android, Qt, and OpenStack projects",
      "Abstract": "Software code review is a well-established software quality practice. Recently, Modern Code Review (MCR) has been widely adopted in both open source and proprietary projects. Our prior work shows that review participation plays an important role in MCR practices, since the amount of review participation shares a relationship with software quality. However, little is known about which factors influence review participation in the MCR process. Hence, in this study, we set out to investigate the characteristics of patches that: (1) do not attract reviewers, (2) are not discussed, and (3) receive slow initial feedback. Through a case study of 196,712 reviews spread across the Android, Qt, and OpenStack open source projects, we find that the amount of review participation in the past is a significant indicator of patches that will suffer from poor review participation. Moreover, we find that the description length of a patch shares a relationship with the likelihood of receiving poor reviewer participation or discussion, while the purpose of introducing new features can increase the likelihood of receiving slow initial feedback. Our findings suggest that the patches with these characteristics should be given more attention in order to increase review participation, which will likely lead to a more responsive review process.",
      "Keywords": "Code review | Developer involvement | Review participation",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Thongtanunam, Patanamon;McIntosh, Shane;Hassan, Ahmed E.;Iida, Hajimu",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84988735029",
      "Primary study DOI": "10.1007/s10664-016-9442-8",
      "Title": "Search-based detection of model level changes",
      "Abstract": "Software models, defined as code abstractions, are iteratively refined, restructured, and evolved due to many reasons such as reflecting changes in requirements or modifying a design to enhance existing features. For understanding the evolution of a model a-posteriori, change detection approaches have been proposed for models. The majority of existing approaches are successful to detect atomic changes. However, composite changes, such as refactorings, are difficult to detect due to several possible combinations of atomic changes or eventually hidden changes in intermediate model versions that may be no longer available. Moreover, a multitude of refactoring sequences may be used to describe the same model evolution. In this paper, we propose a multi-objective approach to detect model changes as a sequence of refactorings. Our approach takes as input an exhaustive list of possible types of model refactoring operations, the initial model, and the revised model, and generates as output a list of refactoring applications representing a good compromise between the following two objectives (i) maximize the similarity between the expected revised model and the generated model after applying the refactoring sequence on the initial model, and (ii) minimize the number of atomic changes used to describe the evolution. In fact, minimizing the number of atomic changes can important since it is maybe easier for a designer to understand and analyze a sequence of refactorings (composite model changes) rather than an equivalent large list of atomic changes (Weissgerber and Diehl 2006). Due to the huge number of possible refactoring sequences, a metaheuristic search method is used to explore the space of possible solutions. To this end, we use the non-dominated sorting genetic algorithm (NSGA-II) to find the best trade-off between our two objectives. The paper reports on the results of an empirical study of our multi-objective model changes detection technique as applied on various versions of real-world models taken from open source projects and one industrial project. We compared our approach to the simple deterministic greedy algorithm, multi-objective particle swarm optimization (MOPSO), an existing mono-objective changes detection approach, and two model changes detection tools not based on computational search. The statistical test results provide evidence to support the claim that our proposal enables the generation of changes detection solutions with correctness higher than 85 %, in average, using a variety of real-world scenarios.",
      "Keywords": "Model changes detection | Multi-objective optimization | Search-based software engineering | Software maintenance",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Kessentini, Marouane;Mansoor, Usman;Wimmer, Manuel;Ouni, Ali;Deb, Kalyanmoy",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84983399519",
      "Primary study DOI": "10.1007/s10664-016-9446-4",
      "Title": "Investigating the use of moving windows to improve software effort prediction: a replicated study",
      "Abstract": "To date most research in software effort estimation has not taken chronology into account when selecting projects for training and validation sets. A chronological split represents the use of a project’s starting and completion dates, such that any model that estimates effort for a new project p only uses as its training set projects that have been completed prior to p’s starting date. A study in 2009 (“S3”) investigated the use of chronological split taking into account a project’s age. The research question investigated was whether the use of a training set containing only the most recent past projects (a “moving window” of recent projects) would lead to more accurate estimates when compared to using the entire history of past projects completed prior to the starting date of a new project. S3 found that moving windows could improve the accuracy of estimates. The study described herein replicates S3 using three different and independent data sets. Estimation models were built using regression, and accuracy was measured using absolute residuals. The results contradict S3, as they do not show any gain in estimation accuracy when using windows for effort estimation. This is a surprising result: the intuition that recent data should be more helpful than old data for effort estimation is not supported. Several factors, which are discussed in this paper, might have contributed to such contradicting results. Some of our future work entails replicating this work using other datasets, to understand better when using windows is a suitable choice for software companies.",
      "Keywords": "Chronological splitting | Effort estimation | Moving window | Regression-based estimation models",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Lokan, Chris;Mendes, Emilia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84976497686",
      "Primary study DOI": "10.1007/s10664-016-9441-9",
      "Title": "Do Programmers do Change Impact Analysis in Debugging?",
      "Abstract": "“Change Impact Analysis” is the process of determining the consequences of a modification to software. In theory, change impact analysis should be done during software maintenance, to make sure changes do not introduce new bugs. Many approaches and techniques are proposed to help programmers do change impact analysis automatically. However, it is still an open question whether and how programmers do change impact analysis. In this paper, we conducted two studies, one in-depth study and one breadth study. For the in-depth study, we recorded videos of nine professional programmers repairing two bugs for two hours. For the breadth study, we surveyed 35 professional programmers using an online system. We found that the programmers in our studies did static change impact analysis before they made changes by using IDE navigational functionalities, and they did dynamic change impact analysis after they made changes by running the programs. We also found that they did not use any change impact analysis tools.",
      "Keywords": "Change impact analysis | Empirical software engineering | Program debugging | Programmer navigation | Software maintenance",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Jiang, Siyuan;McMillan, Collin;Santelices, Raul",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84975113024",
      "Primary study DOI": "10.1007/s10664-016-9437-5",
      "Title": "Robust Statistical Methods for Empirical Software Engineering",
      "Abstract": "There have been many changes in statistical theory in the past 30 years, including increased evidence that non-robust methods may fail to detect important results. The statistical advice available to software engineering researchers needs to be updated to address these issues. This paper aims both to explain the new results in the area of robust analysis methods and to provide a large-scale worked example of the new methods. We summarise the results of analyses of the Type 1 error efficiency and power of standard parametric and non-parametric statistical tests when applied to non-normal data sets. We identify parametric and non-parametric methods that are robust to non-normality. We present an analysis of a large-scale software engineering experiment to illustrate their use. We illustrate the use of kernel density plots, and parametric and non-parametric methods using four different software engineering data sets. We explain why the methods are necessary and the rationale for selecting a specific analysis. We suggest using kernel density plots rather than box plots to visualise data distributions. For parametric analysis, we recommend trimmed means, which can support reliable tests of the differences between the central location of two or more samples. When the distribution of the data differs among groups, or we have ordinal scale data, we recommend non-parametric methods such as Cliff’s δ or a robust rank-based ANOVA-like method.",
      "Keywords": "Empirical software engineering | Robust methods | Robust statistical methods | Statistical methods",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Kitchenham, Barbara;Madeyski, Lech;Budgen, David;Keung, Jacky;Brereton, Pearl;Charters, Stuart;Gibbs, Shirley;Pohthong, Amnart",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84960114008",
      "Primary study DOI": "10.1007/s10664-016-9426-8",
      "Title": "A robust multi-objective approach to balance severity and importance of refactoring opportunities",
      "Abstract": "Refactoring large systems involves several sources of uncertainty related to the severity levels of code smells to be corrected and the importance of the classes in which the smells are located. Both severity and importance of identified refactoring opportunities (e.g. code smells) are difficult to estimate. In fact, due to the dynamic nature of software development, these values cannot be accurately determined in practice, leading to refactoring sequences that lack robustness. In addition, some code fragments can contain severe quality issues but they are not playing an important role in the system. To address this problem, we introduced a multi-objective robust model, based on NSGA-II, for the software refactoring problem that tries to find the best trade-off between three objectives to maximize: quality improvements, severity and importance of refactoring opportunities to be fixed. We evaluated our approach using 8 open source systems and one industrial project, and demonstrated that it is significantly better than state-of-the-art refactoring approaches in terms of robustness in all the experiments based on a variety of real-world scenarios. Our suggested refactoring solutions were found to be comparable in terms of quality to those suggested by existing approaches, better prioritization of refactoring opportunities and to carry an acceptable robustness price.",
      "Keywords": "Refactoring under uncertainty | Robust multi-objective optimization | Search-based software engineering | Software quality",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Mkaouer, Mohamed Wiem;Kessentini, Marouane;Cinnéide, Mel;Hayashi, Shinpei;Deb, Kalyanmoy",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84954357117",
      "Primary study DOI": "10.1007/s10664-015-9422-4",
      "Title": "Generating valid grammar-based test inputs by means of genetic programming and annotated grammars",
      "Abstract": "Automated generation of system level tests for grammar based systems requires the generation of complex and highly structured inputs, which must typically satisfy some formal grammar. In our previous work, we showed that genetic programming combined with probabilities learned from corpora gives significantly better results over the baseline (random) strategy. In this work, we extend our previous work by introducing grammar annotations as an alternative to learned probabilities, to be used when finding and preparing the corpus required for learning is not affordable. Experimental results carried out on six grammar based systems of varying levels of complexity show that grammar annotations produce a higher number of valid sentences and achieve similar levels of coverage and fault detection as learned probabilities.",
      "Keywords": "Genetic programming | Grammar annotations | Grammar based testing",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Kifetew, Fitsum Meshesha;Tiella, Roberto;Tonella, Paolo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84963705438",
      "Primary study DOI": "10.1007/s10664-016-9427-7",
      "Title": "An experimental search-based approach to cohesion metric evaluation",
      "Abstract": "In spite of several decades of software metrics research and practice, there is little understanding of how software metrics relate to one another, nor is there any established methodology for comparing them. We propose a novel experimental technique, based on search-based refactoring, to ‘animate’ metrics and observe their behaviour in a practical setting. Our aim is to promote metrics to the level of active, opinionated objects that can be compared experimentally to uncover where they conflict, and to understand better the underlying cause of the conflict. Our experimental approaches include semi-random refactoring, refactoring for increased metric agreement/disagreement, refactoring to increase/decrease the gap between a pair of metrics, and targeted hypothesis testing. We apply our approach to five popular cohesion metrics using ten real-world Java systems, involving 330,000 lines of code and the application of over 78,000 refactorings. Our results demonstrate that cohesion metrics disagree with each other in a remarkable 55 % of cases, that Low-level Similarity-based Class Cohesion (LSCC) is the best representative of the set of metrics we investigate while Sensitive Class Cohesion (SCOM) is the least representative, and we discover several hitherto unknown differences between the examined metrics. We also use our approach to investigate the impact of including inheritance in a cohesion metric definition and find that doing so dramatically changes the metric.",
      "Keywords": "Empirical studies | Refactoring | Software metrics",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Ó Cinnéide, Mel;Hemati Moghadam, Iman;Harman, Mark;Counsell, Steve;Tratt, Laurence",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84949562691",
      "Primary study DOI": "10.1007/s10664-015-9413-5",
      "Title": "FOREPOST: finding performance problems automatically with feedback-directed learning software testing",
      "Abstract": "A goal of performance testing is to find situations when applications unexpectedly exhibit worsened characteristics for certain combinations of input values. A fundamental question of performance testing is how to select a manageable subset of the input data faster in order to automatically find performance bottlenecks in applications. We propose FOREPOST, a novel solution, for automatically finding performance bottlenecks in applications using black-box software testing. Our solution is an adaptive, feedback-directed learning testing system that learns rules from execution traces of applications. Theses rules are then used to automatically select test input data for performance testing. We hypothesize that FOREPOST can find more performance bottlenecks as compared to random testing. We have implemented our solution and applied it to a medium-size industrial application at a major insurance company and to two open-source applications. Performance bottlenecks were found automatically and confirmed by experienced testers and developers. We also thoroughly studied the factors (or independent variables) that impact the results of FOREPOST.",
      "Keywords": "Feedback-directed learning system | Performance testing",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Luo, Qi;Nair, Aswathy;Grechanik, Mark;Poshyvanyk, Denys",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84976276892",
      "Primary study DOI": "10.1007/s10664-016-9435-7",
      "Title": "An empirical study of emergency updates for top android mobile apps",
      "Abstract": "The mobile app market continues to grow at a tremendous rate. The market provides a convenient and efficient distribution mechanism for updating apps. App developers continuously leverage such mechanism to update their apps at a rapid pace. The mechanism is ideal for publishing emergency updates (i.e., updates that are published soon after the previous update). In this paper, we study such emergency updates in the Google Play Store. Examining more than 44,000 updates of over 10,000 mobile apps in the Google Play Store, we identify 1,000 emergency updates. By studying the characteristics of such emergency updates, we find that the emergency updates often have a long lifetime (i.e., they are rarely followed by another emergency update). Updates preceding emergency updates often receive a higher ratio of negative reviews than the emergency updates. However, the release notes of emergency updates rarely indicate the rationale for such updates. Hence, we manually investigate the binary changes of several of these emergency updates. We find eight patterns of emergency updates. We categorize these eight patterns along two categories “Updates due to deployment issues” and “Updates due to source code changes”. We find that these identified patterns of emergency updates are often associated with simple mistakes, such as using a wrong resource folder (e.g., images or sounds) for an app. We manually examine each pattern and document its causes and impact on the user experience. App developers should carefully avoid these patterns in order to improve the user experience.",
      "Keywords": "Android mobile apps | Emergency updates | Empirical study | Patterns | Permissions | SDK version | Software engineering",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Hassan, Safwat;Shang, Weiyi;Hassan, Ahmed E.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84966320690",
      "Primary study DOI": "10.1007/s10664-016-9429-5",
      "Title": "Characterizing logging practices in Java-based open source software projects   a replication study in Apache Software Foundation",
      "Abstract": "Log messages, which are generated by the debug statements that developers insert into the code at runtime, contain rich information about the runtime behavior of software systems. Log messages are used widely for system monitoring, problem diagnoses and legal compliances. Yuan et al. performed the first empirical study on the logging practices in open source software systems. They studied the development history of four C/C++ server-side projects and derived ten interesting findings. In this paper, we have performed a replication study in order to assess whether their findings would be applicable to Java projects in Apache Software Foundations. We examined 21 different Java-based open source projects from three different categories: server-side, client-side and supporting-component. Similar to the original study, our results show that all projects contain logging code, which is actively maintained. However, contrary to the original study, bug reports containing log messages take a longer time to resolve than bug reports without log messages. A significantly higher portion of log updates are for enhancing the quality of logs (e.g., formatting & style changes and spelling/grammar fixes) rather than co-changes with feature implementations (e.g., updating variable names).",
      "Keywords": "Empirical study | Log messages | Logging code | Mining software engineering data | MSR | Replication",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Chen, Boyuan;(Jack) Jiang, Zhen Ming",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84955252262",
      "Primary study DOI": "10.1007/s10664-015-9421-5",
      "Title": "Learning to rank code examples for code search engines",
      "Abstract": "Source code examples are used by developers to implement unfamiliar tasks by learning from existing solutions. To better support developers in finding existing solutions, code search engines are designed to locate and rank code examples relevant to user’s queries. Essentially, a code search engine provides a ranking schema, which combines a set of ranking features to calculate the relevance between a query and candidate code examples. Consequently, the ranking schema places relevant code examples at the top of the result list. However, it is difficult to determine the configurations of the ranking schemas subjectively. In this paper, we propose a code example search approach that applies a machine learning technique to automatically train a ranking schema. We use the trained ranking schema to rank candidate code examples for new queries at run-time. We evaluate the ranking performance of our approach using a corpus of over 360,000 code snippets crawled from 586 open-source Android projects. The performance evaluation study shows that the learning-to-rank approach can effectively rank code examples, and outperform the existing ranking schemas by about 35.65 % and 48.42 % in terms of normalized discounted cumulative gain (NDCG) and expected reciprocal rank (ERR) measures respectively.",
      "Keywords": "Code example | Code recommendation | Code search | Learning to rank",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Niu, Haoran;Keivanloo, Iman;Zou, Ying",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84966430205",
      "Primary study DOI": "10.1007/s10664-016-9432-x",
      "Title": "An empirical study of supplementary patches in open source projects",
      "Abstract": "Developers occasionally make more than one patch to fix a bug. The related patches sometimes are intentionally separated, but unintended omission errors require supplementary patches. Several change recommendation systems have been suggested based on clone analysis, structural dependency, and historical change coupling in order to reduce or prevent incomplete patches. However, very few studies have examined the reason that incomplete patches occur and how real-world omission errors could be reduced. This paper systematically studies a group of bugs that were fixed more than once in open source projects in order to understand the characteristics of incomplete patches. Our study on Eclipse JDT core, Eclipse SWT, Mozilla, and Equinox p2 showed that a significant portion of the resolved bugs require more than one attempt to fix. Compared to single-fix bugs, the multi-fix bugs did not have a lower quality of bug reports, but more attribute changes (i.e., cc’ed developers or title) were made to the multi-fix bugs than to the single-fix bugs. Multi-fix bugs are more likely to have high severity levels than single-fix bugs. Hence, more developers have participated in discussions about multi-fix bugs compared to single-fix bugs. Multi-fix bugs take more time to resolve than single-fix bugs do. Incomplete patches are longer and more scattered, and they are related to more files than regular patches are. Our manual inspection showed that the causes of incomplete patches were diverse, including missed porting updates, incorrect handling of conditional statements, and incomplete refactoring. Our investigation showed that only 7 % to 17 % of supplementary patches had content similar to their initial patches, which implies that supplementary patch locations cannot be predicted by code clone analysis alone. Furthermore, 16 % to 46 % of supplementary patches were beyond the scope of the immediate structural dependency of their initial patch locations. Historical co-change patterns also showed low precision in predicting supplementary patch locations. Code clones, structural dependencies, and historical co-change analyses predicted different supplementary patch locations, and there was little overlap between them. Combining these analyses did not cover all supplementary patch locations. The present study investigates the characteristics of incomplete patches and multi-fix bugs, which have not been systematically examined in previous research. We reveal that predicting supplementary patch is a difficult problem that existing change recommendation approaches could not cover. New type of approaches should be developed and validated on a supplementary patch data set, which developers failed to make the complete patches at once in practice.",
      "Keywords": "Bug fixes | Empirical study | Patches | Software evolution",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Park, Jihun;Kim, Miryung;Bae, Doo Hwan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84947744892",
      "Primary study DOI": "10.1007/s10664-015-9415-3",
      "Title": "Defect propagation at the project-level: results and a post-hoc analysis on inspection efficiency",
      "Abstract": "Inspections are increasingly utilized to enhance software quality. While the effectiveness of inspections in uncovering defects is widely accepted, there is a lack of research that takes a more holistic approach by considering defect counts from initial phases of the development process (requirements, design, and coding) and examining defect propagation where defect counts are aggregated to the project-level (i.e., application-level). Using inspection data collected from a large software development firm, this paper investigates the extent of defect propagation at the project-level during early lifecycle phases. I argue that defect propagation can be observed from the relationship between defects in the prior phase and the defects in the subsequent phase. Both Ordinary Least Squares and 3-Stage Least Squares analyses support the hypotheses on defect propagation. Moreover, results show that the inspection efficiency (defects per unit inspection time) decreases as the software product progresses from requirements to design to coding. A post-hoc analysis revealed further insights into inspection efficiency. In each phase, as the inspection time increased, efficiency reached an optimal point and then dropped off. In addition, a project’s inspection efficiency generally tends to remain stable from one phase to another. These insights offer managers means to assess inspections, their efficiency, and make adjustments to the time allotted to inspect project’s artifacts in both the current and the subsequent phase. Implications for managers and future research directions are discussed.",
      "Keywords": "Coding | Defect propagation | Design | Inspections | Requirements | Software quality",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Vitharana, Padmal",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84957652388",
      "Primary study DOI": "10.1007/s10664-015-9419-z",
      "Title": "Raters  reliability in clone benchmarks construction",
      "Abstract": "Cloned code often complicates code maintenance and evolution and must therefore be effectively detected. One of the biggest challenges for clone detectors is to reduce the amount of irrelevant clones they found, called false positives. Several benchmarks of true and false positive clones have been introduced, enabling tool developers to compare, assess and fine-tune their tools. Manual inspection of clone candidates is performed by raters that do not have expertise on the underlying code. This way of building benchmarks might be unreliable when considering context-dependent clones i.e., clones valid for a specific purpose. Our goal is to investigate the reliability of rater judgments about context-dependent clones. We randomly select about 600 clones from two projects and ask several raters, including experts of the projects, to manually classify these clones. We observe that judgments of non expert raters are not always repeatable. We also observe that they seldomly agree with each others and with the expert. Finally, we find that the project and the fact that a clone is a true or false positive might have an influence on the agreement between the expert and non experts. Therefore, using non experts to produce clone benchmarks could be unreliable.",
      "Keywords": "Code clone | Duplication | Empirical study | Software metrics",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Charpentier, Alan;Falleri, Jean Rémy;Morandat, Floréal;Ben Hadj Yahia, Elyas;Réveillère, Laurent",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84973160570",
      "Primary study DOI": "10.1007/s10664-016-9434-8",
      "Title": "A stability assessment of solution adaptation techniques for analogy-based software effort estimation",
      "Abstract": "Among numerous possible choices of effort estimation methods, analogy-based software effort estimation based on Case-based reasoning is one of the most adopted methods in both the industry and research communities. Solution adaptation is the final step of analogy-based estimation, employed to aggregate and adapt to solutions derived during the case-based reasoning process. Variants of solution adaptation techniques have been proposed in previous studies; however, the ranking of these techniques is not conclusive and shows conflicting results, since different studies rank these techniques in different ways. This paper aims to find a stable ranking of solution adaptation techniques for analogy-based estimation. Compared with the existing studies, we evaluate 8 commonly adopted solution techniques with more datasets (12), more feature selection techniques included (4), and more stable error measures (5) to a robust statistical test method based on the Brunner test. This comprehensive experimental procedure allows us to discover a stable ranking of the techniques applied, and to observe similar behaviors from techniques with similar adaptation mechanisms. In general, the linear adaptation techniques based on the functions of size and productivity (e.g., regression towards the mean technique) outperform the other techniques in a more robust experimental setting adopted in this study. Our empirical results show that project features with strong correlation to effort, such as software size or productivity, should be utilized in the solution adaptation step to achieve desirable performance. Designing a solution adaptation strategy in analogy-based software effort estimation requires careful consideration of those influential features to ensure its prediction is of relevant and accurate.",
      "Keywords": "Analogy-based estimation | Ranking instability | Robust statistical method | Software effort estimation | Solution adaptation techniques",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Phannachitta, Passakorn;Keung, Jacky;Monden, Akito;Matsumoto, Kenichi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84964337019",
      "Primary study DOI": "10.1007/s10664-016-9430-z",
      "Title": "The structure and dynamics of knowledge network in domain-specific Q&A sites: a case study of stack overflow",
      "Abstract": "Programming-specific Q&A sites (e.g., Stack Overflow) are being used extensively by software developers for knowledge sharing and acquisition. Due to the cross-reference of questions and answers (note that users also reference URLs external to the Q&A site. In this paper, URL sharing refers to internal URLs within the Q&A site, unless otherwise stated), knowledge is diffused in the Q&A site, forming a large knowledge network. In Stack Overflow, why do developers share URLs? How is the community feedback to the knowledge being shared? What are the unique topological and semantic properties of the resulting knowledge network in Stack Overflow? Has this knowledge network become stable? If so, how does it reach to stability? Answering these questions can help the software engineering community better understand the knowledge diffusion process in programming-specific Q&A sites like Stack Overflow, thereby enabling more effective knowledge sharing, knowledge use, and knowledge representation and search in the community. Previous work has focused on analyzing user activities in Q&A sites or mining the textual content of these sites. In this article, we present a methodology to analyze URL sharing activities in Stack Overflow. We use open coding method to analyze why users share URLs in Stack Overflow, and develop a set of quantitative analysis methods to study the structural and dynamic properties of the emergent knowledge network in Stack Overflow. We also identify system designs, community norms, and social behavior theories that help explain our empirical findings. Through this study, we obtain an in-depth understanding of the knowledge diffusion process in Stack Overflow and expose the implications of URL sharing behavior for Q&A site design, developers who use crowdsourced knowledge in Stack Overflow, and future research on knowledge representation and search.",
      "Keywords": "Crowdsourced knowledge | Domain-specific Q&A | Human factors | Mining software repositories | Network analysis | URL sharing",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Ye, Deheng;Xing, Zhenchang;Kapre, Nachiket",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84954492015",
      "Primary study DOI": "10.1007/s10664-015-9417-1",
      "Title": "Extracting and analyzing time-series HCI data from screen-captured task videos",
      "Abstract": "Recent years have witnessed the increasing emphasis on human aspects in software engineering research and practices. Our survey of existing studies on human aspects in software engineering shows that screen-captured videos have been widely used to record developers’ behavior and study software engineering practices. The screen-captured videos provide direct information about which software tools the developers interact with and which content they access or generate during the task. Such Human-Computer Interaction (HCI) data can help researchers and practitioners understand and improve software engineering practices from human perspective. However, extracting time-series HCI data from screen-captured task videos requires manual transcribing and coding of videos, which is tedious and error-prone. In this paper we report a formative study to understand the challenges in manually transcribing screen-captured videos into time-series HCI data. We then present a computer-vision based video scraping technique to automatically extract time-series HCI data from screen-captured videos. We also present a case study of our scvRipper tool that implements the video scraping technique using 29-hours of task videos of 20 developers in two development tasks. The case study not only evaluates the runtime performance and robustness of the tool, but also performs a detailed quantitative analysis of the tool’s ability to extract time-series HCI data from screen-captured task videos. We also study the developer’s micro-level behavior patterns in software development from the quantitative analysis.",
      "Keywords": "HCI data | Online search behavior | Screen-captured video | Video scraping",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Bao, Lingfeng;Li, Jing;Xing, Zhenchang;Wang, Xinyu;Xia, Xin;Zhou, Bo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84973174936",
      "Primary study DOI": "10.1007/s10664-016-9436-6",
      "Title": "Why and how developers fork what from whom in GitHub",
      "Abstract": "Forking is the creation of a new software repository by copying another repository. Though forking is controversial in traditional open source software (OSS) community, it is encouraged and is a built-in feature in GitHub. Developers freely fork repositories, use codes as their own and make changes. A deep understanding of repository forking can provide important insights for OSS community and GitHub. In this paper, we explore why and how developers fork what from whom in GitHub. We collect a dataset containing 236,344 developers and 1,841,324 forks. We make surveys, and analyze programming languages and owners of forked repositories. Our main observations are: (1) Developers fork repositories to submit pull requests, fix bugs, add new features and keep copies etc. Developers find repositories to fork from various sources: search engines, external sites (e.g., Twitter, Reddit), social relationships, etc. More than 42 % of developers that we have surveyed agree that an automated recommendation tool is useful to help them pick repositories to fork, while more than 44.4 % of developers do not value a recommendation tool. Developers care about repository owners when they fork repositories. (2) A repository written in a developer’s preferred programming language is more likely to be forked. (3) Developers mostly fork repositories from creators. In comparison with unattractive repository owners, attractive repository owners have higher percentage of organizations, more followers and earlier registration in GitHub. Our results show that forking is mainly used for making contributions of original repositories, and it is beneficial for OSS community. Moreover, our results show the value of recommendation and provide important insights for GitHub to recommend repositories.",
      "Keywords": "Fork | GitHub | Open source software",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Jiang, Jing;Lo, David;He, Jiahuan;Xia, Xin;Kochhar, Pavneet Singh;Zhang, Li",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84960079137",
      "Primary study DOI": "10.1007/s10664-015-9418-0",
      "Title": "Zen-ReqOptimizer: a search-based approach for requirements assignment optimization",
      "Abstract": "At early phases of a product development lifecycle of large scale Cyber-Physical Systems (CPSs), a large number of requirements need to be assigned to stakeholders from different organizations or departments of the same organization for review, clarification and checking their conformance to standards and regulations. These requirements have various characteristics such as extents of importance to the organization, complexity, and dependencies between each other, thereby requiring different effort (workload) to review and clarify. While working with our industrial partners in the domain of CPSs, we discovered an optimization problem, where an optimal solution is required for assigning requirements to various stakeholders by maximizing their familiarity to assigned requirements, meanwhile balancing the overall workload of each stakeholder. In this direction, we propose a fitness function that takes into account all the above-mentioned factors to guide a search algorithm to find an optimal solution. As a pilot experiment, we first investigated four commonly applied search algorithms (i.e., GA, (1 + 1) EA, AVM, RS) together with the proposed fitness function and results show that (1 + 1) EA performs significantly better than the other algorithms. Since our optimization problem is multi-objective, we further empirically evaluated the performance of the fitness function with six multi-objective search algorithms (CellDE, MOCell, NSGA-II, PAES, SMPSO, SPEA2) together with (1 + 1) EA (the best in the pilot study) and RS (as the baseline) in terms of finding an optimal solution using an real-world case study and 120 artificial problems of varying complexity. Results show that both for the real-world case study and the artificial problems (1 + 1) EA achieved the best performance for each single objective and NSGA-II achieved the best performance for the overall fitness. NSGA-II has the ability to solve a wide range of problems without having their performance degraded significantly and (1 + 1) EA is not fit for problems with less than 250 requirements Therefore we recommend that, if a project manager is interested in a particular objective then (1 + 1) EA should be used; otherwise, NSGA-II should be applied to obtain optimal solutions when putting the overall fitness as the first priority.",
      "Keywords": "Optimization and empirical evaluation | Requirements assignment | Search based software engineering",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Li, Yan;Yue, Tao;Ali, Shaukat;Zhang, Li",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84965019192",
      "Primary study DOI": "10.1007/s10664-016-9431-y",
      "Title": "Stochastic actor-oriented modeling for studying homophily and social influence in OSS projects",
      "Abstract": "Open Source Software projects are communities in which people “learn the ropes” from each other. The social and technical activities of developers evolve together, and as they link to each other they get organized in a network of changing socio-technical connections. Traces of those activities, or behaviors, are typically visible to all, in project repositories and through communication between them. Thus, in principle it may be possible to study those traces to tell which of the observable socio-technical behaviors of developers in these projects are responsible for the forming of persistent links between them. It may also be possible to tell the extent to which links participate in the spread of potential behavioral influences. Since OSS projects change in both social and technical activity over time, static approaches, that either ignore time or simplify it to a few slices, are frequently inadequate to study these networks. On the other hand, ad-hoc dynamic approaches are often only loosely supported by theory and can yield misleading findings. Here we adapt the stochastic actor-oriented models from social network analysis. These models enable the study of the interplay between behavior, influence and network architecture, for dynamic networks, in a statistically sound way. We apply the stochastic actor-oriented models in case studies of two Apache Software Foundation projects, and study code ownership and developer productivity as behaviors. For those, we find evidence of significant social selection effects (homophily) in both projects, but in different directions. However, we find no evidence for the spread (social influence) of either code ownership or developer productivity behaviors through the networks.",
      "Keywords": "Actor oriented models | Apache | Homophily | Open source | Siena | Social influence | Social selection",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Kavaler, David;Filkov, Vladimir",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027709523",
      "Primary study DOI": "10.1109/ICSE.2017.27",
      "Title": "A Guided Genetic Algorithm for Automated Crash Reproduction",
      "Abstract": "To reduce the effort developers have to make for crash debugging, researchers have proposed several solutions for automatic failure reproduction. Recent advances proposed the use of symbolic execution, mutation analysis, and directed model checking as underling techniques for post-failure analysis of crash stack traces. However, existing approaches still cannot reproduce many real-world crashes due to such limitations as environment dependencies, path explosion, and time complexity. To address these challenges, we present EvoCrash, a post-failure approach which uses a novel Guided Genetic Algorithm (GGA) to cope with the large search space characterizing real-world software programs. Our empirical study on three open-source systems shows that EvoCrash can replicate 41 (82%) of real-world crashes, 34 (89%) of which are useful reproductions for debugging purposes, outperforming the state-of-The-Art in crash replication.",
      "Keywords": "Automated Crash Reproduction | Genetic Algorithms | Search-Based Software Testing",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Soltani, Mozhan;Panichella, Annibale;Van Deursen, Arie",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85020739000",
      "Primary study DOI": "10.1109/ICSE.2017.66",
      "Title": "A Test-Suite Diagnosability Metric for Spectrum-Based Fault Localization Approaches",
      "Abstract": "Current metrics for assessing the adequacy of a test-suite plainly focus on the number of components (be it lines, branches, paths) covered by the suite, but do not explicitly check how the tests actually exercise these components and whether they provide enough information so that spectrum-based fault localization techniques can perform accurate fault isolation. We propose a metric, called DDU, aimed at complementing adequacy measurements by quantifying a test-suite's diagnosability, i.e., the effectiveness of applying spectrum-based fault localization to pinpoint faults in the code in the event of test failures. Our aim is to increase the value generated by creating thorough test-suites, so they are not only regarded as error detection mechanisms but also as effective diagnostic AIDS that help widely-used fault-localization techniques to accurately pinpoint the location of bugs in the system. Our experiments show that optimizing a test suite with respect to DDU yields a 34% gain in spectrum-based fault localization report accuracy when compared to the standard branch-coverage metric.",
      "Keywords": "Coverage | Diagnosability | Testing",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Perez, Alexandre;Abreu, Rui;Van Deursen, Arie",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027712617",
      "Primary study DOI": "10.1109/ICSE.2017.36",
      "Title": "A sealant for inter-app security holes in android",
      "Abstract": "Android's communication model has a major security weakness: malicious apps can manipulate other apps into performing unintended operations and can steal end-user data, while appearing ordinary and harmless. This paper presents SEALANT, a technique that combines static analysis of app code, which infers vulnerable communication channels, with runtime monitoring of inter-App communication through those channels, which helps to prevent attacks. SEALANT's extensive evaluation demonstrates that (1) it detects and blocks inter-App attacks with high accuracy in a corpus of over 1,100 real-world apps, (2) it suffers from fewer false alarms than existing techniques in several representative scenarios, (3) its performance overhead is negligible, and (4) end-users do not find it challenging to adopt.",
      "Keywords": "Android | Inter-App vulnerability | Security",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Lee, Youn Kyu;Bang, Jae Young;Safi, Gholamreza;Shahbazian, Arman;Zhao, Yixue;Medvidovic, Nenad",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027719228",
      "Primary study DOI": "10.1109/ICSE.2017.73",
      "Title": "UML Diagram Refinement (Focusing on Class-And Use Case Diagrams)",
      "Abstract": "Large and complicated UML models are not useful, because they are difficult to understand. This problem can be solved by using several diagrams of the same system at different levels of abstraction. Unfortunately, UML does not define an explicit set of rules for ensuring that diagrams at different levels of abstraction are consistent. We define such a set of rules, that we call diagram refinement. Diagram refinement is intuitive, and applicable to several kinds of UML diagrams (mostly to structural diagrams but also to use case diagrams), yet it rests on a solid mathematical basis-the theory of graph homomorphisms. We illustrate its usefulness with a series of examples.",
      "Keywords": "Class diagram | Design patterns | Graph homomorphism | Refinement | UML | Use case diagram",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Faitelson, David;Tyszberowicz, Shmuel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027681076",
      "Primary study DOI": "",
      "Title": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Abstract": "The proceedings contain 68 papers. The topics discussed include: semantically enhanced software traceability using deep learning techniques; can latent topics in source code predict missing architectural tactics?; preventing defects: the impact of traceability completeness on software quality; imprecise matching of requirements specifications for software services using fuzzy logic; analyzing APIs documentation and code to detect directive defects; detecting user story information in developer-client conversations to generate extractive summaries; keyword search for building service-based systems; clone refactoring with lambda expressions; automated refactoring of legacy Java software to default methods; using cohesion and coupling for software remodularization: is it enough?; recommending and localizing change requests for mobile apps based on user reviews; machine learning-based detection of open source license exceptions; supporting change impact analysis using a recommendation system: an industrial case study in a safety-critical context; becoming agile: a grounded theory of agile transitions in practice; from diversity by numbers to diversity as process: supporting inclusiveness in software development teams with brainstorming; and process aspects and social dynamics of contemporary code review: insights from open source development and industrial practice at Microsoft.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027700281",
      "Primary study DOI": "10.1109/ICSE.2017.55",
      "Title": "How Good Is a Security Policy against Real Breaches? A HIPAA Case Study",
      "Abstract": "Policy design is an important part of software development. As security breaches increase in variety, designing a security policy that addresses all potential breaches becomes a nontrivial task. A complete security policy would specify rules to prevent breaches. Systematically determining which, if any, policy clause has been violated by a reported breach is a means for identifying gaps in a policy. Our research goal is to help analysts measure the gaps between security policies and reported breaches by developing a systematic process based on semantic reasoning. We propose SEMAVER, a framework for determining coverage of breaches by policies via comparison of individual policy clauses and breach descriptions. We represent a security policy as a set of norms. Norms (commitments, authorizations, and prohibitions) describe expected behaviors of users, and formalize who is accountable to whom and for what. A breach corresponds to a norm violation. We develop a semantic similarity metric for pairwise comparison between the norm that represents a policy clause and the norm that has been violated by a reported breach. We use the US Health Insurance Portability and Accountability Act (HIPAA) as a case study. Our investigation of a subset of the breaches reported by the US Department of Health and Human Services (HHS) reveals the gaps between HIPAA and reported breaches, leading to a coverage of 65%. Additionally, our classification of the 1,577 HHS breaches shows that 44% of the breaches are accidental misuses and 56% are malicious misuses. We find that HIPAA's gaps regarding accidental misuses are significantly larger than its gaps regarding malicious misuses.",
      "Keywords": "breach ontology | Security and privacy breaches | semantic similarity | social norms",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Kafali, Ozgur;Jones, Jasmine;Petruso, Megan;Williams, Laurie;Singh, Munindar P.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027723153",
      "Primary study DOI": "10.1109/ICSE.2017.51",
      "Title": "ProEva: Runtime proactive performance evaluation based on continuous-time markov chains",
      "Abstract": "Software systems, especially service-based software systems, need to guarantee runtime performance. If their performance is degraded, some reconfiguration countermeasures should be taken. However, there is usually some latency before the countermeasures take effect. It is thus important not only to monitor the current system status passively but also to predict its future performance proactively. Continuous-Time Markov chains (CTMCs) are suitable models to analyze time-bounded performance metrics (e.g., how likely a performance degradation may occur within some future period). One challenge to harness CTMCs is the measurement of model parameters (i.e., transition rates) in CTMCs at runtime. As these parameters may be updated by the system or environment frequently, it is difficult for the model builder to provide precise parameter values. In this paper, we present a framework called ProEva, which extends the conventional technique of time-bounded CTMC model checking by admitting imprecise, interval-valued estimates for transition rates. The core method of ProEva computes asymptotic expressions and bounds for the imprecise model checking output. We also present an evaluation of accuracy and computational overhead for ProEva.",
      "Keywords": "Continuous-Time Markov Chain | Imprecise Parameters | Performance | Quality-of-Service",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Su, Guoxin;Chen, Taolue;Feng, Yuan;Rosenblum, David S.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027725508",
      "Primary study DOI": "10.1109/ICSE.2017.63",
      "Title": "Syntactic and Semantic Differencing for Combinatorial Models of Test Designs",
      "Abstract": "Combinatorial test design (CTD) is an effective test design technique, considered to be a testing best practice. CTD provides automatic test plan generation, but it requires a manual definition of the test space in the form of a combinatorial model. As the system under test evolves, e.g., due to iterative development processes and bug fixing, so does the test space, and thus, in the context of CTD, evolution translates into frequent manual model definition updates. Manually reasoning about the differences between versions of real-world models following such updates is infeasible due to their complexity and size. Moreover, representing the differences is challenging. In this work, we propose a first syntactic and semantic differencing technique for combinatorial models of test designs. We define a concise and canonical representation for differences between two models, and suggest a scalable algorithm for automatically computing and presenting it. We use our differencing technique to analyze the evolution of 42 real-world industrial models, demonstrating its applicability and scalability. Further, a user study with 16 CTD practitioners shows that comprehension of differences between real-world combinatorial model versions is challenging and that our differencing tool significantly improves the performance of less experienced practitioners. The analysis and user study provide evidence for the potential usefulness of our differencing approach. Our work advances the state-of-The-Art in CTD with better capabilities for change comprehension and management.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Tzoref-Brill, Rachel;Maoz, Shahar",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027726725",
      "Primary study DOI": "10.1109/ICSE.2017.41",
      "Title": "Performance diagnosis for inefficient loops",
      "Abstract": "Writing efficient software is difficult. Design and implementation defects cancause severe performance degradation. Unfortunately, existing performance diagnosis techniques like profilers are still preliminary. They can locate code regions that consume resources, but not the ones that waste resources. In this paper, we first design a root-causeand fix-strategy taxonomy for inefficient loops, one of the most common performance problems in the field. We then design a static-dynamic hybrid analysis tool, LDoctor, toprovide accurate performance diagnosis for loops. We further use sampling techniques to lower the run-Time overhead withoutdegrading the accuracy or latency of LDoctor diagnosis. Evaluation using real-world performanceproblems shows that LDoctor can provide better coverage and accuracy thanexisting techniques, with low overhead.",
      "Keywords": "Debugging | Loop Inefficiency | Performance Diagnosis",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Song, Linhai;Lu, Shan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027691396",
      "Primary study DOI": "10.1109/ICSE.2017.42",
      "Title": "How do developers fix cross-project correlated bugs? A case study on the GitHub scientific python ecosystem",
      "Abstract": "GitHub, a popular social-software-development platform, has fostered a variety of software ecosystems where projects depend on one another and practitioners interact with each other. Projects within an ecosystem often have complex inter-dependencies that impose new challenges in bug reporting and fixing. In this paper, we conduct an empirical study on cross-project correlated bugs, i.e., causally related bugs reported to different projects, focusing on two aspects: 1) how developers track the root causes across projects, and 2) how the downstream developers coordinate to deal with upstream bugs. Through manual inspection of bug reports collected from the scientific Python ecosystem and an online survey with developers, this study reveals the common practices of developers and the various factors in fixing cross-project bugs. These findings provide implications for future software bug analysis in the scope of ecosystem, as well as shed light on the requirements of issue trackers for such bugs.",
      "Keywords": "Coordinate | Cross-Project Correlated Bugs | GitHub ecosystems | Root Causes Tracking",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Ma, Wanwangying;Chen, Lin;Zhang, Xiangyu;Zhou, Yuming;Xu, Baowen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85020722422",
      "Primary study DOI": "10.1109/ICSE.2017.61",
      "Title": "An Empirical Study on Mutation, Statement and Branch Coverage Fault Revelation That Avoids the Unreliable Clean Program Assumption",
      "Abstract": "Many studies suggest using coverage concepts, such as branch coverage, as the starting point of testing, while others as the most prominent test quality indicator. Yet the relationship between coverage and fault-revelation remains unknown, yielding uncertainty and controversy. Most previous studies rely on the Clean Program Assumption, that a test suite will obtain similar coverage for both faulty and fixed ('clean') program versions. This assumption may appear intuitive, especially for bugs that denote small semantic deviations. However, we present evidence that the Clean Program Assumption does not always hold, thereby raising a critical threat to the validity of previous results. We then conducted a study using a robust experimental methodology that avoids this threat to validity, from which our primary finding is that strong mutation testing has the highest fault revelation of four widely-used criteria. Our findings also revealed that fault revelation starts to increase significantly only once relatively high levels of coverage are attained.",
      "Keywords": "code coverage | Mutation testing | real faults | test adequacy | test effectiveness",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Chekam, Thierry Titcheu;Papadakis, Mike;Le Traon, Yves;Harman, Mark",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018698735",
      "Primary study DOI": "10.1109/ICSE.2017.21",
      "Title": "Becoming Agile: A Grounded Theory of Agile Transitions in Practice",
      "Abstract": "Agile adoption is typically understood as a oneoff organizational process involving a staged selection of agile development practices. This view of agility fails to explain the differences in the pace and effectiveness of individual teams transitioning to agile development. Based on a Grounded Theory study of 31 agile practitioners drawn from 18 teams across five countries, we present a grounded theory of becoming agile as a network of on-going transitions across five dimensions: software development practices, team practices, management approach, reflective practices, and culture. The unique position of a software team through this network, and their pace of progress along the five dimensions, explains why individual agile teams present distinct manifestations of agility and unique transition experiences. The theory expands the current understanding of agility as a holistic and complex network of on-going multidimensional transitions, and will help software teams, their managers, and organizations better navigate their individual agile journeys.",
      "Keywords": "agile software development | culture | grounded theory | management | selforganizing | teams | theory | transition",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Hoda, Rashina;Noble, James",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027711067",
      "Primary study DOI": "10.1109/ICSE.2017.49",
      "Title": "SPAIN: Security patch analysis for binaries towards understanding the pain and pills",
      "Abstract": "Software vulnerability is one of the major threats to software security. Once discovered, vulnerabilities are often fixed by applying security patches. In that sense, security patches carry valuable information about vulnerabilities, which could be used to discover, understand and fix (similar) vulnerabilities. However, most existing patch analysis approaches work at the source code level, while binary-level patch analysis often heavily relies on a lot of human efforts and expertise. Even worse, some vulnerabilities may be secretly patched without applying CVE numbers, or only the patched binary programs are available while the patches are not publicly released. These practices greatly hinder patch analysis and vulnerability analysis. In this paper, we propose a scalable binary-level patch analysis framework, named SPAIN, which can automatically identify security patches and summarize patch patterns and their corresponding vulnerability patterns. Specifically, given the original and patched versions of a binary program, we locate the patched functions and identify the changed traces (i.e., a sequence of basic blocks) that may contain security or non-security patches. Then we identify security patches through a semantic analysis of these traces and summarize the patterns through a taint analysis on the patched functions. The summarized patterns can be used to search similar patches or vulnerabilities in binary programs. Our experimental results on several real-world projects have shown that: i) SPAIN identified security patches with high accuracy and high scalability, ii) SPAIN summarized 5 patch patterns and their corresponding vulnerability patterns for 5 vulnerability types, and iii) SPAIN discovered security patches that were not documented, and discovered 3 zero-day vulnerabilities.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Xu, Zhengzi;Chen, Bihuan;Chandramohan, Mahinthan;Liu, Yang;Song, Fu",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027711811",
      "Primary study DOI": "10.1109/ICSE.2017.64",
      "Title": "Balancing Soundness and Efficiency for Practical Testing of Configurable Systems",
      "Abstract": "Testing configurable systems is important and challenging due to the enormous space of configurations where errors can hide. Existing approaches to test these systems are often costly or unreliable. This paper proposes S-SPLat, a technique that combines heuristic sampling with symbolic search to obtain both breadth and depth in the exploration of the configuration space. S-SPLat builds on SPLat, our previously developed technique, that explores all reachable configurations from tests. In contrast to its predecessor, S-SPLat sacrifices soundness in favor of efficiency. We evaluated our technique on eight software product lines of various sizes and on a large configurable system-GCC. Considering the results for GCC, S-SPLat was able to reproduce all five bugs that we previously found in a previous study with SPLat but much faster and it was able to find two new bugs in a recent release of GCC. Results suggest that it is preferable to use a combination of simple heuristics to drive the symbolic search as opposed to a single heuristic. S-SPLat and our experimental infrastructure are publicly available.",
      "Keywords": "configuration | sampling | testing",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Souto, Sabrina;D'Amorim, Marcelo;Gheyi, Rohit",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026767161",
      "Primary study DOI": "10.1109/ICSE.2017.52",
      "Title": "Glacier: Transitive class immutability for Java",
      "Abstract": "Though immutability has been long-proposed as a way to prevent bugs in software, little is known about how to make immutability support in programming languages effective for software engineers. We designed a new formalism that extends Java to support transitive class immutability, the form of immutability for which there is the strongest empirical support, and implemented that formalism in a tool called Glacier. We applied Glacier successfully to two real-world systems. We also compared Glacier to Java's final in a user study of twenty participants. We found that even after being given instructions on how to express immutability with final, participants who used final were unable to express immutability correctly, whereas almost all participants who used Glacier succeeded. We also asked participants to make specific changes to immutable classes and found that participants who used final all incorrectly mutated immutable state, whereas almost all of the participants who used Glacier succeeded. Glacier represents a promising approach to enforcing immutability in Java and provides a model for enforcement in other languages.",
      "Keywords": "Empirical Studies Of Programmers | Immutability | Programming Language Usability",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Coblenz, Michael;Nelson, Whitney;Aldrich, Jonathan;Myers, Brad;Sunshine, Joshua",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027712504",
      "Primary study DOI": "10.1109/ICSE.2017.9",
      "Title": "Semantically Enhanced Software Traceability Using Deep Learning Techniques",
      "Abstract": "In most safety-critical domains the need for traceability is prescribed by certifying bodies. Trace links are generally created among requirements, design, source code, test cases and other artifacts, however, creating such links manually is time consuming and error prone. Automated solutions use information retrieval and machine learning techniques to generate trace links, however, current techniques fail to understand semantics of the software artifacts or to integrate domain knowledge into the tracing process and therefore tend to deliver imprecise and inaccurate results. In this paper, we present a solution that uses deep learning to incorporate requirements artifact semantics and domain knowledge into the tracing solution. We propose a tracing network architecture that utilizes Word Embedding and Recurrent Neural Network (RNN) models to generate trace links. Word embedding learns word vectors that represent knowledge of the domain corpus and RNN uses these word vectors to learn the sentence semantics of requirements artifacts. We trained 360 different configurations of the tracing network using existing trace links in the Positive Train Control domain and identified the Bidirectional Gated Recurrent Unit (BI-GRU) as the best model for the tracing task. BI-GRU significantly out-performed state-of-The-Art tracing methods including the Vector Space Model and Latent Semantic Indexing.",
      "Keywords": "Deep Learning | Recurrent Neural Network | Semantic Representation | Traceability",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Guo, Jin;Cheng, Jinghui;Cleland-Huang, Jane",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85025619012",
      "Primary study DOI": "10.1109/ICSE.2017.23",
      "Title": "Classifying Developers into Core and Peripheral: An Empirical Study on Count and Network Metrics",
      "Abstract": "Knowledge about the roles developers play in a software project is crucial to understanding the project's collaborative dynamics. In practice, developers are often classified according to the dichotomy of core and peripheral roles. Typically, count-based operationalizations, which rely on simple counts of individual developer activities (e.g., number of commits), are used for this purpose, but there is concern regarding their validity and ability to elicit meaningful insights. To shed light on this issue, we investigate whether count-based operationalizations of developer roles produce consistent results, and we validate them with respect to developers' perceptions by surveying 166 developers. Improving over the state of the art, we propose a relational perspective on developer roles, using fine-grained developer networks modeling the organizational structure, and by examining developer roles in terms of developers' positions and stability within the developer network. In a study of 10 substantial open-source projects, we found that the primary difference between the count-based and our proposed network-based core-peripheral operationalizations is that the network-based ones agree more with developer perception than count-based ones. Furthermore, we demonstrate that a relational perspective can reveal further meaningful insights, such as that core developers exhibit high positional stability, upper positions in the hierarchy, and high levels of coordination with other core developers, which confirms assumptions of previous work.",
      "Keywords": "classification | developer networks | Developer roles | mining software repositories",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Joblin, Mitchell;Apel, Sven;Hunsen, Claus;Mauerer, Wolfgang",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027718027",
      "Primary study DOI": "10.1109/ICSE.2017.56",
      "Title": "Adaptive Coverage and Operational Profile-Based Testing for Reliability Improvement",
      "Abstract": "We introduce covrel, an adaptive software testing approach based on the combined use of operational profile and coverage spectrum, with the ultimate goal of improving the delivered reliability of the program under test. Operational profile-based testing is a black-box technique that selects test cases having the largest impact on failure probability in operation, as such, it is considered well suited when reliability is a major concern. Program spectrum is a characterization of a program's behavior in terms of the code entities (e.g., branches, statements, functions) that are covered as the program executes. The driving idea of covrel is to complement operational profile information with white-box coverage measures based on count spectra, so as to dynamically select the most effective test cases for reliability improvement. In particular, we bias operational profile-based test selection towards those entities covered less frequently. We assess the approach by experiments with 18 versions from 4 subjects commonly used in software testing research, comparing results with traditional operational and coverage testing. Results show that exploiting operational and coverage data in a combined adaptive way actually pays in terms of reliability improvement, with covrel overcoming conventional operational testing in more than 80% of the cases.",
      "Keywords": "Operational coverage | Operational profile | Program count spectrum | Reliability | Test case selection | Testing",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Bertolino, Antonia;Miranda, Breno;Pietrantuono, Roberto;Russo, Stefano",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027722823",
      "Primary study DOI": "10.1109/ICSE.2017.24",
      "Title": "Decoding the Representation of Code in the Brain: An fMRI Study of Code Review and Expertise",
      "Abstract": "Subjective judgments in software engineering tasks are of critical importance but can be difficult to study with conventional means. Medical imaging techniques hold the promise of relating cognition to physical activities and brain structures. In a controlled experiment involving 29 participants, we examine code comprehension, code review and prose review using functional magnetic resonance imaging. We find that the neural representations of programming languages vs. natural languages are distinct. We can classify which task a participant is undertaking based solely on brain activity (balanced accuracy 79%.",
      "Keywords": "code comprehension | medical imaging | prose review",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Floyd, Benjamin;Santander, Tyler;Weimer, Westley",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027707946",
      "Primary study DOI": "10.1109/ICSE.2017.28",
      "Title": "Stochastic Optimization of Program Obfuscation",
      "Abstract": "Program obfuscation is a common practice in software development to obscure source code or binary code, in order to prevent humans from understanding the purpose or logic of software. It protects intellectual property and deters malicious attacks. While tremendous efforts have been devoted to the development of various obfuscation techniques, we have relatively little knowledge on how to most effectively use them together. The biggest challenge lies in identifying the most effective combination of obfuscation techniques. This paper presents a unified framework to optimize program obfuscation. Given an input program P and a set T of obfuscation transformations, our technique can automatically identify a sequence seq = t1, t2,.., tn (&ForAll;i &Element; [1, n].Ti &Element; T), such that applying ti in order on P yields the optimal obfuscation performance. We model the process of searching for seq as a mathematical optimization problem. The key technical contributions of this paper are: (1) an obscurity language model to assess obfuscation effectiveness/optimality, and (2) a guided stochastic algorithm based on Markov chain Monte Carlo methods to search for the optimal solution seq. We have realized the framework in a tool Closure∗ for JavaScript, and evaluated it on 25 most starred JavaScript projects on GitHub (19K lines of code). Our machinery study shows that Closure∗ outperforms the well-known Google Closure Compiler by defending 26% of the attacks initiated by JSNice. Our human study also reveals that Closure∗ is practical and can reduce the human attack success rate by 30%.",
      "Keywords": "markov chain monte carlo methods | obscurity language model | program obfuscation",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Liu, Han;Sun, Chengnian;Su, Zhendong;Jiang, Yu;Gu, Ming;Sun, Jiaguang",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027687742",
      "Primary study DOI": "10.1109/ICSE.2017.12",
      "Title": "An Unsupervised Approach for Discovering Relevant Tutorial Fragments for APIs",
      "Abstract": "Developers increasingly rely on API tutorials to facilitate software development. However, it remains a challenging task for them to discover relevant API tutorial fragments explaining unfamiliar APIs. Existing supervised approaches suffer from the heavy burden of manually preparing corpus-specific annotated data and features. In this study, we propose a novel unsupervised approach, namely Fragment Recommender for APIs with PageRank and Topic model (FRAPT). FRAPT can well address two main challenges lying in the task and effectively determine relevant tutorial fragments for APIs. In FRAPT, a Fragment Parser is proposed to identify APIs in tutorial fragments and replace ambiguous pronouns and variables with related ontologies and API names, so as to address the pronoun and variable resolution challenge. Then, a Fragment Filter employs a set of non-explanatory detection rules to remove non-explanatory fragments, thus address the non-explanatory fragment identification challenge. Finally, two correlation scores are achieved and aggregated to determine relevant fragments for APIs, by applying both topic model and PageRank algorithm to the retained fragments. Extensive experiments over two publicly open tutorial corpora show that, FRAPT improves the state-of-The-Art approach by 8.77% and 12.32% respectively in terms of F-Measure. The effectiveness of key components of FRAPT is also validated.",
      "Keywords": "Application Programming Interface | PageRank Algorithm | Topic Model | Unsupervised Approaches",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Jiang, He;Zhang, Jingxuan;Ren, Zhilei;Zhang, Tao",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027726819",
      "Primary study DOI": "10.1109/ICSE.2017.37",
      "Title": "An efficient, robust, and scalable approach for analyzing interacting android apps",
      "Abstract": "When multiple apps on an Android platform interact, faults and security vulnerabilities can occur. Software engineers need to be able to analyze interacting apps to detect such problems. Current approaches for performing such analyses, however, do not scale to the numbers of apps that may need to be considered, and thus, are impractical for application to real-world scenarios. In this paper, we introduce JITANA, a program analysis framework designed to analyze multiple Android apps simultaneously. By using a classloader-based approach instead of a compiler-based approach such as SOOT, JITANA is able to simultaneously analyze large numbers of interacting apps, perform on-demand analysis of large libraries, and effectively analyze dynamically generated code. Empirical studies of JITANA show that it is substantially more efficient than a state-of-The-Art approach, and that it can effectively and efficiently analyze complex apps including Facebook, Pokemon Go, and Pandora that the state-of-The-Art approach cannot handle.",
      "Keywords": "Android | Inter-App communication | Program Analysis",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Tsutano, Yutaka;Bachala, Shakthi;Srisa-An, Witawas;Rothermel, Gregg;Dinh, Jackson",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027715334",
      "Primary study DOI": "10.1109/ICSE.2017.57",
      "Title": "RADAR: A Lightweight Tool for Requirements and Architecture Decision Analysis",
      "Abstract": "Uncertainty and conflicting stakeholders' objectives make many requirements and architecture decisions particularly hard. Quantitative probabilistic models allow software architects to analyse such decisions using stochastic simulation and multi-objective optimisation, but the difficulty of elaborating the models is an obstacle to the wider adoption of such techniques. To reduce this obstacle, this paper presents a novel modelling language and analysis tool, called RADAR, intended to facilitate requirements and architecture decision analysis. The language has relations to quantitative AND/OR goal models used in requirements engineering and to feature models used in software product lines. However, it simplifies such models to a minimum set of language constructs essential for decision analysis. The paper presents RADAR's modelling language, automated support for decision analysis, and evaluates its application to four real-world examples.",
      "Keywords": "Decision Analysis | Expected Value of Information | Goal Modelling | Monte-Carlo Simulation | Multi-Objective Optimisation | Requirements Engineering | Search-Based Software Engineering | Software Architecture",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Busari, Saheed A.;Letier, Emmanuel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027715159",
      "Primary study DOI": "10.1109/ICSE.2017.72",
      "Title": "Symbolic Model Extraction for Web Application Verification",
      "Abstract": "Modern web applications use complex data models and access control rules which lead to data integrity and access control errors. One approach to find such errors is to use formal verification techniques. However, as a first step, most formal verification techniques require extraction of a formal model which is a difficult problem in itself due to dynamic features of modern languages, and it is typically done either manually, or using ad hoc techniques. In this paper, we present a technique called symbolic model extraction for extracting formal data models from web applications. The key ideas of symbolic model extraction are 1) to use the source language interpreter for model extraction, which enables us to handle dynamic features of the language, 2) to use code instrumentation so that execution of each instrumented piece of code returns the formal model that corresponds to that piece of code, 3) to instrument the code dynamically so that the models of methods that are created at runtime can also be extracted, and 4) to execute both sides of branches during instrumented execution so that all program behaviors can be covered in a single instrumented execution. We implemented the symbolic model extraction technique for the Rails framework and used it to extract data and access control models from web applications. Our experiments demonstrate that symbolic model extraction is scalable and extracts formal models that are precise enough to find bugs in real-world applications without reporting too many false positives.",
      "Keywords": "Formal Verification | Model Extraction | Web Applications",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Bocic, Ivan;Bultan, Tevfik",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019246649",
      "Primary study DOI": "10.1109/ICSE.2017.40",
      "Title": "Adaptive unpacking of android apps",
      "Abstract": "More and more app developers use the packing services (or packers) to prevent attackers from reverse engineering and modifying the executable (or Dex files) of their apps. At the same time, malware authors also use the packers to hide the malicious component and evade the signature-based detection. Although there are a few recent studies on unpacking Android apps, it has been shown that the evolving packers can easily circumvent them because they are not adaptive to the changes of packers. In this paper, we propose a novel adaptive approach and develop a new system, named PackerGrind, to unpack Android apps. We also evaluate PackerGrind with real packed apps, and the results show that PackerGrind can successfully reveal the packers' protection mechanisms and recover the Dex files with low overhead, showing that our approach can effectively handle the evolution of packers.",
      "Keywords": "App Unpacking | Dynamic Analysis",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Xue, Lei;Luo, Xiapu;Yu, Le;Wang, Shuai;Wu, Dinghao",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027714405",
      "Primary study DOI": "10.1109/ICSE.2017.65",
      "Title": "Automatic Text Input Generation for Mobile Testing",
      "Abstract": "Many designs have been proposed to improve the automated mobile testing. Despite these improvements, providing appropriate text inputs remains a prominent obstacle, which hinders the large-scale adoption of automated testing approaches. The key challenge is how to automatically produce the most relevant text in a use case context. For example, a valid website address should be entered in the address bar of a mobile browser app to continue the testing of the app, a singer's name should be entered in the search bar of a music recommendation app. Without the proper text inputs, the testing would get stuck. We propose a novel deep learning based approach to address the challenge, which reduces the problem to a minimization problem. Another challenge is how to make the approach generally applicable to both the trained apps and the untrained apps. We leverage the Word2Vec model to address the challenge. We have built our approaches as a tool and evaluated it with 50 iOS mobile apps including Firefox and Wikipedia. The results show that our approach significantly outperforms existing automatic text input generation methods.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Liu, Peng;Zhang, Xiangyu;Pistoia, Marco;Zheng, Yunhui;Marques, Manoel;Zeng, Lingfei",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027714489",
      "Primary study DOI": "10.1109/ICSE.2017.59",
      "Title": "Do Developers Read Compiler Error Messages?",
      "Abstract": "In integrated development environments, developers receive compiler error messages through a variety of textual and visual mechanisms, such as popups and wavy red underlines. Although error messages are the primary means of communicating defects to developers, researchers have a limited understanding on how developers actually use these messages to resolve defects. To understand how developers use error messages, we conducted an eye tracking study with 56 participants from undergraduate and graduate software engineering courses at our university. The participants attempted to resolve common, yet problematic defects in a Java code base within the Eclipse development environment. We found that: 1) participants read error messages and the difficulty of reading these messages is comparable to the difficulty of reading source code, 2) difficulty reading error messages significantly predicts participants' task performance, and 3) participants allocate a substantial portion of their total task to reading error messages (13%-25%). The results of our study offer empirical justification for the need to improve compiler error messages for developers.",
      "Keywords": "compiler errors | eye tracking | integrated development environments | programmer comprehension | reading | visual attention",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Barik, Titus;Smith, Justin;Lubick, Kevin;Holmes, Elisabeth;Feng, Jing;Murphy-Hill, Emerson;Parnin, Chris",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026653905",
      "Primary study DOI": "10.1109/ICSE.2017.62",
      "Title": "Evaluating and Improving Fault Localization",
      "Abstract": "Most fault localization techniques take as input a faulty program, and produce as output a ranked list of suspicious code locations at which the program may be defective. When researchers propose a new fault localization technique, they typically evaluate it on programs with known faults. The technique is scored based on where in its output list the defective code appears. This enables the comparison of multiple fault localization techniques to determine which one is better. Previous research has evaluated fault localization techniques using artificial faults, generated either by mutation tools or manually. In other words, previous research has determined which fault localization techniques are best at finding artificial faults. However, it is not known which fault localization techniques are best at finding real faults. It is not obvious that the answer is the same, given previous work showing that artificial faults have both similarities to and differences from real faults. We performed a replication study to evaluate 10 claims in the literature that compared fault localization techniques (from the spectrum-based and mutation-based families). We used 2995 artificial faults in 6 real-world programs. Our results support 7 of the previous claims as statistically significant, but only 3 as having non-negligible effect sizes. Then, we evaluated the same 10 claims, using 310 real faults from the 6 programs. Every previous result was refuted or was statistically and practically insignificant. Our experiments show that artificial faults are not useful for predicting which fault localization techniques perform best on real faults. In light of these results, we identified a design space that includes many previously-studied fault localization techniques as well as hundreds of new techniques. We experimentally determined which factors in the design space are most important, using an overall set of 395 real faults. Then, we extended this design space with new techniques. Several of our novel techniques outperform all existing techniques, notably in terms of ranking defective code in the top-5 or top-10 reports.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Pearson, Spencer;Campos, Jose;Just, Rene;Fraser, Gordon;Abreu, Rui;Ernst, Michael D.;Pang, Deric;Keller, Benjamin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027677105",
      "Primary study DOI": "10.1109/ICSE.2017.17",
      "Title": "Supporting Software Developers with a Holistic Recommender System",
      "Abstract": "The promise of recommender systems is to provide intelligent support to developers during their programming tasks. Such support ranges from suggesting program entities to taking into account pertinent Q&A pages. However, current recommender systems limit the context analysis to change history and developers' activities in the IDE, without considering what a developer has already consulted or perused, e.g., by performing searches from the Web browser. Given the faceted nature of many programming tasks, and the incompleteness of the information provided by a single artifact, several heterogeneous resources are required to obtain the broader picture needed by a developer to accomplish a task. We present Libra, a holistic recommender system. It supports the process of searching and navigating the information needed by constructing a holistic meta-information model of the resources perused by a developer, analyzing their semantic relationships, and augmenting the web browser with a dedicated interactive navigation chart. The quantitative and qualitative evaluation of Libra provides evidence that a holistic analysis of a developer's information context can indeed offer comprehensive and contextualized support to information navigation and retrieval during software development.",
      "Keywords": "Mining unstructured data | Recommender systems",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Ponzanelli, Luca;Scalabrino, Simone;Bavota, Gabriele;Mocci, Andrea;Oliveto, Rocco;Di Penta, Massimiliano;Lanza, Michele",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027730393",
      "Primary study DOI": "10.1109/ICSE.2017.58",
      "Title": "PEoPL: Projectional Editing of Product Lines",
      "Abstract": "The features of a software product line-a portfolio of system variants-can be realized using various implementation techniques (a. k. a., variability mechanisms). Each technique represents the software artifacts of features differently, typically classified into annotative (e.g., C preprocessor) and modular representations (e.g., feature modules), each with distinct advantages and disadvantages. Annotative representations are easy to realize, but annotations clutter source code and hinder program comprehension. Modular representations support comprehension, but are difficult to realize. Most importantly, to engineer feature artifacts, developers need to choose one representation and adhere to it for evolving and maintaining the same artifacts. We present PEoPL, an approach to combine the advantages of annotative and modular representations. When engineering a feature artifact, developers can choose the most-suited representation and even use different representations in parallel. PEoPL relies on separating a product line into an internal and external representation, the latter by providing editable projections used by the developers. We contribute a programming-language-independent internal representation of variability, five editable projections reflecting different variability representations, a supporting IDE, and a tailoring of PEoPL to Java. We evaluate PEoPL's expressiveness, scalability, and flexibility in eight Java-based product lines, finding that all can be realized, that projections are feasible, and that variant computation is fast (",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Behringer, Benjamin;Palz, Jochen;Berger, Thorsten",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027716666",
      "Primary study DOI": "10.1109/ICSE.2017.16",
      "Title": "Automated Refactoring of Legacy Java Software to Default Methods",
      "Abstract": "Java 8 default methods, which allow interfaces to contain (instance) method implementations, are useful for the skeletal implementation software design pattern. However, it is not easy to transform existing software to exploit default methods as it requires analyzing complex type hierarchies, resolving multiple implementation inheritance issues, reconciling differences between class and interface methods, and analyzing tie-breakers (dispatch precedence) with overriding class methods to preserve type-correctness and confirm semantics preservation. In this paper, we present an efficient, fully-Automated, type constraint-based refactoring approach that assists developers in taking advantage of enhanced interfaces for their legacy Java software. The approach features an extensive rule set that covers various corner-cases where default methods cannot be used. To demonstrate applicability, we implemented our approach as an Eclipse plug-in and applied it to 19 real-world Java projects, as well as submitted pull requests to popular GitHub repositories. The indication is that it is useful in migrating skeletal implementation methods to interfaces as default methods, sheds light onto the pattern's usage, and provides insight to language designers on how this new construct applies to existing software.",
      "Keywords": "default methods | interfaces | Java | refactoring",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Khatchadourian, Raffi;Masuhara, Hidehiko",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027674841",
      "Primary study DOI": "10.1109/ICSE.2017.69",
      "Title": "Optimizing Test Placement for Module-Level Regression Testing",
      "Abstract": "Modern build systems help increase developer productivityby performing incremental building and testing. Thesebuild systems view a software project as a group of interdependentmodules and perform regression test selection at themodule level. However, many large software projects have imprecisedependency graphs that lead to wasteful test executions. Ifa test belongs to a module that has more dependencies than theactual dependencies of the test, then it is executed unnecessarilywhenever a code change impacts those additional dependencies. In this paper, we formulate the problem of wasteful testexecutions due to suboptimal placement of tests in modules. We propose a greedy algorithm to reduce the number oftest executions by suggesting test movements while consideringhistorical build information and actual dependencies of tests. Wehave implemented our technique, called TestOptimizer, on topof CloudBuild, the build system developed within Microsoft overthe last few years. We have evaluated the technique on five largeproprietary projects. Our results show that the suggested testmovements can lead to a reduction of 21.66 million test executions(17.09%) across all our subject projects. We received encouragingfeedback from the developers of these projects, they accepted andintend to implement ≈80% of our reported suggestions.",
      "Keywords": "build system | module-level regression testing | regression test selection",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Shi, August;Thummalapenta, Suresh;Lahiri, Shuvendu K.;Bjorner, Nikolaj;Czerwonka, Jacek",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85025175203",
      "Primary study DOI": "10.1109/ICSE.2017.32",
      "Title": "Efficient detection of thread safety violations via coverage-guided generation of concurrent tests",
      "Abstract": "As writing concurrent programs is challenging, developers often rely on thread-safe classes, which encapsulate most synchronization issues. Testing such classes is crucial to ensure the correctness of concurrent programs. An effective approach to uncover otherwise missed concurrency bugs is to automatically generate concurrent tests. Existing approaches either create tests randomly, which is inefficient, build on a computationally expensive analysis of potential concurrency bugs exposed by sequential tests, or focus on exposing a particular kind of concurrency bugs, such as atomicity violations. This paper presents CovCon, a coverage-guided approach to generate concurrent tests. The key idea is to measure how often pairs of methods have already been executed concurrently and to focus the test generation on infrequently or not at all covered pairs of methods. The approach is independent of any particular bug pattern, allowing it to find arbitrary concurrency bugs, and is computationally inexpensive, allowing it to generate many tests in short time. We apply CovCon to 18 thread-safe Java classes, and it detects concurrency bugs in 17 of them. Compared to five state of the art approaches, CovCon detects more bugs than any other approach while requiring less time. Specifically, our approach finds bugs faster in 38 of 47 cases, with speedups of at least 4x for 22 of 47 cases.",
      "Keywords": "Concurrency | Coverage | Test Generation",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Choudhary, Ankit;Lu, Shan;Pradel, Michael",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027688602",
      "Primary study DOI": "10.1109/ICSE.2017.74",
      "Title": "Fuzzy Fine-Grained Code-History Analysis",
      "Abstract": "Existing software-history techniques represent source-code evolution as an absolute and unambiguous mapping of lines of code in prior revisions to lines of code in subsequent revisions. However, the true evolutionary lineage of a line of code is often complex, subjective, and ambiguous. As such, existing techniques are predisposed to, both, overestimate and underestimate true evolution lineage. In this paper, we seek to address these issues by providing a more expressive model of code evolution, the fuzzy history graph, by representing code lineage as a continuous (i.e., fuzzy) metric rather than a discrete (i.e., absolute) one. Using this more descriptive model, we additionally provide a novel multi-revision code-history analysis-fuzzy history slicing. In our experiments over three real-world software systems, we found that the fuzzy history graph provides a tunable balance of precision and recall, and an overall improved accuracy over existing code-evolution models. Furthermore, we found that the use of such a fuzzy model of history provided improved accuracy for code-history analysis tasks.",
      "Keywords": "computer aided software engineering | reasoning about programs | software engineering | software maintenance",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Servant, Francisco;Jones, James A.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027727325",
      "Primary study DOI": "10.1109/ICSE.2017.22",
      "Title": "From Diversity by Numbers to Diversity as Process: Supporting Inclusiveness in Software Development Teams with Brainstorming",
      "Abstract": "Negative experiences in diverse software development teams have the potential to turn off minority participants from future team-based software development activity. We examine the use of brainstorming as one concrete team processes that may be used to improve the satisfaction of minority developers when working in a group. Situating our study in time-intensive hackathon-like environments where engagement of all team members is particularly crucial, we use a combination of survey and interview data to test our propositions. We find that brainstorming strategies are particularly effective for team members who identify as minorities, and support satisfaction with both the process and outcomes of teamwork through different mechanisms.",
      "Keywords": "brainstorming | Diversity | hackathons | satisfaction | software engineering management | teamwork",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Filippova, Anna;Trainer, Erik;Herbsleb, James D.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027690659",
      "Primary study DOI": "10.1109/ICSE.2017.33",
      "Title": "RClassify: Classifying race conditions in web applications via deterministic replay",
      "Abstract": "Race conditions are common in web applicationsbut are difficult to diagnose and repair. Although there existtools for detecting races in web applications, they all report alarge number of false positives. That is, the races they report areeither bogus, meaning they can never occur in practice, or benign, meaning they do not lead to erroneous behaviors. Since manuallydiagnosing them is tedious and error prone, reporting theserace warnings to developers would be counter-productive. Wepropose a platform-Agnostic, deterministic replay-based methodfor identifying not only the real but also the truly harmful raceconditions. It relies on executing each pair of racing events in twodifferent orders and assessing their impact on the program state:we say a race is harmful only if (1) both of the two executions arefeasible and (2) they lead to different program states. We haveevaluated our evidence-based classification method on a large setof real websites from Fortune-500 companies and demonstratedthat it significantly outperforms all state-of-The-Art techniques.",
      "Keywords": "deterministic replay | JavaScript | program repair | Race condition | web application",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Zhang, Lu;Wang, Chao",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027044493",
      "Primary study DOI": "10.1109/ICSE.2017.39",
      "Title": "Analysis and testing of notifications in android wear applications",
      "Abstract": "Android Wear (AW) is Google's platform for developing applications for wearable devices. Our goal is to make a first step toward a foundation for analysis and testing of AW apps. We focus on a core feature of such apps: notifications issued by a handheld device (e.g., a smartphone) and displayed on a wearable device (e.g., a smartwatch). We first define a formal semantics of AW notifications in order to capture the core features and behavior of the notification mechanism. Next, we describe a constraint-based static analysis to build a model of this run-Time behavior. We then use this model to develop a novel testing tool for AW apps. The tool contains a testing framework together with components to support AW-specific coverage criteria and to automate the generation of GUI events on the wearable. These contributions advance the state of the art in the increasingly important area of software for wearable devices.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Zhang, Hailong;Rountev, Atanas",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027689033",
      "Primary study DOI": "10.1109/ICSE.2017.26",
      "Title": "Search-Driven String Constraint Solving for Vulnerability Detection",
      "Abstract": "Constraint solving is an essential technique for detecting vulnerabilities in programs, since it can reason about input sanitization and validation operations performed on user inputs. However, real-world programs typically contain complex string operations that challenge vulnerability detection. State-of-The-Art string constraint solvers support only a limited set of string operations and fail when they encounter an unsupported one, this leads to limited effectiveness in finding vulnerabilities. In this paper we propose a search-driven constraint solving technique that complements the support for complex string operations provided by any existing string constraint solver. Our technique uses a hybrid constraint solving procedure based on the Ant Colony Optimization meta-heuristic. The idea is to execute it as a fallback mechanism, only when a solver encounters a constraint containing an operation that it does not support. We have implemented the proposed search-driven constraint solving technique in the ACO-Solver tool, which we have evaluated in the context of injection and XSS vulnerability detection for Java Web applications. We have assessed the benefits and costs of combining the proposed technique with two state-of-The-Art constraint solvers (Z3-str2 and CVC4). The experimental results, based on a benchmark with 104 constraints derived from nine realistic Web applications, show that our approach, when combined in a state-of-The-Art solver, significantly improves the number of detected vulnerabilities (from 4.7% to 71.9% for Z3-str2, from 85.9% to 100.0% for CVC4), and solves several cases on which the solver fails when used stand-Alone (46 more solved cases for Z3-str2, and 11 more for CVC4), while still keeping the execution time affordable in practice.",
      "Keywords": "search-based software engineering | string constraint solving | vulnerability detection",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Thome, Julian;Shar, Lwin Khin;Bianculli, Domenico;Briand, Lionel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019247447",
      "Primary study DOI": "10.1109/ICSE.2017.38",
      "Title": "LibD: Scalable and precise third-party library detection in android markets",
      "Abstract": "With the thriving of the mobile app markets, third-party libraries are pervasively integrated in the Android applications. Third-party libraries provide functionality such as advertisements, location services, and social networking services, making multi-functional app development much more productive. However, the spread of vulnerable or harmful third-party libraries may also hurt the entire mobile ecosystem, leading to various security problems. The Android platform suffers severely from such problems due to the way its ecosystem is constructed and maintained. Therefore, third-party Android library identification has emerged as an important problem which is the basis of many security applications such as repackaging detection and malware analysis. According to our investigation, existing work on Android library detection still requires improvement in many aspects, including accuracy and obfuscation resilience. In response to these limitations, we propose a novel approach to identifying third-party Android libraries. Our method utilizes the internal code dependencies of an Android app to detect and classify library candidates. Different from most previous methods which classify detected library candidates based on similarity comparison, our method is based on feature hashing and can better handle code whose package and method names are obfuscated. Based on this approach, we have developed a prototypical tool called LibD and evaluated it with an update-To-date and large-scale dataset. Our experimental results on 1,427,395 apps show that compared to existing tools, LibD can better handle multi-package third-party libraries in the presence of name-based obfuscation, leading to significantly improved precision without the loss of scalability.",
      "Keywords": "Android | Software mining | Third-Party Library",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Li, Menghao;Wang, Wei;Wang, Pei;Wang, Shuai;Wu, Dinghao;Liu, Jian;Xue, Rui;Huo, Wei",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027688963",
      "Primary study DOI": "10.1109/ICSE.2017.67",
      "Title": "Automated Transplantation and Differential Testing for Clones",
      "Abstract": "Code clones are common in software. When applying similar edits to clones, developers often find it difficult to examine the runtime behavior of clones. The problem is exacerbated when some clones are tested, while their counterparts are not. To reuse tests for similar but not identical clones, Grafter transplants one clone to its counterpart by (1) identifying variations in identifier names, types, and method call targets, (2) resolving compilation errors caused by such variations through code transformation, and (3) inserting stub code to transfer input data and intermediate output values for examination. To help developers examine behavioral differences between clones, Grafter supports fine-grained differential testing at both the test outcome level and the intermediate program state level. In our evaluation on three open source projects, Grafter successfully reuses tests in 94% of clone pairs without inducing build errors, demonstrating its automated code transplantation capability. To examine the robustness of G RAFTER, we systematically inject faults using a mutation testing tool, Major, and detect behavioral differences induced by seeded faults. Compared with a static cloning bug finder, Grafter detects 31% more mutants using the test-level comparison and almost 2X more using the state-level comparison. This result indicates that Grafter should effectively complement static cloning bug finders.",
      "Keywords": "Code Clones | Code Transplantation | Differential Testing | Test Reuse",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Zhang, Tianyi;Kim, Miryung",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019549570",
      "Primary study DOI": "10.1109/ICSE.2017.44",
      "Title": "Learning syntactic program transformations from examples",
      "Abstract": "Automatic program transformation tools can be valuable for programmers to help them with refactoring tasks, and for Computer Science students in the form of tutoring systems that suggest repairs to programming assignments. However, manually creating catalogs of transformations is complex and time-consuming. In this paper, we present REFAZER, a technique for automatically learning program transformations. REFAZER builds on the observation that code edits performed by developers can be used as input-output examples for learning program transformations. Example edits may share the same structure but involve different variables and subexpressions, which must be generalized in a transformation at the right level of abstraction. To learn transformations, REFAZER leverages state-of-The-Art programming-by-example methodology using the following key components: (a) a novel domain-specific language (DSL) for describing program transformations, (b) domain-specific deductive algorithms for efficiently synthesizing transformations in the DSL, and (c) functions for ranking the synthesized transformations. We instantiate and evaluate REFAZER in two domains. First, given examples of code edits used by students to fix incorrect programming assignment submissions, we learn program transformations that can fix other students' submissions with similar faults. In our evaluation conducted on 4 programming tasks performed by 720 students, our technique helped to fix incorrect submissions for 87% of the students. In the second domain, we use repetitive code edits applied by developers to the same project to synthesize a program transformation that applies these edits to other locations in the code. In our evaluation conducted on 56 scenarios of repetitive edits taken from three large C# open-source projects, REFAZER learns the intended program transformation in 84% of the cases using only 2.9 examples on average.",
      "Keywords": "Program Synthesis | Program transformation | Refactoring | Tutoring Systems",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Rolim, Reudismam;Soares, Gustavo;D'Antoni, Loris;Polozov, Oleksandr;Gulwani, Sumit;Gheyi, Rohit;Suzuki, Ryo;Hartmann, Bjorn",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027682332",
      "Primary study DOI": "10.1109/ICSE.2017.76",
      "Title": "The Evolution of Continuous Experimentation in Software Product Development: From Data to a Data-Driven Organization at Scale",
      "Abstract": "Software development companies are increasingly aiming to become data-driven by trying to continuously experiment with the products used by their customers. Although familiar with the competitive edge that the A/B testing technology delivers, they seldom succeed in evolving and adopting the methodology. In this paper, and based on an exhaustive and collaborative case study research in a large software-intense company with highly developed experimentation culture, we present the evolution process of moving from ad-hoc customer data analysis towards continuous controlled experimentation at scale. Our main contribution is the 'Experimentation Evolution Model' in which we detail three phases of evolution: Technical, organizational and business evolution. With our contribution, we aim to provide guidance to practitioners on how to develop and scale continuous experimentation in software organizations with the purpose of becoming data-driven at scale.",
      "Keywords": "A/B testing | continuous experimentation | continuous product innovation | customer feedback | data science | Experiment Owner | Experimentation Evolution Model | product value",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Fabijan, Aleksander;Dmitriev, Pavel;Olsson, Helena Holmstrom;Bosch, Jan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027721960",
      "Primary study DOI": "10.1109/ICSE.2017.53",
      "Title": "Challenges for static analysis of Java reflection-literature review and empirical study",
      "Abstract": "The behavior of software that uses the Java Reflection API is fundamentally hard to predict by analyzing code. Only recent static analysis approaches can resolve reflection under unsound yet pragmatic assumptions. We survey what approaches exist and what their limitations are. We then analyze how real-world Java code uses the Reflection API, and how many Java projects contain code challenging state-of-The-Art static analysis. Using a systematic literature review we collected and categorized all known methods of statically approximating reflective Java code. Next to this we constructed a representative corpus of Java systems and collected descriptive statistics of the usage of the Reflection API. We then applied an analysis on the abstract syntax trees of all source code to count code idioms which go beyond the limitation boundaries of static analysis approaches. The resulting data answers the research questions. The corpus, the tool and the results are openly available. We conclude that the need for unsound assumptions to resolve reflection is widely supported. In our corpus, reflection can not be ignored for 78% of the projects. Common challenges for analysis tools such as non-exceptional exceptions, programmatic filtering meta objects, semantics of collections, and dynamic proxies, widely occur in the corpus. For Java software engineers prioritizing on robustness, we list tactics to obtain more easy to analyze reflection code, and for static analysis tool builders we provide a list of opportunities to have significant impact on real Java code.",
      "Keywords": "Empirical Study | Java | Reflection | Static Analysis | Systematic Literature Review",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Landman, Davy;Serebrenik, Alexander;Vinju, Jurgen J.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85020744779",
      "Primary study DOI": "10.1109/ICSE.2017.70",
      "Title": "Learning to Prioritize Test Programs for Compiler Testing",
      "Abstract": "Compiler testing is a crucial way of guaranteeing the reliability of compilers (and software systems in general). Many techniques have been proposed to facilitate automated compiler testing. These techniques rely on a large number of test programs (which are test inputs of compilers) generated by some test-generation tools (e.g., CSmith). However, these compiler testing techniques have serious efficiency problems as they usually take a long period of time to find compiler bugs. To accelerate compiler testing, it is desirable to prioritize the generated test programs so that the test programs that are more likely to trigger compiler bugs are executed earlier. In this paper, we propose the idea of learning to test, which learns the characteristics of bug-revealing test programs from previous test programs that triggered bugs. Based on the idea of learning to test, we propose LET, an approach to prioritizing test programs for compiler testing acceleration. LET consists of a learning process and a scheduling process. In the learning process, LET identifies a set of features of test programs, trains a capability model to predict the probability of a new test program for triggering compiler bugs and a time model to predict the execution time of a test program. In the scheduling process, LET prioritizes new test programs according to their bug-revealing probabilities in unit time, which is calculated based on the two trained models. Our extensive experiments show that LET significantly accelerates compiler testing. In particular, LET reduces more than 50% of the testing time in 24.64% of the cases, and reduces between 25% and 50% of the testing time in 36.23% of the cases.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Chen, Junjie;Bai, Yanwei;Hao, Dan;Xiong, Yingfei;Zhang, Hongyu;Xie, Bing",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026639354",
      "Primary study DOI": "10.1109/ICSE.2017.50",
      "Title": "Travioli: A dynamic analysis for detecting data-structure traversals",
      "Abstract": "Traversal is one of the most fundamental operations on data structures, in which an algorithm systematically visits some or all of the data items of a data structure. We propose a dynamic analysis technique, called Travioli, for detecting data-structure traversals. We introduce the concept of acyclic execution contexts, which enables precise detection of traversals of arrays and linked data structures such as lists and trees in the presence of both loops and recursion. We describe how the information reported by Travioli can be used for visualizing data-structure traversals, manually generating performance regression tests, and for discovering performance bugs caused by redundant traversals. We evaluate Travioli on five real-world JavaScript programs. In our experiments, Travioli produced fewer than 4% false positives. We were able to construct performance tests for 93.75% of the reported true traversals. Travioli also found two asymptotic performance bugs in widely used JavaScript frameworks D3 and express.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Padhye, Rohan;Sen, Koushik",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027717403",
      "Primary study DOI": "10.1109/ICSE.2017.31",
      "Title": "On cross-stack configuration errors",
      "Abstract": "Today's web applications are deployed on powerful software stacks such as MEAN (JavaScript) or LAMP (PHP), which consist of multiple layers such as an operating system, web server, database, execution engine and application framework, each of which provide resources to the layer just above it. These powerful software stacks unfortunately are plagued by so-called cross-stack configuration errors (CsCEs), where a higher layer in the stack suddenly starts to behave incorrectly or even crash due to incorrect configuration choices in lower layers. Due to differences in programming languages and lack of explicit links between configuration options of different layers, sysadmins and developers have a hard time identifying the cause of a CsCE, which is why this paper (1) performs a qualitative analysis of 1,082 configuration errors to understand the impact, effort and complexity of dealing with CsCEs, then (2) proposes a modular approach that plugs existing source code analysis (slicing) techniques, in order to recommend the culprit configuration option. Empirical evaluation of this approach on 36 real CsCEs of the top 3 LAMP stack layers shows that our approach reports the misconfigured option with an average rank of 2.18 for 32 of the CsCEs, and takes only few minutes, making it practically useful.",
      "Keywords": "Empirical Study | Multi-layer Systems | PHP | Qualitative Study | Slicing | Software Configuration | Software Stack",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Sayagh, Mohammed;Kerzazi, Noureddine;Adams, Bram",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027719957",
      "Primary study DOI": "10.1109/ICSE.2017.47",
      "Title": "Exploring API embedding for API usages and applications",
      "Abstract": "Word2Vec is a class of neural network models that as being trainedfrom a large corpus of texts, they can produce for each unique word acorresponding vector in a continuous space in which linguisticcontexts of words can be observed. In this work, we study thecharacteristics of Word2Vec vectors, called API2VEC or API embeddings, for the API elements within the API sequences in source code. Ourempirical study shows that the close proximity of the API2VEC vectorsfor API elements reflects the similar usage contexts containing thesurrounding APIs of those API elements. Moreover, API2VEC can captureseveral similar semantic relations between API elements in API usagesvia vector offsets. We demonstrate the usefulness of API2VEC vectorsfor API elements in three applications. First, we build a tool thatmines the pairs of API elements that share the same usage relationsamong them. The other applications are in the code migrationdomain. We develop API2API, a tool to automatically learn the APImappings between Java and C# using a characteristic of the API2VECvectors for API elements in the two languages: semantic relationsamong API elements in their usages are observed in the two vectorspaces for the two languages as similar geometric arrangements amongtheir API2VEC vectors. Our empirical evaluation shows that API2APIrelatively improves 22.6% and 40.1% top-1 and top-5 accuracy over astate-of-The-Art mining approach for API mappings. Finally, as anotherapplication in code migration, we are able to migrate equivalent APIusages from Java to C# with up to 90.6% recall and 87.2% precision.",
      "Keywords": "API embedding | API usages | migration | Word2Vec",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Nguyen, Trong Duc;Nguyen, Anh Tuan;Phan, Hung Dang;Nguyen, Tien N.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026730061",
      "Primary study DOI": "10.1109/ICSE.2017.68",
      "Title": "Code Defenders: Crowdsourcing Effective Tests and Subtle Mutants with a Mutation Testing Game",
      "Abstract": "Writing good software tests is difficult and not every developer's favorite occupation. Mutation testing aims to help by seeding artificial faults (mutants) that good tests should identify, and test generation tools help by providing automatically generated tests. However, mutation tools tend to produce huge numbers of mutants, many of which are trivial, redundant, or semantically equivalent to the original program, automated test generation tools tend to produce tests that achieve good code coverage, but are otherwise weak and have no clear purpose. In this paper, we present an approach based on gamification and crowdsourcing to produce better software tests and mutants: The Code Defenders web-based game lets teams of players compete over a program, where attackers try to create subtle mutants, which the defenders try to counter by writing strong tests. Experiments in controlled and crowdsourced scenarios reveal that writing tests as part of the game is more enjoyable, and that playing Code Defenders results in stronger test suites and mutants than those produced by automated tools.",
      "Keywords": "crowdsourcing | gamification | mutation testing | software testing",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Rojas, Jose Miguel;White, Thomas D.;Clegg, Benjamin S.;Fraser, Gordon",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026675138",
      "Primary study DOI": "10.1109/ICSE.2017.45",
      "Title": "Precise condition synthesis for program repair",
      "Abstract": "Due to the difficulty of repairing defect, many research efforts have been devoted into automatic defect repair. Given a buggy program that fails some test cases, a typical automatic repair technique tries to modify the program to make all tests pass. However, since the test suites in real world projects are usually insufficient, aiming at passing the test suites often leads to incorrect patches. This problem is known as weak test suites or overfitting. In this paper we aim to produce precise patches, that is, any patch we produce has a relatively high probability to be correct. More concretely, we focus on condition synthesis, which was shown to be able to repair more than half of the defects in existing approaches. Our key insight is threefold. First, it is important to know what variables in a local context should be used in an 'if' condition, and we propose a sorting method based on the dependency relations between variables. Second, we observe that the API document can be used to guide the repair process, and propose document analysis technique to further filter the variables. Third, it is important to know what predicates should be performed on the set of variables, and we propose to mine a set of frequently used predicates in similar contexts from existing projects. Based on the insight, we develop a novel program repair system, ACS, that could generate precise conditions at faulty locations. Furthermore, given the generated conditions are very precise, we can perform a repair operation that is previously deemed to be too overfitting: directly returning the test oracle to repair the defect. Using our approach, we successfully repaired 18 defects on four projects of Defects4J, which is the largest number of fully automatically repaired defects reported on the dataset so far. More importantly, the precision of our approach in the evaluation is 78.3%, which is significantly higher than previous approaches, which are usually less than 40%.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Xiong, Yingfei;Wang, Jie;Yan, Runfa;Zhang, Jiachen;Han, Shi;Huang, Gang;Zhang, Lu",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027730989",
      "Primary study DOI": "10.1109/ICSE.2017.46",
      "Title": "Heuristically matching solution spaces of arithmetic formulas to efficiently reuse solutions",
      "Abstract": "Many symbolic program analysis techniques rely on SMT solvers to verify properties of programs. Despite the remarkable progress made in the development of such tools, SMT solvers still represent a main bottleneck to the scalability of these techniques. Recent approaches tackle this bottleneck by reusing solutions of formulas that recur during program analysis, thus reducing the number of queries to SMT solvers. Current approaches only reuse solutions across formulas that are equivalent to, contained in or implied by other formulas, as identified through a set of predefined rules, and cannot reuse solutions across formulas that differ in their structure, even if they share some potentially reusable solutions. In this paper, we propose a novel approach that can reuse solutions across formulas that share at least one solution, regardless of their structural resemblance. Our approach exploits a novel heuristic to efficiently identify solutions computed for previously solved formulas and most likely shared by new formulas. The results of an empirical evaluation of our approach on two different logics show that our approach can identify on average more reuse opportunities and is markedly faster than competing approaches.",
      "Keywords": "SMT solvers | SMT-based program analysis | Solution reuse | Symbolic execution",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Aquino, Andrea;Denaro, Giovanni;Pezze, Mauro",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027683092",
      "Primary study DOI": "10.1109/ICSE.2017.60",
      "Title": "A General Framework for Dynamic Stub Injection",
      "Abstract": "Stub testing is a standard technique to simulate the behavior of dependencies of an application under test such as the file system. Even though existing frameworks automate the actual stub injection, testers typically have to implement manually where and when to inject stubs, in addition to the stub behavior. This paper presents a novel framework that reduces this effort. The framework provides a domain specific language to describe stub injection strategies and stub behaviors via declarative rules, as well as a tool that automatically injects stubs dynamically into binary code according to these rules. Both the domain specific language and the injection are language independent, which enables the reuse of stubs and injection strategies across applications. We implemented this framework for both unmanaged (assembly) and managed (.NET) code and used it to perform fault injection for twelve large applications, which revealed numerous crashes and bugs in error handling code. We also show how to prioritize the analysis of test failures based on a comparison of the effectiveness of stub injection rules across applications.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Christakis, Maria;Emmisberger, Patrick;Godefroid, Patrice;Muller, Peter",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027679282",
      "Primary study DOI": "10.1109/ICSE.2017.13",
      "Title": "Detecting User Story Information in Developer-Client Conversations to Generate Extractive Summaries",
      "Abstract": "User stories are descriptions of functionality that a software user needs. They play an important role in determining which software requirements and bug fixes should be handled and in what order. Developers elicit user stories through meetings with customers. But user story elicitation is complex, and involves many passes to accommodate shifting and unclear customer needs. The result is that developers must take detailed notes during meetings or risk missing important information. Ideally, developers would be freed of the need to take notes themselves, and instead speak naturally with their customers. This paper is a step towards that ideal. We present a technique for automatically extracting information relevant to user stories from recorded conversations between customers and developers. We perform a qualitative study to demonstrate that user story information exists in these conversations in a sufficient quantity to extract automatically. From this, we found that roughly 10.2% of these conversations contained user story information. Then, we test our technique in a quantitative study to determine the degree to which our technique can extract user story information. In our experiment, our process obtained about 70.8% precision and 18.3% recall on the information.",
      "Keywords": "developer communication | productivity | software engineering | transcripts | user story generation",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Rodeghero, Paige;Jiang, Siyuan;Armaly, Ameer;McMillan, Collin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026763563",
      "Primary study DOI": "10.1109/ICSE.2017.30",
      "Title": "Statically Checking Web API Requests in JavaScript",
      "Abstract": "Many JavaScript applications perform HTTP requests to web APIs, relying on the request URL, HTTP method, and request data to be constructed correctly by string operations. Traditional compile-Time error checking, such as calling a non-existent method in Java, are not available for checking whether such requests comply with the requirements of a web API. In this paper, we propose an approach to statically check web API requests in JavaScript. Our approach first extracts a request's URL string, HTTP method, and the corresponding request data using an inter-procedural string analysis, and then checks whether the request conforms to given web API specifications. We evaluated our approach by checking whether web API requests in JavaScript files mined from GitHub are consistent or inconsistent with publicly available API specifications. From the 6575 requests in scope, our approach determined whether the request's URL and HTTP method was consistent or inconsistent with web API specifications with a precision of 96.0%. Our approach also correctly determined whether extracted request data was consistent or inconsistent with the data requirements with a precision of 87.9% for payload data and 99.9% for query data. In a systematic analysis of the inconsistent cases, we found that many of them were due to errors in the client code. The here proposed checker can be integrated with code editors or with continuous integration tools to warn programmers about code containing potentially erroneous requests.",
      "Keywords": "JavaScript | Static analysis | Web APIs",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Wittern, Erik;Ying, Annie T.T.;Zheng, Yunhui;Dolby, Julian;Laredo, Jim A.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027716023",
      "Primary study DOI": "10.1109/ICSE.2017.54",
      "Title": "Machine-Learning-Guided Selectively Unsound Static Analysis",
      "Abstract": "We present a machine-learning-based technique for selectively applying unsoundness in static analysis. Existing bug-finding static analyzers are unsound in order to be precise and scalable in practice. However, they are uniformly unsound and hence at the risk of missing a large amount of real bugs. By being sound, we can improve the detectability of the analyzer but it often suffers from a large number of false alarms. Our approach aims to strike a balance between these two approaches by selectively allowing unsoundness only when it is likely to reduce false alarms, while retaining true alarms. We use an anomaly-detection technique to learn such harmless unsoundness. We implemented our technique in two static analyzers for full C. One is for a taint analysis for detecting format-string vulnerabilities, and the other is for an interval analysis for buffer-overflow detection. The experimental results show that our approach significantly improves the recall of the original unsound analysis without sacrificing the precision.",
      "Keywords": "Bug-finding | Machine Learning | Static Analysis",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Heo, Kihong;Oh, Hakjoo;Yi, Kwangkeun",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027720002",
      "Primary study DOI": "10.1109/ICSE.2017.34",
      "Title": "Repairing event race errors by controlling nondeterminism",
      "Abstract": "Modern web applications are written in an event-driven style, in which event handlers execute asynchronously in response to user or system events. The nondeterminism arising from this programming style can lead to pernicious errors. Recent work focuses on detecting event races and classifying them as harmful or harmless. However, since modifying the source code to prevent harmful races can be a difficult and error-prone task, it may be preferable to steer away from the bad executions. In this paper, we present a technique for automated repair of event race errors in JavaScript web applications. Our approach relies on an event controller that restricts event handler scheduling in the browser according to a specified repair policy, by intercepting and carefully postponing or discarding selected events. We have implemented the technique in a tool called EventRaceCommander, which relies entirely on source code instrumentation, and evaluated it by repairing more than 100 event race errors that occur in the web applications from the largest 20 of the Fortune 500 companies. Our results show that application-independent repair policies usually suffice to repair event race errors without excessive negative impact on performance or user experience, though application-specific repair policies that target specific event races are sometimes desirable.",
      "Keywords": "Automated Repair | Event-Driven Programming | JavaScript",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Adamsen, Christoffer Quist;Moller, Anders;Karim, Rezwana;Sridharan, Manu;Tip, Frank;Sen, Koushik",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027072739",
      "Primary study DOI": "10.1109/ICSE.2017.35",
      "Title": "Making malory behave maliciously: targeted fuzzing of android execution environments",
      "Abstract": "Android applications, or apps, provide useful features to end-users, but many apps also contain malicious behavior. Modern malware makes understanding such behavior challenging by behaving maliciously only under particular conditions. For example, a malware app may check whether it runs on a real device and not an emulator, in a particular country, and alongside a specific target app, such as a vulnerable banking app. To observe the malicious behavior, a security analyst must find out and emulate all these app-specific constraints. This paper presents FuzzDroid, a framework for automatically generating an Android execution environment where an app exposes its malicious behavior. The key idea is to combine an extensible set of static and dynamic analyses through a search-based algorithm that steers the app toward a configurable target location. On recent malware, the approach reaches the target location in 75% of the apps. In total, we reach 240 code locations within an average time of only one minute. To reach these code locations, FuzzDroid generates 106 different environments, too many for a human analyst to create manually.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Rasthofer, Siegfried;Arzt, Steven;Triller, Stefan;Pradel, Michael",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027727722",
      "Primary study DOI": "10.1109/ICSE.2017.14",
      "Title": "Clone Refactoring with Lambda Expressions",
      "Abstract": "Lambda expressions have been introduced in Java 8 to support functional programming and enable behavior parameterization by passing functions as parameters to methods. The majority of software clones (duplicated code) are known to have behavioral differences (i.e., Type-2 and Type-3 clones). However, to the best of our knowledge, there is no previous work to investigate the utility of Lambda expressions for parameterizing such behavioral differences in clones. In this paper, we propose a technique that examines the applicability of Lambda expressions for the refactoring of clones with behavioral differences. Moreover, we empirically investigate the applicability and characteristics of the Lambda expressions introduced to refactor a large dataset of clones. Our findings show that Lambda expressions enable the refactoring of a significant portion of clones that could not be refactored by any other means.",
      "Keywords": "Code duplication | Lambda expressions | Refactoring",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Tsantalis, Nikolaos;Mazinanian, Davood;Rostami, Shahriar",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027703564",
      "Primary study DOI": "10.1109/ICSE.2017.11",
      "Title": "Analyzing APIs Documentation and Code to Detect Directive Defects",
      "Abstract": "Application Programming Interface (API) documents represent one of the most important references for API users. However, it is frequently reported that the documentation is inconsistent with the source code and deviates from the API itself. Such inconsistencies in the documents inevitably confuse the API users hampering considerably their API comprehension and the quality of software built from such APIs. In this paper, we propose an automated approach to detect defects of API documents by leveraging techniques from program comprehension and natural language processing. Particularly, we focus on the directives of the API documents which are related to parameter constraints and exception throwing declarations. A first-order logic based constraint solver is employed to detect such defects based on the obtained analysis results. We evaluate our approach on parts of well documented JDK 1.8 APIs. Experiment results show that, out of around 2000 API usage constraints, our approach can detect 1158 defective document directives, with a precision rate of 81.6%, and a recall rate of 82.0%, which demonstrates its practical feasibility.",
      "Keywords": "API documentation | natural language processing | static analysis",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Zhou, Yu;Gu, Ruihang;Chen, Taolue;Huang, Zhiqiu;Panichella, Sebastiano;Gall, Harald",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027696498",
      "Primary study DOI": "10.1109/ICSE.2017.48",
      "Title": "Unsupervised software-specific morphological forms inference from informal discussions",
      "Abstract": "Informal discussions on social platforms (e.g., Stack Overflow) accumulates a large body of programming knowledge in natural language text. Natural language process (NLP) techniques can be exploited to harvest this knowledge base for software engineering tasks. To make an effective use of NLP techniques, consistent vocabulary is essential. Unfortunately, the same concepts are often intentionally or accidentally mentioned in many different morphological forms in informal discussions, such as abbreviations, synonyms and misspellings. Existing techniques to deal with such morphological forms are either designed for general English or predominantly rely on domain-specific lexical rules. A thesaurus of software-specific terms and commonly-used morphological forms is desirable for normalizing software engineering text, but very difficult to build manually. In this work, we propose an automatic approach to build such a thesaurus. Our approach identifies software-specific terms by contrasting software-specific and general corpuses, and infers morphological forms of software-specific terms by combining distributed word semantics, domain-specific lexical rules and transformations, and graph analysis of morphological relations. We evaluate the coverage and accuracy of the resulting thesaurus against community-curated lists of software-specific terms, abbreviations and synonyms. We also manually examine the correctness of the identified abbreviations and synonyms in our thesaurus. We demonstrate the usefulness of our thesaurus in a case study of normalizing questions from Stack Overflow and CodeProject.",
      "Keywords": "Abbreviation | Morphological Form | Stack Overflow | Synonym | Word embedding",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Chen, Chunyang;Xing, Zhenchang;Wang, Ximing",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027705027",
      "Primary study DOI": "10.1109/ICSE.2017.71",
      "Title": "What Causes My Test Alarm? Automatic Cause Analysis for Test Alarms in System and Integration Testing",
      "Abstract": "Driven by new software development processes and testing in clouds, system and integration testing nowadays tends to produce enormous number of alarms. Such test alarms lay an almost unbearable burden on software testing engineers who have to manually analyze the causes of these alarms. The causes are critical because they decide which stakeholders are responsible to fix the bugs detected during the testing. In this paper, we present a novel approach that aims to relieve the burden by automating the procedure. Our approach, called Cause Analysis Model, exploits information retrieval techniques to efficiently infer test alarm causes based on test logs. We have developed a prototype and evaluated our tool on two industrial datasets with more than 14,000 test alarms. Experiments on the two datasets show that our tool achieves an accuracy of 58.3% and 65.8%, respectively, which outperforms the baseline algorithms by up to 13.3%. Our algorithm is also extremely efficient, spending about 0.1s per cause analysis. Due to the attractive experimental results, our industrial partner, a leading information and communication technology company in the world, has deployed the tool and it achieves an average accuracy of 72% after two months of running, nearly three times more accurate than a previous strategy based on regular expressions.",
      "Keywords": "multiclass classification | software testing | system and integration testing | test alarm analysis",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Jiang, He;Li, Xiaochen;Yang, Zijiang;Xuan, Jifeng",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027701347",
      "Primary study DOI": "10.1109/ICSE.2017.75",
      "Title": "To Type or Not to Type: Quantifying Detectable Bugs in JavaScript",
      "Abstract": "JavaScript is growing explosively and is now used in large mature projects even outside the web domain. JavaScript is also a dynamically typed language for which static type systems, notably Facebook's Flow and Microsoft's TypeScript, have been written. What benefits do these static type systems provide? Leveraging JavaScript project histories, we select a fixed bug and check out the code just prior to the fix. We manually add type annotations to the buggy code and test whether Flow and TypeScript report an error on the buggy code, thereby possibly prompting a developer to fix the bug before its public release. We then report the proportion of bugs on which these type systems reported an error. Evaluating static type systems against public bugs, which have survived testing and review, is conservative: it understates their effectiveness at detecting bugs during private development, not to mention their other benefits such as facilitating code search/completion and serving as documentation. Despite this uneven playing field, our central finding is that both static type systems find an important percentage of public bugs: both Flow 0.30 and TypeScript 2.0 successfully detect 15%!.",
      "Keywords": "Flow | JavaScript | mining software repositories | static type systems | TypeScript",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Gao, Zheng;Bird, Christian;Barr, Earl T.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027714890",
      "Primary study DOI": "10.1109/ICSE.2017.19",
      "Title": "Machine Learning-Based Detection of Open Source License Exceptions",
      "Abstract": "From a legal perspective, software licenses govern the redistribution, reuse, and modification of software as both source and binary code. Free and Open Source Software (FOSS) licenses vary in the degree to which they are permissive or restrictive in allowing redistribution or modification under licenses different from the original one(s). In certain cases, developers may modify the license by appending to it an exception to specifically allow reuse or modification under a particular condition. These exceptions are an important factor to consider for license compliance analysis since they modify the standard (and widely understood) terms of the original license. In this work, we first perform a large-scale empirical study on the change history of over 51K FOSS systems aimed at quantitatively investigating the prevalence of known license exceptions and identifying new ones. Subsequently, we performed a study on the detection of license exceptions by relying on machine learning. We evaluated the license exception classification with four different supervised learners and sensitivity analysis. Finally, we present a categorization of license exceptions and explain their implications.",
      "Keywords": "Classifiers | Empirical Studies | Software Licenses",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Vendome, Christopher;Linares-Vasquez, Mario;Bavota, Gabriele;Di Penta, Massimiliano;German, Daniel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027163553",
      "Primary study DOI": "10.1109/ICSE.2017.20",
      "Title": "Software Development Waste",
      "Abstract": "Context: Since software development is a complex socio-Technical activity that involves coordinating different disciplines and skill sets, it provides ample opportunities for waste to emerge. Waste is any activity that produces no value for the customer or user. Objective: The purpose of this paper is to identify and describe different types of waste in software development. Method: Following Constructivist Grounded Theory, we conducted a two-year five-month participant-observation study of eight software development projects at Pivotal, a software development consultancy. We also interviewed 33 software engineers, interaction designers, and product managers, and analyzed one year of retrospection topics. We iterated between analysis and theoretical sampling until achieving theoretical saturation. Results: This paper introduces the first empirical waste taxonomy. It identifies nine wastes and explores their causes, underlying tensions, and overall relationship to the waste taxonomy found in Lean Software Development. Limitations: Grounded Theory does not support statistical generalization. While the proposed taxonomy appears widely applicable, organizations with different software development cultures may experience different waste types. Conclusion: Software development projects manifest nine types of waste: building the wrong feature or product, mismanaging the backlog, rework, unnecessarily complex solutions, extraneous cognitive load, psychological distress, waiting/multitasking, knowledge loss, and ineffective communication.",
      "Keywords": "Extreme Programming | Lean Software Development | Software engineering waste",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Sedano, Todd;Ralph, Paul;Peraire, Cecile",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027675138",
      "Primary study DOI": "10.1109/ICSE.2017.25",
      "Title": "Understanding the Impressions, Motivations, and Barriers of One Time Code Contributors to FLOSS Projects: A Survey",
      "Abstract": "Successful Free/Libre Open Source Software (FLOSS) projects must attract and retain high-quality talent. Researchers have invested considerable effort in the study of core and peripheral FLOSS developers. To this point, one critical subset of developers that have not been studied are One-Time code Contributors (OTC)-those that have had exactly one patch accepted. To understand why OTCs have not contributed another patch and provide guidance to FLOSS projects on retaining OTCs, this study seeks to understand the impressions, motivations, and barriers experienced by OTCs. We conducted an online survey of OTCs from 23 popular FLOSS projects. Based on the 184 responses received, we observed that OTCs generally have positive impressions of their FLOSS project and are driven by a variety of motivations. Most OTCs primarily made contributions to fix bugs that impeded their work and did not plan on becoming long term contributors. Furthermore, OTCs encounter a number of barriers that prevent them from continuing to contribute to the project. Based on our findings, there are some concrete actions FLOSS projects can take to increase the chances of converting OTCs into long-Term contributors.",
      "Keywords": "FLOSS | Newcomers | One Time Contributors | Open source | OSS | Qualitative Research | Survey",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Lee, Amanda;Carver, Jeffrey C.;Bosu, Amiangshu",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027706903",
      "Primary study DOI": "10.1109/ICSE.2017.29",
      "Title": "ZenIDS: Introspective Intrusion Detection for PHP Applications",
      "Abstract": "Since its first appearance more than 20 years ago, PHP has steadily increased in popularity, and has become the foundation of the Internet's most popular content management systems (CMS). Of the world's 1 million most visited websites, nearly half use a CMS, and WordPress alone claims 25% market share of all websites. While their easy-To-use templates and components have greatly simplified the work of developing high quality websites, it comes at the cost of software vulnerabilities that are inevitable in such large and rapidly evolving frameworks. Intrusion Detection Systems (IDS) are often used to protect Internet-facing applications, but conventional techniques struggle to keep up with the fast pace of development in today's web applications. Rapid changes to application interfaces increase the workload of maintaining an IDS whitelist, yet the broad attack surface of a web application makes for a similarly verbose blacklist. We developed ZenIDS to dynamically learn the trusted execution paths of an application during a short online training period and report execution anomalies as potential intrusions. We implement ZenIDS as a PHP extension supported by 8 hooks instrumented in the PHP interpreter. Our experiments demonstrate its effectiveness monitoring live web traffic for one year to 3 large PHP applications, detecting malicious requests with a false positive rate of less than.01% after training on fewer than 4,000 requests. ZenIDS excludes the vast majority of deployed PHP code from the whitelist because it is never used for valid requests-yet could potentially be exploited by a remote adversary. We observe 5% performance overhead (or less) for our applications vs. an optimized vanilla LAMP stack.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Hawkins, Byron;Demsky, Brian",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027708416",
      "Primary study DOI": "10.1109/ICSE.2017.43",
      "Title": "Feedback-based debugging",
      "Abstract": "Software debugging has long been regarded as a time and effort consuming task. In the process of debugging, developers usually need to manually inspect many program steps to see whether they deviate from their intended behaviors. Given that intended behaviors usually exist nowhere but in human mind, the automation of debugging turns out to be extremely hard, if not impossible. In this work, we propose a feedback-based debugging approach, which (1) builds on light-weight human feedbacks on a buggy program and (2) regards the feedbacks as partial program specification to infer suspicious steps of the buggy execution. Given a buggy program, we record its execution trace and allow developers to provide light-weight feedback on trace steps. Based on the feedbacks, we recommend suspicious steps on the trace. Moreover, our approach can further learn and approximate bug-free paths, which helps reduce required feedbacks to expedite the debugging process. We conduct an experiment to evaluate our approach with simulated feedbacks on 3409 mutated bugs across 3 open source projects. The results show that our feedback-based approach can detect 92.8% of the bugs and 65% of the detected bugs require less than 20 feedbacks. In addition, we implement our proof-of-concept tool, Microbat, and conduct a user study involving 16 participants on 3 debugging tasks. The results show that, compared to the participants using the baseline tool, Whyline, the ones using Microbat can spend on average 55.8% less time to locate the bugs.",
      "Keywords": "Approximation | Debugging | Feedback | Path Pattern | Slicing",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Lin, Yun;Sun, Jun;Xue, Yinxing;Liu, Yang;Dong, Jinsong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027701767",
      "Primary study DOI": "10.1109/ICSE.2017.10",
      "Title": "Can Latent Topics in Source Code Predict Missing Architectural Tactics?",
      "Abstract": "Architectural tactics such as heartbeat, resource pooling, and scheduling provide solutions to satisfy reliability, security, performance, and other critical characteristics of a software system. Current design practices advocate rigorous up-front analysis of the system's quality concerns to identify tactics and where in the code they should be used. In this paper, we explore a bottom-up approach to recommend architectural tactics based on latent topics discovered in the source code of projects. We present a recommender system developed by building predictor models which capture relationships between topical concepts in source code and the use of specific architectural tactics in that code. Based on an extensive analysis of over 116,000 open source systems, we identify significant correlations between latent topics in source code and the usage of architectural tactics. We use this information to construct a predictor for generating tactic recommendations. Our approach is validated through a series of experiments which demonstrate the ability to generate package-level tactic recommendations. We provide further validation via two large-scale studies of Apache Hive and Hadoop to illustrate that our recommender system predicts tactics that are actually implemented by developers in later releases.",
      "Keywords": "Architectural design and implementation | emergent design | tactic recommender",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Gopalakrishnan, Raghuram;Sharma, Palak;Mirakhorli, Mehdi;Galster, Matthias",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027675453",
      "Primary study DOI": "10.1109/ICSE.2017.18",
      "Title": "Recommending and Localizing Change Requests for Mobile Apps Based on User Reviews",
      "Abstract": "Researchers have proposed several approaches to extract information from user reviews useful for maintaining and evolving mobile apps. However, most of them just perform automatic classification of user reviews according to specific keywords (e.g., bugs, features). Moreover, they do not provide any support for linking user feedback to the source code components to be changed, thus requiring a manual, time-consuming, and error-prone task. In this paper, we introduce ChangeAdvisor, a novel approach that analyzes the structure, semantics, and sentiments of sentences contained in user reviews to extract useful (user) feedback from maintenance perspectives and recommend to developers changes to software artifacts. It relies on natural language processing and clustering algorithms to group user reviews around similar user needs and suggestions for change. Then, it involves textual based heuristics to determine the code artifacts that need to be maintained according to the recommended software changes. The quantitative and qualitative studies carried out on 44,683 user reviews of 10 open source mobile apps and their original developers showed a high accuracy of ChangeAdvisor in (i) clustering similar user change requests and (ii) identifying the code components impacted by the suggested changes. Moreover, the obtained results show that ChangeAdvisor is more accurate than a baseline approach for linking user feedback clusters to the source code in terms of both precision (+47%) and recall (+38%).",
      "Keywords": "Impact Analysis | Mining User Reviews | Mobile Apps | Natural Language Processing",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Palomba, Fabio;Salza, Pasquale;Ciurumelea, Adelina;Panichella, Sebastiano;Gall, Harald;Ferrucci, Filomena;De Lucia, Andrea",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026802755",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.16",
      "Title": "Taming google-scale continuous testing",
      "Abstract": "Growth in Google's code size and feature churn rate has seen increased reliance on continuous integration (CI) and testing to maintain quality. Even with enormous resources dedicated to testing, we are unable to regression test each code change individually, resulting in increased lag time between code check-ins and test result feedback to developers. We report results of a project that aims to reduce this time by: (1) controlling test workload without compromising quality, and (2) distilling test results data to inform developers, while they write code, of the impact of their latest changes on quality. We model, empirically understand, and leverage the correlations that exist between our code, test cases, developers, programming languages, and code-change and test-execution frequencies, to improve our CI and development processes. Our findings show: Very few of our tests ever fail, but those that do are generally 'closer' to the code they test, certain frequently modified code and certain users/tools cause more breakages, and code recently modified by multiple developers (more than 3) breaks more often.",
      "Keywords": "continuous integration | selection | software testing",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Memon, Atif;Gao, Zebao;Nguyen, Bao;Dhanda, Sanjeev;Nickell, Eric;Siemborski, Rob;Micco, John",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021833820",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.10",
      "Title": "Software quality concerns in the Italian bank sector: The emergence of a meta-quality dimension",
      "Abstract": "This paper reports on a Delphi-like study about the Italian banking IT sector's greatest concerns. A new research framework was developed to pursue this vertical study: Domain and country specific, using a Mixed Methods approach. Data collection was drawn in four phases starting with a high level randomly stratified panel of 13 senior managers and then a target-panel of 124 carefully selected and well-informed domain experts. We have identified and dealt with 15 concerns about the present situation, they were discussed in a framework inspired by the ISO 25010 standard. After having mapped the concerns within the ISO standard, we identified the emergence of a new meta quality dimension which impacts both on software quality and architectural description. Our inductive outcome lets this meta dimension emerge connecting both ISO 25010 and ISO 42010 standards.",
      "Keywords": "Delphi Study | Information Systems | Mixed Methods | Software Quality",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Russo, Daniel;Ciancarini, Paolo;Falasconi, Tommaso;Tomasi, Massimo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026843019",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.24",
      "Title": "When prototyping meets storytelling: Practices and malpractices in innovating software firms",
      "Abstract": "Storytelling is an important but often underestimated practice in software engineering. Whereas existing research widely regards storytelling as creating a common understanding between developers and users, we argue that storytelling and prototyping are intertwined practices for innovators to persuade decision makers. Based on a two-year qualitative case study in two innovating software firms, we identify and dialectically examine practices of storytelling and prototyping. Our study implies that storytelling and prototyping should be integrated together into software engineering methods.",
      "Keywords": "Case Study | Innovation | Malpractices | Practices | Prototyping | Software Engineering | Storytelling",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Ciriello, Raffaele Fabio;Richter, Alexander;Schwabe, Gerhard",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026828496",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.5",
      "Title": "Zero-downtime SQL database schema evolution for continuous deployment",
      "Abstract": "When a web service or application evolves, its database schema - tables, constraints, and indices - often need to evolve along with it. Depending on the database, some of these changes require a full table lock, preventing the service from accessing the tables under change. To deal with this, web services are typically taken offline momentarily to modify the database schema. However with the introduction of concepts like Continuous Deployment, web services are deployed into their production environments every time the source code is modified. Having to take the service offline - potentially several times a day - to perform schema changes is undesirable. In this paper we introduce QuantumDB - a tool-supported approach that abstracts this evolution process away from the web service without locking tables. This allows us to redeploy a web service without needing to take it offline even when a database schema change is necessary. In addition QuantumDB puts no restrictions on the method of deployment, supports schema changes to multiple tables using changesets, and does not subvert foreign key constraints during the evolution process. We evaluate QuantumDB by applying 19 synthetic and 95 industrial evolution scenarios to our open source implementation of QuantumDB. These experiments demonstrate that QuantumDB realizes zero downtime migrations at the cost of acceptable overhead, and is applicable in industrial continuous deployment contexts.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "De Jong, Michael;Van Deursen, Arie;Cleve, Anthony",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026827677",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.30",
      "Title": "An empirical study of search-based task scheduling in global software development",
      "Abstract": "Scheduling tasks is one of the critical duties of software project managers. The main objective of the scheduling is typically reducing the project's cost and duration. However, the numerous possible assignments of tasks to the team members and the dependencies between tasks make task scheduling an NP-hard problem. In the context of Global Software Development (GSD) projects, specifically, reducing the development time is one of the cornerstones. However, some of the GSD characteristics such as having people from different locations, working in different time zones, and perhaps involved in the same software tasks (Follow the Sun approach) make the scheduling even more difficult for the manager. Recently, several techniques based on evolutionary search algorithms have been proposed to automatically optimize the task scheduling in traditional software development projects. In this paper, we apply the same concepts in the context of GSD projects. We have implemented a Genetic algorithm-based assignment technique that uses a queue-based GSD simulator for fitness function evaluation. Our technique has been evaluated based on three project's datasets from two large-scale organizations that practice GSD. The results show that the search-based approach can in some cases improve the assignments compared to the actual assignments by the managers, in terms of reducing the projects duration. We also report the actual project managers' feedback on the automatic assignments.",
      "Keywords": "follow the sun | genetic algorithm | global software development | project management | task scheduling",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Kroll, Josiane;Friboim, Shai;Hemmati, Hadi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026830618",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.8",
      "Title": "Domain adaptation for test report classification in crowdsourced testing",
      "Abstract": "In crowdsourced testing, it is beneficial to automatically classify the test reports that actually reveal a fault - a true fault, from the large number of test reports submitted by crowd workers. Most of the existing approaches toward this task simply leverage historical data to train a machine learning classifier and classify the new incoming reports. However, our observation on real industrial data reveals that projects under crowdsourced testing come from various domains, and the submitted reports usually contain different technical terms to describe the software behavior for each domain. The different data distribution across domains could significantly degrade the performance of classification models when utilized for cross-domain report classification. To build an effective cross-domain classification model, we leverage deep learning to discover the intermediate representation that is shared across domains, through the co-occurrence between domain-specific terms and domain-unaware terms. Specifically, we use the Stacked Denoising Autoencoders to automatically learn the high-level features from raw textual terms, and utilize these features for classification. Our evaluation on 58 commercial projects of 10 domains from one of the Chinese largest crowdsourced testing platforms shows that our approach can generate promising results, compared to three commonly-used and state-of-the-art baselines. Moreover, we also evaluate its usefulness using real-world case studies. The feedback from real-world testers demonstrates its practical value.",
      "Keywords": "Crowdsourced testing | deep learning | domain adaptation | test report classification",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Wang, Junjie;Cui, Qiang;Wang, Song;Wang, Qing",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026809253",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.25",
      "Title": "Scaling agile development in mechatronic organizations - A comparative case study",
      "Abstract": "Agile software development principles enable companies to successfully and quickly deliver software by meeting their customers' expectations while focusing on high quality. Many companies working with pure software systems have adopted these principles, but implementing them in companies dealing with non-pure software products is challenging. We identified a set of goals and practices to support large-scale agile development in companies that develop software-intense mechatronic systems. We used an inductive approach based on empirical data collected during a longitudinal study with six companies in the Nordic region. The data collection took place over two years through focus group workshops, individual on-site interviews, and complementary surveys. The primary benefit of large-scale agile development is improved quality, enabled by practices that support regular or continuous integration between teams delivering software, hardware, and mechanics. In this regard, the most beneficial integration cycle for deliveries is every four weeks, while continuous integration on a daily basis would favor software teams, other disciplines does not seem to benefit from faster integration cycles. We identified 108 goals and development practices supporting agile principles among the companies, most of them concerned with integration, therefrom, 26 agile practices are unique to the mechatronics domain to support adopting agile beyond pure software development teams. 16 of these practices are considered as key enablers, confirmed by our control cases.",
      "Keywords": "agile software development | embedded software | mechatronics | software engineering | system integration | testing",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Eklund, Ulrik;Berger, Christian",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026819297",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.14",
      "Title": "Supporting defect causal analysis in practice with cross-company data on causes of requirements engineering problems",
      "Abstract": "[Context] Defect Causal Analysis (DCA) represents an efficient practice to improve software processes. While knowledge on cause-effect relations is helpful to support DCA, collecting cause-effect data may require significant effort and time. [Goal] We propose and evaluate a new DCA approach that uses cross-company data to support the practical application of DCA. [Method] We collected cross-company data on causes of requirements engineering problems from 74 Brazilian organizations and built a Bayesian network. Our DCA approach uses the diagnostic inference of the Bayesian network to support DCA sessions. We evaluated our approach by applying a model for technology transfer to industry and conducted three consecutive evaluations: (i) in academia, (ii) with industry representatives of the Fraunhofer Project Center at UFBA, and (iii) in an industrial case study at the Brazilian National Development Bank (BNDES). [Results] We received positive feedback in all three evaluations and the cross-company data was considered helpful for determining main causes. [Conclusions] Our results strengthen our confidence in that supporting DCA with cross-company data is promising and should be further investigated.",
      "Keywords": "Bayesian network | case study | cross-company data | defect causal analysis | requirements engineering | technology transfer",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Kalinowski, Marcos;Curty, Pablo;Paes, Aline;Ferreira, Alexandre;Spinola, Rodrigo;Fernandez, Daniel Mendez;Felderer, Michael;Wagner, Stefan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026799522",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.31",
      "Title": "What is the perception of female and male software professionals on performance, team dynamics and job satisfaction? Insights from the trenches",
      "Abstract": "Research has shown that gender diversity correlates positively with innovation and productivity in many professional engineering and technology domains. Yet, software development teams are dominated by males. In this paper, we aim at understanding whether female software professionals, compared to male, have different perceptions on a) team performance and dynamics, b) their own personal performance, c) their immediate supervisors, and d) accomplishment, recognition, and opportunities. Understanding perceptions of different genders can help software professionals, their supervisors and those responsible for staff create and foster environments in which both females and males are comfortable and perform best. To achieve this aim, we conducted a survey targeted at individual software professionals in technical roles. We collected and analyzed data from 55 female and 69 male respondents. Our results show basic differences in demographics (e.g., males tend to be older, have more senior roles, and have longer tenure with their employer). While we did find some differences around perceptions of spirit of team work, productivity, sense of satisfaction and fairness of reviews from supervisors, in general, females and males do not seem to differ significantly in their perceptions. Based on the results from our survey and insights from the current literature, we discuss commonalities and differences between females and males, and explore potential implications for performance reviews, recognition, and career progression.",
      "Keywords": "descriptive survey | diversity | female and male software professionals | perceptions | software development teams",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "James, Toni;Galster, Matthias;Blincoe, Kelly;Miller, Grant",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026813835",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.26",
      "Title": "Analytics-driven load testing: An industrial experience report on load testing of large-scale systems",
      "Abstract": "Assessing how large-scale software systems behave under load is essential because many problems cannot be uncovered without executing tests of large volumes of concurrent requests. Load-related problems can directly affect the customer-perceived quality of systems and often cost companies millions of dollars. Load testing is the standard approach for assessing how a system behaves under load. However, designing, executing and analyzing a load test can be very difficult due to the scale of the test (e.g., simulating millions of users and analyzing terabytes of data). Over the past decade, we have tackled many load testing challenges in an industrial setting. In this paper, we document the challenges that we encountered and the lessons that we learned as we addressed these challenges. We provide general guidelines for conducting load tests using an analytics-driven approach. We also discuss open research challenges that require attention from the research community. We believe that our experience can be beneficial to practitioners and researchers who are interested in the area of load testing.",
      "Keywords": "load testing | mining software repositories | performance testing | test analysis",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Chen, Tse Hsun;Syer, Mark D.;Shang, Weiyi;Jiang, Zhen Ming;Hassan, Ahmed E.;Nasser, Mohamed;Flora, Parminder",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026811222",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.29",
      "Title": "A three-year participant observation of software startup software evolution",
      "Abstract": "This paper presents a three-year participant observation in which the author acted as CTO of a software startup, spanning more than 9,000 hours of direct experience. The author's emails and diary reflections were analyzed and synthesized into a set of nine claims about software engineering work. These claims help shape software engineering research, practice, and education by provoking new questions about what makes software engineering difficult.",
      "Keywords": "human factors | Management | project management",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Ko, Andrew J.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026844670",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.12",
      "Title": "Exception beyond exception: Crashing android system by trapping in 'uncaught exception'",
      "Abstract": "Android is characterized as a complicated open source software stack created for a wide array of devices with different form of factors, whose latest release has over one hundred million lines of code. Such code is mainly developed with the Java language, which builds complicated logic and brings implicit information flows among components and the inner framework. By studying the source code of system service interfaces, we discovered an unknown type of code flaw, which is named uncaughtException flaw, caused by un-well implemented exceptions that could crash the system and be further vulnerable to system level Denial-of-Service (DoS) attacks. We found that exceptions are used to handle the errors and other exceptional events but sometimes they would kill some critical system services exceptionally. We designed and implemented ExHunter, a new tool for automatic detection of this uncaughtException flaw by dynamically reflecting service interfaces, continuously fuzzing parameters and verifying the running logs. On 11 new popular Android devices, ExHunter extracted 1045 system services, reflected 758 suspicious functions, discovered 132 uncaughtException flaws which are 0-day vulnerabilities that have never been known before and generated 275 system DoS attack exploitations. The results showed that: (1) almost every type of Android phone suffers from this flaw, (2) the flaws are different from phone by phone, and (3) all the vulnerabilities can be exploited by direct/indirect trapping. To mitigate uncaughtException flaws, we further developed ExCatcher to re-catch the exceptions. Finally, we informed four internationally renowned manufacturers and provided secure improvements in their commercial phones.",
      "Keywords": "Android System Service | DoS Attack | Exception | Vulnerability",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Wu, Jingzheng;Liu, Shen;Ji, Shouling;Yang, Mutian;Luo, Tianyue;Wu, Yanjun;Wang, Yongji",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018435815",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.15",
      "Title": "Reducing failure analysis time: An industrial evaluation",
      "Abstract": "Testing and debugging automotive cyber physical systems are challenging. Developing and integrating cyber and physical components require extensive testing to ensure reliable and safe releases. One important cost factor in the debugging process is the time required to analyze failures. Since large number of failures usually happen due to a few underlying faults, clustering failures based on the responsible faults helps reduce analysis time. We focus on the software-in-the-loop and hardware-in-the-loop levels of testing where test execution times are high. We devise a methodology for adapting existing clustering techniques to a real context. We augment an existing clustering approach by a method for selecting representative tests. To analyze failures, rather than investigating all failing tests one by one, testers inspect only these representatives. We report on the results of a large scale industrial case study. We ran experiments on ca. 850 KLOC. Results show that utilizing our clustering tool, testers can reduce failure analysis time by more than 80%.",
      "Keywords": "automotive CPS | failure analysis | failure clustering | HiL testing | SiL testing",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Golagha, Mojdeh;Pretschner, Alexander;Fisch, Dominik;Nagy, Roman",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85025809293",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.7",
      "Title": "Collaborative identification of code smells: A multi-case study",
      "Abstract": "Code smells are anomalous program structures that may indicate software maintainability problems. God Classes and Feature Envies are examples of code smells that frequently become the target of software refactoring. However, smell identification might be harder than expected due to the subjectivity involved in the recognition of the apparently simple structure of each smell. Moreover, smell identification might require the knowledge of multiple program elements, which are better understood by different developers. Thus, the use of collaboration among developers may have the potential to improve effectiveness on smell identification. However, current knowledge, especially empirically developed and evaluated in the industry, is quite scarce. This paper reports an industrial case study aimed at observing how 13 developers individually and collaboratively performed smell identification in five software projects from two software development organizations. Our results suggest that collaboration contributes to improving effectiveness on the identification of a wide range of code smells. We also analyzed the factors contributing to such effectiveness improvement.",
      "Keywords": "Case Study | Collaboration | Identification of Code Smells | Qualitative Analysis",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Oliveira, Roberto;Sousa, Leonardo;De Mello, Rafael;Valentim, Natasha;Lopes, Adriana;Conte, Tayana;Garcia, Alessandro;Oliveira, Edson;Lucena, Carlos",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026753603",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.32",
      "Title": "Automated test input generation for android: Towards getting there in an industrial case",
      "Abstract": "Monkey, a random testing tool from Google, has been popularly used in industrial practices for automatic test input generation for Android due to its applicability to a variety of application settings, e.g., ease of use and compatibility with different Android platforms. Recently, Monkey has been under the spotlight of the research community: Recent studies found out that none of the studied tools from the academia were actually better than Monkey when applied on a set of open source Android apps. Our recent efforts performed the first case study of applying Monkey on WeChat, a popular messenger app with over 800 million monthly active users, and revealed many limitations of Monkey along with developing our improved approach to alleviate some of these limitations. In this paper, we explore two optimization techniques to improve the effectiveness and efficiency of our previous approach. We also conduct manual categorization of not-covered activities and two automatic coverage-analysis techniques to provide insightful information about the not-covered code entities. Lastly, we present findings of our empirical studies of conducting automatic random testing on WeChat with the preceding techniques.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Zheng, Haibing;Li, Dengfeng;Liang, Beihai;Zeng, Xia;Zheng, Wujie;Deng, Yuetang;Lam, Wing;Yang, Wei;Xie, Tao",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026801022",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.20",
      "Title": "Meeting industry-academia research collaboration challenges with agile methodologies",
      "Abstract": "Continuous and long-term collaboration between industry and academia is crucial to develop front-line research in context-dependent areas like software development where both practitioners and researchers are searched for data collection, analysis and results. Despite many mutual benefits, this collaboration is often challenging, not only due to different goals, but also because of different pace in providing the results. The software development industry has during the last decade aligned around and organized their development adopting agile methodologies. For the researchers, the agile methodologies are a topic for a research, rather than a means of performing the research itself. We can state a question, whether the agile methodologies can be a good common ground for enabling successful research collaboration between industry and academia? This paper reports on a longitudinal industry - academia research collaboration case, which has stepwise adapted SCRUM over a six-year period. The implementation of SCRUM and the collaboration successes and challenges are presented, and findings are discussed.",
      "Keywords": "agile methodologies | case study | collaboration | industry-academia research",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Sandberg, Anna Borjesson;Crnkovic, Ivica",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85025833031",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.18",
      "Title": "Crunch time: The reasons and effects of unpaid overtime in the games industry",
      "Abstract": "The games industry is notorious for its intense work ethics with uncompensated overtime and weekends at the office, also known as crunch or crunch time. Since crunch time is so common within the industry, is it possible that the benefits of crunch time outweigh the disadvantages? By studying postmortems and conducting interviews with employees in the industry, we aim to characterise crunch time and discover its effects on the industry. We provide a classification of crunch, i.e., four types of crunch which all have distinct characteristics and affect the product, employees and schedule in various ways. One of the crunch types stands out from the others by only having positive effects on product and schedule. A characteristic that all of the types have in common is an increase in stress levels amongst the employees. We identify a set of reasons for crunch and show that crunch is less pronounced in game studios where prioritisation of features is a regular practice.",
      "Keywords": "Agile development | Games industry | Human factors | Stress | Sustainable pace | Types of crunch | Unpaid overtime",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Edholm, Henrik;Lidstrom, Mikaela;Steghofer, Jan Philipp;Burden, Hakan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026848003",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.21",
      "Title": "Towards continuous delivery by reducing the feature freeze period: A case study",
      "Abstract": "Today, many software companies continuously deliver and deploy new features to their customers. However, many software systems are still released traditionally with long feature freeze periods and time-based releases due to historical reasons. Currently, only a few empirical inquiries of transformations towards continuous delivery exist. In this paper, we aim to understand how feature freeze was practiced and the feature freeze period reduced in an R&D program at Ericsson. The case organization has struggled with the feature freeze approach and is now moving towards the continuous delivery paradigm. We investigated the intended and actual effects of the feature freeze practice, how the feature freeze period was reduced and what effects the reduction had. We interviewed 11 employees, covering all the development teams at the largest site of the distributed organization. In addition, we analyzed data from software repositories to get quantitative triangulation of the qualitative results. Historically, the organization was not able to comply with the intended feature freeze practice, due to pressure for new feature development and long feature freeze periods leaving little time to perform actual development. By implementing test automation, the organization was able to reduce the feature freeze period by 56%, after which the amount of changes during the freeze decreased by 63% and the amount of changes close to the release date by 59%. We conclude that reducing the feature freeze period is possible using test automation, and reducing the freeze time can increase conformance to the intended feature freeze practice. To further reduce feature freeze, attention must be paid to deployment automation and collaboration between development and operations, in addition to test automation.",
      "Keywords": "case study | code freeze | continuous delivery | continuous deployment | continuous integration | devops | feature freeze | release stabilization",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Laukkanen, Eero;Paasivaara, Maria;Itkonen, Juha;Lassenius, Casper;Arvonen, Teemu",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026809849",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.28",
      "Title": "Practices and perceptions of UML use in open source projects",
      "Abstract": "Context: Open Source is getting more and more collaborative with industry. At the same time, modeling is today playing a crucial role in development of, e.g., safety critical software. Goal: However, there is a lack of research about the use of modeling in Open Source. Our goal is to shed some light into the motivation and benefits of the use of modeling and its use within project teams. Method: In this study, we perform a survey among Open Source developers. We focus on projects that use the Unified Modeling Language (UML) as a representative for software modeling. Results: We received 485 answers of contributors of 458 different Open Source projects. Conclusion: Collaboration seems to be the most important motivation for using UML. It benefits new contributors and contributors who do not create models. Teams use UML during communication and planning of joint implementation efforts.",
      "Keywords": "architecture documentation | GitHub | impacts of UML | OSS projects | team communication | UML | UML use",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Ho-Quang, Truong;Hebig, Regina;Robles, Gregorio;Chaudron, Michel R.V.;Fernandez, Miguel Angel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026840620",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.33",
      "Title": "Agile cultural challenges in Europe and Asia: Insights from practitioners",
      "Abstract": "The challenges of adopting agile software development methods vary from one context to another. This work investigates how the European and Asian cultural backgrounds may impact agile practices adoption. The focus is on three countries: Belgium, Malaysia and Singapore. We gathered data about practices, challenges and impediments encountered by software development teams from interviews of 19 practitioners and two agile software development events (discussion groups). The results of the analysis are prioritized and discussed using the Hofstede Model for national cultures comparison. The identified hypotheses can serve as a guidance for contextual agile practices improvement. The results need to be considered carefully since more empirical data are required to confirm the practitioners' opinion. In fact, studying socio-cultural differences is sensitive. Yet such information may be valuable for onshore teams (culturally homogeneous) as well as teams working on globally distributed projects.",
      "Keywords": "Agile Software Development | Challenges | Cultural Differences | Process Improvement",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Ayed, Hajer;Vanderose, Benoit;Habra, Naji",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026821101",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.23",
      "Title": "Crowd Advisor: A framework for freelancer assessment in online marketplace",
      "Abstract": "Hiring is one of the important challenges in the context of online labor marketplace. Unlike traditional hiring, where workers are hired either as a full time employee or as a contractor, hiring from online marketplaces are done for individual jobs of short duration. As these marketplaces are open for anyone, hiring becomes challenging due to the large number of freelancers applying for a posted job. Quite often, clients use ratings of the freelancers while hiring. However, we have observed that ratings are skewed towards higher values and do not provide valuable insights about freelancers' abilities to do a quality work. Therefore, we propose a multidimensional assessment framework which evaluates freelancers on several dimensions. The proposed framework, not only uses the current information about the freelancer, but also utilizes the past jobs he has performed. The framework is evaluated on the data collected from a popular online marketplace. Our analysis, performed on 7254 jobs and 96,271 applicants, shows that the assessment made by the proposed framework outperforms the baseline algorithm.",
      "Keywords": "Crowdsourcing | Data Analytics | Freelancing | Recommendation Systems | Software Engineering",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Abhinav, Kumar;Dubey, Alpana;Jain, Sakshi;Virdi, Gurdeep;Kass, Alex;Mehta, Manish",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026802069",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.6",
      "Title": "Transferring code-clone detection and analysis to practice",
      "Abstract": "During software development, code clones are commonly produced, in the form of a number of the same or similar code fragments spreading within one or many large code bases. Numerous research projects have been carried out on empirical studies or tool support for detecting or analyzing code clones. However, in practice, few such research projects have resulted in substantial industry adoption. In this paper, we report our experiences of transferring XIAO, a code-clone detection and analysis approach and its supporting tool, to road industrial practices: (1) shipped in Visual Studio 2012, a widely used industrial IDE, (2) deployed and intensively used at the Microsoft Security Response Center. According to our experiences, technology transfer is a rather complicated journey that needs significant efforts from both the technical aspect and social aspect. From the technical aspect, significant efforts are needed to adapt a research prototype to a product-quality tool that addresses the needs of real scenarios, to be integrated into a mainstream product or development process. From the social aspect, there are strong needs to interact with practitioners to identify killer scenarios in industrial settings, figure out the gap between a research prototype and a tool fitting the needs of real scenarios, to understand the requirements of releasing with a mainstream product, being integrated into a development process, understanding their release cadence, etc.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Dang, Yingnong;Zhang, Dongmei;Ge, Song;Huang, Ray;Chu, Chengyun;Xie, Tao",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026830461",
      "Primary study DOI": "",
      "Title": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Abstract": "The proceedings contain 30 papers. The topics discussed include: what is the perception of female and male software professionals on performance, team dynamics and job satisfaction? insights from the trenches; towards continuous delivery by reducing the feature freeze period: a case study; transferring code-clone detection and analysis to practice; software quality concerns in the Italian bank sector: the emergence of a meta-quality dimension; CrowdAdvisor: a framework for freelancer assessment in online marketplace; leveraging crowdsourcing for team elasticity: an empirical evaluation at topcoder; who broke the build? automatically identifying changes that induce test failures in continuous integration at Google scale; characterizing experimentation in continuous deployment: a case study on Bing; online robustness testing of distributed embedded systems: an industrial approach; agile cultural challenges in Europe and Asia: insights from practitioners; scaling agile development in mechatronic organizations-a comparative case study; context-based analytics-establishing explicit links between runtime traces and source code; automated example oriented REST API documentation at Cisco; and supporting defect causal analysis in practice with cross-company data on causes of requirements engineering problems.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026844028",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.17",
      "Title": "Online robustness testing of distributed embedded systems: An industrial approach",
      "Abstract": "Having robust systems that behave properly even in presence of faults is becoming increasingly important. This is the case of the system we investigate in this paper, which is an embedded distributed system consisting of components that communicate with each other via messages exchange in the RBS (Radio Based Station) at Ericsson AB in Gothenburg, Sweden. Specifically, this paper describes a novel fault injection approach for testing the robustness of distributed embedded systems with very limited computation power. The new approach is inspired by Netflix's ChaosMonkey, a fault injection approach that has been developed for testing distributed systems hosted in the cloud. However, ChaosMonkey cannot be used in the context of RBS since the latter consists of small-embedded components with specific requirements of performance, programming language, and communication paradigm. This paper reports about the approach called Postmonkey we developed, illustrates the results of applying it to RBS, and discusses the potential of utilizing fault injection to test complex, embedded, and distributed systems. The approach and tool are now adopted by Ericsson.",
      "Keywords": "distributed embedded systems | fault injection | online testing",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Alnawasreh, Khaled;Pelliccione, Patrizio;Hao, Zhenxiao;Range, Marten;Bertolino, Antonia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026798409",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.2",
      "Title": "Leveraging crowdsourcing for team elasticity: An empirical evaluation at topcoder",
      "Abstract": "There is an emergent trend in software development projects that mini-tasks can be crowdsourced to achieve rapid development and delivery. For software managers requesting crowdsourcing services, it is beneficial to be able to evaluate and assure the availability and performance of trustable workers on their tasks. However, existing rating systems are facing challenges such as providing limited information regarding worker's abilities as well as potential threats from workers' gaming or cheating the systems. To develop better understanding of worker performance in software crowdsourcing, this paper reports an empirical study at TopCoder, one of the primary software crowdsourcing platforms. We aim at investigating the following questions: How diverse are crowd workers in terms of skill and experience? How fast do crowd workers respond to a task call? How reliable are crowd workers in submitting tasks? And how much does CSD benefit schedule reduction? The main results of this study showed that on average, (i) 59% of workers respond to a task call in the first 24 hours, (ii) 24% of the workers who registered early will make submissions to tasks, and 76% of them exceeding the acceptance criteria, and (iii) an overall average of 1.82 schedule acceleration rate is observed through organizing mass parallel development in 4 software crowdsourcing projects. Such empirical evidences are beneficial to help exploring resourcing options and improve team elasticity in adaptive software development.",
      "Keywords": "Agility | Crowdsourced software development | Elasticity | Topcoder | Velocity | Worker availability | Worker performance",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Saremi, Razieh L.;Yang, Ye;Ruhe, Guenther;Messinger, David",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026826017",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.11",
      "Title": "Automated example oriented REST API documentation at Cisco",
      "Abstract": "Generating and maintaining an up-to-date API documentation is a challenging problem for evolving REST APIs. At Cisco, we've used SpyREST, an automated REST API documentation tool, via our functional tests to solve this problem with one of our APIs for a cyber security application over the past eighteen months. Using this approach, we've avoided the need for extensive manual effort by leveraging our test code to also generate a continuously updated API documentation as the API evolved. Our always-updated API documentation has helped creating a fast feedback loop between the developers and QA engineers. The findings from this paper can be used by practitioners to introduce automation to reduce the manual effort associated to their REST API documentation process.",
      "Keywords": "API | Automation | Case study | Documentation | HTTP | REST | Test | Tool | Web API",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Sohan, S. M.;Anslow, Craig;Maurer, Frank",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026838215",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.1",
      "Title": "Context-based analytics - Establishing explicit links between runtime traces and source code",
      "Abstract": "Diagnosing problems in large-scale, distributed applications runningin cloud environments requires investigating different sources ofinformation to reason about application state at any given time. Typical sources of information available to developers and operatorsinclude log statements and other runtime information collectedby monitors, such as application and system metrics. Just as importantly, developers rely on information related to changes to the source code andconfiguration files (program code) when troubleshooting. This information is generally scattered, and it is up to the troubleshooterto inspect multiple implicitly-connected fragments thereof. Currently, different tools need to be used in conjunction, e.g., logaggregation tools, source-code management tools, and runtime-metricdashboards, each requiring different data sources and workflows. Notsurprisingly, diagnosing problems is a difficult proposition. In this paper, we propose Context-Based Analytics, an approach that makes the links between runtime informationand program-code fragments explicit by constructing a graph based on anapplication-context model. Implicit connections between informationfragments are explicitly represented as edges in the graph. We designeda framework for expressing application-context models and implemented a prototype. Further, we instantiated our prototype framework with an application-contextmodel for two real cloud applications, one from IBM and another from a major telecommunications provider. We applied context-based analytics to diagnose twoissues taken from the issue tracker of the IBM application and foundthat our approach reduced the effort of diagnosing these issues. In particular, context-based analytics decreased the number of required analysis steps by 48% and the number ofneeded inspected traces by 40% on average as compared to a standard diagnosis approach.",
      "Keywords": "DevOps | Runtime Information | Software Analytics",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Cito, Jurgen;Oliveira, Fabio;Leitner, Philipp;Nagpurkar, Priya;Gall, Harald C.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026838407",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.27",
      "Title": "An industrial evaluation of unit test generation: Finding real faults in a financial application",
      "Abstract": "Automated unit test generation has been extensively studied in the literature in recent years. Previous studies on open source systems have shown that test generation tools are quite effective at detecting faults, but how effective and applicable are they in an industrial application? In this paper, we investigate this question using a life insurance and pension products calculator engine owned by SEB Life & Pension Holding AB Riga Branch. To study fault-finding effectiveness, we extracted 25 real faults from the version history of this software project, and applied two up-to-date unit test generation tools for Java, EVOSUITE and RANDOOP, which implement search-based and feedback-directed random test generation, respectively. Automatically generated test suites detected up to 56.40% (EVOSUITE) and 38.00% (RANDOOP) of these faults. The analysis of our results demonstrates challenges that need to be addressed in order to improve fault detection in test generation tools. In particular, classification of the undetected faults shows that 97.62% of them depend on either 'specific primitive values' (50.00%) or the construction of 'complex state configuration of objects' (47.62%). To study applicability, we surveyed the developers of the application under test on their experience and opinions about the test generation tools and the generated test cases. This leads to insights on requirements for academic prototypes for successful technology transfer from academic research to industrial practice, such as a need to integrate with popular build tools, and to improve the readability of the generated tests.",
      "Keywords": "Automated Tests Generation | Empirical Software Engineering | Random Testing | Search-based Testing",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Almasi, M. Moein;Hemmati, Hadi;Fraser, Gordon;Arcuri, Andrea;Benefelds, Janis",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026831298",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.9",
      "Title": "Risk-based attack surface approximation: How much data is enough?",
      "Abstract": "Proactive security reviews and test efforts are a necessary component of the software development lifecycle. Resource limitations often preclude reviewing the entire code base. Making informed decisions on what code to review can improve a team's ability to find and remove vulnerabilities. Risk-based attack surface approximation (RASA) is a technique that uses crash dump stack traces to predict what code may contain exploitable vulnerabilities. The goal of this research is to help software development teams prioritize security efforts by the efficient development of a risk-based attack surface approximation. We explore the use of RASA using Mozilla Firefox and Microsoft Windows stack traces from crash dumps. We create RASA at the file level for Firefox, in which the 15.8% of the files that were part of the approximation contained 73.6% of the vulnerabilities seen for the product. We also explore the effect of random sampling of crashes on the approximation, as it may be impractical for organizations to store and process every crash received. We find that 10-fold random sampling of crashes at a rate of 10% resulted in 3% less vulnerabilities identified than using the entire set of stack traces for Mozilla Firefox. Sampling crashes in Windows 8.1 at a rate of 40% resulted in insignificant differences in vulnerability and file coverage as compared to a rate of 100%.",
      "Keywords": "attack surface | prediction models | stack traces",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Theisen, Christopher;Herzig, Kim;Murphy, Brendan;Williams, Laurie",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026847860",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.19",
      "Title": "Characterizing experimentation in continuous deployment: A case study on bing",
      "Abstract": "The practice of continuous deployment enables product teams to release content to end users within hours or days, rather than months or years. These faster deployment cycles, along with rich product instrumentation, allows product teams to capture and analyze feature usage measurements. Product teams define a hypothesis and a set of metrics to assess how a code or feature change will impact the user. Supported by a framework, a team can deploy that change to subsets of users, enabling randomized controlled experiments. Based on the impact of the change, the product team may decide to modify the change, to deploy the change to all users, or to abandon the change. This experimentation process enables product teams to only deploy the changes that positively impact the user experience. The goal of this research is to aid product teams to improve their deployment process through providing an empirical characterization of an experimentation process when applied to a large-scale and mature service. Through an analysis of 21,220 experiments applied in Bing since 2014, we observed the complexity of the experimental process and characterized the full deployment cycle (from code change to deployment to all users). The analysis identified that the experimentation process takes an average of 42 days, including multiple iterations of one or two week experiment runs. Such iterations typically indicate that problems were found that could have hurt the users or business if the feature was just launched, hence the experiment provided real value to the organization. Further, we discovered that code changes for experiments are four times larger than other code changes. We identify that the code associated with 33.4% of the experiments is eventually shipped to all users. These fully-deployed code changes are significantly larger than the code changes for the other experiments, in terms of files (35.7%), changesets (80.4%) and contributors (20.0%).",
      "Keywords": "continuous deployment | empirical analysis | experimentation | full deployment cycle",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Kevic, Katja;Murphy, Brendan;Williams, Laurie;Beckmann, Jennifer",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026801807",
      "Primary study DOI": "10.1109/ICSE-SEIP.2017.13",
      "Title": "Who broke the build? Automatically identifying changes that induce test failures in continuous integration at google scale",
      "Abstract": "Quickly identifying and fixing code changes that introduce regressions is critical to keep the momentum on software development, especially in very large scale software repositories with rapid development cycles, such as at Google. Identifying and fixing such regressions is one of the most expensive, tedious, and time consuming tasks in the software development life-cycle. Therefore, there is a high demand for automated techniques that can help developers identify such changes while minimizing manual human intervention. Various techniques have recently been proposed to identify such code changes. However, these techniques have shortcomings that make them unsuitable for rapid development cycles as at Google. In this paper, we propose a novel algorithm to identify code changes that introduce regressions, and discuss case studies performed at Google on 140 projects. Based on our case studies, our algorithm automatically identifies the change that introduced the regression in the top-5 among thousands of candidates 82% of the time, and provides considerable savings on manual work developers need to perform.",
      "Keywords": "Ranking | Software changes | Software debugging | Software fault diagnosis | Software testing | Software tools",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track, ICSE-SEIP 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Ziftci, Celal;Reardon, Jim",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84943730250",
      "Primary study DOI": "",
      "Title": "LILITH: A PERSONAL COMPUTER FOR THE SOFTWARE ENGINEER.",
      "Abstract": "The personal work station offers significant advantages over the large-scale, central computing facility accessed via a terminal. Among them are availability, reliability, simplicity of operation, and a high bandwidth to the user. Modern technology allows to build systems for high-level language programming with significant computing power for a reasonable price. At the Institut fur Informatik of ETH we have designed and built such a personal computer tailored to the language Modula-2. This paper is a report on this project which encompasses language design, development of a compiler and a single-user operating system, design of an architecture suitable for compiling and yielding a high density of code, and the development and construction of the hardware. 20 Lilith computers are now in use at ETH. A principal theme is that the requirements of software engineering influence the design of the language, and that its facilities are reflected by the architecture of the computer and the structure of the hardware. The module structure is used to exemplify this theme. That the hardware should be designed according to the programming language, instead of vice-versa, is particularly relevant in view of the trend towards VLSI technology.",
      "Keywords": "",
      "Publication venue": "",
      "Publication date": "",
      "Publication type": "",
      "Authors": "Wirth, N.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027693932",
      "Primary study DOI": "10.1109/ICSE.2017.15",
      "Title": "Characterizing and Detecting Anti-Patterns in the Logging Code",
      "Abstract": "Snippets of logging code are output statements (e.g., LOG.info or System.out.println) that developers insert into a software system. Although more logging code can provide more execution context of the system's behavior during runtime, it is undesirable to instrument the system with too much logging code due to maintenance overhead. Furthermore, excessive logging may cause unexpected side-effects like performance slow-down or high disk I/O bandwidth. Recent studies show that there are no well-defined coding guidelines for performing effective logging. Previous research on the logging code mainly tackles the problems of where-To-log and what-To-log. There are very few works trying to address the problem of how-To-log (developing and maintaining high-quality logging code). In this paper, we study the problem of how-To-log by characterizing and detecting the anti-patterns in the logging code. As the majority of the logging code is evolved together with the feature code, the remaining set of logging code changes usually contains the fixes to the anti-patterns. We have manually examined 352 pairs of independently changed logging code snippets from three well-maintenance open source systems: ActiveMQ, Hadoop and Maven. Our analysis has resulted in six different anti-patterns in the logging code. To demonstrate the value of our findings, we have encoded these anti-patterns into a static code analysis tool, LCAnalyzer. Case studies show that LCAnalyzer has an average recall of 95% and precision of 60% and can be used to automatically detect previously unknown anti-patterns in the source code. To gather feedback, we have filed 64 representative instances of the logging code anti-patterns from the most recent releases of ten open source software systems. Among them, 46 instances (72%) have already been accepted by their developers.",
      "Keywords": "anti-patterns | empirical studies | logging code | logging practices | software maintenance",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering, ICSE 2017",
      "Publication date": "2017-07-19",
      "Publication type": "Conference Paper",
      "Authors": "Chen, Boyuan;Jiang, Zhen Ming",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040581486",
      "Primary study DOI": "10.1109/ICSME.2017.76",
      "Title": "The evaluation of an approach for automatic generated documentation",
      "Abstract": "Two studies are conducted to evaluate an approach to automatically generate natural language documentation summaries for C++ methods. The documentation approach relies on a method's stereotype information. First, each method is automatically assigned a stereotype(s) based on static analysis and a set of heuristics. Then, the approach uses the stereotype information, static analysis, and predefined templates to generate a natural-language summary/documentation for each method. This documentation is automatically added to the code base as a comment for each method. The result of the first study reveals that the generated documentation is accurate, does not include unnecessary information, and does a reasonable job describing what the method does. Based on statistical analysis of the second study, the most important part of the documentation is the short description as it describes the intended behavior of a method.",
      "Keywords": "Method stereotypes | Program comprehension | Source-code summarization | Static analysis",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Abid, Nahla;Dragan, Natalia;Collard, Michael L.;Maletic, Jonathan I.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040574156",
      "Primary study DOI": "10.1109/ICSME.2017.38",
      "Title": "Understanding spreadsheet evolution in practice",
      "Abstract": "As a special kind of software, spreadsheets have been evolving during their life cycle. Understanding spreadsheet evolution can help facilitate spreadsheet design, maintenance and fault detection. However, understanding spreadsheet evolution is challenging in practice. There are many factors that hinder spreadsheet evolution comprehension, such as, lack of version information, complicated structure changes during evolution, etc. Thus, we propose this work to facilitate the understanding of spreadsheet evolution, including developing semi-automated technique to build versioned spreadsheet corpora, characterizing and understanding how spreadsheet templates are reused, developing automated tools for spreadsheet comparison, and new approaches for fault detection during evolution.",
      "Keywords": "Comparison | Empirical study | Evolution | Spreadsheet | Template | Version",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Xu, Liang",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040628967",
      "Primary study DOI": "10.1109/ICSME.2017.17",
      "Title": "On-demand developer documentation",
      "Abstract": "We advocate for a paradigm shift in supporting the information needs of developers, centered around the concept of automated on-demand developer documentation. Currently, developer information needs are fulfilled by asking experts or consulting documentation. Unfortunately, traditional documentation practices are inefficient because of, among others, the manual nature of its creation and the gap between the creators and consumers. We discuss the major challenges we face in realizing such a paradigm shift, highlight existing research that can be leveraged to this end, and promote opportunities for increased convergence in research on software documentation.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Robillard, Martin P.;Marcus, Andrian;Treude, Christoph;Bavota, Gabriele;Chaparro, Oscar;Ernst, Neil;Gerosall, Marco Aurélio;Godfrey, Michael;Lanza, Michele;Linares-Vásquez, Mario;Murphy, Gail C.;Moreno, Laura;Shepherd, David;Wong, Edmund",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040561934",
      "Primary study DOI": "10.1109/ICSME.2017.25",
      "Title": "Reviewing career paths of the openstack developers",
      "Abstract": "Career perspectives are known to motivate software engineers. However, so far, career perspectives have been mostly studied within traditional software development companies. In our work we take a complementary approach and study career paths of open source developers, focusing on their advancement through the code review hierarchy, from developers to reviewers and further to core reviewers. To gain understanding of code review career paths we conduct an exploratory case study of the OpenStack community. Based on the case study we have publicized anonymized research data and formulated four hypotheses pertaining to career paths of contributors in modern multi-company open source projects. We conjecture that (i) developers and reviewers are separate subpopulations with little movement between them, (ii-a) the turnover of the core reviewers is high and rapid, (ii-b) companies are interested in having core reviewers among their staff, and (iii) being a core reviewer is beneficial for career. Validity of those hypotheses in other multi-company open source projects should be investigated in the follow-up studies.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Van Wesel, Perry;Lin, Bin;Robles, Gregorio;Serebrenik, Alexander",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040557409",
      "Primary study DOI": "10.1109/ICSME.2017.41",
      "Title": "Predicting and evaluating software model growth in the automotive industry",
      "Abstract": "The size of a software artifact influences the software quality and impacts the development process. In industry, when software size exceeds certain thresholds, memory errors accumulate and development tools might not be able to cope anymore, resulting in a lengthy program start up times, failing builds, or memory problems at unpredictable times. Thus, foreseeing critical growth in software modules meets a high demand in industrial practice. Predicting the time when the size grows to the level where maintenance is needed prevents unexpected efforts and helps to spot problematic artifacts before they become critical. Although the amount of prediction approaches in literature is vast, it is unclear how well they fit with prerequisites and expectations from practice. In this paper, we perform an industrial case study at an automotive manufacturer to explore applicability and usability of prediction approaches in practice. In a first step, we collect the most relevant prediction approaches from literature, including both, approaches using statistics and machine learning. Furthermore, we elicit expectations towards predictions from practitioners using a survey and stakeholder workshops. At the same time, we measure software size of 48 software artifacts by mining four years of revision history, resulting in 4,547 data points. In the last step, we assess the applicability of state-of-the-art prediction approaches using the collected data by systematically analyzing how well they fulfill the practitioners' expectations. Our main contribution is a comparison of commonly used prediction approaches in a real world industrial setting while considering stakeholder expectations. We show that the approaches provide significantly different results regarding prediction accuracy and that the statistical approaches fit our data best.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Schroeder, Jan;Berger, Christian;Knauss, Alessia;Preenja, Harri;Ali, Mohammad;Staron, Miroslaw;Herpel, Thomas",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040630908",
      "Primary study DOI": "10.1109/ICSME.2017.45",
      "Title": "Automated repair of high inaccuracies in numerical programs",
      "Abstract": "Rounding errors are introduced pervasively when using floating-point arithmetic to approximate real arithmetic. The accumulation or catastrophic cancellation of rounding errors in numerical programs may produce high inaccuracy results, which can cause serious software failures once being triggered. High inaccuracies are known hard to debug and fix manually for developers. Hence, the automated techniques are desired for solving the high inaccuracy problem. In this paper, we propose a novel framework for automated repair of high-inaccuracy bugs in numerical programs. The framework includes the phases of detecting high-inaccuracy bugs, localizing the buggy code, generating and validating the patches, and synthesizing the repaired program at last. Based on this framework, we develop a prototype tool for repairing high inaccuracies in numerical programs. Our preliminary experimental results are encouraging.",
      "Keywords": "Automated repair | Dynamic analysis | Floating-point | Numerical code",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Yi, Xin;Chen, Liqian;Mao, Xiaoguang;Ji, Tao",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040585168",
      "Primary study DOI": "10.1109/ICSME.2017.39",
      "Title": "Improving software maintenance using process mining and predictive analytics",
      "Abstract": "This research focuses on analyzing and improving maintenance process by exploring novel applications of process mining and predictive analytics. We analyze the software maintenance process by applying process mining on software repositories, and address the identified inefficiencies using predictive analytics. To drive our research, we engage with practitioners from large, global IT companies and emphasize on the practical usability of the proposed solution approaches, which are evaluated by conducting a series of case studies on open source and commercial projects.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Gupta, Monika;Serebrenik, Alexander;Jalote, Pankaj",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040538692",
      "Primary study DOI": "10.1109/ICSME.2017.28",
      "Title": "On the optimal order of reading source code changes for review",
      "Abstract": "Change-based code review, e.g., in the form of pull requests, is the dominant style of code review in practice. An important option to improve review's efficiency is cognitive support for the reviewer. Nevertheless, review tools present the change parts under review sorted in alphabetical order of file path, thus leaving the effort of understanding the construction, connections, and logic of the changes on the reviewer. This leads to the question: How should a code review tool order the parts of a code change to best support the reviewer? We answer this question with a middle-range theory, which we generated inductively in a mixed methods study, based on interviews, an online survey, and existing findings from related areas. Our results indicate that an optimal order is mainly an optimal grouping of the change parts by relatedness. We present our findings as a collection of principles and formalize them as a partial order relation among review orders.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Baum, Tobias;Schneider, Kurt;Bacchelli, Alberto",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040547234",
      "Primary study DOI": "10.1109/ICSME.2017.22",
      "Title": "RCIA: Automated change impact analysis to facilitate a practical cancer registry system",
      "Abstract": "The Cancer Registry of Norway (CRN) employs a cancer registry system to collect cancer patient data (e.g., diagnosis and treatments) from various medical entities (e.g., clinic hospitals). The collected data are then checked for validity (i.e., validation) and assembled as cancer cases (i.e., aggregation) based on more than 1000 cancer coding rules in the system. However, it is frequent in practice that the collected cancer data changes due to various reasons (e.g., different treatments) and the cancer coding rules can also change/evolve due to new medical knowledge. Thus, such a cancer registry system requires an efficient means to automatically analyze these changes and provide consequent impacts to medical experts for further actions. This paper proposes an automated Rule-based Change Impact Analysis (CIA) approach named RCIA that includes: 1) a change classification to capture the potential changes that can occur at CRN; 2) in total 80 change impact analysis rules including 50 dependency rules and 30 impact rules; and 3) an efficient algorithm to analyze changes and produce consequent impacts. We evaluate RCIA via a case study with 12 real change sets from CRN and a conducted interview. The results showed that RCIA managed to produce 100% actual change impacts and the medical expert at CRN is quite positive to apply RCIA to facilitate their cancer registry system. We also shared a set of lessons learned based on the collaboration with CRN.",
      "Keywords": "Cancer coding rules | Cancer registry system | Change classification | Change impact analysis",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Wang, Shuai;Schwitalla, Thomas;Yue, Tao;Ali, Shaukat;Nygård, Jan F.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040557867",
      "Primary study DOI": "10.1109/ICSME.2017.31",
      "Title": "Understanding android application programming and security: A dynamic study",
      "Abstract": "Most existing research for Android focuses on particular security issues, yet there is little broad understanding of Android application run-time characteristics and their implications. To mitigate this gap, we present the first systematic dynamic characterization study of Android apps that targets a broad understanding of application behaviors in Android. Through lightweight method-level profiling, we collected 59GB traces of method calls and Intent-based inter-component communication (ICC) from 125 popular Android apps and 62 pairs among them that enabled an intensive empirical investigation of their run-time behaviors. Our study revealed that, among other findings, (1) the application executions were overwhelmingly dominated by the Android framework, (2) Activity components dominated over other types of components and were responsible for most lifecycle callbacks (3) most event handlers dealt with user interactions as opposed to system events, (4) the majority of exercised ICCs did not carry any data payloads, and (5) sensitive data sources and sinks targeted only one/two dominant categories of information or operations. We also discuss the implications of our results for cost-effective program analysis and security defense for Android.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Cai, Haipeng;Ryder, Barbara G.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040559958",
      "Primary study DOI": "10.1109/ICSME.2017.81",
      "Title": "How long and how much: What to expect from Summer of Code participants?",
      "Abstract": "Open Source Software (OSS) communities depend on continually recruiting new contributors. Some communities promote initiatives such as Summers of Code to foster contribution, but little is known about how successful these initiatives are. As a case study, we chose Google Summer of Code (GSoC), which is a three-month internship promoting software development by students in several OSS projects. We quantitatively investigated different aspects of students' contribution, including number of commits, code churn, and contribution date intervals. We found that 82% of the studied OSS projects merged at least one commit in codebase. When only newcomers are considered, ∼54% of OSS projects merged at least one commit. We also found that ∼23% of newcomers contributed to GSoC projects before knowing they would be accepted. Additionally, we found that the amount of commits and code of students with experience in the GSoC projects are strongly correlated with how much code they produced and how long they remained during and after GSoC. OSS communities can take advantage of our results to balance the trade-offs involved in entering CCEs, to set the communities' expectations about how much contribution they can expect to achieve, and for how long students will probably engage.",
      "Keywords": "Community code engagement | Google Summer of Code | Mining software repositories | Newcomers | Open source software | Sustainability",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Silva, Jefferson O.;Wiese, Igor;German, Daniel;Steinmacher, Igor;Gerosa, Marco A.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030760681",
      "Primary study DOI": "10.1109/ICSME.2017.27",
      "Title": "Continuous, evolutionary and large-scale: A new perspective for automated mobile app testing",
      "Abstract": "Mobile app development involves a unique set of challenges including device fragmentation and rapidly evolving platforms, making testing a difficult task. The design space for a comprehensive mobile testing strategy includes features, inputs, potential contextual app states, and large combinations of devices and underlying platforms. Therefore, automated testing is an essential activity of the development process. However, current state of the art of automated testing tools for mobile apps posses limitations that has driven a preference for manual testing in practice. As of today, there is no comprehensive automated solution for mobile testing that overcomes fundamental issues such as automated oracles, history awareness in test cases, or automated evolution of test cases. In this perspective paper we survey the current state of the art in terms of the frameworks, tools, and services available to developers to aid in mobile testing, highlighting present shortcomings. Next, we provide commentary on current key challenges that restrict the possibility of a comprehensive, effective, and practical automated testing solution. Finally, we offer our vision of a comprehensive mobile app testing framework, complete with research agenda, that is succinctly summarized along three principles: Continuous, Evolutionary and Large-scale (CEL).",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Linares-Vásquez, Mario;Moran, Kevin;Poshyvanyk, Denys",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040549294",
      "Primary study DOI": "10.1109/ICSME.2017.92",
      "Title": "Message from the industry track co-chairs",
      "Abstract": "Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Editorial",
      "Authors": "Ozkaya, Ipek;Robinson, Brian",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85032496394",
      "Primary study DOI": "10.1109/ICSME.2017.20",
      "Title": "REPERSP: Recommending personalized software projects on GitHub",
      "Abstract": "In the open source community such as GitHub, developers usually need to find projects similar to their work, with the aim to reuse their functions and explore ideas of features that could be possibly added into their project at hand. Traditional text search engine can help detect similar resources. However, it is difficult for developers to use in open source community because a few query words cannot describe the whole features of a project. In this paper, we present a practical software recommendation system, REPERSP, which is used to recommend personalized software projects in GitHub. According to the features of projects created by developers and their behavior to other known projects, REPERSP recommends the top N relevant and personalized software projects. Moreover, REPERSP is implemented with the MapReduce parallel processing frame - Apache Spark for large-scale data, which can be scaled to a large number of users and projects for practical usage. Empirical results show that REPERSP can recommend more accurate results compared with other two recommendation algorithms, i.e., UserCF (user collaborative filtering) and ItemCF (item collaborative filtering). Video of our demo is available at https://youtu.be/WKigSUV4UA0.",
      "Keywords": "GitHub | Personalized recommendation | Project recommendation",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Xu, Wenyuan;Sun, Xiaobing;Hu, Jiajun;Li, Bin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040543079",
      "Primary study DOI": "10.1109/ICSME.2017.52",
      "Title": "Learning to predict severity of software vulnerability using only vulnerability description",
      "Abstract": "Software vulnerabilities pose significant security risks to the host computing system. Faced with continuous disclosure of software vulnerabilities, system administrators must prioritize their efforts, triaging the most critical vulnerabilities to address first. Many vulnerability scoring systems have been proposed, but they all require expert knowledge to determine intricate vulnerability metrics. In this paper, we propose a deep learning approach to predict multi-class severity level of software vulnerability using only vulnerability description. Compared with intricate vulnerability metrics, vulnerability description is the \"surface level\" information about how a vulnerability works. To exploit vulnerability description for predicting vulnerability severity, discriminative features of vulnerability description have to be defined. This is a challenging task due to the diversity of software vulnerabilities and the richness of vulnerability descriptions. Instead of relying on manual feature engineering, our approach uses word embeddings and a one-layer shallow Convolutional Neural Network (CNN) to automatically capture discriminative word and sentence features of vulnerability descriptions for predicting vulnerability severity. We exploit large amounts of vulnerability data from the Common Vulnerabilities and Exposures (CVE) database to train and test our approach.",
      "Keywords": "Deep learning | Mining software repositories | Multi-class classification | Vulnerability severity prediction",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Han, Zhuobing;Li, Xiaohong;Xing, Zhenchang;Liu, Hongtao;Feng, Zhiyong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85037094010",
      "Primary study DOI": "10.1109/ICSME.2017.59",
      "Title": "Semantics-aware machine learning for function recognition in binary code",
      "Abstract": "Function recognition in program binaries serves as the foundation for many binary instrumentation and analysis tasks. However, as binaries are usually stripped before distribution, function information is indeed absent in most binaries. By far, identifying functions in stripped binaries remains a challenge. Recent research work proposes to recognize functionsinbinary code through machine learning techniques. The recognition model, including typical function entry point patterns, is automatically constructed through learning. However, we observed that as previous work only leverages syntax-level features to train the model, binary obfuscation techniques can undermine the prelearned models in real-world usage scenarios. In this paper, we propose FID, a semantics-based method to recognize functions in stripped binaries. We leverage symbolic execution to generate semantic information and learn the function recognition model through well-performing machine learning techniques. FID extracts semantic information from binary code and, therefore, is effectively adapted to different compilers and optimizations. Moreover, we also demonstrate that FID has high recognition accuracy on binaries transformed by widely-used obfuscation techniques. We evaluate FID with over four thousand test cases. Our evaluation shows that FID is comparable with previous work on normal binaries and it notably outperforms existing tools on obfuscated code.",
      "Keywords": "Function recognition | Machine learning | Reverse engineering",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Wang, Shuai;Wang, Pei;Wu, Dinghao",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040636121",
      "Primary study DOI": "10.1109/ICSME.2017.86",
      "Title": "TraceLab components for generating extractive summaries of user stories",
      "Abstract": "This artifact is a reproducibility package for experiments in user stories summarization. We implemented and packaged the artifact as a set of reusable TraceLab components. The existing implementation of the artifact was relatively difficult to use because it required the user to coordinate several different programming languages and dependencies. This artifact, available via our online appendix, provides the components, a detailed tutorial with screenshots that show exactly where to click and what to enter, and an example virtual machine image.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Krasniqi, Rrezarta;Jiang, Siyuan;McMillan, Collin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040634374",
      "Primary study DOI": "10.1109/ICSME.2017.82",
      "Title": "Is it safe to uplift this patch? An empirical study on mozilla firefox",
      "Abstract": "In rapid release development processes, patches that fix critical issues, or implement high-value features are often promoted directly from the development channel to a stabilization channel, potentially skipping one or more stabilization channels. This practice is called patch uplift. Patch uplift is risky, because patches that are rushed through the stabilization phase can end up introducing regressions in the code. This paper examines patch uplift operations at Mozilla, with the aim to identify the characteristics of uplifted patches that introduce regressions. Through statistical and manual analyses, we quantitatively and qualitatively investigate the reasons behind patch uplift decisions and the characteristics of uplifted patches that introduced regressions. Additionally, we interviewed three Mozilla release managers to understand organizational factors that affect patch uplift decisions and outcomes. Results show that most patches are uplifted because of a wrong functionality or a crash. Uplifted patches that lead to faults tend to have larger patch size, and most of the faults are due to semantic or memory errors in the patches. Also, release managers are more inclined to accept patch uplift requests that concern certain specific components, and-or that are submitted by certain specific developers.",
      "Keywords": "Mining software repositories | Patch uplift | Release engineering | Urgent update",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Castelluccio, Marco;An, Le;Khomh, Foutse",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040609765",
      "Primary study DOI": "10.1109/ICSME.201751",
      "Title": "Supervised vs unsupervised models: A holistic look at effort-aware just-in-time defect prediction",
      "Abstract": "Effort-aware just-in-time (JIT) defect prediction aims at finding more defective software changes with limited code inspection cost. Traditionally, supervised models have been used; however, they require sufficient labelled training data, which is difficult to obtain, especially for new projects. Recently, Yang et al. proposed an unsupervised model (LT) and applied it to projects with rich historical bug data. Interestingly, they reported that, under the same inspection cost (i.e., 20 percent of the total lines of code modified by all changes), it could find more defective changes than a state-of-the-art supervised model (i.e., EALR). This is surprising as supervised models that benefit from historical data are expected to perform better than unsupervised ones. Their finding suggests that previous studies on defect prediction had made a simple problem too complex. Considering the potential high impact of Yang et al.'s work, in this paper, we perform a replication study and present the following new findings: (1) Under the same inspection budget, LT requires developers to inspect a large number of changes necessitating many more context switches. (2) Although LT finds more defective changes, many highly ranked changes are false alarms. These initial false alarms may negatively impact practitioners' patience and confidence. (3) LT does not outperform EALR when the harmonic mean of Recall and Precision (i.e., F1-score) is considered. Aside from highlighting the above findings, we propose a simple but improved supervised model called CBS. When compared with EALR, CBS detects about 15% more defective changes and also significantly improves Precision and F1-score. When compared with LT, CBS achieves similar results in terms of Recall, but it significantly reduces context switches and false alarms before first success. Finally, we also discuss the implications of our findings for practitioners and researchers.",
      "Keywords": "Bias | Change classification | Cost effectiveness | Evaluation",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Huang, Qiao;Xia, Xin;Lo, David",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040609724",
      "Primary study DOI": "10.1109/ICSME.2017.42",
      "Title": "The pricey bill of Technical Debt - When and by whom will it be paid?",
      "Abstract": "Software companies need to support continuous and fast delivery of customer value both in short and a long-term perspective. However, this can be hindered by evolution limitations and high maintenance efforts due to internal software quality issues by what is described as Technical Debt. Although significant theoretical work has been undertaken to describe the negative effects of Technical Debt, these studies tend to have a weak empirical basis and often lack quantitative data. The aim of this study is to estimate wasted time, caused by the Technical Debt interest during the software life-cycle. This study also investigates how practitioners perceive and estimate the impact of the negative consequences due to Technical Debt during the software development process. This paper reports the results of both an online web-survey provided quantitative data from 258 participants and follow-up interviews with 32 industrial software practitioners. The importance and originality of this study contributes and provides novel insights into the research on Technical Debt by quantifying the perceived interest and the negative effects it has on the software development life-cycle. The findings show that on average, 36 % of all development time is estimated to be wasted due to Technical Debt; Complex Architectural Design and Requirement Technical Debt generates most negative effect; and that most time is wasted on understanding and/or measuring the Technical Debt. Moreover, the analysis of the professional roles and the age of the software system in the survey revealed that different roles are affected differently and that the consequences of Technical Debt are also influenced by the age of the software system.",
      "Keywords": "Component | Development cost | Empirical study | Qualitative data | Quantitative data | Software development | Survey | Technical Debt | Wasted time",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Besker, Terese;Martini, Antonio;Bosch, Jan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040563526",
      "Primary study DOI": "10.1109/ICSME.2017.29",
      "Title": "SimEvo: Testing evolving multi-process software systems",
      "Abstract": "Regression testing is used to perform re-validation of evolving software. However, most existing techniques for regression testing focus exclusively on single-process applications, but to date, no work has considered regression testing for software involving multiple processes or event handlers (e.g., software signals) at the system-level. The unique characteristics of concurrency control mechanism employed at the system-level can affect the static and dynamic analysis techniques on which existing regression testing approaches rely. Therefore, applying these approaches can result in inadequately tested software during maintenance, and ultimately impair software quality. In this paper, we propose SimEvo, the first regression testing techniques for multi-process applications. SimEvo employs novel impact analysis techniques to identify system-level concurrent events that are affected by the changes. It then reuses existing test cases, as well as generating new test cases, focused on the set of impacted events, to effectively and efficiently explore the newly updated concurrent behaviors. Our empirical study on a set of real-world Linux applications shows that SimEvo is more cost-effective in achieving high inter-process coverage and revealing real world system-level concurrency faults than other approaches.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Yu, Tingting",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040611476",
      "Primary study DOI": "10.1109/ICSME.2017.69",
      "Title": "Towards accurate duplicate bug retrieval using deep learning techniques",
      "Abstract": "Duplicate Bug Detection is the problem of identifying whether a newly reported bug is a duplicate of an existing bug in the system and retrieving the original or similar bugs from the past. This is required to avoid costly rediscovery and redundant work. In typical software projects, the number of duplicate bugs reported may run into the order of thousands, making it expensive in terms of cost and time for manual intervention. This makes the problem of duplicate or similar bug detection an important one in Software Engineering domain. However, an automated solution for the same is not quite accurate yet in practice, in spite of many reported approaches using various machine learning techniques. In this work, we propose a retrieval and classification model using Siamese Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) for accurate detection and retrieval of duplicate and similar bugs. We report an accuracy close to 90% and recall rate close to 80%, which makes possible the practical use of such a system. We describe our model in detail along with related discussions from the Deep Learning domain. By presenting the detailed experimental results, we illustrate the effectiveness of the model in practical systems, including for repositories for which supervised training data is not available.",
      "Keywords": "Convolutional neural networks | Deep learning | Duplicate bug detection | Information retrieval | Long short term memory | Natural language processing | Siamese networks | Word embeddings",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Deshmukh, Jayati;Annervaz, K. M.;Podder, Sanjay;Sengupta, Shubhashis;Dubash, Neville",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040588479",
      "Primary study DOI": "10.1109/ICSME.2017.18",
      "Title": "Mean average distance to resolver: An evaluation metric for ticket routing in expert network",
      "Abstract": "In the technical support division of a large enterprise software provider, customers' technical incidents, problems, and change requests are processed as tickets. Each ticket is assigned to a support engineer for processing. Due to the limited expertise of individuals, resolving a ticket may involve routing the ticket among multiple groups of engineers. Each routing step costs time and resources. It is desirable for experts to route a ticket to its most likely resolver with minimum steps. Automated or semi-automated systems are proposed to improve routing efficiency. To evaluate the performance of any system, including human routing, two metrics are commonly used, namely Mean Steps to Resolver (MSTR) and Resolution Rate (RR). The two measures are designed independently, with different objectives and at different scales, making it difficult to compare systems. Moreover, the current measures only consider the resolver group as the ground truth, even during path-level evaluation. They disregard the contribution of intermediate groups during the ticket resolution. In this paper, we propose a distance-based unified evaluation measure named Mean Average Distance to Resolver (MADR). This new framework addresses the aforementioned limitations, and it can be easily modified to adapt to different business requirements in different organizations. In addition, existing evaluation paradigm does not consider human routing steps except the resolver. We argue that the predicted paths may not be followed exactly by expert groups in real operation. An assistive routing evaluation framework is therefore designed to take into account expert's choice when recommendation fails, for each routing. Experiments using proprietary data from a large enterprise demonstrate that MADR can be used to benchmark and compare routing systems.",
      "Keywords": "Expert network | MADR | MSTR | RR | Ticket routing",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Han, Jianglei;Sun, Aixin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040553831",
      "Primary study DOI": "10.1109/ICSME.2017.56",
      "Title": "NLP2Code: Code snippet content assist via natural language tasks",
      "Abstract": "Developers increasingly take to the Internet for code snippets to integrate into their programs. To save developers the time required to switch from their development environments to a web browser in the quest for a suitable code snippet, we introduce NLP2Code, a content assist for code snippets. Unlike related tools, NLP2Code integrates directly into the source code editor and provides developers with a content assist feature to close the vocabulary gap between developers' needs and code snippet meta data. Our preliminary evaluation of NLP2Code shows that the majority of invocations lead to code snippets rated as helpful by users and that the tool is able to support a wide range of tasks.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Campbell, Brock Angus;Treude, Christoph",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040626965",
      "Primary study DOI": "10.1109/ICSME.2017.37",
      "Title": "An empirical study on the usage of fault localization in automated program repair",
      "Abstract": "Spectrum-based fault localization (SFL), the technique producing a rank list of statements in descending order of their suspiciousness values, is nowadays widely used in current automated program repair tools. There are two different algorithms for these tools to choose statements selected for modification to produce candidate patches from the list: one is the rank-first algorithm based on suspiciousness rankings of statements, the other is the suspiciousness-first algorithm based on suspiciousness value of statements. However, to our knowledge there is no research work implementing the two algorithms in the same repair tool or comparing their effectiveness. In this paper, we conduct an empirical research based on the automated repair tool Nopol with the benchmark set of Defects4J to compare these two algorithms. Preliminary results suggest that the suspiciousness-first algorithm is not equivalent to the rank-first algorithm and behaves better in parallel repair and patch diversity.",
      "Keywords": "Automated program repair | Rank-first algorithm | Spectrum-based fault localization | Suspiciousness-first algorithm",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Yang, Deheng;Qi, Yuhua;Mao, Xiaoguang",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040186398",
      "Primary study DOI": "10.1109/ICSME.2017.68",
      "Title": "What are the testing habits of developers? A case study in a large IT company",
      "Abstract": "Tests are considered important to ensure the good behavior of applications and improve their quality. But development in companies also involves tight schedules, old habits, less-trained developers, or practical difficulties such as creating a test database. As a result, good testing practices are not always used as often as one might wish. With a major IT company, we are engaged in a project to understand developers testing behavior, and whether it can be improved. Some ideas are to promote testing by reducing test session length, or by running automatically tests behind the scene and send warnings to developers about the failing ones. Reports on developers testing habits in the literature focus on highly distributed open-source projects, or involve students programmers. As such they might not apply to our industrial, closed source, context. In this paper, we take inspiration from experiments of two papers of the literature to enhance our comprehension of the industrial environment. We report the results of a field study on how often the developers use tests in their daily practice, whether they make use of tests selection and why they do. Results are reinforced by interviews with developers involved in the study. The main findings are that test practice is in better shape than we expected; developers select tests \"ruthlessly\" (instead of launching an entire test suite); although they are not accurate in their selection, and; contrary to expectation, test selection is not influenced by the size of the test suite nor the duration of the tests.",
      "Keywords": "Case study | Interviews | IT company | Regression test selection",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Blondeau, Vincent;Etien, Anne;Anquetil, Nicolas;Cresson, Sylvain;Croisy, Pascal;Ducasse, Stéphane",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040594632",
      "Primary study DOI": "10.1109/ICSME.2017.60",
      "Title": "An experiment comparing lifted and delayed variability-aware program analysis",
      "Abstract": "Today's software systems need to be highly flexible and managing their variability plays an essential role during development. Variability-aware program analysis techniques have been proposed to support developers in understanding codelevel variability by analyzing the space of program variants. Such techniques are highly beneficial, e.g., when determining the impact of changes during maintenance and evolution. Two strategies have been proposed in the literature to make existing program analysis techniques variability-aware: (i) program analysis can be lifted by considering variability already in the parsing stage; or (ii) analysis can be delayed by considering and recovering variability only when needed. Both strategies have advantages and disadvantages, however, a systematic comparison is still missing. The contributions of this paper are an in-depth comparison of SPLLIFT and COACH, two existing approaches representing these two strategies, and an analysis and discussion of the trade-offs regarding precision and run-time performance. The results of our experiment show that the delayed strategy is significantly faster but typically less precise. Our findings are intended for researchers and practitioners deciding which strategy to select for their purpose and context.",
      "Keywords": "Configuration | Maintenance and evolution of product lines | Variability-aware program analysis",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Angerer, Florian;Grünbacher, Paul;Prähofer, Herbert;Linsbauer, Lukas",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040561396",
      "Primary study DOI": "10.1109/ICSME.2017.26",
      "Title": "Coarse Hierarchical delta debugging",
      "Abstract": "This paper introduces the Coarse Hierarchical Delta Debugging algorithm for efficient test case reduction. It can be used as a test case simplification algorithm in its own right if theoretical minimality is not a strict requirement, or it can act as a preprocessing step to the original Hierarchical Delta Debugging algorithm. Evaluation of artificial and real test cases shows that a coarse variant can produce reduced test cases with significantly fewer testing steps than the original algorithm (58% gain on average, 79% maximum), while still keeping the outputs acceptably small (never increasing the reduced test cases by more than 0.36% of the input).",
      "Keywords": "Coarse algorithm | Hierarchical delta debugging | Test case reduction",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Hodován, Renata;Kiss, Ákos;Gyimóthy, Tibor",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040612378",
      "Primary study DOI": "10.1109/ICSME.2017.100",
      "Title": "Using Observed Behavior to reformulate queries during text retrieval-based bug localization",
      "Abstract": "Text Retrieval (TR)-based approaches for bug localization rely on formulating an initial query based on a bug report. Often, the query does not return the buggy software artifacts at or near the top of the list (i.e., it is a low-quality query). In such cases, the query needs reformulation. Existing research on supporting developers in the reformulation of queries focuses mostly on leveraging relevance feedback from the user or expanding the original query with additional information (e.g., adding synonyms). In many cases, the problem with such low-quality queries is the presence of irrelevant terms (i.e., noise) and previous research has shown that removing such terms from the queries leads to substantial improvement in code retrieval. Unfortunately, the current state of research lacks methods to identify the irrelevant terms. Our research aims at addressing this problem and our conjecture is that reducing a low-quality query to only the terms describing the Observed Behavior (OB) can improve TR-based bug localization. To verify our conjecture, we conducted an empirical study using bug data from 21 open source systems to reformulate 451 low-quality queries. We compare the accuracy achieved by four TR-based bug localization approaches at three code granularities (i.e., files, classes, and methods), when using the complete bug reports as queries versus a reduced version corresponding to the OB only. The results show that the reformulated queries improve TR-based bug localization for all approaches by 147.4% and 116.6% on average, in terms of MRR and MAP, respectively. We conclude that using the OB descriptions is a simple and effective technique to reformulate low-quality queries during TR-based bug localization.",
      "Keywords": "Bug localization | Observed Behavior | Query reformulation | Text retrieval",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Chaparro, Oscar;Florez, Juan Manuel;Marcus, Andrian",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040590726",
      "Primary study DOI": "10.1109/ICSME.2017.58",
      "Title": "Flexfringe: A passive automaton learning package",
      "Abstract": "Finite state models, such as Mealy machines or state charts, are often used to express and specify protocol and software behavior. Consequently, these models are often used in verification, testing, and for assistance in the development and maintenance process. Reverse engineering these models from execution traces and log files, in turn, can accelerate and improve the software development and inform domain experts about the processes actually executed in a system. We present flexfringe, an open-source software tool to learn variants of finite state automata from traces using a state-of-the-art evidence-driven state-merging algorithm at its core. We embrace the need for customized models and tailored learning heuristics in different application domains by providing a flexible, extensible interface.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Verwer, Sicco;Hammerschmidt, Christian A.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040575916",
      "Primary study DOI": "10.1109/ICSME.2017.46",
      "Title": "CCLearner: A deep learning-based clone detection approach",
      "Abstract": "Programmers produce code clones when developing software. By copying and pasting code with or without modification, developers reuse existing code to improve programming productivity. However, code clones present challenges to software maintenance: they may require consistent application of the same or similar bug fixes or program changes to multiple code locations. To simplify the maintenance process, various tools have been proposed to automatically detect clones [1], [2], [3], [4], [5], [6]. Some tools tokenize source code, and then compare the sequence or frequency of tokens to reveal clones [1], [3], [4], [5]. Some other tools detect clones using tree-matching algorithms to compare the Abstract Syntax Trees (ASTs) of source code [2], [6]. In this paper, we present CCLEARNER, the first solely token-based clone detection approach leveraging deep learning. CCLEARNER extracts tokens from known method-level code clones and non-clones to train a classifier, and then uses the classifier to detect clones in a given codebase. To evaluate CCLEARNER, we reused BigCloneBench [7], an existing large benchmark of real clones. We used part of the benchmark for training and the other part for testing, and observed that CCLEARNER effectively detected clones. With the same data set, we conducted the first systematic comparison experiment between CCLEARNER and three popular clone detection tools. Compared with the approaches not using deep learning, CCLEARNER achieved competitive clone detection effectiveness with low time cost.",
      "Keywords": "Clone detection | Deep learning | Empirical",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Li, Liuqing;Feng, He;Zhuang, Wenjie;Meng, Na;Ryder, Barbara",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040587848",
      "Primary study DOI": "10.1109/ICSME.2017.34",
      "Title": "Forecasting the duration of incremental build jobs",
      "Abstract": "Build systems automate the process of compiling, testing, packaging, and deploying modern software systems. While building a simple program may only take a few seconds on most modern computers, it may take hours, if not days, to build large software systems. Since modern build tools do not provide estimates of how long a build job will take, development and release teams cannot plan human and computer resources optimally. To fill this gap, we propose BUILDMÉTÉO - a tool to forecast the duration of incremental build jobs. BUILDMé TÉO analyzes a timing-annotated Build Dependency Graph (BDG) that we extract from the build system to forecast build job duration. We evaluate BUILDMÉTÉO by comparing forecasts to the timed execution of 2,163 incremental build jobs derived from replayed commits of the GLIB and VTK open source systems. We find that: (a) 87% of the studied commits do not change the BDG, suggesting that reasoning about build job duration using the BDG is a sensible starting point; (b) 94% of incremental build jobs that do not change the BDG have an estimation error of under ten seconds; and (c) build jobs with larger sets of modified files tend to yield more accurate duration forecasts. These results suggest that BUILDMÉTÉO can improve the transparency of build jobs, and thus, aid practitioners in build-related decision making.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Cao, Qi;Wen, Ruiyin;McIntosh, Shane",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040597683",
      "Primary study DOI": "10.1109/ICSME.2017.57",
      "Title": "The utility challenge of privacy-preserving data-sharing in cross-company defect prediction An empirical study of the CLIFF&MORPH algorithm",
      "Abstract": "In practice, the data owners of source projects may need to share data without disclosing sensitive information. Therefore, privacy-preserving data-sharing becomes an important topic in cross-company defect prediction (CCDP). In this context, the challenge is how to achieve a high privacy-preserving level while ensuring the utility of the shared privatized data for CCDP. CLIFF&MORPH is a recently proposed state-of-the-art privacy-preserving data-sharing algorithm for CCDP. It has been reported that the CLIFF&MORPH CCDP model produces a promising defect prediction performance. However, we find that ManualDown, a simple (unsupervised) module size model, built on the target projects has a comparable or even better defect prediction performance. Since ManualDown does not require any source project data to build the model, it is free of the privacy-preserving data-sharing challenges for CCDP. This means that, for practitioners, the motivation of applying privacy-preserving data-sharing algorithms to CCDP could not be well justified if the utility challenge is not addressed. We analyze the implications of our findings and outline the directions for future research. In particular, we strongly suggest that future studies at least use ManualDown as a baseline model for comparison to help develop practical privacy-preserving datasharing algorithms for CCDP.",
      "Keywords": "Cross-project | Defect prediction | Model | Privacy",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Fan, Yi;Lv, Chenxi;Zhang, Xu;Zhou, Guoqiang;Zhou, Yuming",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040540682",
      "Primary study DOI": "10.1109/ICSME.2017.10",
      "Title": "GEAS: Generic adaptive scheduling for high-efficiency context inconsistency detection",
      "Abstract": "Context-aware applications adapt their behavior based on collected contexts. However, contexts can be inaccurate due to sensing noise, which might cause applications to misbehave. One promising approach is to check contexts against consistency constraints at runtime, so as to detect context inconsistencies for applications and resolve them in time. The checking is typically immediate upon each collected context change. Such a scheduling strategy is intuitive for avoiding missing context inconsistencies in the detection, but may cause low-efficiency problems for heavy-workload checking scenarios, even if equipped with existing incremental or parallel constraint checking techniques. One may choose to check contexts in a batch way to increase the efficiency by reducing the number of constraint checking. However, this can easily cause missed context inconsistencies, denying the purpose of inconsistency detection. In this paper, we propose a novel scheduling strategy GEAS of two nice properties: (1) adaptively tuning the batch window to avoid missing any context inconsistency; (2) generic to checking techniques with no or little adjustment. We experimentally evaluated GEAS against the immediate strategy with existing constraint checking techniques. The experimental results show that GEAS achieved 143-645% efficiency improvement without missing any context inconsistency, while alternatives caused 39.2-65.3% loss of detected context inconsistencies.",
      "Keywords": "Context inconsistency | Scheduling strategy | Suspicious pair",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Guo, Bingying;Wang, Huiyan;Xu, Chang;Lu, Jian",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040570206",
      "Primary study DOI": "10.1109/ICSME.2017.43",
      "Title": "Interaction-based tracking of program entities for test case evolution",
      "Abstract": "After changes are made to a system, developers typically perform regression testing to uncover the regression faults in previously existing functionality of the system. However, during software evolution, the program entities (i.e., classes/methods) realizing such functionality might be modified/replaced by other entities. Thus, in the new version, existing test cases containing obsolete class references or method calls might be broken or might not test the intended functionality. To repair the broken method calls in those test cases, for each obsolete class/method, a tester needs to find the corresponding entity that provides the same/similar function or has the same role in the new version. To automate that task, we present ITRACK, a novel tool for matching program entities across versions, which mainly relies on their interactions in the code. The key idea is that the role and functionality of an entity correlate with its interactions with other entities (e.g., how it uses or is used by others). Two entities in two versions are matched based on the similarity of their interactions with other entities in the respective versions via our novel iterative matching algorithm. Our empirical evaluation shows that ITRACK achieves from 84-99% accuracy in identifying the calls in previous test cases that need to be adapted in accordance with the replacements of entities and provide such matching to support repairing broken method calls.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Nguyen, Hoan Anh;Nguyen, Tung Thanh;Nguyen, Tien N.;Nguyen, Hung Viet",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040600354",
      "Primary study DOI": "10.1109/ICSME.2017.63",
      "Title": "Supporting microservice evolution",
      "Abstract": "Microservices have become a popular pattern for deploying scale-out application logic and are used at companies like Netflix, IBM, and Google. An advantage of using microservices is their loose coupling, which leads to agile and rapid evolution, and continuous re-deployment. However, developers are tasked with managing this evolution and largely do so manually by continuously collecting and evaluating low-level service behaviors. This is tedious, error-prone, and slow. We argue for an approach based on service evolution modeling in which we combine static and dynamic information to generate an accurate representation of the evolving microservice-based system. We discuss how our approach can help engineers manage service upgrades, architectural evolution, and changing deployment trade-offs.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Sampaio, Adalberto R.;Kadiyala, Harshavardhan;Hu, Bo;Steinbacher, John;Erwin, Tony;Rosa, Nelson;Beschastnikh, Ivan;Rubin, Julia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040611701",
      "Primary study DOI": "10.1109/ICSME.2017.78",
      "Title": "KOWALSKI: Collecting API clients in easy mode",
      "Abstract": "Understanding API usage is important for upstream and downstream developers. However, compiling a dataset of API clients is often a tedious task, especially since one needs many clients to draw a representative picture of the API usage. In this paper, we present KOWALSKI, a tool that takes the name of an API, then finds and downloads client binaries by exploiting the Maven dependency management system. As a case study, we collect clients of Apache Lucene, the de facto standard for full-text search, analyze the binaries, and create a typed call graph that allows developers to identify hotspots in the API. A video demonstrating how KOWALSKI is used for this experiment can be found at https://youtu.be/zdx28GnoSRQ.",
      "Keywords": "API client collection | API usage analysis | Dependency management systems | Repository mining",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Leuenberger, Manuel;Osman, Haidar;Ghafari, Mohammad;Nierstrasz, Oscar",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040605526",
      "Primary study DOI": "10.1109/ICSME.2017.9",
      "Title": "The co-evolution of test maintenance and code maintenance through the lens of fine-grained semantic changes",
      "Abstract": "Automatic testing is a widely adopted technique for improving software quality. Software developers add, remove and update test methods and test classes as part of the software development process as well as during the evolution phase, following the initial release. In this work we conduct a large scale study of 61 popular open source projects and report the relationships we have established between test maintenance, production code maintenance, and semantic changes (e.g, statement added, method removed, etc.). performed in developers' commits. We build predictive models, and show that the number of tests in a software project can be well predicted by employing code maintenance profiles (i.e., how many commits were performed in each of the maintenance activities: corrective, perfective, adaptive). Our findings also reveal that more often than not, developers perform code fixes without performing complementary test maintenance in the same commit (e.g., update an existing test or add a new one). When developers do perform test maintenance, it is likely to be affected by the semantic changes they perform as part of their commit. Our work is based on studying 61 popular open source projects, comprised of over 240, 000 commits consisting of over 16, 000, 000 semantic change type instances, performed by over 4, 000 software engineers.",
      "Keywords": "Human factors | Mining software repositories | Predictive models | Software maintenance | Software metrics | Software testing",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Levin, Stanislav;Yehudai, Amiram",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040569179",
      "Primary study DOI": "10.1109/ICSME.2017.32",
      "Title": "Combining evolutionary algorithms with constraint solving for configuration optimization",
      "Abstract": "In Search based Software Engineering, well-known evolutionary algorithms are utilized to find the optimal solutions and address the configuration optimization problem for software product lines and trade off multiple often competing objectives. Previous work by Henard et al. showed the weakness of the constraint expressiveness and the optimality and speed. In this work, we propose a multi-objective evolutionary algorithm, which significantly improves the expressiveness from Boolean constraints to quantifier-free first-order constraints, particularly without sacrificing much performance. Furthermore, we propose a parallel portfolio approach. Empirical results demonstrate that this approach presents the performance superiority compared with the state-off-the-art and improves optimality as far as possible within a limited time budget. Finally, we present an overview of challenges in future.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Shi, Kai;Yu, Huiqun;Guo, Jianmei",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040543522",
      "Primary study DOI": "10.1109/ICSME.2017.54",
      "Title": "Graph data management of evolving dependency graphs for multi-versioned codebases",
      "Abstract": "Frappé is a code comprehension tool developed by Oracle Labs that extracts the code dependencies from a codebase and stores them in a graph database enabling advanced comprehension tasks. In addition to traditional text-based queries, such context-sensitive tools allow developers to express navigational queries of the form Does function X or something it calls write to global variable Y? providing more insight into the underlying codebases. Frappé captures the dependencies based on the most recent snapshot of the codebase. In this work we focus on the challenges associated with the management of multiple source code revisions, and investigate strategies to enable advanced code comprehension when the underlying codebase evolves over time. To find the deltas, we detail how entities can be resolved across versions, and propose a model for representing evolving dependency graphs. Our versioned graphs are built using snapshots of large codebases in the order of 13 million lines of code. We show growth and storage benefits of versioned graphs compared to independently storing individual snapshots. We also demonstrate how existing Frappé queries can be executed on versioned graphs and new queries can retrieve a history of changes in a function for a code review use case.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Goonetilleke, Oshini;Meibusch, David;Barham, Ben",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040543168",
      "Primary study DOI": "10.1109/ICSME.2017.80",
      "Title": "Recommending framework extension examples",
      "Abstract": "The use of software frameworks enables the delivery of common functionality but with significantly less effort than when developing from scratch. To meet application specific requirements, the behavior of a framework needs to be customized via extension points. A common way of customizing framework behavior is by passing a framework related object as an argument to an API call. Such an object can be created by subclassing an existing framework class or interface, or by directly customizing an existing framework object. However, to do this effectively requires developers to have extensive knowledge of the framework's extension points and their interactions. To aid the developers in this regard, we propose and evaluate a graph mining approach for extension point management. Specifically, we propose a taxonomy of extension patterns to categorize the various ways an extension point has been used in the code examples. Our approach mines a large amount of code examples to discover all extension points and patterns for each framework class. Given a framework class that is being used, our approach AIDS the developer by following a two-step recommendation process. First, it recommends all the extension points that are available in the class. Once the developer chooses an extension point, our approach then discovers all of its usage patterns and recommends the best code examples for each pattern. Using five frameworks, we evaluate the performance of our two-step recommendation, in terms of precision, recall, and F-measure. We also report several statistics related to framework extension points.",
      "Keywords": "API | Code example | Extension pattern | Extension point | Framework | Graph mining | Recommender | Reuse",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Asaduzzaman, Muhammad;Roy, Chanchal K.;Schneider, Kevin A.;Hou, Daqing",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040591330",
      "Primary study DOI": "10.1109/ICSME.2017.72",
      "Title": "AIMDROID: Activity-insulated multi-level automated testing for android applications",
      "Abstract": "Activities are the fundamental components of Android applications (apps). However, existing approaches to automated testing for Android apps cannot effectively manage the transitions between activities, e.g., too rarely or too often. Besides, some techniques need to repeatedly restart from scratch and revisit every intermediate activity to reach a specific one, which leads to unnecessarily long transitions and wasted time. To address these problems, we propose AIMDROID, a practical model-based approach to automated testing for Android apps that aims to manage the exploration of activities and meantime minimize unnecessary transitions between them. Specifically, AIMDROID applies an activity-insulated multi-level strategy during testing and replaying. It systematically discovers unexplored activities and then intensively exploits every discovered individual with a reinforcement learning guided random algorithm. We conduct comprehensive experiments on 50 popular closed-source commercial apps that in total have billions of daily usages in China. The results demonstrate that AIMDROID outperforms both SAPIENZ and Monkey in activity, method and instruction coverage, respectively. In addition, AIMDROID also reports more crashes than the other two.",
      "Keywords": "Android application testing | Model-based testing | Reinforcement learning",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Gu, Tianxiao;Cao, Chun;Liu, Tianchi;Sun, Chengnian;Deng, Jing;Ma, Xiaoxing;Lü, Jian",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040548570",
      "Primary study DOI": "10.1109/ICSME.2017.48",
      "Title": "Towards activity-aware tool support for change tasks",
      "Abstract": "To complete a change task, software developers perform a number of activities, such as locating and editing the relevant code. While there is a variety of approaches to support developers for change tasks, these approaches mainly focus on a single activity each. Given the wide variety of activities during a change task, a developer has to keep track of and switch between the different approaches. By knowing more about a developer's activities and in particular by knowing when she is working on which activity, we would be able to provide better and more tailored tool support, thereby reducing developer effort. In our research we investigate the characteristics of these activities, whether they can be identified, and whether we can use this additional information to improve developer support for change tasks. We conducted two exploratory studies with a total of 21 software developers collecting data on activities in the lab and field. An empirical analysis of the data shows, amongst other results, that activities comprise a consistently small amount of code elements across all developers and tasks (approx. 8.7 elements). Further analysis of the data shows, that we can automatically detect the boundaries and types of activities, and that the information on activity types can be used to improve the identification of relevant code elements.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Kevic, Katja;Fritz, Thomas",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040567114",
      "Primary study DOI": "10.1109/ICSME.2017.53",
      "Title": "Software practitioner perspectives on merge conflicts and resolutions",
      "Abstract": "Merge conflicts occur when software practitioners need to work in parallel and are inevitable in software development. Tool builders and researchers have focused on the prevention and resolution of merge conflicts, but there is little empirical knowledge about how practitioners actually approach and perform merge conflict resolution. Without such knowledge, tool builders might be building on wrong assumptions and researchers might miss opportunities for improving the state of the art. We conducted semi-structured interviews of 10 software practitioners across 7 organizations, including both open-source and commercial projects. We identify the key concepts and perceptions from practitioners, which we then validated via a survey of 162 additional practitioners. We find that practitioners are directly impacted by their perception of the complexity of the conflicting code, and may alter the timeline in which to resolve these conflicts, as well as the methods employed for conflict resolution based upon that initial perception. Practitioners' perceptions alter the impact of tools and processes that have been designed to preemptively and efficiently resolve merge conflicts. Understanding whether practitioners will react according to standard use cases is important when creating human-oriented tools to support development processes.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "McKee, Shane;Nelson, Nicholas;Sarma, Anita;Dig, Danny",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040613687",
      "Primary study DOI": "10.1109/ICSME.2017.71",
      "Title": "An experience report on applying passive learning in a large-scale payment company",
      "Abstract": "Passive learning techniques infer graph models on the behavior of a system from large trace logs. The research community has been dedicating great effort in making passive learning techniques more scalable and ready to use by industry. However, there is still a lack of empirical knowledge on the usefulness and applicability of such techniques in large scale real systems. To that aim, we conducted action research over nine months in a large payment company. Throughout this period, we iteratively applied passive learning techniques with the goal of revealing useful information to the development team. In each iteration, we discussed the findings and challenges to the expert developer of the company, and we improved our tools accordingly. In this paper, we present evidence that passive learning can indeed support development teams, a set of lessons we learned during our experience, a proposed guide to facilitate its adoption, and current research challenges.",
      "Keywords": "Dfasat | Experience report | Passive learning",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Wieman, Rick;Aniche, Maurício;Lobbezoo, Willem;Verwer, Sicco;Van Deursen, Arie",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040599854",
      "Primary study DOI": "10.1109/ICSME.2017.14",
      "Title": "Bug or not? Bug Report classification using N-gram IDF",
      "Abstract": "Previous studies have found that a significant number of bug reports are misclassified between bugs and nonbugs, and that manually classifying bug reports is a time-consuming task. To address this problem, we propose a bug reports classification model with N-gram IDF, a theoretical extension of Inverse Document Frequency (IDF) for handling words and phrases of any length. N-gram IDF enables us to extract key terms of any length from texts, these key terms can be used as the features to classify bug reports. We build classification models with logistic regression and random forest using features from N-gram IDF and topic modeling, which is widely used in various software engineering tasks. With a publicly available dataset, our results show that our N-gram IDF-based models have a superior performance than the topic-based models on all of the evaluated cases. Our models show promising results and have a potential to be extended to other software engineering tasks.",
      "Keywords": "Bug report classification | Bug reports | N-gram IDF",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Terdchanakul, Pannavat;Hata, Hideaki;Phannachitta, Passakorn;Matsumoto, Kenichi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040571715",
      "Primary study DOI": "10.1109/ICSME.2017.75",
      "Title": "An empirical study of local database usage in android applications",
      "Abstract": "Local databases have become an important component within mobile applications. Developers use local databases to provide mobile users with a responsive and secure service for data storage and access. However, using local databases comes with a cost Studies have shown that they are one of the most energy consuming components on mobile devices and misuse of their APIs can lead to performance and security problems. In this paper, we report the results of a large scale empirical study on 1,000 top ranked apps from the Google Play app store. Our results present a detailed look into the practices, costs, and potential problems associated with local database usage in deployed apps. We distill our findings into actionable guidance for developers and motivate future areas of research related to techniques to support mobile app developers.",
      "Keywords": "Database | Empirical study | Energy | Mobile applications | Performance | Security",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Lyu, Yingjun;Gui, Jiaping;Wan, Mian;Halfond, William G.J.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040598464",
      "Primary study DOI": "10.1109/ICSME.2017.55",
      "Title": "Constraints based approach to interactive feature location",
      "Abstract": "Feature location is a maintenance task to identify the implementation of a feature within the source code. To automate or support the task, extensive studies have been conducted on feature location techniques. In this paper, we focus on certain static and dynamic constraints regarding feature additions to object-oriented programs, and construct an interactive feature location procedure based on the constraints. We manually conducted a case study for several features of a real-world program on the assumption that the user always correctly answers the questions asked by the procedure. The results show that over 75% of the feature's implementation could be efficiently covered by the procedure with relatively small number of execution traces.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Fujioka, Daiki;Nitta, Naoya",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040611845",
      "Primary study DOI": "10.1109/ICSME.2017.24",
      "Title": "Understanding stack overflow code fragments",
      "Abstract": "Code fragments posted in answers on Q&A forums can form an important source of developer knowledge. However, effective reuse of code fragments found online often requires information other than the code fragment alone. We report on the results of a survey-based study to investigate to what extent developers perceive Stack Overflow code fragments to be self-explanatory. As part of the study, we also investigated the types of information missing from fragments that were not self-explanatory. We find that less than half of the Stack Overflow code fragments in our sample are considered to be self-explanatory by the 321 participants who answered our survey, and that the main issues that negatively affect code fragment understandability include incomplete fragments, code quality, missing rationale, code organization, clutter, naming issues, and missing domain information. This study is a step towards understanding developers' information needs as they relate to code fragments, and how these needs can be addressed.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Treude, Christoph;Robillard, Martin P.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040545409",
      "Primary study DOI": "10.1109/ICSME.2017.30",
      "Title": "Automating aggregation for Software Quality modeling",
      "Abstract": "Software Quality model is a well-accepted way for assessing high-level quality characteristics (e.g., maintainability) by aggregation from low-level metrics. Aggregation method in a software quality model denotes how to aggregate low-level metrics to high-level quality characteristics. Most of the existing quality models adopt the weighted linear aggregation method. The main drawback of weighted linear method is that it suffers from a lack of consensus in how to decide the correct weights. To address this issue, we present an automated aggregation method which adopts a kind of probabilistic weight instead of the subjective weight in previous aggregation methods. In particular, we leverage a topic modeling technique to estimate the probabilistic weight by learning from a software benchmark. In this manner, our approach can enable automated quality assessment by using the learned probabilistic relationship without manual effort. To evaluate the effectiveness of proposed aggregation approach, we conduct an empirical study on assessing one typical high-level quality characteristic (i.e., maintainability) which is regarded as an important characteristic defined in ISO 9126. The achieved results on 10 open source projects with totally 269 versions show that our method can reveal maintainability well and it outperforms a weighted linear aggregation method baseline in most of the projects.",
      "Keywords": "Aggregation method | Software Quality modeling | Topic model",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Yan, Meng;Xia, Xin;Zhang, Xiaohong;Yang, Dan;Xu, Ling",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040608482",
      "Primary study DOI": "10.1109/ICSME.2017.49",
      "Title": "Mining AndroZoo: A retrospect",
      "Abstract": "This paper presents a retrospect of an Android app collection named AndroZoo and some research works conducted on top of the collection. AndroZoo is a growing collection of Android apps from various markets including the official Google Play. At the moment, over five million Android apps have been collected. Based on AndroZoo, we have explored several directions that mine Android apps for resolving various challenges. In this work, we summarize those resolved mining challenges in three research dimensions, including code analysis, app evolution analysis, malware analysis, and present in each dimension several case studies that experimentally demonstrate the usefulness of AndroZoo.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Li, Li",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040639267",
      "Primary study DOI": "10.1109/ICSME.2017.65",
      "Title": "Flattening code for metrics measurement and analysis",
      "Abstract": "When we measure code metrics or analyze source code, code normalization is occasionally performed as a preprocessing. Code normalization means removing untargeted program elements, formatting source code, or transforming source code with specific rules. Code normalization makes measurement and analysis results more significant. Existing code normalization mainly targets program elements not influencing program behavior (e.g., code comments and blank lines) or program tokens (e.g., variable names and literals). In this paper, we propose a new code normalization technique targeting program structure. Our proposed technique transforms a complex program statement to simple ones. We call this transformation flattening. By flattening code, we can obtain source code including only simple program statements. As applications of the code flattening, we report how it changes LOC metric and clone detection results.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Higo, Yoshiki;Kusumoto, Shinji",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040591711",
      "Primary study DOI": "10.1109/ICSME.2017.15",
      "Title": "Embroidery: Patching vulnerable binary code of fragmentized android devices",
      "Abstract": "The rapid-iteration, web-style update cycle of Android helps fix revealed security vulnerabilities for its latest version. However, such security enhancements are usually only available for few Android devices released by certain manufacturers (e.g., Google's official Nexus devices). More manufactures choose to stop providing system update service for their obsolete models, remaining millions of vulnerable Android devices in use. In this situation, a feasible solution is to leverage existing source code patches to fix outdated vulnerable devices. To implement this, we introduce EMBROIDERY, a binary rewriting based vulnerability patching system for obsolete Android devices without requiring the manufacturer's source code against Android fragmentation. EMBROIDERY patches the known critical framework and kernel vulnerabilities in Android using both static and dynamic binary rewriting techniques. It transplants official patches (CVE source code patches) of known vulnerabilities to different devices by adopting heuristic matching strategies to deal with the code diversity introduced by Android fragmentation, and fulfills a complex dynamic memory modification to implement kernel vulnerabilities patching. We employ EMBROIDERY to patch sophisticated Android kernel and framework vulnerabilities for various manufactures' obsolete devices ranging from Android 4.2 to 5.1. The result shows the patched devices are able to defend against known exploits and the normal functions are not affected.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Zhang, Xuewen;Zhang, Yuanyuan;Li, Juanru;Hu, Yikun;Li, Huayi;Gu, Dawu",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040576689",
      "Primary study DOI": "10.1109/ICSME.2017.61",
      "Title": "Composite software diversification",
      "Abstract": "Many techniques of software vulnerability exploitation rely on deep and comprehensive analysis of vulnerable program binaries. If a copy of the vulnerable software is available to attackers, they can compose their attack scripts and payloads by studying the sample copy and launch attacks on other copies of the same software in deployment. By transforming software into different forms before deployment, software diversification is considered as an effective mitigation of attacks originated from malicious binary analyses. Essentially, developing a software diversification transformation is nontrivial because it has to preserve the original functionality, provide strong enough unpredictability, and introduce negligible cost. Enlightened by research in other areas, we seek to apply different diversification transformations to the same program for a synergy effect such that the resulting hybrid transformations can have boosted diversification effects with modest cost. We name this approach the composite software diversification. Although the concept is straightforward, it becomes challenging when searching for satisfactory compositions of primitive transformations that maximize the synergy effect and make a balance between effectiveness and cost. In this work, we undertake an in-depth study and develop a reasonably well working selection strategy to find a transformation composition that performs better than any single transformation used in the composition. We believe our work can provide guidelines for practitioners who would like to improve the design of diversification tools in the future.",
      "Keywords": "Binary instrumentation | Reverse engineering | Software diversification",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Wang, Shuai;Wang, Pei;Wu, Dinghao",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040583420",
      "Primary study DOI": "10.1109/ICSME.2017.50",
      "Title": "Personality and project success: Insights from a large-scale study with professionals",
      "Abstract": "A software project is typically completed as a result of a collective effort done by individuals of different personalities. Personality reflects differences among people in behaviour patterns, communication, cognition and emotion. It often impacts relationships and collaborative work, and software engineering teamwork is no exception. Some personalities are more likely to click while others to clash. A number of studies have investigated the relationship between personality and collaborative work success. However, most of them are done in a laboratory setting, do not involve professionals, or consider non software engineering tasks. Additionally, they only answer a limited set of questions, and many other questions remain open. To enrich the existing body of work, we study professionals working on real software projects, answering a new set of research questions that assess linkages between project manager personality and team personality composition and project success. In particular, our study investigates 28 recently completed software projects, which contain a total of 346 professionals, in 2 large IT companies. We asked project members to do a DISC (Dominance, Influence, Steadiness, and Compliant) personality test, and correlated the test outcomes with project success scores measured in six different dimensions. The scores were given by managers of three office as part of their regular day-today work. Our results show that project teams with dominant managers, along with those with more influential members and less dominant members, have higher success scores. This work provides new insights to construct a personality matching strategy that can contribute to building an effective project team.",
      "Keywords": "Personality | Project success | Software project | Team formation",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Xia, Xin;Lo, David;Bao, Lingfeng;Sharma, Abhishek;Li, Shanping",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040591802",
      "Primary study DOI": "10.1109/ICSME.2017.44",
      "Title": "Recommending when design technical debt should be self-admitted",
      "Abstract": "Previous research has shown how developers \"self-admit\" technical debt introduced in the source code, commenting why such code represents a workaround or a temporary, incomplete solution. This paper investigates the extent to which previously self-admitted technical debt can be used to provide recommendations to developers when they write new source code, suggesting them when to \"self-admit\" design technical debt, or possibly when to improve the code being written. To achieve this goal, we have developed a machine learning approach named TEDIOUS (TEchnical Debt IdentificatiOn System), which leverages various kinds of method-level features as independent variables, including source code structural metrics, readability metrics and, last but not least, warnings raised by static analysis tools. We assessed TEDIOUS on data from nine open source projects for which there are available tagged self-admitted technical debt instances, also comparing the performances of different machine learners. Results of the study indicate that TEDIOUS achieves, when recommending self-admitted technical debts within a single project, an average precision of about 50% and a recall of 52%. When predicting cross-projects, TEDIOUS improves, achieving an average precision of 67% and a recall of 55%. Last, but not least, we noticed how TEDIOUS leverages readability, size and complexity metrics, as well as some warnings raised by static analysis tools.",
      "Keywords": "Recommender systems | Self-admitted technical debt | Static analysis tools",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Zampetti, Fiorella;Noiseux, Cedric;Antoniol, Giuliano;Khomh, Foutse;Di Penta, Massimiliano",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040612636",
      "Primary study DOI": "10.1109/ICSME.2017.21",
      "Title": "SimPact: Impact analysis for simulink models",
      "Abstract": "With the increasing use of Simulink modeling in embedded system development, there comes a need for effective techniques and tools to support managing these models and their related artifacts. Because maintenance of models makes up such a large portion of the cost and effort of the system as a whole, it is increasingly important to ensure that the process of managing models is as simple, intuitive and efficient as possible. Part of model management comes in the form of impact analysis - the ability to determine the impact of a change to a model on related artifacts such as test cases and other models. This paper presents an approach to impact analysis for Simulink models, and a tool to implement it (SimPact). We validate our tool as an impact predictor against the maintenance history of a large set of industrial models and their tests. The results show a high level of both precision and recall in predicting actual impact of model changes on tests.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Rapos, Eric J.;Cordy, James R.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040622740",
      "Primary study DOI": "10.1109/ICSME.2017.62",
      "Title": "Behavior metrics for prioritizing investigations of exceptions",
      "Abstract": "Many software development teams collect product defect reports, which can either be manually submitted or automatically created from product logs. Periodically, the teams use the collected defect reports to prioritize which defect to address next. We present a set of behavior-based metrics that can be used in this process. These metrics are based on the insight that development teams can estimate user inconvenience from user and application behavior in interaction logs. To estimate user inconvenience, the behavior metrics capture important user and application behavior after exceptions (the defects of interest in our case). We validated these metrics through a survey of how developers would incorporate the behavior metrics into their prioritization decisions. We found that developers change their priority of investigating an exception about 31% of the time after including the behavior metrics in the priority decision. These findings provide evidence that behavior metrics provide a promising advance towards prioritizing application exceptions.",
      "Keywords": "Behavior metrics | Bug triage | Exceptions | IDE usage data | Stack traces",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Coker, Zack;Damevski, Kostadin;Le Goues, Claire;Kraft, Nicholas A.;Shepherd, David;Pollock, Lori",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040541904",
      "Primary study DOI": "10.1109/ICSME.2017.16",
      "Title": "A characterization study of repeated bug fixes",
      "Abstract": "Programmers always fix bugs when maintaining software. Previous studies showed that developers apply repeated bug fixes-similar or identical code changes-to multiple locations. Based on the observation, researchers built tools to identify code locations in need of similar changes, or to suggest similar bug fixes to multiple code fragments. However, some fundamental research questions, such as what are the characteristics of repeated bug fixes, are still unexplored. In this paper, we present a comprehensive empirical study with 341,856 bug fixes from 3 open source projects to investigate repeated fixes in terms of their frequency, edit locations, and semantic meanings. Specifically, we sampled bug reports and retrieved the corresponding fixing patches in version history. Then we chopped patches into smaller fixes (edit fragments). Among all the fixes related to a bug, we identified repeated fixes using clone detection, and put a fix and its repeated ones into one repeated-fix group. With these groups, we characterized the edit locations, and investigated the common bug patterns as well as common fixes. Our study on Eclipse JDT, Mozilla Firefox, and LibreOffice shows that (1) 15-20% of bugs involved repeated fixes; (2) 73-92% of repeated-fix groups were applied purely to code clones; and (3) 39% of manually examined groups focused on bugs relevant to additions or deletions of whole if-structures. These results deepened our understanding of repeated fixes. They enabled us to assess the effectiveness of existing tools, and will further provide insights for future research directions in automatic software maintenance and program repair.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Yue, Ruru;Meng, Na;Wang, Qianxiang",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040637055",
      "Primary study DOI": "10.1109/ICSME.201767",
      "Title": "A tale of CI build failures: An open source and a financial organization perspective",
      "Abstract": "Continuous Integration (CI) and Continuous Delivery (CD) are widespread in both industrial and open-source software (OSS) projects. Recent research characterized build failures in CI and identified factors potentially correlated to them. However, most observations and findings of previous work are exclusively based on OSS projects or data from a single industrial organization. This paper provides a first attempt to compare the CI processes and occurrences of build failures in 349 Java OSS projects and 418 projects from a financial organization, ING Nederland. Through the analysis of 34, 182 failing builds (26% of the total number of observed builds), we derived a taxonomy of failures that affect the observed CI processes. Using cluster analysis, we observed that in some cases OSS and ING projects share similar build failure patterns (e.g., few compilation failures as compared to frequent testing failures), while in other cases completely different patterns emerge. In short, we explain how OSS and ING CI processes exhibit commonalities, yet are substantially different in their design and in the failures they report.",
      "Keywords": "Agile development | Build failures | Continuous Delivery | Continuous Integration",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Vassallo, Carmine;Schermann, Gerald;Zampetti, Fiorella;Romano, Daniele;Leitner, Philipp;Zaidman, Andy;Di Penta, Massimiliano;Panichella, Sebastiano",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040539918",
      "Primary study DOI": "10.1109/ICSME.2017.35",
      "Title": "DROIDFAX: A toolkit for systematic characterization of android applications",
      "Abstract": "As the Android app market keeps growing, there is a pressing need for automated tool supports to empower Android developers to produce quality apps with higher productivity. Yet existing tools for Android mostly aim at security and privacy protection, primarily targeting end users and security analysts. Towards filling this gap, we present DROIDFAX, a toolkit that targets the developers to help them comprehensively understand Android apps regarding their code structure and behavioral traits. To that end, DROIDFAX features a systematic app characterization in multiple dimensions and views, through lightweight code analysis and profiling of both ordinary method calls (including those via reflection and exceptional control flows) and inter-component communications (including those within and across apps). The toolkit also includes a statement coverage tracker that works directly on bytecode and a dedicated tracer of events occurred during app executions. Applying DROIDFAX in two use cases has resulted in important findings about app behavioral patterns and an advanced security defense technique for Android. Empirical results also showed promising efficiency and scalability of DROIDFAX for practical adoption. A demo video for DROIDFAX can be viewed here or downloaded here.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Cai, Haipeng;Ryder, Barbara G.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040559968",
      "Primary study DOI": "10.1109/ICSME.2017.8",
      "Title": "An empirical study on the removal of Self-Admitted Technical Debt",
      "Abstract": "Technical debt refers to the phenomena of taking shortcuts to achieve short term gain at the cost of higher maintenance efforts in the future. Recently, approaches were developed to detect technical debt through code comments, referred to as Self-Admitted Technical Debt (SATD). Due to its importance, several studies have focused on the detection of SATD and examined its impact on software quality. However, preliminary findings showed that in some cases SATD may live in a project for a long time, i.e., more than 10 years. These findings clearly show that not all SATD may be regarded as 'bad' and some SATD needs to be removed, while other SATD may be fine to take on. Therefore, in this paper, we study the removal of SATD. In an empirical study on five open source projects, we examine how much SATD is removed and who removes SATD? We also investigate for how long SATD lives in a project and what activities lead to the removal of SATD? Our findings indicate that the majority of SATD is removed and that the majority is self-removed (i.e., removed by the same person that introduced it). Moreover, we find that SATD can last between approx. 18-172 days, on median. Finally, through a developer survey, we find that developers mostly use SATD to track future bugs and areas of the code that need improvements. Also, developers mostly remove SATD when they are fixing bugs or adding new features. Our findings contribute to the body of empirical evidence on SATD, in particular evidence pertaining to its removal.",
      "Keywords": "Mining software repositories | Self-Admitted Technical Debt | Source code quality",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Da Maldonado, Everton S.;Abdalkareem, Rabe;Shihab, Emad;Serebrenik, Alexander",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040626682",
      "Primary study DOI": "10.1109/ICSME.2017.79",
      "Title": "Deep green: Modelling time-series of software energy consumption",
      "Abstract": "Inefficient mobile software kills battery life. Yet, developers lack the tools necessary to detect and solve energy bugs in software. In addition, developers are usually tasked with the creation of software features and triaging existing bugs. This means that most developers do not have the time or resources to research, build, or employ energy debugging tools. We present a new method for predicting software energy consumption to help debug software energy issues. Our approach enables developers to align traces of software behavior with traces of software energy consumption. This allows developers to match run-time energy hot spots to the corresponding execution. We accomplish this by applying recent neural network models to predict time series of energy consumption given a software's behavior. We compare our time series models to prior state-of-the-art models that only predict total software energy consumption. We found that machine learning based time series based models, and LSTM based time series based models, can often be more accurate at predicting instantaneous power use and total energy consumption.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Romansky, Stephen;Borle, Neil C.;Chowdhury, Shaiful;Hindle, Abram;Greiner, Russ",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040624154",
      "Primary study DOI": "10.1109/ICSME.2017.23",
      "Title": "Atlantis: Improving the analysis and visualization of large assembly execution traces",
      "Abstract": "Assembly execution trace analysis is an effective approach for discovering potential software vulnerabilities. However, the size of the execution traces and the lack of source code makes this a manual, labor-intensive process. Instead of browsing billions of instructions one by one, software security analysts need higher-level information that can provide an overview of the execution of a program to assist in the identification of patterns of interest. The tool we present in this paper, Atlantis, is our trace analysis environment for multi-gigabyte assembly traces, and it contains a number of new features that make it particularly successful in meeting this goal. The contributions of this continuous work fall into three main categories: a) the ability to efficiently reconstruct and navigate the memory state of a program at any point in a trace; b) the ability to reconstruct and navigate functions and processes; and c) a powerful search facility to query and navigate traces. These contributions are not only novel for Atlantis but also for the field of assembly trace analysis. Software is becoming increasingly complex and many applications are designed as collaborative systems or modules interacting with each other, which makes the discovery of vulnerabilities extremely difficult. With the novel features we describe in this paper, our tool extends the security analyst's ability to investigate vulnerabilities of real-world large execution traces and can lay the groundwork for supporting trace analysis of interacting programs in the future.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Huang, Huihui Nora;Verbeek, Eric;German, Daniel;Storey, Margaret Anne;Salois, Martin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040615045",
      "Primary study DOI": "10.1109/ICSME.2017.40",
      "Title": "Confusion detection in code reviews",
      "Abstract": "Code reviews are an important mechanism for assuring quality of source code changes. Reviewers can either add general comments pertaining to the entire change or pinpoint concerns or shortcomings about a specific part of the change using inline comments. Recent studies show that reviewers often do not understand the change being reviewed and its context. Our ultimate goal is to identify the factors that confuse code reviewers and understand how confusion impacts the efficiency and effectiveness of code review(er)s. As the first step towards this goal we focus on the identification of confusion in developers' comments. Based on an existing theoretical framework categorizing expressions of confusion, we manually classify 800 comments from code reviews of the Android project. We observe that confusion can be reasonably well-identified by humans: raters achieve moderate agreement (Fleiss' kappa 0.59 for the general comments and 0.49 for the inline ones). Then, for each kind of comment we build a series of automatic classifiers that, depending on the goals of the further analysis, can be trained to achieve high precision (0.875 for the general comments and 0.615 for the inline ones), high recall (0.944 for the general comments and 0.988 for the inline ones), or substantial precision and recall (0.696 and 0.542 for the general comments and 0.434 and 0.583 for the inline ones, respectively). These results motivate further research on the impact of confusion on the code review process. Moreover, other researchers can employ the proposed classifiers to analyze confusion in other contexts where software development-related discussions occur, such as mailing lists.",
      "Keywords": "Code review | Confusion | Machine learning",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Ebert, Felipe;Castor, Fernando;Novielli, Nicole;Serebrenik, Alexander",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040634957",
      "Primary study DOI": "10.1109/ICSME.2017.73",
      "Title": "Behavior-informed algorithms for automatic documentation generation",
      "Abstract": "Programmers rely on source code documentation to quickly understand what the source code does and how they would use it. Unfortunately, many programmers do not have the time to write and maintain source code documentation. A solution to this problem is to document and summarize source code automatically. Unfortunately, research efforts to automatically generate documentation have stalled recently because the research community does not know exactly what a summary of source code should include. To solve this problem, my overall strategy is to study programmer behavior in order to write algorithms that mimic that behavior. I have four key areas of work in which I execute that strategy: First, I determine what areas of code programmers read when they create documentation. Second, I find patterns in programmers' eye movements when they reading code. Third, I use recordings of developer-client meetings to extract user story information. Finally, I propose to conduct a grounded theory study at a medium sized software company to determine whether factors outside the code influence source code summarization. This paper discusses the foundation for my career in the software engineering community, and I seek the community's advice.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Rodeghero, Paige",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040636022",
      "Primary study DOI": "10.1109/ICSME.2017.74",
      "Title": "Dissecting android inter-component communications via interactive visual explorations",
      "Abstract": "Inter-component communication (ICC) serves as a key element of any Android app's implementation. Specifically, an Android app uses Intents as the main mechanism for ICC to complete tasks such as switching between different user interfaces, starting background services, communicating to other apps on the Android device, and saving or retrieving data from device storage. Thus, dissecting how an app uses ICCs to accomplish its tasks is fundamental to understanding the app's underlying behaviors. Existing works involving ICCs focus on resolving Intents and/or mapping ICCs for security analysis purposes. While the ICC analysis result is potentially informative, it is difficult to digest on its own and has not been utilized for app/ICC comprehension. Also, the result is based on static analysis, and thus does not inform of run-time app behaviors exercised via ICCs. We propose the first approach to dissecting Android ICCs via interactive, dynamic visualizations, empowered by static and dynamic ICC analysis combined. Through multiple semantically linked views and in-situ interaction features, our approach enables real-time visual explorations of ICCs as they are triggered by user inputs to the app under analysis. It conveys rich ICC information while managing limited visual space through various visualization design strategies. Our case studies with a number of commonly used apps have showed promising merits of the approach for both deep ICC comprehension and security vulnerability inspection, as well as practical scalability. Our tool prototype of the approach has enabled quick revelation of inefficient, intrusive, and malicious behaviors in several popular apps that were normally hidden to users.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Jenkins, John;Cai, Haipeng",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040613801",
      "Primary study DOI": "10.1109/ICSME.2017.11",
      "Title": "Detecting DOM-sourced cross-site scripting in browser extensions",
      "Abstract": "In recent years, with the advances in JavaScript engines and the adoption of HTML5 APIs, web applications begin to show a tendency to shift their functionality from the server side towards the client side, resulting in dense and complex interactions with HTML documents using the Document Object Model (DOM). As a consequence, client-side vulnerabilities become more and more prevalent. In this paper, we focus on DOM-sourced Cross-site Scripting (XSS), which is a kind of severe but not well-studied vulnerability appearing in browser extensions. Comparing with conventional DOM-based XSS, a new attack surface is introduced by DOM-sourced XSS where the DOM could become a vulnerable source as well besides common sources such as URLs and form inputs. To discover such vulnerability, we propose a detecting framework employing hybrid analysis with two phases. The first phase is the lightweight static analysis consisting of a text filter and an abstract syntax tree parser, which produces potential vulnerable candidates. The second phase is the dynamic symbolic execution with an additional component named shadow DOM, generating a document as a proof-of-concept exploit. In our large-scale real-world experiment, 58 previously unknown DOM-sourced XSS vulnerabilities were discovered in user scripts of the popular browser extension Greasemonkey.",
      "Keywords": "Browser extension vulnerability | DOM-sourced XSS | Dynamic symbolic execution | JavaScript | Shadow DOM | Web security",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Pan, Jinkun;Mao, Xiaoguang",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040542582",
      "Primary study DOI": "10.1109/ICSME.2017.19",
      "Title": "Heterogeneous defect prediction through multiple kernel learning and ensemble learning",
      "Abstract": "Heterogeneous defect prediction (HDP) aims to predict defect-prone software modules in one project using heterogeneous data collected from other projects. Recently, several HDP methods have been proposed. However, these methods do not sufficiently incorporate the two characteristics of the defect prediction data: (1) data could be linearly inseparable, and (2) data could be highly imbalanced. These two data characteristics make it challenging to build an effective HDP model. In this paper, we propose a novel Ensemble Multiple Kernel Correlation Alignment (EMKCA) based approach to HDP, which takes into consideration the two characteristics of the defect prediction data. Specifically, we first map the source and target project data into high dimensional kernel space through multiple kernel leaning, where the defective and non-defective modules can be better separated. Then, we design a kernel correlation alignment method to make the data distribution of the source and target projects similar in the kernel space. Finally, we integrate multiple kernel classifiers with ensemble learning to relieve the influence caused by class imbalance problem, which can improve the accuracy of the defect prediction model. Consequently, EMKCA owns the advantages of both multiple kernel learning and ensemble learning. Extensive experiments on 30 public projects show that EMKCA outperforms the related competing methods.",
      "Keywords": "Class imbalance | Ensemble learning | Heterogeneous defect prediction | Kernel correlation alignment | Linearly inseparable | Multiple kernel learning",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Li, Zhiqiang;Jing, Xiao Yuan;Zhu, Xiaoke;Zhang, Hongyu",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040606094",
      "Primary study DOI": "10.1109/ICSME.2017.47",
      "Title": "How do developers test android applications?",
      "Abstract": "Enabling fully automated testing of mobile applications has recently become an important topic of study for both researchers and practitioners. A plethora of tools and approaches have been proposed to aid mobile developers both by augmenting manual testing practices and by automating various parts of the testing process. However, current approaches for automated testing fall short in convincing developers about their benefits, leading to a majority of mobile testing being performed manually. With the goal of helping researchers and practitioners - who design approaches supporting mobile testing - to understand developer's needs, we analyzed survey responses from 102 open source contributors to Android projects about their practices when performing testing. The survey focused on questions regarding practices and preferences of developers/testers in-the-wild for (i) designing and generating test cases, (ii) automated testing practices, and (iii) perceptions of quality metrics such as code coverage for determining test quality. Analyzing the information gleaned from this survey, we compile a body of knowledge to help guide researchers and professionals toward tailoring new automated testing approaches to the need of a diverse set of open source developers.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Linares-Vásquez, Mario;Bernal-Cárdenas, Carlos;Moran, Kevin;Poshyvanyk, Denys",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040575311",
      "Primary study DOI": "10.1109/ICSME.2017.36",
      "Title": "Artifacts for dynamic analysis of android apps",
      "Abstract": "We describe a set of artifacts for dynamic analysis of Android apps, including a dataset used in a dynamic characterization study, source code used for performing the study, an Android inter-app benchmark suite, and definition of Android behavioral metrics.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Cai, Haipeng;Ryder, Barbara G.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040619464",
      "Primary study DOI": "10.1109/ICSME.2017.13",
      "Title": "An exploratory study of performance regression introducing code changes",
      "Abstract": "Performance is an important aspect of software quality. In fact, large software systems failures are often due to performance issues rather than functional bugs. One of the most important performance issues is performance regression. Examples of performance regressions are response time degradation and increased resource utilization. Although performance regressions are not all bugs, they often have a direct impact on users' experience of the system. Due to the possible large impact of performance regressions, prior research proposes various automated approaches that detect performance regressions. However, the detection of performance regressions is conducted after the fact, i.e., after the system is built and deployed in the field or dedicated performance testing environments. On the other hand, there exists rich software quality research that examines the impact of code changes on software quality; while a majority of prior findings do not use performance regression as a sign of software quality degradation. In this paper, we perform an exploratory study on the source code changes that introduce performance regressions. We conduct a statistically rigorous performance evaluation on 1, 126 commits from ten releases of Hadoop and 135 commits from five releases of RxJava. In particular, we repetitively run tests and performance micro-benchmarks for each commit while measuring response time, CPU usage, Memory usage and I/O traffic. We identify performance regressions in each test or performance micro-benchmark if there exists statistically significant degradation with medium or large effect sizes, in any performance metric. We find that performance regressions widely exist during the development of both subject systems. By manually examining the issue reports that are associated with the identified performance regression introducing commits, we find that the majority of the performance regressions are introduced while fixing other bugs. In addition, we identify six root-causes of performance regressions. 12.5% of the examined performance regressions can be avoided or their impact may be reduced during development. Our findings highlight the need for performance assurance activities during development. Developers should address avoidable performance regressions and be aware of the impact of unavoidable performance regressions.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Chen, Jinfu;Shang, Weiyi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040584332",
      "Primary study DOI": "10.1109/ICSME.2017.77",
      "Title": "Evaluating state-of-the-art free and open source static analysis tools against buffer errors in android apps",
      "Abstract": "Modern mobile apps incorporate rich and complex features, opening the doors for different security concerns. Android is the dominant platform in mobile app markets, and enhancing its apps security is a considerable area of research. Android malware (introduced intentionally by developers) has been well studied and many tools are available to detect them. However, little attention has been directed to address vulnerabilities caused unintentionally by developers in Android apps. Static analysis has been one way to detect such vulnerabilities in traditional desktop and server side desktop. Therefore, our research aims at assessing static analysis tools that could be used by Android developers. Our preliminary analysis revealed that Buffer Errors are the most frequent type of vulnerabilities that threaten Android apps. Also, we found that Buffer Errors in Android apps have the highest risk on Android that affects data integrity, confidentiality, and availability. Our main study therefore tested whether state-of-the-art static analysis tools could detect Buffer Errors in Android apps. We investigated 6 static analysis tools that are designed to detect Buffer Errors. The study shows that the free and open source state-of-the-art static analysis tools do not efficiently discover Buffer Error vulnerabilities in Android apps. We analyzed the tools carefully to see why they could not discover Buffer Errors and found that the lack of semantic analysis capabilities, inapplicability to Android apps, and the gap between native code and other contexts were some of the reasons. Thus, we concluded that there is a need to build better free and open source static analysis tools for detecting Buffer Errors in Android apps.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Aloraini, Bushra;Nagappan, Meiyappan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040599897",
      "Primary study DOI": "10.1109/ICSME.2017.33",
      "Title": "Bug propagation through code cloning: An empirical study",
      "Abstract": "Code clones are defined to be the identical or nearly similar code fragments in a code-base. According to a number of existing studies, code clones are directly related to bugs and inconsistencies in software systems. Code cloning (i.e., creating code clones) is suspected to propagate temporarily hidden bugs from one code fragment to another. However, there is no study on the intensity of bug-propagation through code cloning. In this paper we present our empirical study on bug-propagation through code cloning. We define two clone evolution patterns that reasonably indicate bug propagation through code cloning. We first identify code clones that experienced bug-fix changes by analyzing software evolution history, and then determine which of these code clones evolved following the bug propagation patterns. According to our study on thousands of commits of four open-source subject systems written in Java, up to 33% of the clone fragments that experience bug-fix changes can contain propagated bugs. Around 28.57% of the bug-fixes experienced by the code clones can occur for fixing propagated bugs. We also find that near-miss clones are primarily involved with bug-propagation rather than identical clones. The clone fragments involved with bug propagation are mostly method clones. Bug propagation is more likely to occur in the clone fragments that are created in the same commit operation rather than in different commits. Our findings are important for prioritizing code clones for refactoring and tracking from the perspective of bug propagation.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Mondal, Manishankar;Roy, Chanchal K.;Schneider, Kevin A.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040544741",
      "Primary study DOI": "10.1109/ICSME.2017.70",
      "Title": "CityVR: Gameful software visualization",
      "Abstract": "Gamification of software engineering tasks improve developer engagement, but has been limited to mechanisms such as points and badges. We believe that a tool that provides developers an interface analogous to computer games can represent the gamification of software engineering tasks more effectively via software visualization. We introduce CityVR - an interactive software visualization tool that implements the city metaphor technique using virtual reality in an immersive 3D environment medium to boost developer engagement in software comprehension tasks. We evaluated our tool with a case study based on ArgoUML. We measured engagement in terms of feelings, interaction, and time perception. We report on how our design choices relate to developer engagement. We found that developers i) felt curious, immersed, in control, excited, and challenged, ii) spent considerable interaction time navigating and selecting elements, and iii) perceived that time passed faster than in reality, and therefore were willing to spend more time using the tool to solve software engineering tasks.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Merino, Leonel;Ghafari, Mohammad;Anslow, Craig;Nierstrasz, Oscar",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "",
      "Primary study DOI": "10.1109/ICSME.2017.12",
      "Title": "Does refactoring of test smells induce fixing flaky tests?",
      "Abstract": "Retracted.",
      "Keywords": "",
      "Publication venue": "",
      "Publication date": "",
      "Publication type": "",
      "Authors": "Palomba, F.;Zaidman, A.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040586242",
      "Primary study DOI": "10.1109/ICSME.2017.83",
      "Title": "Refactoring asynchrony in JavaScript",
      "Abstract": "JavaScript is a widely used programming language that makes extensive use of asynchronous computation, particularly in the form of asynchronous callbacks. These callbacks are used to handle tasks, from GUI events to network messages, in a non-blocking fashion. Asynchronous callbacks present developers with two challenges. First, JavaScript's try/catch error-handling mechanism is not sufficient for proper error handling in asynchronous contexts. In response, the JavaScript community has come to rely on the error-first protocol, an informal programming idiom that is not enforced or checked by the runtime. Second, JavaScript callbacks are frequently nested, making them difficult to handle (also known as callback hell). Fortunately, a recent language extension called promises provides an alternative to asynchronous callbacks. The adoption of promises, however, has been slow as refactoring existing code to use promises is a complex task. We present a set of program analysis techniques to detect instances of asynchronous callbacks and to refactor such callbacks, including callbacks with the error-first protocol, into promises. We implement our techniques in a tool called PROMISESLAND. We perform a manual analysis of four JavaScript applications to evaluate the tool's precision and recall, which are, on average, 100% and 83%, respectively. We evaluate PROMISESLAND on 21 large JavaScript applications, and find that PROMISESLAND (1) correctly refactors callbacks to promises, (2) outperforms a recent related refactoring technique, and (3) runs in under three seconds on all of our evaluation targets.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Gallaba, Keheliya;Hanam, Quinn;Mesbah, Ali;Beschastnikh, Ivan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040575712",
      "Primary study DOI": "10.1109/ICSME.2017.64",
      "Title": "Revisiting turnover-induced knowledge loss in software projects",
      "Abstract": "In large software projects, tacit knowledge of the system is threatened by developer turnover. When a developer leaves the project, their knowledge may be lost if the other developers do not understand the design decisions made by the leaving developer. Understanding the source code written by leaving developers thus becomes a burden for their successors. In a previous paper, Rigby et al. reported on a case study of turnover-induced knowledge loss in two large projects, Chromium and a project at Avaya, using risk evaluation methods usually applied to financial systems. They found that the two projects were susceptible to large knowledge losses that are more than three times the average loss. We report on a replication of their study on the Chromium project, as well as seven other large and medium-sized open source projects. We also extended their work by studying two variations of the knowledge loss metric, as well as the location and persistence of abandoned files. We found that all projects had a similar knowledge loss probability distribution, but extreme knowledge loss can be more severe than those originally discovered in Chromium and the project at Avaya. We also found that, in the systems under study, abandoned files often remained in the system for long periods.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Nassif, Mathieu;Robillard, Martin P.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013070479",
      "Primary study DOI": "10.1109/ICSME.2016.25",
      "Title": "An empirical study on the characteristics of python fine-grained source code change types",
      "Abstract": "Software has been changing during its whole life cycle. Therefore, identification of source code changes becomes a key issue in software evolution analysis. However, few current change analysis research focus on dynamic language software. In this paper, we pay attention to the fine-grained source code changes of Python software. We implement an automatic tool named PyCT to extract 77 kinds of fine-grained source code change types from commit history information. We conduct an empirical study on ten popular Python projects from five domains, with 132294 commits, to investigate the characteristics of dynamic software source code changes. Analyzing the source code changes in four aspects, we distill 11 findings, which are summarized into two insights on software evolution: change prediction and fault code fix. In addition, we provide direct evidence on how developers use and change dynamic features. Our results provide useful guidance and insights for improving the understanding of source code evolution of dynamic language software.",
      "Keywords": "Fine-grained change types | Python | Software evolution",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Lin, Wei;Chen, Zhifei;Ma, Wanwangying;Chen, Lin;Xu, Lei;Xu, Baowen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013070465",
      "Primary study DOI": "10.1109/ICSME.2016.66",
      "Title": "Enhancing automated program repair with deductive verification",
      "Abstract": "Automated program repair (APR) is a challenging process of detecting bugs, localizing buggy code, generating fix candidates and validating the fixes. Effectiveness of program repair methods relies on the generated fix candidates, and the methods used to traverse the space of generated candidates to search for the best ones. Existing approaches generate fix candidates based on either syntactic searches over source code or semantic analysis of specification, e.g., test cases. In this paper, we propose to combine both syntactic and semantic fix candidates to enhance the search space of APR, and provide a function to effectively traverse the search space. We present an automated repair method based on structured specifications, deductive verification and genetic programming. Given a function with its specification, we utilize a modular verifier to detect bugs and localize both program statements and sub-formulas in the specification that relate to those bugs. While the former are identified as buggy code, the latter are transformed as semantic fix candidates. We additionally generate syntactic fix candidates via various mutation operators. Best candidates, which receives fewer warnings via a static verification, are selected for evolution though genetic programming until we find one satisfying the specification. Another interesting feature of our proposed approach is that we efficiently ensure the soundness of repaired code through modular (or compositional) verification. We implemented our proposal and tested it on C programs taken from the SIR benchmark that are seeded with bugs, achieving promising results.",
      "Keywords": "Automated repair | Deductive verification | Genetic programming | Sound repair",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Le, Xuan Bach D.;Le, Quang Loc;Lo, David;Le Goues, Claire",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013080585",
      "Primary study DOI": "10.1109/ICSME.2016.45",
      "Title": "Continuous maintenance",
      "Abstract": "There are many \"continuous\" practices in software engineering, for example continuous integration (CI), continuous delivery (CD), continuous release (CR), and DevOps. However, the maintenance aspect of continuity is rarely mentioned in publication or education. The continuous practices and applications depend on many repositories and artifacts, such as databases, servers, virtual machines, storage, data, meta-data, various logs, and reports. Continuous maintenance (CM) seeks to maintain these repositories and artifacts properly and consistently through automation, summarization, compaction, archival, and removal. For example, retaining builds and test results created by CI consumes storage. An automated CM process can remove the irrelevant artifacts and compact the relevant artifacts to reduce storage usage. Proper CM is essential for applications' long term sustainability. There are two sides of CM: pre-production and post-production. During the pre-production phase, CM maintains the health of the development environments and the relevant processes. Then during the postproduction phase, CM maintains the health of the applications. This paper defines CM for developers. CM complements and completes continuous practices.",
      "Keywords": "Continuous delivery | Continuous integration | Continuous maintenance | Continuous release | DevOps | Software engineering",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Pang, Candy;Hindle, Abram",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013032060",
      "Primary study DOI": "10.1109/ICSME.2016.57",
      "Title": "Evolving requirements-to-code trace links across versions of a software system",
      "Abstract": "Trace links provide critical support for numerous software engineering activities including safety analysis, compliance verification, test-case selection, and impact prediction. However, as the system evolves over time, there is a tendency for the quality of trace links to degrade into a tangle of inaccurate and untrusted links. This is especially true with the links between source-code and upstream artifacts such as requirements - because developers frequently refactor and change code without updating the links. We present TLE (Trace Link Evolver), a solution for automating the evolution of trace links as changes are introduced to source code. We use a set of heuristics, open source tools, and information retrieval methods to detect common change scenarios across different versions of software. Each change scenario is then associated with a set of link evolution heuristics which are used to evolve trace links. We evaluate our approach through a controlled experiment and also through applying it across 27 releases of the Cassandra Database System. Results show that the trace links evolved using our approach are significantly more accurate than those generated using information retrieval alone.",
      "Keywords": "Evolution | Maintenance | Traceability",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Rahimi, Mona;Goss, William;Cleland-Huang, Jane",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013124289",
      "Primary study DOI": "10.1109/ICSME.2016.40",
      "Title": "Using topic model to suggest fine-grained source code changes",
      "Abstract": "Prior research has shown that source code and its changes are repetitive. Several approaches have leveraged that phenomenon to detect and recommend change/fix patterns. In this paper, we propose TasC, a model that leverages the context of change tasks in development history to suggest fine-grained code change/fix at the program statement level. We use Latent Dirichlet Allocation (LDA) to capture the change task context via co-occurring program elements in the changes in a context. We also propose a novel technique for measuring the similarity of code fragments and code changes using the task context. We conducted an empirical evaluation on a large dataset of 88 open-source Java projects containing more than 200 thousand source files and 3.5 million source lines of code in their last revisions with 423 thousand changed methods. Our result shows that TasC relatively improves recommendation accuracy up to 130%-250% in comparison with the base models that do not use task context. Compared with other types of contexts, TasC outperforms the models using structural and co-change contexts.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Nguyen, Hoan Anh;Nguyen, Anh Tuan;Nguyen, Tien N.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013116264",
      "Primary study DOI": "10.1109/ICSME.2016.68",
      "Title": "Empirical study on synthesis engines for semantics-based program repair",
      "Abstract": "Automatic Program Repair (APR) is an emerging and rapidly growing research area, with many techniques proposed to repair defective software. One notable state-of-the-art line of APR approaches is known as semantics-based techniques, e.g., Angelix, which extract semantics constraints, i.e., specifications, via symbolic execution and test suites, and then generate repairs conforming to these constraints using program synthesis. The repair capability of such approaches-expressive power, output quality, and scalability-naturally depends on the underlying synthesis technique. However, despite recent advances in program synthesis, not much attention has been paid to assess, compare, or leverage the variety of available synthesis engine capabilities in an APR context. In this paper, we empirically compare the effectiveness of different synthesis engines for program repair. We do this by implementing a framework on top of the latest semantics-based APR technique, Angelix, that allows us to use different such engines. For this preliminary study, we use a subset of bugs in the IntroClass benchmark, a dataset of many small programs recently proposed for use in evaluating APR techniques, with a focus on assessing output quality. Our initial findings suggest that different synthesis engines have their own strengths and weaknesses, and future work on semantics-based APR should explore innovative ways to exploit and combine multiple synthesis engines.",
      "Keywords": "Automated Program Repair | Empirical study | Program synthesis engine",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Le, Xuan Bach D.;Lo, David;Le Goues, Claire",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013128181",
      "Primary study DOI": "10.1109/ICSME.2016.76",
      "Title": "A case study of automated feature location techniques for industrial cost estimation",
      "Abstract": "We present a case study of feature location in industry. We study two off-the-shelf feature location algorithms for use as input to a software cost estimator. The feature location algorithms that we studied map program requirements to one or more function points. The cost estimator product, which is the industrial context in which we study feature location, transforms the list of function points into an estimate of the resources necessary to implement that requirement. We chose the feature location algorithms because they are simple to explain, deploy and maintain as a project evolves and personnel rotate on and off. We tested both feature location algorithms against a large software system with a development lifespan of over 20 years. We compared both algorithms by surveying our industrial partner about the accuracy of the list of function points produced by each algorithm. To provide further evidence, we compared both algorithms against an open source benchmarking dataset. Finally, we discuss the requirements of the industrial environment and the ways in which it differs from the academic environment. Our industrial partner elected to use Lucene combined with the PageRank algorithm as their feature location algorithm because it balanced accuracy with simplicity.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Armaly, Ameer;Klaczynski, John;McMillan, Collin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013113170",
      "Primary study DOI": "10.1109/ICSME.2016.41",
      "Title": "A tiered approach towards an incremental bpel to BPMN 2.0 migration",
      "Abstract": "This report describes the challenges and experiences with the incremental migration of a BPEL to a BPMN 2.0 process engine. The transition is motivated by a strategic reorientation towards the new standard as well as end of life of the previous product. The solution reflects the preliminary steps of integrating the new platform into the existing application and support for parallel operation. This paper further describes the incrementally executed reverse engineering of process definitions and migration of instances by applying four different, tiered strategies in an economically viable way. The report concludes by detailing the lessons learned to provide additional guidance for attempts to apply the detailed approach.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Strobl, Stefan;Zoffi, Markus;Bernhart, Mario;Grechenig, Thomas",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013105572",
      "Primary study DOI": "10.1109/ICSME.2016.12",
      "Title": "A validated set of smells in model-view-controller architectures",
      "Abstract": "Code smells are symptoms of poor design and implementation choices that may hinder code comprehension, and possibly increase change- and defect-proneness. A vast catalogue of smells has been defined in the literature, and it includes smells that can be found in any kind of system (e.g., God Classes), regardless of their architecture. On the other hand, software systems adopting specific architectures (e.g., the Model-View-Controller pattern) can be also affected by other types of poor practices. We surveyed and interviewed 53 MVC developers to collect bad practices to avoid while working on Web MVC applications. Then, we followed an open coding procedure on the collected answers to define a catalogue of six Web MVC smells, namely BRAIN REPOSITORY, FAT REPOSITORY, PROMISCUOUS CONTROLLER, BRAIN CONTROLLER, LABORI-OUS REPOSITORY METHOD, and MEDDLING SERVICE. Then, we ran a study on 100 MVC projects to assess the impact of these smells on code change- and defect-proneness. In addition, we surveyed 21 developers to verify their perception of the defined smells. The achieved results show that the Web MVC smells (i) more often than not, increase change- and defect-proneness of classes, and (ii) are perceived by developers as severe problems.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Aniche, Maurício;Bavota, Gabriele;Treude, Christoph;Van Deursen, Arie;Gerosa, Marco Aurélio",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013032112",
      "Primary study DOI": "10.1109/ICSME.2016.46",
      "Title": "An automated approach for recommending when to stop performance tests",
      "Abstract": "Performance issues are often the cause of failures in today's large-scale software systems. These issues make performance testing essential during software maintenance. However, performance testing is faced with many challenges. One challenge is determining how long a performance test must run. Although performance tests often run for hours or days to uncover performance issues (e.g., memory leaks), much of the data that is generated during a performance test is repetitive. Performance analysts can stop their performance tests (to reduce the time to market and the costs of performance testing) if they know that continuing the test will not provide any new information about the system's performance. To assist performance analysts in deciding when to stop a performance test, we propose an automated approach that measures how much of the data that is generated during a performance test is repetitive. Our approach then provides a recommendation to stop the test when the data becomes highly repetitive and the repetitiveness has stabilized (i.e., little new information about the systems' performance is generated). We performed a case study on three open source systems (i.e., CloudStore, PetClinic and Dell DVD Store). Our case study shows that our approach reduces the duration of 24 hour performance tests by 75% while preserving more than 91.9% of the information about the system's performance. In addition, our approach recommends a stopping time that is close to the most cost-effective stopping time (i.e., the stopping time that minimize the duration of the test and maximizes the amount of information about the system's performance provided by performance testing).",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Al Ghmadi, Hammam M.;Syer, Mark D.;Shang, Weiyi;Hassan, Ahmed E.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013040909",
      "Primary study DOI": "10.1109/ICSME.2016.14",
      "Title": "Hug the elephant: Migrating a legacy data analytics application to hadoop ecosystem",
      "Abstract": "Big data applications that rely on relational databases gradually expose limitations on scalability and performance. In recent years, Hadoop ecosystem has been widely adopted as an evolving solution. This paper presents the migration of a legacy data analytics application in a provincial data center. The target platform follows \"no one size fits all\" method. Considering different workloads, data storage is hybrid with distributed file system (HDFS) and distributed NoSQL database. Beyond the architecture re-design, we focus on the problem of data model transformation from relational database to NoSQL database. We propose a query-aware approach to free developers from tedious manual work. The approach generates query-specific views (NoView) for NoSQL and re-structures the views to align with NoSQL's data model. Our results show that the migrated application achieves high scalability and high performance. We believe that our practice provides valuable insights (such as NoSQL data modeling methodology), and the techniques can be easily applied to other similar migrations.",
      "Keywords": "Data model | Hadoop | Migration | NoSQL database",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Zhu, Feng;Liu, Jie;Wang, Sa;Xu, Jiwei;Xu, Lijie;Ren, Jixin;Ye, Dan;Wei, Jun;Huang, Tao",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013083047",
      "Primary study DOI": "10.1109/ICSME.2016.48",
      "Title": "An optimization approach for matching textual domain models with existing code",
      "Abstract": "We address the task of mapping a given textual domain model with the source code of an application which is in the same domain but was developed independently of the domain model. The key novelty of our approach is to use mathematical optimization to find a mapping between the elements in the two sides that maximizes the instances of clusters of related elements on each side being mapped to clusters of similarly related elements on the other side. We describe experiments wherein we apply our approach to the task of matching two real, open-source applications to corresponding industry-standard domain models. In comparison with previous approaches that leverage relationships, but are formulated as heuristics rather than as a principled optimization problem, our approach gives up to 40% higher precision given a desired level of recall.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Patil, Tejas;Komondoor, Raghavan;D'Souza, Deepak;Bhattacharya, Indrajit",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013076920",
      "Primary study DOI": "10.1109/ICSME.2016.71",
      "Title": "Recommending code changes for automatic backporting of linux device drivers",
      "Abstract": "Device drivers are essential components of any operating system (OS). They specify the communication protocol that allows the OS to interact with a device. However, drivers for new devices are usually created for a specific OS version. These drivers often need to be backported to the older versions to allow use of the new device. Backporting is often done manually, and is tedious and error prone. To alleviate this burden on developers, we propose an automatic recommendation system to guide the selection of backporting changes. Our approach analyzes the version history for cues to recommend candidate changes. We have performed an experiment on 100 Linux driver files and have shown that we can give a recommendation containing the correct backport for 68 of the drivers. For these 68 cases, 73.5%, 85.3%, and 88.2% of the correct recommendations are located in the Top-1, Top-2, and Top-5 positions of the recommendation lists respectively. The successful cases cover various kinds of changes including change of record access, deletion of function argument, change of a function name, change of constant, and change of if condition. Manual investigation of failed cases highlights limitations of our approach, including inability to infer complex changes, and unavailability of relevant cues in version history.",
      "Keywords": "Backporting | Device Drivers | Linux | Recommendation system",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Thung, Ferdian;Le, Xuan Bach D.;Lo, David;Lawall, Julia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013030401",
      "Primary study DOI": "10.1109/ICSME.2016.86",
      "Title": "Artifacts for \"A Comprehensive Study on the Energy Efficiency of Java's Thread-Safe Collections\"",
      "Abstract": "Analyzing the energy consumption of application level software is an emerging direction. This artifact makes available all the toolset and raw data needed to reproduce the main findings of our research paper. The artifact consists of: ● The source code of the micro-benchmarks analyzed, ● The source code of the case study used, ● The jRAPL tool, ● The raw energy data generated by the jRAPL tool with the source code of the experiments, ● The plotting scripts used to create the figures of the paper, based on the raw energy data.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Pinto, Gustavo;Liu, Kenan;Castor, Fernando;Liu, Yu David",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013059523",
      "Primary study DOI": "10.1109/ICSME.2016.78",
      "Title": "How does the shift to GitHub impact project collaboration?",
      "Abstract": "Social coding environments such as GitHub and Bitbucket are changing the way software is built. They are not only lowering the barriers for placing changes, but also making open-source contributions more visible and traceable. Not surprisingly, several mature, active, non-trivial open-source software projects are switching their decades of software history to these environments. There is a belief that these environments have the potential of attracting new contributors to open-source projects. However, there is little empirical evidence to support these claims. In this paper, we quantitatively and qualitatively studied a curated set of open-source projects that made the move to GitHub, aiming at understanding whether and how this migration fostered collaboration. Our results suggest that although interaction in some projects increased after migrating to GitHub, the rise of contributions is not straightforward.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Dias, Luiz Felipe;Steinmacher, Igor;Pinto, Gustavo;Da Costa, Daniel Alencar;Gerosa, Marco",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013119842",
      "Primary study DOI": "10.1109/ICSME.2016.80",
      "Title": "Part of speech tagging Java method names",
      "Abstract": "Numerous software engineering tools for evolution and comprehension, including code search, comment generation, and analyzing bug reports, make use of part-of-speech (POS) information. However, many POS taggers are developed for, and trained on, natural language. In this paper, we investigate the accuracy of 9 POS taggers on over 200 source code identifiers taken from method names in open source Java programs. The set of taggers includes traditional POS taggers for English as well as some tuned to source code identifiers. Our results indicate that taggers tailored for source code are significantly more effective.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Olney, Wyatt;Hill, Emily;Thurber, Chris;Lemma, Bezalem",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013037478",
      "Primary study DOI": "10.1109/ICSME.2016.52",
      "Title": "Improving code maintainability: A case study on the impact of refactoring",
      "Abstract": "It is a fact that a lot of software is written by people without a formal education in software engineering. As an example, material scientists often capture their knowledge in the form of simulation software that contains sophisticated algorithms representing complex physical concepts. Since software engineering is typically not a core skill of these scientists, there is a risk that their software becomes unmaintainable once it reaches a substantial size or structural complexity. This paper reports on a case study in which software engineers consulted magnetics researchers in refactoring their simulation software. This software had grown to 30 kloc of Java and was considered unmaintainable by the stakeholders of the research project The case study describes the process of refactoring a system under the guidance of a software engineer with results supported by static analysis and software metrics. It shows how software engineers evaluated and selected refactorings to apply to the system using their expert judgment with input from static analysis tools and discusses the outcome of refactoring as evaluated by code owners and reported via static analysis metrics.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Wahler, Michael;Drofenik, Uwe;Snipes, Will",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013130630",
      "Primary study DOI": "10.1109/ICSME.2016.64",
      "Title": "On the reaction to deprecation of 25,357 clients of 4+1 popular Java APIs",
      "Abstract": "Application Programming Interfaces (APIs) are a tremendous resource-that is, when they are stable. Several studies have shown that this is unfortunately not the case. Of those, a large-scale study of API changes in the Pharo Smalltalk ecosystem documented several findings about API deprecations and their impact on API clients. We conduct a partial replication of this study, considering more than 25,000 clients of five popular Java APIs on GitHub. This work addresses several shortcomings of the previous study, namely: a study of several distinct API clients in a popular, statically-typed language, with more accurate version information. We compare and contrast our findings with the previous study and highlight new ones, particularly on the API client update practices and the startling similarities between reaction behavior in Smalltalk and Java.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Sawant, Anand Ashok;Robbes, Romain;Bacchelli, Alberto",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013080720",
      "Primary study DOI": "10.1109/ICSME.2016.53",
      "Title": "Industrial application of automated regression testing in test-driven ETL development",
      "Abstract": "While there are many commercial Extraction, Transformation and Loading (ETL) platforms that facilitate fast and easy development of ETL applications, ETL testing is largely a manual process. In order to allow frequent releases of reliable ETL applications, an automated regression testing approach is needed. This paper presents 1TH, one such test harness tool and a methodology for agile ETL development. 1TH builds on the existing techniques in testing and database research areas. It addresses practical challenges with nondeterministic values and, to a limited extent, test dependence. 1TH is applied in an industrial ETL development, and evaluated in a case study involving production releases once every couple of weeks for more than three years. 1TH introduced minimal delays in the development process and caught numerous regressions before they escaped to production.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Dzakovic, Miroslav",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013059693",
      "Primary study DOI": "10.1109/ICSME.2016.16",
      "Title": "A multiagent-based framework for self-adaptive software with search-based optimization",
      "Abstract": "Planning a suitable solution to adapt to software changes is the most important and fundamental ability of self-adaptive software (SAS). However, with the increasing complexities of managed resources, context and user preferences, existing self-adaptive planning approaches need to be improved to deal with the complex changes which are multiple, interrelated and evolving. Search-based optimization (SBO) is well-suited to deal with multiple and complex problems. Hence, using SBO as a new self-adaptive planning approach may be a particularly promising research trajectory. This paper proposes a multi-agent framework for SAS with SBO to deal with complex changes, reduce maintenance time and cost, and enhance software quality. This framework defines a special software architecture of SAS to choose different planning approaches, uses the SBO to plan solutions for complex changes, and supports the online planning by multi agents. In addition, a corresponding workbench is being established to develop SAS according to this framework.",
      "Keywords": "Multi-agent systems | Search-based optimization | Search-based software engineering | Self-adaptive planning approaches | Self-adaptive systems",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Wang, Lu;Li, Qingshan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013123665",
      "Primary study DOI": "10.1109/ICSME.2016.19",
      "Title": "An ecosystemic and socio-technical view on software maintenance and evolution",
      "Abstract": "In this invited paper I focus on the difficulties of maintaining and evolving software systems that are part of a larger ecosystem. While not every software system falls under this category, software ecosystems are becoming ubiquitous due to the omnipresence of open source software. I present several challenges that arise during maintenance and evolution of software ecosystems, and I argue how some of these challenges should be addressed by adopting a socio-technical view and by relying on a multidisciplinary and mixed methods research approach. My arguments are accompanied by an extensive, though unavoidably incomplete, set of references to the state-of-the-art research in this domain.",
      "Keywords": "Collaborative software engineering | Empirical software engineering | Interdisciplinary research | Mixed methods research | Socio-technical network | Software ecosystem",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Mens, Tom",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013130532",
      "Primary study DOI": "10.1109/ICSME.2016.65",
      "Title": "Search-based peer reviewers recommendation in modern code review",
      "Abstract": "Code review is of primary importance in modern software development. It is widely recognized that peer review is an efficient and effective practice for improving software quality and reducing defect proneness. For successful review process, peer reviewers should have a deep experience and knowledge with the code being reviewed, and familiar to work and collaborate together. However, one of the main challenging tasks in modern code review is to find the most appropriate reviewers for submitted code changes. So far, reviewers assignment is still a manual, costly and time-consuming task. In this paper, we introduce a search-based approach, namely RevRec, to provide decision-making support for code change submitters and/or reviewers assigners to identify most appropriate peer reviewers for their code changes. RevRec aims at finding reviewers to be assigned for a code change based on their expertise and collaboration in past reviews using genetic algorithm (GA). We evaluated our approach on a benchmark of three open-source software systems, Android, OpenStack, and Qt. Results indicate that RevRec accurately recommends code reviewers with up to 59% of precision and 74% of recall. Our experiments provide evidence that leveraging reviewers expertise from their prior reviews and the socio-technical aspects of the team work and collaboration is relevant in improving the performance of peer reviewers recommendation in modern code review.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Ouni, Ali;Kula, Raula Gaikovina;Inoue, Katsuro",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013124320",
      "Primary study DOI": "10.1109/ICSME.2016.67",
      "Title": "\"Automated debugging considered harmful\" considered harmful: A user study revisiting the usefulness of spectra-based fault localization techniques with professionals using real bugs from large systems",
      "Abstract": "Due to the complexity of software systems, bugs are inevitable. Software debugging is tedious and time consuming. To help developers perform this crucial task, a number of spectra-based fault localization techniques have been proposed. In general, spectra-based fault localization helps developers to find the location of a bug given its symptoms (e.g., program failures). A previous study by Parnin and Orso however implies that several assumptions made by existing work on spectra-based fault localization do not hold in practice, which hinders the practical usage of these tools. Moreover, a recent study by Xie et al. claims that spectra-based fault localization can potentially \"weaken programmers' abilities in fault detection\". Unfortunately, these studies are performed either using only 2 bugs from small systems (Parnin and Orso's study) or synthetic bugs injected into toy programs (Xie et al.'s study), only involve students, and use dated spectra-based fault localization tools. Thus, the question whether spectra-based fault localization techniques can help professionals to improve their debugging efficiency in a reasonably large project is still insufficiently answered. In this paper, we perform a more realistic investigation of how professionals can use and benefit from spectra-based fault localization techniques. We perform a user study of spectra-based fault localization with a total of 16 real bugs from 4 reasonably large open-source projects, with 36 professionals, amounting to 80 recorded debugging hours. The 36 professionals are divided into 3 groups, i.e., those that use an accurate fault localization tool, use a mediocre fault localization tool, and do not use any fault localization tool. Our study finds that both the accurate and mediocre spectra-based fault localization tools can help professionals to save their debugging time, and the improvements are statistically significant and substantial. We also discuss implications of our findings to future directions of spectra-based fault localization.",
      "Keywords": "Automated debugging | Empirical study | Spectra-based fault localization | User study",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Xia, Xin;Bao, Lingfeng;Lo, David;Li, Shanping",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013059396",
      "Primary study DOI": "10.1109/ICSME.2016.49",
      "Title": "A quantitative and qualitative investigation of performance-related commits in android apps",
      "Abstract": "Performance is nowadays becoming a crucial issue for mobile apps, as they are often implementing computational-intensive features, are being used for mission-critical tasks, and, last but not least, a pleasant user experience often is a key factor to determine the success of an app. This paper reports a study aimed at preliminarily investigating to what extent developers take care of performance issues in their commits, and explicitly document that. The study has been conducted on commits of 2,443 open source Android apps, of which 180 turned out to contain a total of 457 documented performance problems. We classified performance-related commits using a card sorting approach, and found that the most predominant kinds of performance-related changes include GUI-related changes, fixing code smells, network-related code, and memory management.",
      "Keywords": "Android | App store mining | Mobile performance issues",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Das, Teerath;Di Penta, Massimiliano;Malavolta, Ivano",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013040821",
      "Primary study DOI": "10.1109/ICSME.2016.39",
      "Title": "Recovering commit branch of origin from GitHub repositories",
      "Abstract": "An approach to automatically recover the name of the branch where a given commit is originally made within a GitHub repository is presented and evaluated. This is a difficult task because in Git, the commit object does not store the name of the branch when it is created. Here this is termed the commit's branch of origin. Developers typically use branches in Git to group sets of changes that are related by task or concern. The approach recovers the branch of origin only within the scope of a single repository. The recovery process first uses Git's default merge commit messages and then examines the relationships between neighboring commits. The evaluation includes a simulation, an empirical examination of 40 repositories of open-source systems, and a manual verification. The evaluations show that the average accuracy exceeds 97% of all commits and the average precision exceeds 80%.",
      "Keywords": "Branching | Git | Merging | Mining software repositories | Version control",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Michaud, Heather M.;Guarnera, Drew T.;Collard, Michael L.;Maletic, Jonathan I.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013032140",
      "Primary study DOI": "10.1109/ICSME.2016.50",
      "Title": "Introducing traceability and consistency checking for change impact analysis across engineering tools in an automation solution company: An experience report",
      "Abstract": "In today's engineering projects, companies continuously have to adapt their systems to changing customer or market requirements. This requires a flexible, iterative development process in which different parts of the system under construction are built and updated concurrently. However, concurrent engineering is quite problematic in domains where different engineering domains and different engineering tools come together. In this paper, we discuss experiences with Van Hoecke Automation, a leading company in the areas of production automation and product processing, in maintaining the consistency between electrical models and the corresponding software controller when both are subject to continuous change. The paper discusses how we let engineers describe the relationships between electrical model and software code in form of links and consistency rules; and how through continuous consistency checking our approach then notified those engineers of the erroneous impact of changes in either electrical model or code.",
      "Keywords": "Incremental consistency checking | Model-driven engineering | Software evolution | Traceability",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Demuth, Andreas;Kretschmer, Roland;Egyed, Alexander;Maes, Davy",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013042160",
      "Primary study DOI": "10.1109/ICSME.2016.54",
      "Title": "Towards accurate binary correspondence using runtime-observed values",
      "Abstract": "Establishing binary correspondence is the process of finding corresponding pairs of program elements, e.g., functions or individual instructions, between two semantically equivalent (or nearly-equivalent) but syntactically different program binaries. The binary-correspondence problem has applications in many fields, e.g., plagiarism and clone detection, reverse engineering, and security, and has therefore received significant attention both in industry and academia. Most binary-correspondence methods used in practice today are based on static analysis of the control structure in binaries. Unfortunately, such methods are often highly sensitive to syntactic differences between binaries, and discrepancies in the control structure due to, for example, using different compilers or optimization levels often severely reduce their accuracy. Several recent works have therefore proposed using dynamic analysis and comparing runtime-observed results of computations to establish binary correspondence. In this paper, we study the discriminative power of runtime-values for matching instructions in binaries, and propose several ways to increase the accuracy of value-based analyses. By utilizing techniques from the field of information retrieval combined with dynamic data-flow analysis, we improve matching accuracy by up to 55% in our experiments.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Kargén, Ulf;Shahmehri, Nahid",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013052660",
      "Primary study DOI": "10.1109/ICSME.2016.79",
      "Title": "Automated GUI testing of android apps: From research to practice",
      "Abstract": "The last decade has seen tremendous proliferation of mobile computing in our society. Billions of users have access to millions of mobile applications that can be installed directly on their mobile devices and electrical appliances such as TV set-top boxes. Factors such as new monetization/revenue models, programming models, and distribution infrastructures contribute to an \"attractive\" movement that captivates new and traditional developers, as well as a crowd of other professionals that explore, design, and implement mobile apps [9]. Also, the need for \"enterprise apps\" that support start-ups or serve as a new front-end for traditional companies is pushing software-related professionals to embrace the mobile technologies [9]. However, the nature of the economy (devices, apps, markets) imposes new challenges on how mobile apps are envisioned, designed, implemented, tested, released, and maintained. This technology briefing aims to help address the challenges of testing and maintaining mobile apps by providing participants from both academic and industrial backgrounds with information on the state-of-art and state-of-practice mobile testing and maintenance techniques. Specifically, we aim to highlight two things: first, new techniques and methodologies for making effective automated testing of mobile apps practical and accessible to developers, and second, open academic research questions related to such technology transfer.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Moran, Kevin;Linares-Vásquez, Mario;Poshyvanyk, Denys",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013052784",
      "Primary study DOI": "10.1109/ICSME.2016.42",
      "Title": "Artifact: Cassandra source code, feature descriptions across 27 versions, with starting and ending version trace matrices",
      "Abstract": "To facilitate research into trace link evolution we present 27 versions of Cassandra source code, feature descriptions for each version, deltas between versions, structured descriptions of each version, and trace links between a subset of 48 features and source code for the starting and ending versions.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Rahimi, Mona;Cleland-Huang, Jane",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013127871",
      "Primary study DOI": "10.1109/ICSME.2016.23",
      "Title": "NullTerminator: Pseudo-automatic refactoring to null object design pattern",
      "Abstract": "Restructuring legacy code to improve its structure and understandability is difficult and adequate tool support is required. While the advantages of the Null Object pattern are widely recognized, the first tool support has only recently emerged. We complement it with NULLTERMINATOR, a prototype tool to assist developers in the instantiation of the Null Object design pattern in Java programs. We describe the main functionalities of the tool and some important internal details on an accompanying example. The demo concludes presenting some initial results.",
      "Keywords": "Design patterns | Null object | Refactorings",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Medeleanu, Ştefan;Mihancea, Petru Florin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013076936",
      "Primary study DOI": "10.1109/ICSME.2016.29",
      "Title": "Detecting function constructors in JavaScript",
      "Abstract": "Prior to the recent updates of the JavaScript language specifications, developers had to use custom solutions to emulate constructs such as classes, modules, and namespaces in JavaScript programs. This paper introduces JSDEODORANT, an automatic approach for detecting function constructors declared locally, under a namespace, or even in other modules. The comparison with the state-of-the-art tool, JSClassFinder, shows that while the precision of the tools is very similar (97% and 98%), the recall of JSDEODORANT (98%) is much higher than JSClassFinder (61%).",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Rostami, Shahriar;Eshkevari, Laleh;Mazinanian, Davood;Tsantalis, Nikolaos",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013059237",
      "Primary study DOI": "10.1109/ICSME.2016.88",
      "Title": "Concepts, operations, and feasibility of a projection-based variation control system",
      "Abstract": "Highly configurable software often uses preprocessor annotations to handle variability. However, understanding, maintaining, and evolving code with such annotations is difficult, mainly because a developer has to work with all variants at a time. Dedicated methods and tools that allow working on a subset of all variants could ease the engineering of highly configurable software. We investigate the potential of one kind of such tools: projection-based variation control systems. For such systems we aim to understand: (i) what end-user operations they need to support, and (ii) whether they can realize the actual evolution of real-world, highly configurable software. We conduct an experiment that investigates variability-related evolution patterns and that evaluates the feasibility of a projection-based variation control system by replaying parts of the history of a highly configurable real-world 3D printer firmware project. Among others, we show that the prototype variation control system does indeed support the evolution of a highly configurable system and that in general, it does not degrade the code.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Stǎnciulescu, Ştefan;Berger, Thorsten;Walkingshaw, Eric;Wąsowski, Andrzej",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013104274",
      "Primary study DOI": "10.1109/ICSME.2016.47",
      "Title": "Evolving NoSQL databases without downtime",
      "Abstract": "NoSQL databases like Redis, Cassandra, and MongoDB are increasingly popular because they are flexible, lightweight, and easy to work with. Applications that use these databases will evolve over time, sometimes necessitating (or preferring) a change to the format or organization of the data. The problem we address in this paper is: How can we support the evolution of high-availability applications and their NoSQL data online, without excessive delays or interruptions, even in the presence of backward-incompatible data format changes? We present KVolve, an extension to the popular Redis NoSQL database, as a solution to this problem. KVolve permits a developer to submit an upgrade specification that defines how to transform existing data to the newest version. This transformation is applied lazily as applications interact with the database, thus avoiding long pause times. We demonstrate that KVolve is expressive enough to support substantial practical updates, including format changes to RedisFS, a Redis-backed file system, while imposing essentially no overhead in general use and minimal pause times during updates.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Saur, Karla;Dumitraş, Tudor;Hicks, Michael",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013059756",
      "Primary study DOI": "10.1109/ICSME.2016.87",
      "Title": "Maintenance effort estimation for open source software: A systematic literature review",
      "Abstract": "Open Source Software (OSS) is distributed and maintained collaboratively by developers all over the world. However, frequent personnel turnover and lack of organizational management makes it difficult to capture the actual development effort. Various OSS maintenance effort estimation approaches have been developed to provide a way to understand and estimate development effort. The goal of this study is to identify the current state of art of the existing maintenance effort estimation approaches for OSS. We performed a systematic literature review on the relevant studies published in the period between 2000-2015 by both automatic and manual searches from different sources. We derived a set of keywords from the research questions and established selection criteria to carefully choose the papers to evaluate. 29 out of 3,312 papers were selected based on a well designed selection process. Our results show that the commonly used OSS maintenance effort estimation methods are actual effort estimation and maintenance activity time prediction; the most commonly used metrics and factors for actual effort estimation are source code measurements and people related metrics; the most commonly mentioned activity for maintenance activity time prediction is bug fixing. Accuracy measures and cross validation is used for validating the estimation models. Based on the above findings, we identified the issues in evaluation methods for actual maintenance effort estimations and the needs for quantitative OSS maintenance effort inference from size-related metrics. Meanwhile, we highlighted individual contribution and performance measurement as a novel and promising research area.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Wu, Hong;Shi, Lin;Chen, Celia;Wang, Qing;Boehm, Barry",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013028973",
      "Primary study DOI": "10.1109/ICSME.2016.58",
      "Title": "Understanding variable code: Reducing the complexity by integrating variability information",
      "Abstract": "Software product lines often use preprocessor statements as a basis for representing variability, which makes understanding the artifacts rather complex. An approach that has been proposed in the past to improve the understanding of code with preprocessor statements is formal concept analysis. This approach has been applied to a number of causes in reengineering. However, the lattices constructed by this approach can become rather large and complex. Hence, any approach that helps to reduce them can be beneficial to understanding the preprocessor-dependencies contained in the code. Here, we show how consistency analysis both within code variability and between code and a variability model can be used to reduce the complexity of a lattice, supporting the analysis of product-line code. We apply our approach to Linux, one of the largest open-source product lines, and analyze both multiple versions and different architectures. We show that our approach typically leads to reductions of the concept lattice and identify situations in which the savings can be rather significant. This leads to a reduction of any efforts for followup analysis or reverse engineering.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Lüdemann, Dierk;Asad, Nazish;Schmid, Klaus;Voges, Christopher",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013078278",
      "Primary study DOI": "10.1109/ICSME.2016.85",
      "Title": "Configuring and assembling information retrieval based solutions for software engineering tasks",
      "Abstract": "Information Retrieval (IR) approaches are used to leverage textual or unstructured data generated during the software development process to support various software engineering (SE) tasks (e.g., concept location, traceability link recovery, change impact analysis, etc.). Two of the most important steps for applying IR techniques to support SE tasks are preprocessing the corpus and configuring the IR technique, and these steps can significantly influence the outcome and the amount of effort developers have to spend for these maintenance tasks. We present the use of Genetic Algorithms (GAs) to automatically configure and assemble an IR process to support SE tasks. The approach named IR-GA determines the (near) optimal solution to be used for each step of the IR process without requiring any training. We applied IR-GA on three different SE tasks and the results of the study indicate that IR-GA outperforms approaches previously used in the literature, and that it does not significantly differ from an ideal upper bound that could be achieved by a supervised approach and a combinatorial approach.",
      "Keywords": "Information retrieval | Parametrization | Reproducibility of experiments | Search-based software engineering | Text-based software engineering",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Dit, Bogdan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013115767",
      "Primary study DOI": "10.1109/ICSME.2016.89",
      "Title": "Do contexts help in phrase-based, statistical source code migration?",
      "Abstract": "Prior research showed that to migrate Java code to C# by directly applying phrase-based statistical machine translation (SMT) on the lexemes of source code produces much semantically incorrect code. In this work, we conduct empirical studies on several open-source projects to investigate the use of well-defined semantics in programming languages to guide the translation process in SMT. We have investigated five types of features forming the contexts involving the (semantic) relations among code tokens including occurrence association among code tokens, data and control dependencies among program entities, visibility constraints of entities, and the consistency in declarations and accesses of variables, fields and methods. We use the Direct Maximum Entropy (DME) approach for feature integration. Our empirical results show that as individual features added to the baseline SMT model, token association and data dependencies contribute much with highest relative improvement in semantic correctness of up to 18.3% and 18.5%, respectively. The integration of three feature types (token association, data dependencies, and visibility) into the baseline model has highest relative improvement with up to 26.4% improvement in semantic correctness. Generally, 43.5-80.7% of the total translated methods are semantically correct. Our results show a good direction of using SMT with semantic features at different levels of abstraction to improve its accuracy.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Nguyen, Anh Tuan;Tu, Zhaopeng;Nguyen, Tien N.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013054093",
      "Primary study DOI": "10.1109/ICSME.2016.44",
      "Title": "On the vocabulary agreement in software issue descriptions",
      "Abstract": "Many software comprehension tasks depend on how stakeholders textually describe their problems. These textual descriptions are leveraged by Text Retrieval (TR)-based solutions to more than 20 software engineering tasks, such as duplicate issue detection. The common assumption of such methods is that text describing the same issue in multiple places will have a common vocabulary. This paper presents an empirical study aimed at verifying this assumption and discusses the impact of the common vocabulary on duplicate issue detection. The study investigated 13K+ pairs of duplicate bug reports and Stack Overflow (SO) questions. We found that on average, more than 12.2% of the duplicate pairs do not have common terms. The other duplicate issue descriptions share, on average, 30% of their vocabulary. The good news is that these duplicates have significantly more terms in common than the non-duplicates. We also found that the difference between the lexical agreement of duplicate and non-duplicate pairs is a good predictor for the performance of TR-based duplicate detection.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Chaparro, Oscar;Florez, Juan Manuel;Marcus, Andrian",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013123088",
      "Primary study DOI": "10.1109/ICSME.2016.77",
      "Title": "Effect of time window on the performance of continuous regression testing",
      "Abstract": "Test prioritization is an effective technique used to reduce the amount of work required to support regression testing in continuous integration development. It aims at finding an optimal order of tests that can detect regressions faster, potentially increasing the frequency of software releases. Prioritization techniques based on test execution history use the results of preceding executions to determine an optimal order of regression tests in the succeeding test executions. In this paper, we investigate how can execution history be optimally used to increase the effectiveness of regression test prioritization. We analyze the effect of history time window on the fault detection effectiveness of prioritized regression tests. We report an experimental study using a data set from Cisco. The results suggest that varying the size of the window can considerably change the performance of regression testing. Our findings will potentially help developers and test teams in adjusting test prioritization techniques for achieving higher cost-effectiveness in continuous regression testing.",
      "Keywords": "Continuous integration | Regression testing | Test prioritization",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Marijan, Dusica;Liaaen, Marius",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013052821",
      "Primary study DOI": "10.1109/ICSME.2016.24",
      "Title": "Key elements extraction and traces comprehension using gestalt theory and the helmholtz principle",
      "Abstract": "Trace analysis techniques are used by software engineers to understand the behaviour of large systems. This understanding can facilitate various software maintenance activities including debugging and feature enhancement. However, traces usually tend to be very large, which makes it difficult for software engineers to unveil the key logic and functionalities embedded in a program's execution. Hence, it is necessary to develop methods and tools that can efficiently identify the important information contained in a large trace. In this paper, we propose an approach that builds on the concept of trace segmentation to extract the major components of a traced scenario. Our approach is based on Gestalt theory and the Helmholtz principle. We show the effectiveness of our approach by applying it to a dataset of large traces.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Khoury, Raphaël;Shi, Lei;Hamou-Lhadj, Abdelwahab",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013036501",
      "Primary study DOI": "10.1109/ICSME.2016.11",
      "Title": "Learning to extract API mentions from informal natural language discussions",
      "Abstract": "When discussing programming issues on social platforms (e.g, Stack Overflow, Twitter), developers often mention APIs in natural language texts. Extracting API mentions in natural language texts is a prerequisite for effective indexing and searching for API-related information in software engineering social content. However, the informal nature of social discussions creates two fundamental challenges for API extraction: common-word polysemy and sentence-format variations. Common-word polysemy refers to the ambiguity between the API sense of a common word and the normal sense of the word (e.g., append, apply and merge). Sentence-format variations refer to the lack of consistent sentence writing format for inferring API mentions. Existing API extraction techniques fall short to address these two challenges, because they assume distinct API naming conventions (e.g., camel case, underscore) or structured sentence format (e.g., code-like phrase, API annotation, or full API name). In this paper, we propose a semi-supervised machine-learning approach that exploits name synonyms and rich semantic context of API mentions to extract API mentions in informal social text. The key innovation of our approach is to exploit two complementary unsupervised language models learned from the abundant un-labeled text to model sentence-format variations and to train a robust model with a small set of labeled data and an iterative self-training process. The evaluation of 1,205 API mentions of the three libraries (Pandas, Numpy, and Matplotlib) in Stack Overflow texts shows that our approach significantly outperforms existing API extraction techniques based on language-convention and sentence-format heuristics and our earlier machine-learning based method for named-entity recognition.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Ye, Deheng;Xing, Zhenchang;Foo, Chee Yong;Li, Jing;Kapre, Nachiket",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013105631",
      "Primary study DOI": "10.1109/ICSME.2016.83",
      "Title": "Why are commits being reverted? A comparative study of industrial and open source projects",
      "Abstract": "Software development is a cyclic process of integrating new features while introducing and fixing defects. During development, commits that modify source code files are uploaded to version control systems. Occasionally, these commits need to be reverted, i.e., the code changes need to be completely backed out of the software project. While one can often speculate about the purpose of reverted commits (e.g., the commit may have caused integration or build problems), little empirical evidence exists to substantiate such claims. The goal of this paper is to better understand why commits are reverted in large software systems. To that end, we quantitatively and qualitatively study two proprietary and four open source projects to measure: (1) the proportion of commits that are reverted, (2) the amount of time that commits that are eventually reverted linger within a codebase, and (3) the most frequent reasons why commits are reverted. Our results show that 1%-5% of the commits in the studied systems are reverted. Those commits that are eventually reverted linger within the studied codebases for 1-35 days (median). Furthermore, we identify 13 common reasons for reverting commits, and observe that the frequency of reverted commits of each reason varies broadly from project to project. A complementary qualitative analysis suggests that many reverted commits could have been avoided with better team communication and change awareness. Our findings made Sony Mobile's stakeholders aware that internally reverted commits can be reduced by paying more attention to their own changes. On the other hand, externally reverted commits could be minimized only if external stakeholders are involved to improve inter-company communication or requirements elicitation.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Shimagaki, Junji;Kamei, Yasutaka;McIntosh, Shane;Pursehouse, David;Ubayashi, Naoyasu",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013031154",
      "Primary study DOI": "10.1109/ICSME.2016.91",
      "Title": "Experimental data for the A?B?A pattern in CSS: Inputs and outputs @leonardpunt",
      "Abstract": "This dataset is used to detect undoing style in CSS code. In total, this dataset contains 41 subjects. Each subject has its own folder, which contains the captured states, a states.html file, is used to load all captured states in one document, and a folder called results, which contains the detected undoing styles, the refactored style sheets and the detected semantic changes. The file states.html was used as an input for our detection tool.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Punt, Leonard;Visscher, Sjoerd;Zaytsev, Vadim",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013037437",
      "Primary study DOI": "10.1109/ICSME.2016.15",
      "Title": "Automated extraction of mixins in cascading style sheets",
      "Abstract": "Cascading style sheets (CSS) is a language that describes the presentation of web documents. CSS is widely adopted in web development and it is now common for web projects to have several thousands of CSS lines of code. Because the language lacks advanced features to allow code reuse, several languages such as Sass and Less have emerged as extensions to CSS. They provide mechanisms such as mixins to enable reuse. However, when a developer wants to migrate her web project from CSS to one of these extension languages, identifying mixins is a challenging task. In this paper, we describe an automated approach to extract mixins from CSS code. We have developed a tool that identifies mixins in CSS files and automatically generates Sass code. Our technique enables a fine-grained control on the generated code tailored to developer needs. We evaluate our approach on more than a hundred CSS files and conduct several case studies to assess its real-world relevance.",
      "Keywords": "Code duplication | CSS | Mixin",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Charpentier, Alan;Falleri, Jean Rémy;Réveillère, Laurent",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013040499",
      "Primary study DOI": "10.1109/ICSME.2016.28",
      "Title": "Detect cross-browser issues for javascript-based web applications based on record/replay",
      "Abstract": "With the advent of Web 2.0 application, and the increasing number of browsers and platforms on which the applications can be executed, cross-browser incompatibilities (XBIs) are becoming a serious problem for organizations to develop web-based software. Although some techniques and tools have been proposed to identify XBIs, a number of false positives and false negatives still exist as they cannot assure the same execution when the application runs across different browsers. To address this limitation, leveraging existing record/replay technique, we developed X-Check, a novel cross-browser testing technique and tool, which supports automated XBIs detection with high accuracy. Our empirical evaluation shows that X-Check is effective and improves the state of the art.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Wu, Guoquan;He, Meimei;Tang, Hongyin;Wei, Jun",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013053101",
      "Primary study DOI": "10.1109/ICSME.2016.92",
      "Title": "Mrstudyr: Retrospectively studying the effectiveness of mutant reduction techniques",
      "Abstract": "Mutation testing is a well-known method for measuring a test suite's quality. However, due to its computational expense and intrinsic difficulties (e.g., detecting equivalent mutants and potentially checking a mutant's status for each test), mutation testing is often challenging to practically use. To control the computational cost of mutation testing, many reduction strategies have been proposed (e.g., uniform random sampling over mutants). Yet, a stand-alone tool to compare the efficiency and effectiveness of these methods is heretofore unavailable. Since existing mutation testing tools are often complex and language-dependent, this paper presents a tool, called mrstudyr, that enables the \"retrospective\" study of mutant reduction methods using the data collected from a prior analysis of all mutants. Focusing on the mutation operators and the mutants that they produce, the presented tool allows developers to prototype and evaluate mutant reducers without being burdened by the implementation details of mutation testing tools. Along with describing mrstudyr's design and overviewing the experimental results from using it, this paper inaugurates the public release of this open-source tool.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "McCurdy, Colton J.;McMinn, Phil;Kapfhammer, Gregory M.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013103855",
      "Primary study DOI": "10.1109/ICSME.2016.37",
      "Title": "A tool for efficiently reverse engineering accurate UML class diagrams",
      "Abstract": "A tool that reverse engineers UML class diagrams from C++ source code is presented. The tool takes srcML as input and produces yUML as output. srcML is an XML representation of the abstract syntactic information of source code. The srcML parser (srcML.org) is highly scalable, efficient, and robust. yUML is a textual format for UML class diagrams that can be easily rendered into a graphical diagram via a web service (yUML.me) or a tool such as Graphvis. The approach utilizes efficient SAX (Simple API for XML) parsing to collect the information needed to construct the class diagram. Currently it supports the following UML features: differentiating between class, data type, or interface; identifying design level attributes, multiplicity and type; determining parameter direction; and identification of the relationships aggregation, composition, generalization, and realization. The tool produces yUML for all of Calligra (∼1,144KLOC) in under 20 seconds (including translation into srcML). The tool is open source under a GPL license and available for download at srcML.org.",
      "Keywords": "Reverse engineering | SrcML | UML class diagrams | YUML",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Decker, Michael John;Swartz, Kyle;Collard, Michael L.;Maletic, Jonathan I.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013073464",
      "Primary study DOI": "10.1109/ICSME.2016.51",
      "Title": "Inferring links between concerns and methods with multi-abstraction vector space model",
      "Abstract": "Concern localization refers to the process of locating code units that match a particular textual description. It takes as input textual documents such as bug reports and feature requests and outputs a list of candidate code units that are relevant to the bug reports or feature requests. Many information retrieval (IR) based concern localization techniques have been proposed in the literature. These techniques typically represent code units and textual descriptions as a bag of tokens at one level of abstraction, e.g., each token is a word, or each token is a topic. In this work, we propose a multi-abstraction concern localization technique named MULAB. MULAB represents a code unit and a textual description at multiple abstraction levels. Similarity of a textual description and a code unit is now made by considering all these abstraction levels. We combine a vector space model and multiple topic models to compute the similarity and apply a genetic algorithm to infer semi-optimal topic model configurations. We have evaluated our solution on 136 concerns from 8 open source Java software systems. The experimental results show that MULAB outperforms the state-of-art baseline PR, which is proposed by Scanniello et al. in terms of effectiveness and rank.",
      "Keywords": "Concern localization | Multi-abstraction | Text retrieval | Topic modeling",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Zhang, Yun;Lo, David;Xia, Xin;Le, Tien Duy B.;Scanniello, Giuseppe;Sun, Jianling",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013074509",
      "Primary study DOI": "10.1109/ICSME.2016.73",
      "Title": "The A?B?A pattern: Undoing style in css and refactoring opportunities it presents",
      "Abstract": "Cascading Style Sheets (CSS) is a language widely used in contemporary web applications for defining the presentation semantics of web documents. Despite its relatively simple syntax, the language has a number of complex features like inheritance, cascading and specificity, which make CSS code challenging to understand and maintain. It has been noted in prior research that CSS code is prone to contain code smells which indicate design weaknesses and maintainability issues. In this paper we focus on one of those code smells called undoing style. It happens when a property is set to a value A, then overridden to another value B, possibly multiple times, and then set back to the original value of A. We refer to this pattern as the A?B∗A pattern. We propose a technique that detects undoing style in CSS code and recommends refactoring opportunities to eliminate instances of undoing style while preserving the semantics of the web application. We evaluate our technique on 41 real-world web applications, and outline a proof of correctness for our refactoring. Our findings show that undoing style is quite prominent in CSS code. Additionally, there are many refactorings that can be applied while hardly introducing any errors.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Punt, Leonard;Visscher, Sjoerd;Zaytsev, Vadim",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013103870",
      "Primary study DOI": "10.1109/ICSME.2016.31",
      "Title": "Understanding the factors that impact the popularity of GitHub repositories",
      "Abstract": "Software popularity is a valuable information to modern open source developers, who constantly want to know if their systems are attracting new users, if new releases are gaining acceptance, or if they are meeting user's expectations. In this paper, we describe a study on the popularity of software systems hosted at GitHub, which is the world's largest collection of open source software. GitHub provides an explicit way for users to manifest their satisfaction with a hosted repository: the stargazers button. In our study, we reveal the main factors that impact the number of stars of GitHub projects, including programming language and application domain. We also study the impact of new features on project popularity. Finally, we identify four main patterns of popularity growth, which are derived after clustering the time series representing the number of stars of 2,279 popular GitHub repositories. We hope our results provide valuable insights to developers and maintainers, which could help them on building and evolving systems in a competitive software market.",
      "Keywords": "GitHub | Open source software | Social coding | Software popularity",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Borges, Hudson;Hora, Andre;Valente, Marco Tulio",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013092921",
      "Primary study DOI": "10.1109/ICSME.2016.43",
      "Title": "How can we help software rearchitecting efforts? Study of an industrial case",
      "Abstract": "Legacy software systems are valuable assets for organisations and are sometimes their main source of incomes. From time to time, renewing legacy software system architecture becomes necessary in order to offer them a new future. Migrating the architecture of a legacy software system is a difficult task. It involves understanding and aggregating a large set of data (the entire source code, dependencies, etc.); it may have a profound impact on the system's behaviour; and because it occurs very rarely in the life of a system, it is hard to gain experience in this domain. Based on the study of an industrial architecture migration case, we discuss how this essentially manual effort could be helped with automated tools and a better defined process. We identified several issues raised during the task, characterized their impact, and proposed possible solutions.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Govin, Brice;Anquetil, Nicolas;Etien, Anne;Ducasse, Stephane;Monegier, Arnaud",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013074464",
      "Primary study DOI": "10.1109/ICSME.2016.18",
      "Title": "Measuring code similarity in large-scaled code corpora",
      "Abstract": "Source code similarity measurement is a fundamental technique in software engineering research. Techniques to measure code similarity have been invented and applied to various research areas such as code clone detection, finding bug fixes, and software plagiarism detection. We perform an evaluation of 30 similarity analysers for source code. The results show that specialised tools including clone and plagiarism detectors, with proper parameter tuning, outperform general techniques such as string matching. Although these specialised tools can handle code similarity in local code bases, they fail to locate similar code artefacts from large-scaled corpora. This is increasingly important considering the rising amount of online code artefacts. We propose a scalable search system specifically designed for source code. It lays a foundation to discovering online code reuse, large-scale code clone detection, finding usage examples, detecting software plagiarism, and finding software licensing conflicts. Our proposed code search framework is a hybrid of information retrieval and code clone detection techniques. This framework will be able to locate similar code artefacts instantly. The search is not only based on textual similarity, but also syntactic and structural similarity. It is resilient to incomplete code fragments that are normally found on the Internet.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Ragkhitwetsagul, Chaiyong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013135300",
      "Primary study DOI": "10.1109/ICSME.2016.38",
      "Title": "SrcType: A tool for efficient static type resolution",
      "Abstract": "An efficient, static type resolution tool is presented. The tool is implemented on top of srcML; an XML representation of source code and abstract syntax. The approach computes the type of every identifier (i.e., function names and variable names) within the provided body of code. The result is a dictionary that can be used to lookup the type of each name. Type information includes metadata such as constness, class membership, aliasing, line number, file, and namespace. The approach is highly scalable and can generate a dictionary for Linux (13 MLOC) in less than 7 minutes. The tool is open source under a GPL license and available for download at srcML.org.",
      "Keywords": "SrcML | Static analysis tool | Static type resolution",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Newman, Christian D.;Maletic, Jonathan I.;Collard, Michael L.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013041909",
      "Primary study DOI": "10.1109/ICSME.2016.75",
      "Title": "Repairing intricate faults in code using machine learning and path exploration",
      "Abstract": "Debugging remains costly and tedious, especially for code that performs intricate operations that are conceptually complex to reason about. We present MLR, a novel approach for repairing faults in such operations, specifically in the context of complex data structures. Our focus is on faults in conditional statements. Our insight is that an integrated approach based on machine learning and systematic path exploration can provide effective repairs. MLR mines the data-spectra of the passing and failing executions of conditional branches to prune the search space for repair and generate patches that are likely valid beyond the existing test-suite. We apply MLR to repair faults in small but complex data structure subjects to demonstrate its efficacy. Experimental results show that MLR has the potential to repair this fault class more effectively than state-of-the-art repair tools.",
      "Keywords": "Condition faults | Data-structures | Decision-tree learning | JPF | Program repair | Semi-supervised learning",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Gopinath, Divya;Wang, Kaiyuan;Hua, Jinru;Khurshid, Sarfraz",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013129713",
      "Primary study DOI": "10.1109/ICSME.2016.22",
      "Title": "Data and analysis code for GP EFSM inference",
      "Abstract": "This artifact captures the workflow that we adopted for our experimental evaluation in our ICSME paper on inferring state transition functions during EFSM inference. To summarise, the paper uses Genetic Programming to infer data transformations, to enable the inference of fully 'computational' extended finite state machine models. This submission shows how we generated, transformed, analysed, and visualised our raw data. It includes everything needed to generate raw results and provides the relevant R code in the form of a re-usable Jupyter Notebook (accompanied by a descriptive narrative).",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Hall, Mathew;Walkinshaw, Neil",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013055721",
      "Primary study DOI": "10.1109/ICSME.2016.74",
      "Title": "Inferring computational state machine models from program executions",
      "Abstract": "The challenge of inferring state machines from log data or execution traces is well-established, and has led to the development of several powerful techniques. Current approaches tend to focus on the inference of conventional finite state machines or, in few cases, state machines with guards. However, these machines are ultimately only partial, because they fail to model how any underlying variables are computed during the course of an execution; they are not computational. In this paper we introduce a technique based upon Genetic Programming to infer these data transformation functions, which in turn render inferred automata fully computational. Instead of merely determining whether or not a sequence is possible, they can be simulated, and be used to compute the variable values throughout the course of an execution. We demonstrate the approach by using a Cross-Validation study to reverse-engineer complete (computational) EFSMs from traces of established implementations.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Walkinshaw, Neil;Hall, Mathew",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013115700",
      "Primary study DOI": "10.1109/ICSME.2016.17",
      "Title": "TechLand: Assisting technology landscape inquiries with insights from stack overflow",
      "Abstract": "Understanding the technology landscape is crucial for the success of the software-engineering project or organization. However, it can be difficult, even for experienced developers, due to the proliferation of similar technologies, the complex and often implicit dependencies among technologies, and the rapid development in which technology landscape evolves. Developers currently rely on online documents such as tutorials and blogs to find out best available technologies, technology correlations, and technology trends. Although helpful, online documents often lack objective, consistent summary of the technology landscape. In this paper, we present the TechLand system for assisting technology landscape inquiries with categorical, relational and trending knowledge of technologies that is aggregated from millions of Stack Overflow questions mentioning the relevant technologies. We implement the TechLand system and evaluate the usefulness of the system against the community answers to 100 technology questions on Stack Overflow and by field deployment and a lab study. Our evaluation shows that the TechLand system can assist developers in technology landscape inquiries by providing direct, objective, and aggregated information about available technologies, technology correlations and technology trends.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Chen, Chunyang;Xing, Zhenchang;Han, Lei",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013073312",
      "Primary study DOI": "10.1109/ICSME.2016.34",
      "Title": "A comprehensive study on the energy efficiency of Java's thread-safe collections",
      "Abstract": "Java programmers are served with numerous choices of collections, varying from simple sequential ordered lists to sophisticated hashtable implementations. These choices are well-known to have different characteristics in terms of performance, scalability, and thread-safety, and most of them are well studied. This paper analyzes an additional dimension, energy efficiency. We conducted an empirical investigation of 16 collection implementations (13 thread-safe, 3 non-thread-safe) grouped under 3 commonly used forms of collections (lists, sets, and mappings). Using micro- and real world-benchmarks (TOMCAT and XALAN), we show that our results are meaningful and impactful. In general, we observed that simple design decisions can greatly impact energy consumption. In particular, we found that using a newer hashtable version can yield a 2.19x energy savings in the micro-benchmarks and up to 17% in the real world-benchmarks, when compared to the old associative implementation. Also, we observed that different implementations of the same thread-safe collection can have widely different energy consumption behaviors. This variation also applies to the different operations that each collection implements, e.g., a collection implementation that performs traversals very efficiently can be more than an order of magnitude less efficient than another implementation of the same collection when it comes to insertions.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Pinto, Gustavo;Liu, Kenan;Castor, Fernando;Liu, Yu David",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013080864",
      "Primary study DOI": "10.1109/ICSME.2016.30",
      "Title": "Customized regression testing using telemetry usage patterns",
      "Abstract": "Pervasive telemetry in modern applications is providing new possibilities in the application of regression testing techniques. Similar to how research in bioinformatics is leading to personalized medicine, tailored to individuals, usage telemetry in modern software allows for custom regression testing, tailored to the usage patterns of an installation. By customizing regression testing based on software usage, the effectiveness of regression testing techniques can be greatly improved, leading to reduced testing costs and enhanced detection of defects that are most important to that customer. In this research, we introduce the concept of fingerprinting software usage patterns through telemetry. We provide various algorithms to compute fingerprints and conduct an empirical study that shows that fingerprints are effective in identifying distinct usage patterns. Further, we discuss how usage fingerprints can be used to improve regression test prioritization run time by over 30 percent compared to traditional prioritization techniques.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Anderson, Jeff;Do, Hyunsook;Salem, Saeed",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013117856",
      "Primary study DOI": "10.1109/ICSME.2016.59",
      "Title": "ICON: Inferring temporal constraints from natural language API descriptions",
      "Abstract": "Temporal constraints of an Application Programming Interface (API) are the allowed sequences of method invocations in the API governing the secure and robust operation of client software using the API. These constraints are typically described informally in natural language API documents, and therefore are not amenable to existing constraint-checking tools. Manually identifying and writing formal temporal constraints from API documents can be prohibitively time-consuming and error-prone. To address this issue, we propose ICON: an approach based on Machine Learning (ML) and Natural Language Processing (NLP) for identifying and inferring formal temporal constraints. To evaluate our approach, we use ICON to infer and formalize temporal constraints from the Amazon S3 REST API, the PayPal Payment REST API, and the java.io package in the JDK API. Our results indicate that ICON can effectively identify temporal constraint sentences (from over 4000 human-annotated API sentences) with the average 79.0% precision and 60.0% recall. Furthermore, our evaluation demonstrates that ICON achieves an accuracy of 70% in inferring 77 formal temporal constraints from these APIs.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Pandita, Rahul;Taneja, Kunal;Tung, Teresa;Williams, Laurie",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013124309",
      "Primary study DOI": "10.1109/ICSME.2016.55",
      "Title": "An empirical study of internationalization failures in the Web",
      "Abstract": "Web application internationalization frameworks allow businesses to more easily market and sell their products and services around the world. However, internationalization can lead to problems. Text expansion and contraction after translation may result in a distortion of the layout of the translated versions of a webpage, which can reduce their usability and aesthetics. In this paper, we investigate and report on the frequency and severity of different types of failures in webpages' user interfaces that are due to internationalization. In our study, we analyzed 449 real world internationalized webpages. Our results showed that internationalization failures occur frequently and they range significantly in terms of severity and impact on the web applications. These findings motivate and guide future work in this area.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Alameer, Abdulmajeed;Halfond, William G.J.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013056311",
      "Primary study DOI": "10.1109/ICSME.2016.21",
      "Title": "Using temporal and semantic developer-level information to predict maintenance activity profiles",
      "Abstract": "Predictive models for software projects' characteristics have been traditionally based on project-level metrics, employing only little developer-level information, or none at all. In this work we suggest novel metrics that capture temporal and semantic developer-level information collected on a per developer basis. To address the scalability challenges involved in computing these metrics for each and every developer for a large number of source code repositories, we have built a designated repository mining platform. This platform was used to create a metrics dataset based on processing nearly 1000 highly popular open source GitHub repositories, consisting of 147 million LOC, and maintained by 30,000 developers. The computed metrics were then employed to predict the corrective, perfective, and adaptive maintenance activity profiles identified in previous works. Our results show both strong correlation and promising predictive power with R2 values of 0.83, 0.64, and 0.75. We also show how these results may help project managers to detect anomalies in the development process and to build better development teams. In addition, the platform we built has the potential to yield further predictive models leveraging developer-level metrics at scale.",
      "Keywords": "Human factors | Mining software repositories | Predictive models | Software maintenance | Software metrics",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Levin, Stanislav;Yehudai, Amiram",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013124163",
      "Primary study DOI": "10.1109/ICSME.2016.20",
      "Title": "TraceLab components for reproducing source code summarization experiments",
      "Abstract": "This artifact is a reproducibility package for experiments in source code summarization. The artifact is implemented as a set of components for the TraceLab research infrastructure. We have converted two implementations of state-of-the-art source code summarization into prepackaged and easily-reusable TraceLab components. Prior to this conversion, the implementations were accessible but difficult to use, being scattered across numerous scripts in various languages with many dependencies. We provide the components, detailed tutorials, and two example virtual machine images via our online appendix.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Cruz, Breno Dantas;McBurney, Paul W.;McMillan, Collin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013124219",
      "Primary study DOI": "10.1109/ICSME.2016.35",
      "Title": "Accessing inaccessible android APIs: An empirical study",
      "Abstract": "As Android becomes a de-facto choice of development platform for mobile apps, developers extensively leverage its accompanying Software Development Kit to quickly build their apps. This SDK comes with a set of APIs which developers may find limited in comparison to what system apps can do or what framework developers are preparing to harness capabilities of new generation devices. Thus, developers may attempt to explore in advance the normally \"inaccessible\" APIs for building unique API-based functionality in their app. The Android programming model is unique in its kind. Inaccessible APIs, which however are used by developers, constitute yet another specificity of Android development, and is worth investigating to understand what they are, how they evolve over time, and who uses them. To that end, in this work, we empirically investigate 17 important releases of the Android framework source code base, and we find that inaccessible APIs are commonly implemented in the Android framework, which are further neither forward nor backward compatible. Moreover, a small set of inaccessible APIs can eventually become publicly accessible, while most of them are removed during the evolution, resulting in risks for such apps that have leveraged inaccessible APIs. Finally, we show that inaccessible APIs are indeed accessed by third-party apps, and the official Google Play store has tolerated the proliferation of apps leveraging inaccessible API methods.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Li, Li;Bissyandé, Tegawendé F.;Le Traon, Yves;Klein, Jacques",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013116271",
      "Primary study DOI": "10.1109/ICSME.2016.70",
      "Title": "Integration of static and dynamic code analysis for understanding legacy source code",
      "Abstract": "In software development we are faced with the problem to comprehend and take over source code from other developers. The key challenge is to understand the underlying specification implemented by the software system. Regaining this understanding is more difficult when the source code is the only reliable source of information, documentation is outdated or only present in fragments, and original developers are not available anymore. Unfortunately, we encounter such situations frequently for scientific and engineering software systems, developed in industry. For instance, process models in the steelmaking domain are developed and maintained over decades by single engineers. If such an engineer leaves the company, he/she literally leaves behind a legacy system for another person (or team). We propose tool support combining static and dynamic program analysis to tackle this challenge. Using static program analysis we extract the input/output behavior from program source code and present the extracted information besides the analyzed source code, providing seamless navigation between both views. Dynamic program analysis allows developers to examine input/output behavior for single program executions and thereby gain insight into standard behavior and exceptional cases. In this paper we present requirements on tool support integrating static and dynamic code analysis, briefly describe the implementation of the tool and report on its application to a C++ program source in the industry. Furthermore, we discuss challenges in the present implementation as well as the potential and limitations of using the tool in general.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Kirchmayr, Wilhelm;Moser, Michael;Nocke, Ludwig;Pichler, Josef;Tober, Rudolf",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013124232",
      "Primary study DOI": "10.1109/ICSME.2016.72",
      "Title": "Continuous delivery practices in a large financial organization",
      "Abstract": "Continuous Delivery is an agile software development practice in which developers frequently integrate changes into the main development line and produce releases of their software. An automated Continuous Integration infrastructure builds and tests these changes. Claimed advantages of CD include early discovery of (integration) errors, reduced cycle time, and better adoption of coding standards and guidelines. This paper reports on a study in which we surveyed 152 developers of a large financial organization (ING Nederland), and investigated how they adopt a Continuous Integration and delivery pipeline during their development activities. In our study, we focus on topics related to managing technical debt, as well as test automation practices. The survey results shed light on the adoption of some agile methods in practice, and sometimes confirm, while in other cases, confute common wisdom and results obtained in other studies. For example, we found that refactoring tends to be performed together with other development activities, technical debt is almost always \"self-admitted\", developers timely document source code, and assure the quality of their product through extensive automated testing, with a third of respondents dedicating more than 50% of their time to do testing activities.",
      "Keywords": "Agile development | Continuous delivery | Continuous integration | DevOps | Refactoring | Technical debt | Test-driven development | Testing",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Vassallo, Carmine;Zampetti, Fiorella;Romano, Daniele;Beller, Moritz;Panichella, Annibale;Di Penta, Massimiliano;Zaidman, Andy",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013029053",
      "Primary study DOI": "10.1109/ICSME.2016.32",
      "Title": "A complete operator library for DSL evolution specification",
      "Abstract": "Domain-specific languages (DSLs) allow users to model systems using concepts from a specific domain. Evolution of DSLs triggers co-evolution of models developed in these languages. Manual co-evolution of the thousands of models is unfeasible, calling for an automated support. A prerequisite to automating model co-evolution with respect to DSL evolution is the ability to formally specify DSL evolution, e.g., using predefined evolution operators. Success or failure of the practical application of the operator-based approach therefore depends heavily on the operators offered by the operator library at hand. In this paper we evaluate the completeness of the state-of-the-art operator library claimed to be \"practically complete\" (which we denote as H) by using it to specify evolution of an ecosystem of 22 commercial DSLs over the period of four years. We observe that 11% of the changes cannot be specified. However, there is no guarantee that extending the library with the identified deficiencies will be sufficient to specify evolution of other DSLs. To mitigate this, we design a theoretically complete library of operators, R. We observe that 77% of the operators from R are absent from H. Of the deficiencies in H, 72% could not be revealed by means of studying the extensive industrial ecosystem above. Our study suggests that the existing operator libraries are not extensive enough to specify evolution of large model-driven software ecosystems. Since extending operator libraries on a per-case study basis does not yield satisfactory results so far, we advocate an alternative, i.e., a theoretically complete library of operators R.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Mengerink, J. G.M.;Serebrenik, A.;Schiffelers, R. R.H.;Van Den Brand, M. G.J.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013080842",
      "Primary study DOI": "10.1109/ICSME.2016.56",
      "Title": "Automatically documenting software artifacts",
      "Abstract": "Software artifacts constantly change during evolution and maintenance of software systems. One critical artifact that developers need to be able to maintain during evolution and maintenance of software systems is up-to-date and complete documentation. However, recent studies on the co-evolution of comments and code showed that the comments are rarely maintained or updated when the respective source code is changed. In order to understand developer practices regarding documenting two kinds of software artifacts, unit test cases and database-related operations, we designed two empirical studies both composed of (i) an online survey with contributors of open source projects and (ii) a mining-based analysis of method comments in these projects. Later, motivated by the findings of the studies, we proposed two novel approaches. UnitTestScribe is an approach for automatically documenting test cases, while DBScribe is an approach for automatically documenting test cases. We evaluated our tools by means of an online survey with industrial developers and graduate students. In general, participants indicated that descriptions generated by our tools are complete, concise, and easy to read.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Li, Boyang;Poshyvanyk, Denys",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013077442",
      "Primary study DOI": "10.1109/ICSME.2016.62",
      "Title": "BigCloneEval: A clone detection tool evaluation framework with BigCloneBench",
      "Abstract": "Many clone detection tools have been proposed in the literature. However, our knowledge of their performance in real software systems is limited, particularly their recall. We previously introduced our BigCloneBench, a big clone benchmark of over 8 million clones within a large inter-project Java repository containing 25,000 open-source Java systems. In this paper we present BigCloneEval, a framework for evaluating clone detection tools with BigCloneBench. BigCloneEval makes it very easy for clone detection researchers to evaluate and compare clone detection tools. It automates the execution and evaluation of clone detection tools against the reference clones of BigCloneBench, and summarizes recall performance from a variety of perspectives, including per clone type, and per syntactical similarity regions.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Svajlenko, Jeffrey;Roy, Chanchal K.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013080990",
      "Primary study DOI": "10.1109/ICSME.2016.93",
      "Title": "SchemaAnalyst: Search-based test data generation for relational database schemas",
      "Abstract": "Data stored in relational databases plays a vital role in many aspects of society. When this data is incorrect, the services that depend on it may be compromised. The database schema is the artefact responsible for maintaining the integrity of stored data. Because of its critical function, the proper testing of the database schema is a task of great importance. Employing a search-based approach to generate high-quality test data for database schemas, SchemaAnalyst is a tool that supports testing this key software component. This presented tool is extensible and includes both an evaluation framework for assessing the quality of the generated tests and full-featured documentation. In addition to describing the design and implementation of SchemaAnalyst and overviewing its efficiency and effectiveness, this paper coincides with the tool's public release, thereby enhancing practitioners' ability to test relational database schemas.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "McMinn, Phil;Wright, Chris J.;Kinneer, Cody;McCurdy, Colton J.;Camara, Michael;Kapfhammer, Gregory M.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013042364",
      "Primary study DOI": "10.1109/ICSME.2016.82",
      "Title": "A taxonomy for program metamodels in program reverse engineering",
      "Abstract": "To support program comprehension, maintenance, and evolution, metamodels are frequently used during program reverse engineering activities to describe and analyze constituents of a program and their relations. Reverse engineering tools often define their own metamodels according to the intended purposes and features. Although each metamodel has its own advantages, its limitations may be addressed by other metamodels. Existing works have evaluated and compared metamodels and tools, but none have considered all the possible characteristics and limitations to provide a comprehensive guideline for classifying, comparing, reusing, and extending program metamodels. To aid practitioners and researchers in classifying, comparing, reusing, and extending program metamodels and their corresponding reverse engineering tools according to the intended goals, we establish a conceptual framework with definitions of program metamodels and related concepts. Then this framework is used to provide a comprehensive taxonomy, named Program Metamodel TAxonomy (ProMeTA), which incorporates newly identified characteristics into those stated in previous works, which were identified via a systematic literature survey on program metamodels, while keeping the orthogonality of the entire taxonomy. Additionally, we validate the taxonomy in terms of its orthogonality and usefulness through the classification of popular metamodels.",
      "Keywords": "Program comprehension and analysis | Program metamodels | Reverse engineering | Taxonomy",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Washizaki, Hironori;Guéhéneuc, Yann Gaël;Khomh, Foutse",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013058550",
      "Primary study DOI": "10.1109/ICSME.2016.63",
      "Title": "From quick fixes to slow fixes: Reimagining static analysis resolutions to enable design space exploration",
      "Abstract": "Quick Fixes as implemented by IDEs today prioritize the speed of applying the fix as a primary criteria for success. In this paper, we argue that when tools over-optimize this criteria, such tools neglect other dimensions that are important to successfully applying a fix, such as being able to explore the design space of multiple fixes. This is especially true in cases where a fix only partially implements the intention of the developer. In this paper, we implement an extension to the FindBugs defect finding tool, called FIXBUGS, an interactive resolution approach within the Eclipse development environment that prioritizes other design criteria to the successful application of suggested fixes. Our empirical evaluation method of 12 developers suggests that FIXBUGS enables developers to explore alternative designs and balances the benefits of manual fixing with automated fixing, without having to compromise in either effectiveness or efficiency. Our analytic evaluation method with six usability experts identified trade-offs between FIXBUGS and Quick Fix, and suggests ways in which FIXBUGS and Quick Fix can offer complementary capabilities to better support developers.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Barik, Titus;Song, Yoonki;Johnson, Brittany;Murphy-Hill, Emerson",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013057111",
      "Primary study DOI": "10.1109/ICSME.2016.60",
      "Title": "What is the cause for a defect to be re-assigned?",
      "Abstract": "Software development organizations often need to balance productivity and sustainability. To keep this balance, attention needs to be given as to why defects are re-assigned. This paper presents an empirical study to explore the causal relationships, other than probabilistic dependencies, between re-assigned/fixed defects and defect attributes: (i) Priority, (ii) Severity, (iii) Density, and (iv) Source. We pursue a quantitative approach based on Bayesian belief networks to uncover the inferential information encoded by these networks. The data source of this research comes from an issue tracking system repository of a proprietary and enterprise level software life-cycle management tool. The causal structure of the defect attributes in our domain has been estimated statistically and the results are plotted. The causes of a defect to be fixed and to be fixed after it has been re-assigned have been explored as well in this study. It has been observed that severity is an effect rather than a cause that significantly affect a defect to be re-assigned in our domain. However, the change of the bug severity has been observed to be a direct cause for a defect to be fixed. We believe that understanding the basis and causes of re-assigning and fixing bugs would help software development organizations to better allocate their resources in the software maintenance phase.",
      "Keywords": "Data analysis | Enterprise resource planning | Software maintenance",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Abdou, Tamer;Soltanifar, Behjat;Bener, Ayse;Neal, Adam",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013040924",
      "Primary study DOI": "10.1109/ICSME.2016.84",
      "Title": "An empirical evaluation of models of programmer navigation",
      "Abstract": "In this paper, we report an evaluation study of predictive models of programmer navigation. In particular, we compared two operationalizations of navigation from the literature (click-based versus view-based) to see which more accurately records a developer's navigation behaviors. Moreover, we also compared the predictive accuracy of seven models of programmer navigation from the literature, including ones based on navigation history and code-structural relationships. To address our research goals, we performed a controlled laboratory study of the navigation behavior of 10 participants engaged in software evolution tasks. The study was a partial replication of a previous comprehensive evaluation of predictive models by Piorkowski et al., and also served to test the generalizability of their results. Key findings of the study included that the click-based navigations agreed closely with those reported by human observers, whereas view-based navigations diverged significantly. Furthermore, our data showed that the predictive model based on recency was significantly more accurate than the other models, suggesting the strong potential for tools that leverage recency-type models. Finally, our model-accuracy results had a strong correlation with the Piorkowski results; however, our results differed in several noteworthy ways, potentially caused by differences in task type and code familiarity.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Singh, Alka;Henley, Austin Z.;Fleming, Scott D.;Luong, Maria V.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013119772",
      "Primary study DOI": "10.1109/ICSME.2016.94",
      "Title": "Comparing quality metrics for cloned and non cloned Java methods: A large scale empirical study",
      "Abstract": "In this paper, we conduct a large scale statistical study to explore if there exists any difference between the quality of cloned methods and non cloned methods. The dataset consists of 4,421 open source Java projects containing 644,830 cloned and 842,052 non cloned methods. The study uses 27 software metrics as a proxy for quality, spanning across complexity, modularity, and documentation (code-comments) categories. We did not find any statistically significant difference (p<0.05, r>0.1) between the quality of cloned and non cloned methods for most of the metrics, except for 3 metrics. We, however, found that the cloned methods are on an average 20% smaller than the non cloned methods.",
      "Keywords": "Code clones | Open source software | Quality metrics",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Saini, Vaibhav;Sajnani, Hitesh;Lopes, Cristina",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013078254",
      "Primary study DOI": "10.1109/ICSME.2016.13",
      "Title": "Who is who in the mailing list? Comparing six disambiguation heuristics to identify multiple addresses of a participant",
      "Abstract": "Many software projects adopt mailing lists for the communication of developers and users. Researchers have been mining the history of such lists to study communities' behavior, organization, and evolution. A potential threat of this kind of study is that users often use multiple email addresses to interact in a single mailing list. This can affect the results and tools, when, for example, extracting social networks. This issue is particularly relevant for popular and long-term Open Source Software (OSS) projects, which attract participation of thousands of people. Researchers have proposed heuristics to identify multiple email addresses from the same participant, however there are few studies analyzing the effectiveness of these heuristics. In addition, many studies still do not use any heuristics for authors' disambiguation, which can compromise the results. In this paper, we compare six heuristics from the literature using data from 150 mailing lists from Apache Software Foundation projects. We found that the heuristics proposed by Oliva et al. and a Naïve heuristic outperformed the others in most cases, when considering the F-measure metric. We also found that the time window and the size of the dataset influence the effectiveness of each heuristic. These results may help researchers and tool developers to choose the most appropriate heuristic to use, besides highlighting the necessity of dealing with identity disambiguation, mainly in open source software communities with a large number of participants.",
      "Keywords": "Apache software foundation | Email address disambiguation | Mailing lists | Mining software repositories",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Wiese, Igor Scaliante;Da Silva, José Teodoro;Steinmacher, Igor;Treude, Christoph;Gerosa, Marco Aurélio",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013054074",
      "Primary study DOI": "10.1109/ICSME.2016.81",
      "Title": "Use cases of a generic model interpreter in an automotive software setting",
      "Abstract": "Model based approaches are widely used to develop today's automotive software systems. They promise to reduce development effort by raising the level of abstraction and making the software better accessible to domain experts. On the other hand, the same effects as for classical code occur: Complexity inevitably increases over time, and models become hard to understand. This is where software analysis can help. Other use cases of software analysis, like test case generation or quality assurance, are of similar importance. Unfortunately, only limited support by analysis tools is available on model level - mainly because each modeling language is different and would require a specific analysis tool. Analyzing the generated code is also not an option, because that code can be arbitrarily far away from the models. We introduce an abstract model interpreter as a way to get out of this situation. It exploits the specialties of automotive embedded software and allows efficient realization of a whole class of software analyses on the model level. In this paper, we introduce the ideas of the interpreter and describe some of its real-world use cases for software maintenance and calibration.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Quante, Jochen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013127651",
      "Primary study DOI": "10.1109/ICSME.2016.69",
      "Title": "Modular change impact analysis for configurable software",
      "Abstract": "Slicing-based change impact analysis is an important and established technique to assess the effects of modifications to source code. Program slicing is usually done by performing a graph reachability analysis on a system dependence graph representing the control and data flow dependencies between statements. However, analyzing large-scale software systems can lead to performance issues, resulting in huge dependence graphs and long analysis times. In this paper we present an approach that exploits the modularity of large-scale systems to first perform program analysis for individual modules, and later compose the pre-computed analysis results. However, partitioning a system dependence graph is not straightforward, as it carries presence conditions representing variability. We thus use placeholders that are resolved when composing the pre-computed SDG modules during configuration-aware program analysis. Our approach is particularly useful in the context of product lines, when product variants are derived by composing modules depending on specific customer requirements. We present preliminary results of an evaluation based on a product line from an industrial partner.",
      "Keywords": "Change impact | Configuration | Maintenance | Modularity | Program analysis",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Angerer, Florian;Prähofer, Herbert;Grünbacher, Paul",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013087078",
      "Primary study DOI": "10.1109/ICSME.2016.90",
      "Title": "A tool for detecting and refactoring the A?B?A pattern in CSS @leonardpunt",
      "Abstract": "This tool detects undoing style in CSS code and is able to apply refactoring opportunities to eliminate a subset of these instances of undoing style, while preserving the semantics of the web application.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Punt, Leonard;Visscher, Sjoerd;Zaytsev, Vadim",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040632990",
      "Primary study DOI": "10.1109/ICSME.2017.66",
      "Title": "How do developers select and prioritize code smells? A preliminary study",
      "Abstract": "Code smells are considered to be indicators of design flaws or problems in source code. Various tools and techniques have been proposed for detecting code smells. The number of code smells detected by these tools is generally large, so approaches have also been developed for prioritizing and filtering code smells. However, the lack of empirical data regarding how developers select and prioritize code smells hinders improvements to these approaches. In this study, we investigated professional developers to determine the factors they use for selecting and prioritizing code smells. We found that Task relevance and Smell severity were most commonly considered during code smell selection, while Module importance and Task relevance were employed most often for code smell prioritization. These results may facilitate further research into code smell detection, prioritization, and filtration to better focus on the actual needs of developers.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Maintenance and Evolution, ICSME 2017",
      "Publication date": "2017-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Sae-Lim, Natthawute;Hayashi, Shinpei;Saeki, Motoshi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013078209",
      "Primary study DOI": "10.1109/ICSME.2016.27",
      "Title": "Smells like teen spirit: Improving bug prediction performance using the intensity of code smells",
      "Abstract": "Code smells are symptoms of poor design and implementation choices. Previous studies empirically assessed the impact of smells on code quality and clearly indicate their negative impact on maintainability, including a higher bug-proneness of components affected by code smells. In this paper we capture previous findings on bug-proneness to build a specialized bug prediction model for smelly classes. Specifically, we evaluate the contribution of a measure of the severity of code smells (i.e., code smell intensity) by adding it to existing bug prediction models and comparing the results of the new model against the baseline model. Results indicate that the accuracy of a bug prediction model increases by adding the code smell intensity as predictor. We also evaluate the actual gain provided by the intensity index with respect to the other metrics in the model, including the ones used to compute the code smell intensity. We observe that the intensity index is much more important as compared to other metrics used for predicting the buggyness of smelly classes.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Palomba, Fabio;Zanoni, Marco;Fontana, Francesca Arcelli;De Lucia, Andrea;Oliveto, Rocco",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84964876126",
      "Primary study DOI": "10.1109/ISSRE.2015.7381819",
      "Title": "Experience report: Evaluating the effectiveness of decision trees for detecting code smells",
      "Abstract": "Developers continuously maintain software systems to adapt to new requirements and to fix bugs. Due to the complexity of maintenance tasks and the time-to-market, developers make poor implementation choices, also known as code smells. Studies indicate that code smells hinder comprehensibility, and possibly increase change- and fault-proneness. Therefore, they must be identified to enable the application of corrections. The challenge is that the inaccurate definitions of code smells make developers disagree whether a piece of code is a smell or not, consequently, making difficult creation of a universal detection solution able to recognize smells in different software projects. Several works have been proposed to identify code smells but they still report inaccurate results and use techniques that do not present to developers a comprehensive explanation how these results have been obtained. In this experimental report we study the effectiveness of the Decision Tree algorithm to recognize code smells. For this, it was applied in a dataset containing 4 open source projects and the results were compared with the manual oracle, with existing detection approaches and with other machine learning algorithms. The results showed that the approach was able to effectively learn rules for the detection of the code smells studied. The results were even better when genetic algorithms are used to pre-select the metrics to use.",
      "Keywords": "Code Smells | Decision Tree | Genetic Algorithm | Software Quality",
      "Publication venue": "2015 IEEE 26th International Symposium on Software Reliability Engineering, ISSRE 2015",
      "Publication date": "2016-01-13",
      "Publication type": "Conference Paper",
      "Authors": "Amorim, Lucas;Costa, Evandro;Antunes, Nuno;Fonseca, Baldoino;Ribeiro, Márcio",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013113212",
      "Primary study DOI": "10.1109/ICSME.2016.33",
      "Title": "Automatic detection of instability architectural smells",
      "Abstract": "Code smells represent well known symptoms of problems at code level, and architectural smells can be seen as their counterpart at architecture level. If identified in a system, they are usually considered more critical than code smells, for their effect on maintainability issues. In this paper, we introduce a tool for the detection of architectural smells that could have an impact on the stability of a system. The detection techniques are based on the analysis of dependency graphs extracted from compiled Java projects and stored in a graph database. The results combine the information gathered from dependency and instability metrics to identify flaws hidden in the software architecture. We also propose some filters trying to avoid possible false positives.",
      "Keywords": "Architectural smells | Design metrics | Software architecture evaluation",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Fontana, Francesca Arcelli;Pigazzini, Ilaria;Roveda, Riccardo;Zanoni, Marco",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013127714",
      "Primary study DOI": "",
      "Title": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Abstract": "The proceedings contain 82 papers. The topics discussed include: automated extraction of mixins in cascading style sheets; detect cross-browser issues for JavaScript-based web applications based on record/replay; inferring links between concerns and methods with multi-abstraction vector space model; inferring computational state machine models from program executions; an optimization approach for matching textual domain models with existing code; evolving NoSQL databases without downtime; from quick fixes to slow fixes: reimagining static analysis resolutions to enable design space exploration; recommending code changes for automatic backporting of Linux device drivers; smells like teen spirit: improving bug prediction performance using the intensity of code smells; comparing quality metrics for cloned and non cloned Java methods: a large scale empirical study; concepts, operations, and feasibility of a projection-based variation control system; who is who in the mailing list? comparing six disambiguation heuristics to identify multiple addresses of a participant; TechLand: assisting technology landscape inquiries with insights from stack overflow; learning to extract api mentions from informal natural language discussions; accessing inaccessible android APIs: an empirical study; and a quantitative and qualitative investigation of performance-related commits in Android apps.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028725240",
      "Primary study DOI": "10.1016/j.jss.2017.08.032",
      "Title": "Towards collaborative storage scheduling using alternating direction method of multipliers for mobile edge cloud",
      "Abstract": "Performance of cloud computing would be much improved by extending storage capabilities to devices at the edge of network. Unfortunately, the commonly employed algorithms fail to be adaptive to the new storage pattern on mobile edge cloud. To address this issue, we propose a collaborative storage architecture model and an alternating-direction-method-of-multipliers-based collaborative storage scheduling algorithm called ACMES (Algorithm of Collaborative Mobile Edge Storage), in which heterogeneous information of nodes in mobile edge cloud is considered and integrated to make decisions. Besides, feasible solutions for storage will be acquired after iterations of computing. By formulating the collaborative storage scheduling problem in the mobile edge cloud and designing the collaborative decision-making process with the theory of Alternating Direction Method of Multipliers (ADMM), the proposed ACMES is able to minimize power usage and the risk of node withdrawal without reducing the reliability of node storage, and meanwhile make storage scheduling decisions at the edge environment directly and work in a distributed and parallel way. The convergence analysis shows that ACMES has the ability to solve complicated mobile edge cloud storage problems in reality. Extensive experiments validate its effectiveness as well as its superiority to three existing strategies (ADM, RDM and ERASURE) in total cost, reliability, power usage and withdrawal risks.",
      "Keywords": "ADMM | Collaborative decision-making | Data storage | Mobile edge cloud",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Wu, Guanlin;Chen, Junjie;Bao, Weidong;Zhu, Xiaomin;Xiao, Wenhua;Wang, Ji",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030453862",
      "Primary study DOI": "10.1016/j.jss.2017.09.022",
      "Title": "Improving feature location in long-living model-based product families designed with sustainability goals",
      "Abstract": "The benefits of Software Product Lines (SPL) are very appealing: software development becomes better, faster, and cheaper. Unfortunately, these benefits come at the expense of a migration from a family of products to a SPL. Feature Location could be useful in achieving the transition to SPLs. This work presents our FeLLaCaM approach for Feature Location. Our approach calculates similarity to a description of the feature to locate, occurrences where the candidate features remain unchanged, and changes performed to the candidate features throughout the retrospective of the product family. We evaluated our approach in two long-living industrial domains: a model-based family of firmwares for induction hobs that was developed over more than 15 years, and a model-based family of PLC software to control trains that was developed over more than 25 years. In our evaluation, we compare our FeLLaCaM approach with two other approaches for Feature Location: (1) FLL (Feature Location through Latent Semantic Analysis) and (2) FLC (Feature Location through Comparisons). We measure the performance of FeLLaCaM, FLL, and FLC in terms of recall, precision, Matthews Correlation Coefficient, and Area Under the Receiver Operating Characteristics curve. The results show that FeLLaCaM outperforms FLL and FLC.",
      "Keywords": "Architecture sustainability | Feature location | Long-Living software systems | Model–Driven engineering | Software product lines",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Cetina, Carlos;Font, Jaime;Arcega, Lorena;Pérez, Francisca",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028944868",
      "Primary study DOI": "10.1016/j.jss.2017.08.025",
      "Title": "The Bayesian Network based program dependence graph and its application to fault localization",
      "Abstract": "Fault localization is an important and expensive task in software debugging. Some probabilistic graphical models such as probabilistic program dependence graph (PPDG) have been used in fault localization. However, PPDG is insufficient to reason across nonadjacent nodes and only support making inference about local anomaly. In this paper, we propose a novel probabilistic graphical model called Bayesian Network based Program Dependence Graph (BNPDG) that has the excellent inference capability for reasoning across nonadjacent nodes. We focus on applying the BNPDG to fault localization. Compared with the PPDG, our BNPDG-based fault localization approach overcomes the reasoning limitation across nonadjacent nodes and provides more precise fault localization by taking its output nodes as the common conditions to calculate the conditional probability of each non-output node. The experimental results show that our BNPDG-based fault localization approach can significantly improve the effectiveness of fault localization.",
      "Keywords": "Bayesian network | Fault localization | Program analysis",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Yu, Xiao;Liu, Jin;Yang, Zijiang;Liu, Xiao",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030467719",
      "Primary study DOI": "10.1016/j.jss.2017.09.011",
      "Title": "Time series forecasting for dynamic quality of web services: An empirical study",
      "Abstract": "Web Services (WSs) constitute a critical component of modern software development. Knowing the dynamic qualities of WSs is mandatory during use, and these qualities vary continuously over time. However, in most cases, the quality information furnished by service providers is static and does not consider dynamic variations in quality over time. Thus, it is necessary to determine a method for acquiring accurate quality values for WSs. The motivation for this research is that the most suitable time-series method for dynamic quality prediction of WSs remains unknown because no comprehensive empirical comparison of the representative time-series methods has been performed. Therefore, in this paper, we implement all the representative time-series methods and compare their dynamic quality predictions for WSs using a real-world quality dataset. For empirical comparison, we have ensured that our study is reproducible and referenceable by providing diverse specifics and evaluating their validity in detail. The experimental results and diverse discussions presented in this paper may act as a valuable reference to both academic researchers and WS consumers and providers in industry because they can depend on the results when selecting the most suitable time-series method for direct use or as a starting point for further modifications. Based on our experimental results, among the included time-series forecasting approaches, genetic programming (GP) can achieve the highest quality of service (QoS) forecasting accuracy (in our experiments, the average mean absolute error and mean absolute percentage error are 1258 and 20%, respectively); however, this approach also requires the longest time to produce a QoS predictor (67.7 s on average). Though auto-regressive integrated moving average (ARIMA, with average error measures of 1343 and 25.4%) and exponential smoothing (ES, with average error measures of 1354 and 25.7%) present slightly worse accuracy than GP, ARIMA and ES require much less time to generate a predictor than GP (on average, 0.1612 and 0.1519 s, respectively); thus, these approaches might represent a compromise between forecasting accuracy and cost.",
      "Keywords": "QoS prediction | Time-series forecasting | Web service",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Syu, Yang;Kuo, Jong Yih;Fanjiang, Yong Yi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028881203",
      "Primary study DOI": "10.1016/j.jss.2017.08.044",
      "Title": "A theory of power in emerging software ecosystems formed by small-to-medium enterprises",
      "Abstract": "In a software ecosystem, partner companies rely on each other to succeed and survive. This scenario of mutual dependence entails a flow of power among companies. Power is an intrinsic property of their relationship and an asset to be exercised with a degree of intentionality. This paper presents a substantive theory to explain how power and dependence manifest in partnerships among small-to-medium enterprises (SMEs) building a software ecosystem. We performed exploratory case studies of two emerging software ecosystems formed by SMEs. We interpreted the results in light of a theoretical framework underpinned by French and Raven's power taxonomy. Finally, we performed a cross-case analysis to evaluate our findings and build the theory. The proposed theory highlights the interactions among different forms of power and corresponding sources of power employed by companies. It provides a better understanding on how power and dependence influence the behaviour and coordination of companies within a software ecosystem. The theory is a useful lens for researchers to explore ecosystem partnerships by understanding the structure and impact of power relationships between partners. In addition, it is a valuable tool for companies to analyse power distribution and define sustainable strategies for software ecosystem governance.",
      "Keywords": "Dependence | Multiple case studies | Power | Small-to-medium enterprises | Software ecosystem",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Valença, George;Alves, Carina",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028704529",
      "Primary study DOI": "10.1016/j.jss.2017.08.038",
      "Title": "Self-organizing multi-agent systems for the control of complex systems",
      "Abstract": "Because of the law of requisite variety, designing a controller for complex systems implies designing a complex system. In software engineering, usual top-down approaches become inadequate to design such systems. The Adaptive Multi-Agent Systems (AMAS) approach relies on the cooperative self-organization of autonomous micro-level agents to tackle macro-level complexity. This bottom-up approach provides adaptive, scalable, and robust systems. This paper presents a complex system controller that has been designed following this approach, and shows results obtained with the automatic tuning of a real internal combustion engine.",
      "Keywords": "Complex systems | Control | Internal combustion engines | Multi-Agent systems | Self-organization",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Boes, Jérémy;Migeon, Frédéric",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029718105",
      "Primary study DOI": "10.1016/j.jss.2017.09.001",
      "Title": "Task Scheduling in Big Data Platforms: A Systematic Literature Review",
      "Abstract": "Context: Hadoop, Spark, Storm, and Mesos are very well known frameworks in both research and industrial communities that allow expressing and processing distributed computations on massive amounts of data. Multiple scheduling algorithms have been proposed to ensure that short interactive jobs, large batch jobs, and guaranteed-capacity production jobs running on these frameworks can deliver results quickly while maintaining a high throughput. However, only a few works have examined the effectiveness of these algorithms. Objective: The Evidence-based Software Engineering (EBSE) paradigm and its core tool, i.e., the Systematic Literature Review (SLR), have been introduced to the Software Engineering community in 2004 to help researchers systematically and objectively gather and aggregate research evidences about different topics. In this paper, we conduct a SLR of task scheduling algorithms that have been proposed for big data platforms. Method: We analyse the design decisions of different scheduling models proposed in the literature for Hadoop, Spark, Storm, and Mesos over the period between 2005 and 2016. We provide a research taxonomy for succinct classification of these scheduling models. We also compare the algorithms in terms of performance, resources utilization, and failure recovery mechanisms. Results: Our searches identifies 586 studies from journals, conferences and workshops having the highest quality in this field. This SLR reports about different types of scheduling models (dynamic, constrained, and adaptive) and the main motivations behind them (including data locality, workload balancing, resources utilization, and energy efficiency). A discussion of some open issues and future challenges pertaining to improving the current studies is provided.",
      "Keywords": "Hadoop | Mesos | Spark | Storm | Systematic Literature Review | Task Scheduling",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Soualhia, Mbarka;Khomh, Foutse;Tahar, Sofiène",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030668983",
      "Primary study DOI": "10.1016/j.jss.2017.09.018",
      "Title": "QuickFuzz testing for fun and profit",
      "Abstract": "Fuzzing is a popular technique to find flaws in programs using invalid or erroneous inputs but not without its drawbacks. At one hand, mutational fuzzers require a set of valid inputs as a starting point, in which modifications are then introduced. On the other hand, generational fuzzing allows to synthesize somehow valid inputs according to a specification. Unfortunately, this requires to have a deep knowledge of the file formats under test to write specifications of them to guide the test case generation process. In this paper we introduce an extended and improved version of QuickFuzz, a tool written in Haskell designed for testing unexpected inputs of common file formats on third-party software, taking advantage of off-the-self well known fuzzers. Unlike other generational fuzzers, QuickFuzz does not require to write specifications for the file formats in question since it relies on existing file-format-handling libraries available on the Haskell code repository. It supports almost 40 different complex file-types including images, documents, source code and digital certificates. In particular, we found QuickFuzz useful enough to discover many previously unknown vulnerabilities on real-world implementations of web browsers and image processing libraries among others.",
      "Keywords": "Fuzzing | Haskell | QuickCheck | Testing",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Grieco, Gustavo;Ceresa, Martín;Mista, Agustín;Buiras, Pablo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030093980",
      "Primary study DOI": "10.1016/j.jss.2017.09.010",
      "Title": "A semi-automatic maintenance and co-evolution of OCL constraints with (meta)model evolution",
      "Abstract": "Metamodels are core components of modeling languages to define structural aspects of a business domain. As a complement, OCL constraints are used to specify detailed aspects of the business domain, e.g. more than 750 constraints come with the UML metamodel. As the metamodel evolves, its OCL constraints may need to be co-evolved too. Our systematic analysis shows that semantically different resolutions can be applied depending not only on the metamodel changes, but also on the user intent and on the structure of the impacted constraints. In this paper, we first investigate the syntactical reasons that lead to apply different resolutions. We then propose a co-evolution approach that offers alternative resolutions while allowing the user to choose the best applicable one. We evaluated our approach on six case studies of metamodel evolution and their OCL constraints co-evolution. The results show the usefulness of alternative resolutions along with user decision to cope with real co-evolution scenarios. Within our six case studies our approach led to an average of 92% (syntactically) and 93% (semantically) matching co-evolution w.r.t. the user intent.",
      "Keywords": "Co-evolution | Constraints | Evolution | Metamodel | OCL",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Khelladi, Djamel Eddine;Bendraou, Reda;Hebig, Regina;Gervais, Marie Pierre",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029586970",
      "Primary study DOI": "10.1016/j.jss.2017.09.009",
      "Title": "Reusability of open source software across domains: A case study",
      "Abstract": "Exploiting the enormous amount of open source software (OSS) as a vehicle for reuse is a promising opportunity for software engineers. However, this task is far from trivial, since such projects are sometimes not easy to understand and adapt to target systems, whereas at the same time the reusable assets are not obvious to identify. In this study, we assess open source software projects, with respect to their reusability, i.e., the easiness to adapt them in a new system. By taking into account that domain-specific reuse is more beneficial than domain-agnostic; we focus this study on identifying the application domains that contain the most reusable software projects. To achieve this goal, we compared the reusability of approximately 600 OSS projects from ten application domains through a case study. The results of the study suggested that in every aspect of reusability, there are different dominant application domains. However, Science and Engineering Applications and Software Development Tools, have proven to be the ones that are the most reuse-friendly. Based on this observation, we suggest software engineers, who are focusing on the specific application domains, to consider reusing assets from open source software projects.",
      "Keywords": "Application domains | Open source | Reusability",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Paschali, Maria Eleni;Ampatzoglou, Apostolos;Bibi, Stamatia;Chatzigeorgiou, Alexander;Stamelos, Ioannis",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028526955",
      "Primary study DOI": "10.1016/j.jss.2017.08.043",
      "Title": "Stochastic modeling for performance and availability evaluation of hybrid storage systems",
      "Abstract": "Improvements in computational systems may be constrained by the efficiency of storage drives. Therefore, replacing hard disk drives (HDD) with solid-state drives (SSD) may also be an effective way to improve system performance, but the higher cost per gigabyte and reduced lifetime of SSDs hinder a thorough replacement. To mitigate these issues, several architectures have been conceived based on hybrid storage systems, but performance and availability models have not been proposed to better assess such different architectures. This paper presents an approach based on stochastic models (i.e., stochastic Petri nets and reliability block diagrams) for performance and availability evaluation of hybrid storage systems. The proposed models can represent write and read operations, and they may estimate response time, throughput and availability. A case study based on OpenStack Swift platform is adopted to demonstrate the feasibility of the proposed approach.",
      "Keywords": "Availability | Cloud computing | Hybrid storage | Performance evaluation | Reliability block diagrams | Stochastic petri nets",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Borba, Eric;Tavares, Eduardo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028968180",
      "Primary study DOI": "10.1016/j.jss.2017.08.042",
      "Title": "Understanding the interplay between the logical and structural coupling of software classes",
      "Abstract": "During the lifetime of object-Oriented (OO) software systems, new classes are added to increase functionality, also increasing the inter-dependencies between classes. Logical coupling depicts the change dependencies between classes, while structural coupling measures source code dependencies induced via the system architecture. The relationship or dependency between logical and structural coupling have been debated in the past, but no large study has confirmed yet their interplay. In this study, we have analysed 79 open-source software projects of different sizes to investigate the interplay between the two types of coupling. First, we quantified the overlapping or intersection of structural and logical class dependencies. Second, we statistically computed the correlation between the strengths of logical and structural dependencies. Third, we propose a simple technique to determine the stability of OO software systems, by clustering the pairs of classes as “stable” or “unstable”, based on their co-change pattern. The results from our statistical analysis show that although there is no strong evidence of a linear correlation between the strengths of the coupling types, there is substantial evidence to conclude that structurally coupled class pairs usually include logical dependencies. However, not all co-changed class pairs are also linked by structural dependencies. Finally, we identified that only a low proportion of structural coupling shows excessive instability in the studied OSS projects.",
      "Keywords": "Co-changed structural dependencies (CSD) | Coupled logical dependencies (CLD) | Object-oriented (OO) | Open-source software (OSS) | References | Structural coupling",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Ajienka, Nemitari;Capiluppi, Andrea",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029575478",
      "Primary study DOI": "10.1016/j.jss.2017.09.007",
      "Title": "An industrial case study on an architectural assumption documentation framework",
      "Abstract": "As an important type of architectural knowledge, documenting architectural assumptions (AAs) is critical to the success of projects. In this work, we proposed and validated an Architectural Assumption Documentation Framework (AADF), which is composed of four viewpoints (i.e., the Detail, Relationship, Tracing, and Evolution viewpoint), to document AAs in projects. One case study with two cases was conducted at two companies from different domains and countries. The main findings are: (1) AADF can be understood by architects in a short time (i.e., a half day workshop); (2) the AA Evolution view requires the least time to create, followed by the AA Detail view and the AA Relationship view; (3) AADF can help stakeholders to identify risks and understand AAs documented by other stakeholders; and (4) understanding and applying AADF is related to various factors, including factors regarding the framework per se (e.g., tutorial, examples, concepts, and terms), personal experience, resources (e.g., time), tool support, and project context (e.g., project size and number of AAs). Adjusting these factors in an appropriate way can facilitate the usage of AADF and further benefit the projects.",
      "Keywords": "Architectural assumption | Case study | Documentation framework | Software architecture",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Yang, Chen;Liang, Peng;Avgeriou, Paris;Eliasson, Ulf;Heldal, Rogardt;Pelliccione, Patrizio;Bi, Tingting",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030565352",
      "Primary study DOI": "10.1016/j.jss.2017.09.020",
      "Title": "An empirical investigation of the influence of persona with personality traits on conceptual design",
      "Abstract": "Persona, an archetypical user, is increasingly becoming a popular tool for Software Engineers to design and communicate with stakeholders. A persona is a representative of a class of end users of a product or service. However, the majority of personas presented in the literature do not take into consideration that the personality of users affects the way they interact with a product or service. This study empirically explores variations in conceptual design based on the personality of a persona. We carried out two studies in Australia and one study in Denmark. We presented four personas with different personalities to 91 participants who collectively completed 218 design artifacts. The results from the studies indicate that the participants' views and prioritization of the needs and system requirements were influenced by the personality traits of the provided personas. For an introverted and emotionally unstable personality, inclusion of confidence building and socializer design features had a higher priority compared with the identified requirements for an extravert and emotionally stable personality. The findings support the proposition that personas with personality traits can aid software engineers to produce conceptual designs tailored to the needs of specific personalities.",
      "Keywords": "Conceptual design | Design features | Empirical study | Holistic Persona | Persona | Personality traits | User-centered design",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Anvari, Farshid;Richards, Deborah;Hitchens, Michael;Babar, Muhammad Ali;Tran, Hien Minh Thi;Busch, Peter",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028911281",
      "Primary study DOI": "10.1016/j.jss.2017.08.045",
      "Title": "Predicting change consistency in a clone group",
      "Abstract": "Code cloning has been accepted as one of the general code reuse methods in software development, thanks to the increasing demand in rapid software production. The introduction of clone groups and clone genealogies enable software developers to be aware of the presence of and changes to clones as a collective group; they also allow developers to understand how clone groups evolve throughout software life cycle. Due to similarity in codes within a clone group, a change in one piece of the code may require developers to make consistent change to other clones in the group. Failure in making such consistent change to a clone group when necessary is commonly known as “clone consistency-defect”, which can adversely impact software reusability. In this work, we propose an approach to predict the need for making consistent change in clones within a clone group at the time when changes have been made to one of its clones. We build a variant of clone genealogies to collect all consistent/inconsistent changes to clone groups, and extract three attribute sets from clone groups as input for predicting the need for consistent clone change. These three attribute sets are code attributes, context attributes and evolution attributes respectively. Together, they provide a holistic view about clone changes. We conduct experiments on four open source projects. Our experiments show that our approach has reasonable precision and recall in predicting whether a clone group requires (or is free of) consistent change. This holistic approach can aid developers in maintaining clone changes, and avoid potential consistency-defect, which can improve software quality and reusability.",
      "Keywords": "Bayesian network | Clone attributes | Clone maintenance | Code clones | Consistency-requirement prediction | Software reuse",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Zhang, Fanlong;Khoo, Siau cheng;Su, Xiaohong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030973152",
      "Primary study DOI": "10.1016/j.jss.2017.09.021",
      "Title": "Enhancing developer recommendation with supplementary information via mining historical commits",
      "Abstract": "Given a software issue request, one important activity is to recommend suitable developers to resolve it. A number of approaches have been proposed on developer recommendation. These developer recommendation techniques tend to recommend experienced developers, i.e., the more experienced a developer is, the more possible he/she is recommended. However, if the experienced developers are hectic, the junior developers may be employed to finish the incoming issue. But they may have difficulty in these tasks for lack of development experience. In this article, we propose an approach, EDR_SI, to enhance developer recommendation by considering their expertise and developing habits. Furthermore, EDR_SI also provides the personalized supplementary information for developers to use, such as personalized source code files, developer network and source-code change history. An empirical study on five open source subjects is conducted to evaluate the effectiveness of EDR_SI. In our study, EDR_SI is also compared with the state-of-art developer recommendation techniques, iMacPro, Location and ABA-Time-tf-idf, to evaluate the effectiveness of developer recommendation, and the results show that EDR_SI can not only improve the accuracy of developer recommendation, but also effectively provide useful supplementary information for them to use when they implement the incoming issue requests.",
      "Keywords": "Bug assignment | Collaborative topic modeling | Commit repository | Developer recommendation | Personalized recommendation | Supplementary information recommendation",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Sun, Xiaobing;Yang, Hui;Xia, Xin;Li, Bin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028936139",
      "Primary study DOI": "10.1016/j.jss.2017.08.009",
      "Title": "Striving for balance: A look at gameplay requirements of massively multiplayer online role-playing games",
      "Abstract": "Engineering gameplay requirements is the most important task for game development organizations. Game industry discourse is concerned with continuous redesign of gameplay to enhance players’ experience and boost game's appeal. However, accounts of gameplay requirements practices are rare. In responding to calls for more research into gameplay requirements engineering, we performed an exploratory study in the context of massively multiplayer online role-playing games (MMORPGs), from the perspective of practitioners involved in the field. Sixteen practitioners from three leading MMORPG-producing companies were interviewed and their gameplay requirements documents were reviewed. Interviewing and qualitative data analysis occurred in a cyclical process with results at each stage of the study informing decisions about data collection and analysis in the next. The analysis revealed a process of striving to reach a balance among three perspectives of gameplay requirements: a process perspective, an artifact perspective and a player-designer relationship perspective. This balance-driven process is co-created by game developers and players, is endless within the MMORPG, and is happening both in-game and off-game. It heavily relies on ‘paper-prototyping’ and play-testing for the purpose of gameplay requirements validation. The study concludes with discussion on validity threats and on implications for requirements engineering research, practice and education.",
      "Keywords": "Empirical research method | Gameplay requirements | Qualitative interview-based study | Requirements elicitation | Requirements engineering",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Daneva, Maya",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029941831",
      "Primary study DOI": "10.1016/j.jss.2017.09.015",
      "Title": "Minimum-cost deployment of adjustable readers to provide complete coverage of tags in RFID systems",
      "Abstract": "In Internet of things (IoT), radio frequency identification (RFID) plays an important role to help people rapidly obtain information of objects associated with tags. Passive tags are cheap and require no batteries to operate, so they are widely used in RFID applications. Readers, on the other hand, have to provide power to activate passive tags to get their data. However, collision occurs when two readers send signals to a tag at the same time. Therefore, it is critical to decide the locations of readers, namely reader deployment, to avoid collision. This paper considers adjustable readers, whose transmitted power is configurable to provide different communication range, and proposes a minimum-cost RFID reader deployment (MR2D) problem. Given the positions of tags, it determines how to deploy readers and adjust their transmitted power to cover all tags, such that we can use the minimum number of readers and save their energy. To facilitate data transmission and reduce hardware cost, we restrict the number of tags that each reader can cover and allow readers to have few overlapped tags in their communication range. Then, we develop an efficient solution to the MR2D problem by clustering tags into groups and placing a reader to cover each group to meet the above conditions. Simulation results show that our proposed solution not only saves the number of RFID readers but also reduces their energy consumption, as compared with existing methods.",
      "Keywords": "Collision | Coverage | Passive tag | Reader deployment | RFID",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Wang, You Chiun;Liu, Shu Ju",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029168443",
      "Primary study DOI": "10.1016/j.jss.2017.08.016",
      "Title": "Multi-cloud service composition using Formal Concept Analysis",
      "Abstract": "Recent years have witnessed a rapid growth in exploiting Cloud environments to deliver various types of resources as services. To improve the efficiency of software development, service reuse and composition is viewed as a powerful means. However, effectively composing services from multiple clouds has not been solved yet. Indeed, existing solutions assume that the services participating to a composition come from a single cloud. This approach is unrealistic since the other existing clouds may host more suitable services. In order to deliver high quality service compositions, the user request must be checked against the services in the multi-cloud environment (MCE) or at least clouds in the availability zone of the user. In this paper, we propose a multi-cloud service composition (MCSC) approach based on Formal Concept Analysis (FCA). We use FCA to represent and combine information of multiple clouds. FCA is based on the concept lattice which is a powerful mean to classify clouds and services information. We first model the multi-cloud environment as a set of formal contexts. Then, we extract and combine candidate clouds from formal concepts. Finally, the optimal cloud combination is selected and the MCSC is transformed into a classical service composition problem. Conducted experiments proved the effectiveness and the ability of FCA based method to regroup and find cloud combinations with a minimal number of clouds and a low communication cost. Also, the comparison with two well-known combinatorial optimization approaches showed that the adopted top-down strategy allowed to rapidly select services hosted on the same and closest clouds, which directly reduced the inter-cloud communication cost, compared to existing approaches.",
      "Keywords": "Cloud computing | Formal concept analysis | Lattice theory | Multi-cloud | Service composition",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Mezni, Haithem;Sellami, Mokhtar",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029543862",
      "Primary study DOI": "10.1016/j.jss.2017.09.008",
      "Title": "Efficient exact Boolean schedulability tests for fixed priority preemption threshold scheduling",
      "Abstract": "Fixed priority preemption threshold scheduling (PTS) is effective in scalable real-time system design, which requires system tuning processes, where the performance of schedulability tests for PTS matters much. This paper proposes five methods for efficient exact Boolean schedulability tests for PTS, which returns the Boolean result that tells whether the given task is schedulable or not. We regard the sufficient test for fully preemptive fixed priority scheduling (FPS) and the early exit in the response time calculations as the conventional approach. We propose (1) a new sufficient test, (2) new initial values for the start/finish times, (3) pre-calculation of the interference time within the start time, (4) incremental start/finish time calculation, and (5) early exits in start/finish time calculations. These are based on some previous work for FPS. The new initial start time, pre-calculation, and the incremental calculations also can be used for the exact response time analysis for PTS. Our empirical results show that the overall proposed methods reduce the iteration count/run time of the conventional test by about 60%/40%, regardless of the number of tasks and the total utilization.",
      "Keywords": "Fixed priority scheduling | Preemption threshold scheduling | Real-time systems and embedded systems | System integration and implementation",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Kim, Saehwa",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030538772",
      "Primary study DOI": "10.1016/j.jss.2017.09.019",
      "Title": "Measuring social networks when forming information system project teams",
      "Abstract": "Despite the advances in the project management field, little is known about how creating performing software teams in a systematical and repeatable way. The technical dimension is not enough to achieve this. Software development is a complex collaborative process where people and interpersonal relationships – i.e., how people interact, behave and organize – significantly influence the project success. In this paper, we define a framework to assign people to projects from this socio-technical perspective. Social networks characterizing the interplay between teammates are built and analyzed to predict productive collaborations and identify adequate team-members depending on the organization needs and the kind of project. A noteworthy novelty of these social networks is that they estimate compatibility between coworkers according to previous collaborations, but also according to individuals' social skills. This allows analyzing the compatibility among people who have not worked together before. We present results of using the proposed framework in a multinational corporation during a more-than-two-year period. Our in-company experiments emphasize that we can significantly improve the expected outcomes characterizing and measuring the social interaction among coworkers. Social aspects discussed may be highly relevant in the context of distributed software engineering, since it implies new challenges in the interplay among coworkers.",
      "Keywords": "Collaboration network | Interpersonal relationships | Social network analysis | Social skills | Team formation problem | Team member allocation",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Latorre, Roberto;Suárez, Javier",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026450983",
      "Primary study DOI": "10.1016/j.jss.2017.07.013",
      "Title": "A novel analysis approach for the design and the development of context-aware applications",
      "Abstract": "In this paper, we propose a novel analysis approach, called ANALOG, for the design and the development of context-aware applications able to detect context change and to predict context parameter values. Our approach is described by two analysis procedures, (a) an analysis procedure for detection and (b) an analysis procedure for prediction. The proposed analysis procedures aim to offer a support for application designers allowing them to design easily context-aware applications. Each procedure is achieved by a sequence of steps performed by the designers. We describe first our analysis approach presented by the analysis procedures. Then, we give some implementation details of our approach. Afterwards, we show the usefulness of the analysis approach through presenting two execution scenarios related to a smart building case study. Finally, to illustrate the effectiveness of our approach, we present different experiments related to (i) the processing time of the analysis approach and (ii) the CPU and the memory overhead introduced by our approach.",
      "Keywords": "Adaptive thresholds | Analysis procedure for detection | Analysis procedure for prediction | Context-aware applications | Extreme Value Theory | Prediction models",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Khabou, Nesrine;Bouassida Rodriguez, Ismael;Jmaiel, Mohamed",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85025115622",
      "Primary study DOI": "10.1016/j.jss.2017.07.008",
      "Title": "Transition of organizational roles in Agile transformation process: A grounded theory approach",
      "Abstract": "This study aims to identify how traditional organizational roles are transformed towards Agile roles. A grounded theory study with 5 software teams in a large software development company in Spain was conducted. We interviewed 21 people, participated in Agile Coaching sessions and observed daily activities in the company during a six-month period. The transition of organizational roles during the Agile transformation process is influenced by many circumstances such as: higher management commitment and support, knowledge about Agile, project team size and culture. The theory demonstrates how enterprise factors influence the transition of organizational roles and how practitioners cope with the challenges during the Agile transformation process. Based on our results, we proposed recommendations and guidelines for organizational roles transition during the Agile method adoption.",
      "Keywords": "Agile transformation process | Grounded theory | Organizational roles | Software development",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Jovanović, Miloš;Mas, Antonia;Mesquida, Antoni Lluís;Lalić, Bojan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028058482",
      "Primary study DOI": "10.1016/j.jss.2017.08.017",
      "Title": "Automatic clustering constraints derivation from object-oriented software using weighted complex network with graph theory analysis",
      "Abstract": "Constrained clustering or semi-supervised clustering has received a lot of attention due to its flexibility of incorporating minimal supervision of domain experts or side information to help improve clustering results of classic unsupervised clustering techniques. In the domain of software remodularisation, classic unsupervised software clustering techniques have proven to be useful to aid in recovering a high-level abstraction of the software design of poorly documented or designed software systems. However, there is a lack of work that integrates constrained clustering for the same purpose to help improve the modularity of software systems. Nevertheless, due to time and budget constraints, it is laborious and unrealistic for domain experts who have prior knowledge about the software to review each and every software artifact and provide supervision on an on-demand basis. We aim to fill this research gap by proposing an automated approach to derive clustering constraints from the implicit structure of software system based on graph theory analysis of the analysed software. Evaluations conducted on 40 open-source object-oriented software systems show that the proposed approach can serve as an alternative solution to derive clustering constraints in situations where domain experts are non-existent, thus helping to improve the overall accuracy of clustering results.",
      "Keywords": "Complex network | Constrained clustering | Graph theory | Software clustering | Software remodularisation",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Chong, Chun Yong;Lee, Sai Peck",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026838411",
      "Primary study DOI": "10.1016/j.jss.2017.07.032",
      "Title": "Reliability and temperature constrained task scheduling for makespan minimization on heterogeneous multi-core platforms",
      "Abstract": "We study the problem of scheduling tasks onto a heterogeneous multi-core processor platform for makespan minimization, where each cluster on the platform has a probability of failure governed by an exponential law and the processor platform has a thermal constraint specified by a peak temperature threshold. The goal of our work is to design algorithms that optimize makespan under the constraints of reliability and temperature. We first provide a mixed-integer linear programming (MILP) formulation for assigning and scheduling independent tasks with reliability and temperature constraints on the heterogeneous platform to minimize the makespan. However, MILP takes exponential time to finish. We then propose a two-stage heuristic that determines the assignment, replication, operating frequency, and execution order of tasks to minimize the makespan while satisfying the real-time, reliability, and temperature constraints based on the analysis of the effects of task assignment on makespan, reliability, and temperature. We finally carry out extensive simulation experiments to validate our proposed MILP formulation and two-stage heuristic. Simulation results demonstrate that the proposed MILP formulation can achieve the best performance in reducing makespan among all the methods used in the comparison. The results also show that the proposed two-stage heuristic has a close performance as the representative existing approach ESTS and a better performance when compared to the representative existing approach RBSA, in terms of reducing makespan. In addition, the proposed two-stage heuristic has the highest feasibility as compared to RBSA and ESTS.",
      "Keywords": "Makespan minimization | Reliability | Task assignment and scheduling | Temperature",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Zhou, Junlong;Cao, Kun;Cong, Peijin;Wei, Tongquan;Chen, Mingsong;Zhang, Gongxuan;Yan, Jianming;Ma, Yue",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85002194726",
      "Primary study DOI": "10.1016/j.jss.2016.10.003",
      "Title": "Going with the flow: An activity theory analysis of flow techniques in software development",
      "Abstract": "Managing flow is fundamental to continuous development, particularly in knowledge intensive work activities such as software development. However, while numerous articles describe flow tools and practice there is little research on their application in context. This is a significant limitation given that software development is a highly complex and socially embedded activity. This research applies activity theory (AT) to examine the adoption of flow techniques by using the multiple-case method in two companies. AT is particularly pertinent in this study as it identifies contradictions, which manifest themselves as problems such as errors or a breakdown of communication in the organisation and congruencies between flow techniques and the development context and indeed contradictions between components of flow techniques themselves. Rather than view contradictions as a threat to flow or as an argument to abandon, a theoretical contribution of this study is that it shows how contradictions and congruencies can be used to reflect, learn, and identify new ways of structuring and enacting the flow activity. It also provides an immediate practical contribution by identifying a set of lessons drawn from the cases studied that may be applicable in future implementations of flow techniques.",
      "Keywords": "Activity theory | Continuous software development | Flow | Kanban | Lean",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Dennehy, Denis;Conboy, Kieran",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85025679495",
      "Primary study DOI": "10.1016/j.jss.2017.07.009",
      "Title": "Introducing continuous experimentation in large software-intensive product and service organisations",
      "Abstract": "Software development in highly dynamic environments imposes high risks to development organizations. One such risk is that the developed software may be of only little or no value to customers, wasting the invested development efforts. Continuous experimentation, as an experiment-driven development approach, may reduce such development risks by iteratively testing product and service assumptions that are critical to the success of the software. Although several experiment-driven development approaches are available, there is little guidance available on how to introduce continuous experimentation into an organization. This article presents a multiple-case study that aims at better understanding the process of introducing continuous experimentation into an organization with an already established development process. The results from the study show that companies are open to adopting such an approach and learning throughout the introduction process. Several benefits were obtained, such as reduced development efforts, deeper customer insights, and better support for development decisions. Challenges included complex stakeholder structures, difficulties in defining success criteria, and building experimentation skills. Our findings indicate that organizational factors may limit the benefits of experimentation. Moreover, introducing continuous experimentation requires fundamental changes in how companies operate, and a systematic introduction process can increase the chances of a successful start.",
      "Keywords": "Agile software development | Continuous experimentation | Experiment-driven software development | Lean software development | Lean startup | Product management",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Yaman, Sezin Gizem;Munezero, Myriam;Münch, Jürgen;Fagerholm, Fabian;Syd, Ossi;Aaltola, Mika;Palmu, Christina;Männistö, Tomi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027717116",
      "Primary study DOI": "10.1016/j.jss.2017.08.026",
      "Title": "A multi-level feedback approach for the class integration and test order problem",
      "Abstract": "Class integration and test order (CITO) problem is to devise an optimal inter-class order which can minimize stubbing efforts. The existing approach for this problem, whether it is graph-based or search-based, usually wastes a significant amount of time and efforts in finding test orders, and sometimes may devise sub-optimal solutions. To overcome this limitation, we introduce a multi-level feedback approach to better solve the CITO problem. In this method, we use a multi-level feedback strategy to calculate test profit for each class, and according to test profit, propose a reward and punishment mechanism to assess the performance of class and set its test priority. Instead of breaking cycles or searching for optimum in the previous methods, our method integrates classes by their test priority, therefore significantly reduces the running time. The experiments are conducted on five benchmark programs and eight industrial programs, and the obtained results are compared with graph-based and search-based approaches. The results indicate that our approach can minimize the stubbing cost efficiently for most programs of all typical approaches compared in this work.",
      "Keywords": "Feedback | Stub minimization | Test cost | Test order",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Zhang, Miao;Jiang, Shujuan;Zhang, Yanmei;Wang, Xingya;Yu, Qiao",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85009477583",
      "Primary study DOI": "10.1016/j.jss.2017.01.002",
      "Title": "User acceptance testing for Agile-developed web-based applications: Empowering customers through wikis and mind maps",
      "Abstract": "User Acceptance Testing (UAT) involves validating software in a real setting by the intended audience. The aim is not so much to check the defined requirements but to ensure that the software satisfies the customer's needs. Agile methodologies put stringent demands on UAT, if only for the frequency at which it needs to be conducted due to the iterative development of small product releases. In this setting, traditional in-person meetings might not scale up well. Complementary ways are needed to reduce the costs of developer-customer collaboration during UAT. This work introduces a wiki-based approach where customers and developers asynchronously collaborate: developers set the UAT scaffolding that will later shepherd customers when testing. To facilitate understanding, mind maps are used to represent UAT sessions. To facilitate engagement, a popular mind map editor, FreeMind, is turned into an editor for FitNesse, the wiki engine in which these ideas are borne out. The approach is evaluated through a case study involving three real customers. First evaluations are promising. Though at different levels of completeness, the three customers were able to complete a UAT. Customers valued asynchronicity, mind map structuredness, and the transparent generation of documentation out of the UAT session.",
      "Keywords": "Agile development | Test automation | User acceptance testing",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Otaduy, I.;Diaz, O.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027980934",
      "Primary study DOI": "10.1016/j.jss.2017.08.024",
      "Title": "Mining domain knowledge from app descriptions",
      "Abstract": "Domain analysis aims at gaining knowledge to a particular domain in the early stage of software development. A key challenge in domain analysis is to extract features automatically from related product artifacts. Compared with other kinds of artifacts, high volume of descriptions can be collected from App marketplaces (such as Google Play and Apple Store) easily when developing a new mobile application (App), so it is essential for the success of domain analysis to gain features and relationships from them using data analysis techniques. In this paper, we propose an approach to mine domain knowledge from App descriptions automatically, where the information of features in a single App description is firstly extracted and formally described by a Concern-based Description Model (CDM), which is based on predefined rules of feature extraction and a modified topic modeling method; then the overall knowledge in the domain is identified by classifying, clustering and merging the knowledge in the set of CDMs and topics, and the results are formalized by a Data-based Raw Domain Model (DRDM). Furthermore, we propose a quantified evaluation method for prioritizing the knowledge in DRDM. The proposed approach is validated by a series of experiments.",
      "Keywords": "App descriptions | Data analysis | Domain analysis | Feature extraction",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Liu, Yuzhou;Liu, Lei;Liu, Huaxiao;Wang, Xiaoyu;Yang, Hongji",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028078659",
      "Primary study DOI": "10.1016/j.jss.2017.07.041",
      "Title": "Analysing and modelling runtime architectural stability for self-adaptive software",
      "Abstract": "With the increased dependence on software, there is a pressing need for engineering long-lived software. As architectures have a profound effect on the life-span of the software and the provisioned quality of service, stable architectures are significant assets. Architectural stability tends to reflect the success of the system in supporting continuous changes without phasing-out. For self-adaptive architectures, the behavioural aspect of stability is essential for seamless operation, to continuously keep the provision of quality requirements stable and prevent unnecessary adaptations that will risk degrading the system. In this paper, we introduce a systematic approach for analysing and modelling architectural stability. Specifically, we leverage architectural concerns and viewpoints to explicitly analyse stability attributes of the intended behaviour. Due to the probabilistic nature of systems’ behaviour, stability modelling is based on a probabilistic relational model for knowledge representation of stability multiple viewpoints. The model, empowered by the quantitative analysis of Bayesian networks, is capable to conduct runtime inference for reasoning about stability under runtime uncertainty. To illustrate the applicability and evaluate the proposed approach, we consider the case of cloud architectures. The results show that the approach increases the efficiency of the architecture in keeping the expected behaviour stable during runtime operation.",
      "Keywords": "Architectural stability | Cloud architecture | Quality of service | Self-adaptive architecture | Software architecture | Sustainability",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Salama, Maria;Bahsoon, Rami",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026858829",
      "Primary study DOI": "10.1016/j.jss.2017.08.013",
      "Title": "A framework for dynamic selection of backoff stages during initial ranging process in wireless networks",
      "Abstract": "The only available solution in the IEEE 802.22 standard for avoiding collision amongst various contending customer premises equipment (CPEs) attempting to associate with a base station (BS) is binary exponential random backoff process in which the contending CPEs retransmit their association requests. The number of attempts the CPEs send their requests to the BS are fixed in an IEEE 802.22 network. This paper presents a mathematical framework that helps the BS in determining at which attempt the majority of the CPEs become part of the wireless regional area network from a particular number of contending CPEs. Based on a particular attempt, the ranging request collision probability for any number of contending CPEs with respect to contention window size is approximated. The numerical results validate the effectiveness of the approximation. Moreover, the average ranging success delay experienced by the majority of the CPEs is also determined.",
      "Keywords": "average ranging success delay | Contention window | Customer premises equipment | ranging request collision probability",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Mufti, Muhammad Rafiq;Afzal, Humaira;Awan, Irfan;Cullen, Andrea",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028333037",
      "Primary study DOI": "10.1016/j.jss.2017.06.069",
      "Title": "Be more familiar with our enemies and pave the way forward: A review of the roles bugs played in software failures",
      "Abstract": "There has been an increasing frequency of failures due to defective software that cost millions of dollars. Recent high profile incidents have drawn increased attention to the risks of failed software systems to the public. Yet aside from the Therac-25 case, very few incidents of software failure causing humans harm have been proven and widely reported. With increased government oversight and the expanded use of social networking for real time reporting of problems, we are only beginning to understand the potential for major injury or death related to software failures. However, debugging defective software can be costly and time consuming. Moreover, undetected bugs could induce great harm to the public when software systems are applied in safety-critical areas, such as consumer products, public infrastructure, transportation systems, etc. Therefore, it is vital that we remove these bugs as early as possible. To gain more understanding of the nature of these bugs, we review the reported software failures that have impacted the health, safety, and welfare of the public. A focus on lessons learned and implications for future software systems is also provided which acts as guidelines for engineers to improve the quality of their products and avoid similar failures from happening.",
      "Keywords": "Accidents | Bugged software systems | Lessons learned | Mishaps | Software failures",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Wong, W. Eric;Li, Xuelin;Laplante, Philip A.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019863772",
      "Primary study DOI": "10.1016/j.jss.2017.05.042",
      "Title": "Reverse engineering language product lines from existing DSL variants",
      "Abstract": "The use of domain-specific languages (DSLs) has become a successful technique to develop complex systems. In this context, an emerging phenomenon is the existence of DSL variants, which are different versions of a DSL adapted to specific purposes but that still share commonalities. In such a case, the challenge for language designers is to reuse, as much as possible, previously defined language constructs to narrow implementation from scratch. To overcome this challenge, recent research in software languages engineering introduced the notion of language product lines. Similarly to software product lines, language product lines are often built from a set of existing DSL variants. In this article, we propose a reverse-engineering technique to ease-off such a development scenario. Our approach receives a set of DSL variants which are used to automatically recover a language modular design and to synthesize the corresponding variability models. The validation is performed in a project involving industrial partners that required three different variants of a DSL for finite state machines. This validation shows that our approach is able to correctly identify commonalities and variability.",
      "Keywords": "Domain-specific languages | Language product lines | Reverse-engineering | Software languages engineering",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-11-01",
      "Publication type": "Article",
      "Authors": "Méndez-Acuña, David;Galindo, José A.;Combemale, Benoît;Blouin, Arnaud;Baudry, Benoît",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85022015886",
      "Primary study DOI": "10.1016/j.jss.2017.06.072",
      "Title": "Patterns of developers behaviour: A 1000-hour industrial study",
      "Abstract": "Monitoring developers’ activity in the Integrated Development Environment (IDE) and, in general, in their working environment, can be useful to provide context to recommender systems, and, in perspective, to develop smarter IDEs. This paper reports results of a long (about 1000 h) observational study conducted in an industrial environment, in which we captured developers’ interaction with the IDE, with various applications available in their workstation, and related them with activities performed on source code files. Specifically, the study involved six developers working on three software systems and investigated (i) how much time developers spent on various activities and how they shift from one activity to another (ii) how developers navigate through the software architecture during their task, and (iii) how the complexity and readability of source code may trigger further actions, such as requests for help or browsing/changing other files. Results of our study suggest that: (i) not surprisingly, developers spend most or their time (∼ 61%) in development activities while the usage of online help is limited (2%) but intensive in specific development sessions; (ii) developers often execute the system under development after working on code, likely to verify the effect of applied changes on the system's behaviour; (iii) while working on files having a high complexity, developers tend to more frequently execute the system as well as to use more online help websites.",
      "Keywords": "Case study | Monitoring developers’ activities",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Astromskis, Saulius;Bavota, Gabriele;Janes, Andrea;Russo, Barbara;Di Penta, Massimiliano",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85023597142",
      "Primary study DOI": "10.1016/j.jss.2017.06.060",
      "Title": "Predictive runtime verification of timed properties",
      "Abstract": "Runtime verification (RV) techniques are used to continuously check whether the (un-trustworthy) output of a black-box system satisfies or violates a desired property. When we consider runtime verification of timed properties, physical time elapsing between actions influences the satisfiability of the property. This paper introduces predictive runtime verification of timed properties where the system is not entirely a black-box but something about its behaviour is known a priori. A priori knowledge about the behaviour of the system allows the verification monitor to foresee the satisfaction (or violation) of the monitored property. In addition to providing a conclusive verdict earlier, the verification monitor also provides additional information such as the minimum (maximum) time when the property can be violated (satisfied) in the future. The feasibility of the proposed approach is demonstrated by a prototype implementation, which is able to synthesize predictive runtime verification monitors from timed automata.",
      "Keywords": "Monitoring | Real-time systems | Runtime verification | Timed automata",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Pinisetty, Srinivas;Jéron, Thierry;Tripakis, Stavros;Falcone, Yliès;Marchand, Hervé;Preoteasa, Viorel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85024895767",
      "Primary study DOI": "10.1016/j.jss.2017.07.011",
      "Title": "Scalable model exploration for model-driven engineering",
      "Abstract": "Model-Driven Engineering (MDE) promotes the use of models to conduct all phases of software development in an automated way. However, for complex systems, these models may become large and unwieldy, and hence difficult to process and comprehend. In order to alleviate this situation, we combine model fragmentation strategies – to split models into more manageable chunks – with model abstraction and visualisation mechanisms, able to provide simpler views of the models. In this paper, we describe the underlying methods and techniques, as well as the supporting tools. The feasibility and benefits of our approach are confirmed based on evaluations in the embedded systems, and the reverse engineering domains, where large benefits in terms of visualisation time (speeds up of up to 55 × ), and reduction in memory consumption (reduction of 97%) are obtained.",
      "Keywords": "Model abstraction | Model fragmentation | Model scalability | Model visualisation | Model-driven engineering",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Jiménez-Pastor, Antonio;Garmendia, Antonio;de Lara, Juan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026228060",
      "Primary study DOI": "10.1016/j.jss.2017.07.006",
      "Title": "Adaptive Ensemble Undersampling-Boost: A novel learning framework for imbalanced data",
      "Abstract": "As one of the most challenging and attractive problems in the pattern recognition and machine intelligence field, imbalanced classification has received a large amount of research attention for many years. In binary classification tasks, one class usually tends to be underrepresented when it consists of far fewer patterns than the other class, which results in undesirable classification results, especially for the minority class. Several techniques, including resampling, boosting and cost-sensitive methods have been proposed to alleviate this problem. Recently, some ensemble methods that focus on combining individual techniques to obtain better performance have been observed to present better classification performance on the minority class. In this paper, we propose a novel ensemble framework called Adaptive Ensemble Undersampling-Boost for imbalanced learning. Our proposal combines the Ensemble of Undersampling (EUS) technique, Real Adaboost, cost-sensitive weight modification, and adaptive boundary decision strategy to build a hybrid algorithm. The superiority of our method over other state-of-the-art ensemble methods is demonstrated by experiments on 18 real world data sets with various data distributions and different imbalance ratios. Given the experimental results and further analysis, our proposal is proven to be a promising alternative that can be applied to various imbalanced classification domains.",
      "Keywords": "Adaptive decision boundary | Classification | Ensemble Undersampling | Imbalanced data sets | Real Adaboost | Voting algorithm",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Lu, Wei;Li, Zhe;Chu, Jinghui",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021411634",
      "Primary study DOI": "10.1016/j.jss.2017.06.021",
      "Title": "A framework for gamification in software engineering",
      "Abstract": "Gamification seeks for improvement of the user's engagement, motivation, and performance when carrying out a certain task; it does so by incorporating game mechanics and elements, thus making that task more attractive. The application of gamification in Software Engineering can be promising; software projects can be organized as a set of challenges which can be ordered and that need to be fulfilled, for which some skills, and mainly much collective effort, are required. The objective of this paper is to propose a complete framework for the introduction of gamification in software engineering environments. This framework is composed of an ontology, a methodology guiding the process, and a support gamification engine. We carried out a case study in which the proposed framework was applied in a real company. In this project the company used the framework to gamify the areas of project management, requirements management, and testing. As a result, the methodology has clearly enabled the company to introduce gamification in its work environment, achieving a quality solution with appropriate design and development effort. The support tool allowed the company to gamify its current tools very easily.",
      "Keywords": "Gamification | Methodology | Ontology | Software engineering",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "García, Félix;Pedreira, Oscar;Piattini, Mario;Cerdeira-Pena, Ana;Penabad, Miguel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85022032339",
      "Primary study DOI": "10.1016/j.jss.2017.06.095",
      "Title": "Evolution of the R software ecosystem: Metrics, relationships, and their impact on qualities",
      "Abstract": "Software ecosystems are an important new concept for collaborative software development, and empirical studies on their development are important towards understanding the underlying dynamics and modelling their behaviour. We conducted an explorative analysis of the R ecosystem as an exemplar on high-level, ecosystem-wide assessment. Based principally on the documentation metadata of the R packages, we generated a variety of metrics that allow the quantification of the R ecosystem. We also categorized the ecosystem participants, both in the software marketplace and in the developer community, by characteristics that measure their activity and impact. By viewing our metrics across the ecosystem's lifecycle for the various participant categories, we discovered interrelationships between them and determined the contribution of each category to the ecosystem as a whole.",
      "Keywords": "Empirical study | Evolution | Quantitative analysis | R | Software ecosystems",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Plakidas, Konstantinos;Schall, Daniel;Zdun, Uwe",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85015765249",
      "Primary study DOI": "10.1016/j.jss.2017.03.010",
      "Title": "A method to localize faults in concurrent C programs",
      "Abstract": "We describe a new approach to localize faults in concurrent programs, which is based on bounded model checking and sequentialization techniques. The main novelty is the idea of reproducing a faulty behavior, in a sequential version of a concurrent program. In order to pinpoint faulty lines, we analyze counterexamples generated by a model checker, to the new instrumented sequential program, and search for a diagnostic value, which corresponds to actual lines in a program. This approach is useful to improve debugging processes for concurrent programs, since it tells which line should be corrected and what values lead to a successful execution. We implemented this approach as a code-to-code transformation from concurrent into non-deterministic sequential programs, which are used as inputs to existing verification tools. Experimental results show that our approach is effective and capable of identifying faults in our benchmark set, which was extracted from the SV-COMP 2016 suite.",
      "Keywords": "Bounded model checking | Concurrent software | Fault localization | Non-determinism | Sequentialization",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Erickson, Erickson H.da;Cordeiro, Lucas C.;Eddie, Eddie B.de",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85025602647",
      "Primary study DOI": "10.1016/j.jss.2017.05.017",
      "Title": "Taxonomy of workflow partitioning problems and methods in distributed environments",
      "Abstract": "A workflow model is the computerized representation of a business or scientific process. It defines the starting and ending conditions of the process, the activities in the process, control flow and data flow among these activities, etc. A partitioning method creates workflow fragments that group some of the workflow model elements (activities, control flows, data flows). Workflow partitioning forms the foundation of decentralised workflow execution and increases scalability, and reuse of partitions. In the literature, different methods have been presented for workflow partitioning and offer a variety of execution approaches; however, there is no existing comprehensive survey and taxonomy of workflow partitioning methods. This article presents an overview of taxonomies characterizing the key concepts of the workflow life cycle and workflow partitioning methods through a comprehensive survey of business and scientific domains in decentralised environments. This in-depth analysis of taxonomies can provide researchers, designers and applications developers with clear guideline to compare current workflow partitioning methods to choose, reuse and compose more vigorous approaches. The article further presents research discussions and future challenges in this area.",
      "Keywords": "Decentralised environments | Workflow models | Workflow partitioning method | Workflow taxonomy",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Khorsand, Reihaneh;Safi-Esfahani, Faramarz;Nematbakhsh, Naser;Mohsenzade, Mehran",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85022069038",
      "Primary study DOI": "10.1016/j.jss.2017.06.070",
      "Title": "A feature matching and transfer approach for cross-company defect prediction",
      "Abstract": "Software defect prediction has drawn much attention of researchers in software engineering. Traditional defect prediction methods aim to build the prediction model based on historical data. For a new project or a project with limited historical data, we cannot build a good prediction model. Therefore, researchers have proposed the cross-project defect prediction (CPDP) and cross-company defect prediction (CCDP) methods to share the historical data among different projects. However, the features of cross-company datasets are often heterogeneous, which may affect the feasibility of CCDP. To address the heterogeneous features of CCDP, this paper presents a feature matching and transfer (FMT) approach. First, we conduct feature selection for the source project and get the distribution curves of selected features. Similarly, we also get the distribution curves of all features in the target project. Second, according to the ‘distance’ of different distribution curves, we design a feature matching algorithm to convert the heterogeneous features into the matched features. Finally, we can achieve feature transfer from the source project to the target project. All experiments are conducted on 16 datasets from NASA and PROMISE, and the results show that FMT is effective for CCDP.",
      "Keywords": "Feature matching | Feature transfer | Heterogeneous features | Software defect prediction",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Yu, Qiao;Jiang, Shujuan;Zhang, Yanmei",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026478388",
      "Primary study DOI": "10.1016/j.jss.2017.07.031",
      "Title": "Ontology-based recommender system for COTS components",
      "Abstract": "Commercial Off-The-Shelf (COTS) components are coarse-grained software components that satisfy high-level requirements by integrating several services and offering several interfaces. They are usually used to build larger systems. The paper proposes an ontology-based recommender system for COTS components, that contributes to COTS-based development by improving COTS components identification. It combines into a single framework information retrieval technologies and knowledge about COTS components and users in order to provide the most relevant COTS components meeting users needs. The recommender system is based on (1) an ontology of COTS components, named ONTOCOTS, that describes COTS components and unifies their heterogeneous descriptions available on the Web, and (2) a user model that represents user preferences and interest domains. The proposed recommender system is broken down on two main processes. The first one is responsible for extracting information about COTS components from COTS repositories and representing it as ONTOCOTS instances. The second one is the recommendation process during which the user query is expanded using the linguistic ontology WordNet, and is used along with the user profile and the domain ontology ODP (Open Directory Project) to generate a formal query. Results list is ranked according to the satisfaction degree of user requirements and preferences. Experimentations show an amelioration in recommendations relevance by placing the relevant COTS components at the top of the recommendation list.",
      "Keywords": "COTS component | Identification | Information extraction | Ontology | Recommender system | User model",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Yanes, Nacim;Ben Sassi, Sihem;Hajjami Ben Ghezala, Henda",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021624190",
      "Primary study DOI": "10.1016/j.jss.2017.06.024",
      "Title": "Modeling and automatic code generation for wireless sensor network applications using model-driven or business process approaches: A systematic mapping study",
      "Abstract": "This systematic mapping study investigates the modeling and automatic code generation initiatives for wireless sensor network applications based on the IEEE 802.15.4 standard, trying to understand the reasons, characteristics and methods used in the approaches available in the scientific literature, identifying research gaps and potential approaches that can be better exploited, indicating new possibilities of research. The focus is on studies that follow the Model-Driven or Business Process approaches.",
      "Keywords": "Business process | Model-driven development | Systematic mapping study | Wireless sensor networks",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Teixeira, Sergio;Agrizzi, Bruno Alves;Filho, José Gonçalves Pereira;Rossetto, Silvana;Baldam, Roquemar de Lima",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026757561",
      "Primary study DOI": "10.1016/j.jss.2017.07.010",
      "Title": "Refining a model for sustained usage of agile methodologies",
      "Abstract": "This paper refines a model of Sustained Agile Usage to present a comprehensive understanding of the key factors that are pertinent to the sustained usage of agile methodologies. It describes our qualitative study which involves: (i) a focus group with twenty-nine software industry agile practitioners, and (ii) semi-structured interviews with twenty agile practitioners from five different organizational backgrounds. Data from both methods is used to develop the refined model of Sustained Agile Usage. The refined Sustained Agile Usage Model includes the following three categories: Agile Team Factors, Technological Factors, and Organizational Factors. These revisions are discussed in this research. Finally, we see implications for research: the study offers a useful complement to the few studies that have examined the long-term acceptance of agile methods. The refined model can be used as a reference model to guide future studies to understand sustained usage in different agile domains (e.g. Kanban). Additionally, implications for practice include valuable insights that can help agile teams and others (e.g. top management) to better understand and benchmark how agile methods can be effectively sustained in organizations.",
      "Keywords": "Agile assimilation | Agile effectiveness | Multi-method research | Post-adoptive agile method use | Sustained agile usage",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Senapathi, Mali;Drury-Grogan, Meghann L.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85024501531",
      "Primary study DOI": "10.1016/j.jss.2017.05.125",
      "Title": "Quality of service approaches in IoT: A systematic mapping",
      "Abstract": "In an Internet of Things (IoT) environment, the existence of a huge number of heterogeneous devices, which are potentially resource-constrained and/or mobile has led to quality of service (QoS) concerns. Quality approaches have been proposed at various layers of the IoT architecture and take into consideration a number of different QoS factors. This paper evaluates the current state of the art of proposed QoS approaches in the IoT, specifically: (1) What layers of the IoT architecture have had the most research on QoS? (2) What quality factors do the quality approaches take into account when measuring performance? (3) What types of research have been conducted in this area? We have conducted a systematic mapping using a number of automated searches from the most relevant academic databases to address these questions. This mapping has identified a number of state of the art approaches which provides a good reference for researchers. The paper also identifies a number of gaps in the research literature at specific layers of the IoT architecture. It identifies which quality factors, research and contribution facets have been underutilised in the state of the art.",
      "Keywords": "Internet of things (IoT) | Monitoring | Quality model | Quality of service (QoS) | Systematic mapping",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "White, Gary;Nallur, Vivek;Clarke, Siobhán",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85023616412",
      "Primary study DOI": "10.1016/j.jss.2017.06.071",
      "Title": "A statistical analysis approach to predict user's changing requirements for software service evolution",
      "Abstract": "Evolution is inevitable for almost all software, and may be driven by users’ continuous requests for changes and improvement, the enablement of technology development, among other factors. The evolution of software services can be seen as the evolution of system-user interactions. The capability to accurately and efficiently observe users’ volatile requirements is critical to making timely system improvements to adapt to rapidly changing environments. In this paper, we propose a methodology that employs Conditional Random Fields (CRF) as a means to provide quantitative exploration of system-user interactions that often lead to the discovery of users’ potential needs and requirements. By analyzing users’ run-time behavioral patterns, domain experts can make prompt predictions on how users’ intentions shift, and timely propose system improvements or remedies to help address emerging needs. Our ultimate research goal is to speed up software service evolution to a great extent with automated tools, knowing that the challenge can be undoubtedly steep. The evolution of an online research library service is used to illustrate and evaluate the proposed approach in detail.",
      "Keywords": "Conditional Random Fields | Goal inference | Human intention detection | Requirements | Service | Software evolution",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Xie, Haihua;Yang, Jingwei;Chang, Carl K.;Liu, Lin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85024833152",
      "Primary study DOI": "10.1016/j.jss.2017.07.012",
      "Title": "Cross-validation based K nearest neighbor imputation for software quality datasets: An empirical study",
      "Abstract": "Being able to predict software quality is essential, but also it pose significant challenges in software engineering. Historical software project datasets are often being utilized together with various machine learning algorithms for fault-proneness classification. Unfortunately, the missing values in datasets have negative impacts on the estimation accuracy and therefore, could lead to inconsistent results. As a method handling missing data, K nearest neighbor (KNN) imputation gradually gains acceptance in empirical studies by its exemplary performance and simplicity. To date, researchers still call for optimized parameter setting for KNN imputation to further improve its performance. In the work, we develop a novel incomplete-instance based KNN imputation technique, which utilizes a cross-validation scheme to optimize the parameters for each missing value. An experimental assessment is conducted on eight quality datasets under various missingness scenarios. The study also compared the proposed imputation approach with mean imputation and other three KNN imputation approaches. The results show that our proposed approach is superior to others in general. The relatively optimal fixed parameter settings for KNN imputation for software quality data is also determined. It is observed that the classification accuracy is improved or at least maintained by using our approach for missing data imputation.",
      "Keywords": "Cross-validation | Empirical software engineering estimation | Imputation | KNN | Missing data",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Huang, Jianglin;Keung, Jacky Wai;Sarro, Federica;Li, Yan Fu;Yu, Y. T.;Chan, W. K.;Sun, Hongyi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021251249",
      "Primary study DOI": "10.1016/j.jss.2017.06.067",
      "Title": "Towards a standardized cloud service description based on USDL",
      "Abstract": "In recent years, cloud computing paradigm has attracted a lot of attention from both industry and academia. However, each cloud provider uses its own techniques (languages, standards, ontologies, or models, etc.) to describe cloud services. The diversity of these techniques leads to the vendor lock-in problem, and thus, the lack of a cloud service description standardization. In addition, existing service descriptions cover only particular aspects and neglect others. For example, WSDL covers only technical aspect and does not cover business and semantic ones. Our objective is to define a standardized cloud service description that covers technical, operational, business, and semantic aspects. In this paper, we introduce different approaches that have dealt with cloud service description, and thus, we adopt USDL language as an appropriate technique to describe cloud services thanks to its expressivity by covering three perspectives (technical, operational, and business). But, USDL is still limited because it cannot cover semantic aspect and it is not intended for cloud computing domain. After that, we highlight USDL limitations that can appear in cloud computing domain and that should be taken into consideration in our research work. This paper will focus on establishing a WSMO-based ontology to define semantically cloud services. This new cloud service description is based on USDL and we will enhance it by taking into consideration some USDL limitations. Finally, we test our proposed cloud service description model on a case study to prove its applicability.",
      "Keywords": "Cloud computing | Cloud service | Generic cloud service description | Semantic service description | USDL | WSMO Ontology",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Ghazouani, Souad;Slimani, Yahya",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85022098813",
      "Primary study DOI": "10.1016/j.jss.2017.05.035",
      "Title": "Resource management in cloud platform as a service systems: Analysis and opportunities",
      "Abstract": "Platform-as-a-Service (PaaS) clouds offer services to automate the deployment and management of applications, relieving application owners of the complexity of managing the underlying infrastructure resources. However, application owners have an increasingly larger diversity and volume of workloads, which they want to execute at minimum cost while maintaining desired performance guarantees. In this paper we investigate how existing PaaS systems cope with this challenge. In particular, we first present a taxonomy of commonly-encountered design decisions regarding how PaaS systems manage underlying resources. We then use this taxonomy to analyze an extensive set of PaaS systems targeting different application domains. Based on this analysis, we identify several future research opportunities in the PaaS design space, which will enable PaaS owners to reduce hosting costs while coping with the workload variety.",
      "Keywords": "Cloud computing | Platform-as-a-service | Resource management",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Costache, Stefania;Dib, Djawida;Parlavantzas, Nikos;Morin, Christine",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85024097362",
      "Primary study DOI": "10.1016/j.jss.2017.07.005",
      "Title": "Efficient query processing on large spatial databases: A performance study",
      "Abstract": "Processing of spatial queries has been studied extensively in the literature. In most cases, it is accomplished by indexing spatial data using spatial access methods. Spatial indexes, such as those based on the Quadtree, are important in spatial databases for efficient execution of queries involving spatial constraints and objects. In this paper, we study a recent balanced disk-based index structure for point data, called xBR+-tree, that belongs to the Quadtree family and hierarchically decomposes space in a regular manner. For the most common spatial queries, like Point Location, Window, Distance Range, Nearest Neighbor and Distance-based Join, the R-tree family is a very popular choice of spatial index, due to its excellent query performance. For this reason, we compare the performance of the xBR+-tree with respect to the R*-tree and the R+-tree for tree building and processing the most studied spatial queries. To perform this comparison, we utilize existing algorithms and present new ones. We demonstrate through extensive experimental performance results (I/O efficiency and execution time), based on medium and large real and synthetic datasets, that the xBR+-tree is a big winner in execution time in all cases and a winner in I/O in most cases.",
      "Keywords": "Performance evaluation | Quadtrees | Query processing | R-trees | Spatial access methods | Spatial databases | xBR-trees",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Roumelis, George;Vassilakopoulos, Michael;Corral, Antonio;Manolopoulos, Yannis",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021675237",
      "Primary study DOI": "10.1016/j.jss.2017.05.097",
      "Title": "Environmental factors analysis and comparison affecting software reliability in development of multi-release software",
      "Abstract": "As the application of the principles of agile and lean software development, software multiple release becomes very common in the modern society. Short iteration and short release cycle have driven the significant changes of the development process of multi-release software product, compared with single release software product. Thus, it is time to conduct a new study investigating the impact level of environmental factors on affecting software reliability in the development of multi-release software to provide a sound and concise guidance to software practitioners and researchers. Statistical learning methods, like principle component analysis, stepwise backward elimination, lasso regression, multiple linear regression, and Tukey method, are applied in this study. Comparisons regarding significant environmental factors during the whole development process, principle components, significant environmental factors in each development phase and significance level of each development phase between the development of single release software and multi-release software are also discussed.",
      "Keywords": "Environmental factors | Lasso regression | Multi-release software | Multiple linear regression | Principle component analysis | Stepwise backward elimination",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Zhu, Mengmeng;Pham, Hoang",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021632215",
      "Primary study DOI": "10.1016/j.jss.2017.06.059",
      "Title": "A PSO-GA approach targeting fault-prone software modules",
      "Abstract": "We present an algorithm to classify software modules as fault-prone or not using object-oriented metrics. Our algorithm is a combination of particle swarm intelligence and genetic algorithms. We empirically validate it on eight different data sets. We also compare it to well known classification techniques. Results show that our algorithm has several advantages over other techniques.",
      "Keywords": "Classification | Fault-proneness | Genetic algorithms | Heuristics | Metrics | Particle swarm optimization | Swarm intelligence",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Moussa, Rebecca;Azar, Danielle",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85017441420",
      "Primary study DOI": "10.1016/j.jss.2017.03.011",
      "Title": "Implementation relations and probabilistic schedulers in the distributed test architecture",
      "Abstract": "We present a complete framework to formally test systems with distributed ports where some choices are probabilistically quantified while other choices are non-deterministic. We define different implementation relations, that is, relations that state what it means for a system to be a valid implementation of a specification. We also study how these relate. In order to define these implementation relations we use probabilistic schedulers, a more powerful version, including probabilistic choices, of a notion of scheduler introduced in our previous work. Probabilistic schedulers, when applied to either a specification or an implementation, resolve all the possible non-determinism, so that we can compare purely probabilistic systems.",
      "Keywords": "Distributed systems | Model-based testing | Probabilistic systems",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Hierons, Robert M.;Núñez, Manuel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019656115",
      "Primary study DOI": "10.1016/j.jss.2017.05.050",
      "Title": "A training process for improving the quality of software projects developed by a practitioner",
      "Abstract": "Background The quality of a software product depends on the quality of the software process followed in developing the product. Therefore, many higher education institutions (HEI) and software organizations have implemented software process improvement (SPI) training courses to improve the software quality. Objective Because the duration of a course is a concern for HEI and software organizations, we investigate whether the quality of software projects will be improved by reorganizing the activities of the ten assignments of the original personal software process (PSP) course into a modified PSP having fewer assignments (i.e., seven assignments). Method The assignments were developed by following a modified PSP with fewer assignments but including the phases, forms, standards, and logs suggested in the original PSP. The measurement of the quality of the software assignments was based on defect density. Results When the activities in the original PSP were reordered into fewer assignments, as practitioners progress through the PSP training, the defect density improved with statistical significance. Conclusions Our modified PSP could be applied in academy and industrial environments which are concerned in the sense of reducing the PSP training time.",
      "Keywords": "Personal software process | Software engineering education and training | Software process improvement | Software quality improvement",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "López-Martín, Cuauhtémoc;Nassif, Ali Bou;Abran, Alain",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84981736041",
      "Primary study DOI": "10.1016/j.jss.2016.07.033",
      "Title": "A Formal Approach to implement java exceptions in cooperative systems",
      "Abstract": "The increasing number of systems that work on the top of cooperating elements have required new techniques to control cooperation on both normal and abnormal behaviors of systems. The controllability of the normal behaviors has received more attention because they are concerned with the users expectations, while for the abnormal behaviors it is left to designers and programmers. However, for cooperative systems, the abnormal behaviors, mostly represented by exceptions at programming level, become an important issue in software development because they can affect the overall system behavior. If an exception is raised and not handled accordingly, the system may collapse. To avoid such situation, certain concepts and models have been proposed to coordinate propagation and recovering of exceptional behaviors, including the Coordinated Atomic Actions (CAA). Regardless of the effort in creating these conceptual models, an actual implementation of them in real systems is not very straightforward. This article provides a reliable framework for the implementation of Java exceptions propagation and recovery using CAA concepts. To do this, a Java framework (based on a formal specification) is presented, together with a set of properties to be preserved and proved with the Java Pathfinder (JPF) model checker. In practice, to develop new systems based on the given coordination concepts, designers/programmers can instantiate the framework to implement the exceptional behavior and then verify the correctness of the resulting code using JPF. Therefore, by using the framework, designers/programmers can reuse the provided CAA implementation and instantiate fault-tolerant Java systems.",
      "Keywords": "concurrent exception handling | Coordinated atomic actions model | java framework | program verification",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Hanazumi, Simone;de Melo, Ana C.V.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021182926",
      "Primary study DOI": "10.1016/j.jss.2017.06.003",
      "Title": "Examining decision characteristics & challenges for agile software development",
      "Abstract": "Although agile software development is often associated with improved decision making, existing studies tend to focus on narrow aspects of decision making in such environments. There is a lack of clarity on how teams make and evaluate a myriad of decisions from software feature inception to product delivery and refinement. Indeed there is relatively little known about a) the decision characteristics related to agile values, and b) the challenges they present for decision making on agile teams. We present an in-depth exploratory case study based on a pluralistic approach comprising semi-structured interviews, focus groups, team meeting observations, and document analysis. The study identifies failings of decision making in an agile setting. Explicitly considering the decision process, information intelligence used in decision making, and decision quality, the key contribution of this paper is the development of an over-arching framework of agile decision making, which identifies particular decision characteristics across 4 key agile values and the related challenges for agile team decision making. It provides a framework for researchers and practitioners to evaluate the decision challenges of an agile software development team and to improve decision quality.",
      "Keywords": "Agile decision making | Agile software development | Decision characteristics | Decision intelligence | Decision process | Decision quality",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Drury-Grogan, Meghann L.;Conboy, Kieran;Acton, Tom",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84979497676",
      "Primary study DOI": "10.1016/j.jss.2016.07.010",
      "Title": "An exploratory study on the usage of common interface elements in android applications",
      "Abstract": "The number of mobile applications has increased drastically in the past few years. A recent study has shown that reusing source code is a common practice for Android application development. However, reuse in mobile applications is not necessarily limited to the source code (i.e., program logic). User interface (UI) design plays a vital role in constructing the user-perceived quality of a mobile application. The user-perceived quality reflects the users’ opinions of a product. For mobile applications, it can be quantified by the number of downloads and raters. In this study, we extract commonly used UI elements, denoted as Common Element Sets (CESs), from user interfaces of applications. Moreover, we highlight the characteristics of CESs that can result in a high user-perceived quality by proposing various metrics. Through an empirical study on 1292 mobile applications, we observe that (i) CESs of mobile applications widely occur among and across different categories; (ii) certain characteristics of CESs can provide a high user-perceived quality; and (iii) through a manual analysis, we recommend UI templates that are extracted and summarized from CESs for developers. Developers and quality assurance personnel can use our guidelines to improve the quality of mobile applications.",
      "Keywords": "Common UI elements | Mobile applications | User-perceived quality",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Taba, Seyyed Ehsan Salamati;Keivanloo, Iman;Zou, Ying;Wang, Shaohua",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84997796738",
      "Primary study DOI": "10.1016/j.jss.2016.07.034",
      "Title": "A method to generate reusable safety case argument-fragments from compositional safety analysis",
      "Abstract": "Safety-critical systems usually need to be accompanied by an explained and well-founded body of evidence to show that the system is acceptably safe. While reuse within such systems covers mainly code, reusing accompanying safety artefacts is limited due to a wide range of context dependencies that need to be satisfied for safety evidence to be valid in a different context. Currently, the most commonly used approaches that facilitate reuse lack support for systematic reuse of safety artefacts. To facilitate systematic reuse of safety artefacts we provide a method to generate reusable safety case argument-fragments that include supporting evidence related to compositional safety analysis. The generation is performed from safety contracts that capture safety-relevant behaviour of components in assumption/guarantee pairs backed up by the supporting evidence. We evaluate the feasibility of our approach in a real-world case study where a safety related component developed in isolation is reused within a wheel-loader.",
      "Keywords": "Component-based architectures | Compositional safety analysis | Contract-based architectures | Modular argumentation | Safety argumentation reuse",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Šljivo, Irfan;Gallina, Barbara;Carlson, Jan;Hansson, Hans;Puri, Stefano",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85020481032",
      "Primary study DOI": "10.1016/j.jss.2016.09.061",
      "Title": "Energy-efficient heterogeneous resource management for wireless monitoring systems",
      "Abstract": "Various energy-saving designs have been proposed for reducing the power consumption of processors through dynamic voltage and frequency scaling (DVFS). When dynamic random access memory (DRAM) or peripheral power consumption is high, dynamic power management (DPM) can be adopted to dynamically activate or deactivate devices or to switch them into energy-saving states during idle periods. This paper proposes a heterogeneous resource management mechanism to manage device scheduling for multiple tasks and task scheduling in a processor. A wireless network monitoring system was analyzed as a case study, wherein a resource sharing mechanism was developed for managing the scheduling of multiple wireless adapters, and the concept of instantaneous utilization was leveraged to enable chain-based task scheduling. This paper explores DVFS and DPM energy saving techniques for peripherals and a processor by considering both the required device time and processor time for each task without violating performance requirements under constraints of buffer size. The proposed algorithms were then implemented on a wireless network monitoring system and real traces were collected from a laboratory and downloaded from the UMass Trace Repository for use as inputs. A series of experiments was conducted to evaluate the quality of our algorithms for energy saving within the constraints of system performance requirements and hardware resources.",
      "Keywords": "Dynamic power management | Dynamic voltage and frequency scaling | Energy-saving designs | Heterogeneous resource management | Resource scheduling | Wireless monitoring systems",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Chang, Che Wei;Liu, Chun Yi;Yang, Chuan Yue",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85003510868",
      "Primary study DOI": "10.1016/j.jss.2016.06.064",
      "Title": "Automating the license compatibility process in open source software with SPDX",
      "Abstract": "Free and Open Source Software (FOSS) promotes software reuse and distribution at different levels for both creator and users, but at the same time imposes some challenges in terms of FOSS licenses that can be selected and combined. The main problem linked to this selection is the presence of a large set of licenses that define different rights and obligations in software use. The problem becomes more evident in cases of complex combinations of software that carries different – often conflicting – licenses. In this paper we are presenting our work on automating license compatibility by proposing a process that examines the structure of Software Package Data Exchange (SPDX) for license compatibility issues assisting in their correct use and combination. We are offering the possibility to detect license violations in existing software projects and make suggestions on appropriate combinations of different software packages. We are also elaborating on the complexity and ambiguity of licensing detection in software products through representative case studies. Our work constitutes a useful process towards automating the analysis of software systems in terms of license use and compatibilities.",
      "Keywords": "License compatibility | License violations | Open Source Software | Software Package Data Exchange",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Kapitsaki, Georgia M.;Kramer, Frederik;Tselikas, Nikolaos D.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85020945450",
      "Primary study DOI": "10.1016/j.jss.2017.06.020",
      "Title": "MeshFS: A distributed file system for cloud-based wireless mesh network",
      "Abstract": "Wireless mesh networks have attracted considerable interest in recent years in both the academic and industrial communities. As wireless mesh routers can be interconnected through wireless links, wireless mesh networks provide greater flexibility and better cost-effectiveness. In particular, due to their ease of installation and maintenance, they can be used in different environments, especially where cable installation is difficult. Apart from providing routing service, wireless mesh networks can also be employed to provide other value-added services. Inspired by cloud computing and other distributed file systems, this paper presents the design and implementation of MeshFS, a distributed file system specifically for wireless mesh networks. A key technical challenge is to develop a lightweight software system that can be implemented over memory-limited wireless mesh network environments. With the aim of providing a lightweight distributed file system, allowing limited resources to be utilized more effectively, MeshFS integrates scattered storage resources from wireless mesh routers to provide a mountable file system interface to Unix/Linux file system with fault-tolerant capabilities and cloud computing-like storage functions.",
      "Keywords": "Cloud computing | Data management | Distributed file system | Wireless mesh network",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Yang, Shengtao;Chan, Henry C.B.;Lam, Patrick P.;Chong, Peter H.J.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019753039",
      "Primary study DOI": "10.1016/j.jss.2017.05.081",
      "Title": "Embedding statecharts into Teleo-Reactive programs to model interactions between agents",
      "Abstract": "Context The Teleo-Reactive (TR) approach offers many possibilities for goal-oriented modeling of reactive systems, but it also has drawbacks when the number of interactions among agents is high, leading to barely legible specifications and losing the original benefits of the approach. Objective This work combines the TR paradigm with statecharts and provides advantages for modeling reactive systems and removing the shortcomings detected. Method A basic example is adopted to reveal the problem that appears when agents are modeled only with the TR approach and have frequent interactions with others. This paper proposes an extension to the TR approach that integrates the modeling using statecharts. A transformation procedure from statecharts to TR programs makes it possible to continue using the infrastructure of existing execution platforms such as TeleoR. The approach has been validated for a particular domain by considering a more complex case study in which traditionally there have been no results on the application of the TR paradigm. A survey was carried out on students to verify the benefits of the approach. Results A method to consider statecharts when modeling TR programs. Conclusions Statecharts can facilitate the adoption of the TR approach.",
      "Keywords": "Modeling reactive-systems | Statecharts | Teleo-Reactive",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Sánchez, Pedro;Álvarez, Bárbara;Martínez, Ramón;Iborra, Andrés",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021148728",
      "Primary study DOI": "10.1016/j.jss.2017.05.098",
      "Title": "A framework for automatically ensuring the conformance of agent designs",
      "Abstract": "Multi-agent systems are increasingly being used in complex applications due to features such as autonomy, pro-activity, flexibility, robustness and social ability. These very features also make verifying multi-agent systems a challenging task. In this article, we propose a mechanism, including automated tool support, for early phase defect detection by comparing the plan structures of a Belief-Desire-Intention agent design against the requirements models and interaction protocols. The basic intuition of our approach is to extract sets of possible behaviour runs from the agents’ behaviour models and to verify whether these runs conform to the specifications of the system-to-be or not. This approach is applicable at design time, not requiring source code, thus enabling detection and removal of some defects at an early phase of the software development lifecycle. We followed an experimental approach for evaluating the proposed verification framework. Our evaluation shows that even simple system's specifications developed by relatively experienced developers are prone to defects, and our approach is successful in uncovering most of these defects. In addition, we conducted a scalability analysis on the approach, and the outcomes show that our approach can scale when designs grow in size.",
      "Keywords": "Agent-oriented software engineering | Multi-agent systems | Verification",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Abushark, Yoosef;Thangarajah, John;Harland, James;Miller, Tim",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019744077",
      "Primary study DOI": "10.1016/j.jss.2017.05.051",
      "Title": "Modularity and architecture of PLC-based software for automated production Systems: An analysis in industrial companies",
      "Abstract": "Adaptive and flexible production systems require modular and reusable software especially considering their long-term life cycle of up to 50 years. SWMAT4aPS, an approach to measure Software Maturity for automated Production Systems is introduced. The approach identifies weaknesses and strengths of various companies’ solutions for modularity of software in the design of automated Production Systems (aPS). At first, a self-assessed questionnaire is used to evaluate a large number of companies concerning their software maturity. Secondly, we analyze PLC code, architectural levels, workflows and abilities to configure code automatically out of engineering information in four selected companies. In this paper, the questionnaire results from 16 German world-leading companies in machine and plant manufacturing and four case studies validating the results from the detailed analyses are introduced to prove the applicability of the approach and give a survey of the state of the art in industry.",
      "Keywords": "Automated production systems | Control software | Factory automation | Maturity | Modularity | Programmable logic controller",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Vogel-Heuser, Birgit;Fischer, Juliane;Feldmann, Stefan;Ulewicz, Sebastian;Rösch, Susanne",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85003977009",
      "Primary study DOI": "10.1016/j.jss.2016.06.062",
      "Title": "Construction and utilization of problem-solving knowledge in open source software environments",
      "Abstract": "Open Source Software (OSS) has become an important environment where developers can share reusable software assets in a collaborative manner. Although developers can find useful software assets to reuse in the OSS environment, they may face difficulties in finding solutions to problems that occur while integrating the assets with their own software. In OSS, sharing the experiences of solving similar problems among developers usually plays an important role in reducing problem-solving efforts. We analyzed how developers interact with each other to solve problems in OSS, and found that there is a common pattern of exchanging information about symptoms and causes of a problem. In particular, we found that many problems involve multiple symptoms and causes and it is critical to identify those symptoms and causes early to solve the problems more efficiently. We developed a Bayesian network based approach to semi-automatically construct a knowledge base for dealing with problems, and to recommend potential causes of a problem based on multiple symptoms reported in OSS. Our experiments showed that the approach is effective to recommend the core causes of a problem, and contributes to solving the problem in an efficient manner.",
      "Keywords": "Bayesian network | Knowledge-based software reuse | Open source software | Software reuse",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Koo, Hyung Min;Ko, In Young",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019716385",
      "Primary study DOI": "10.1016/j.jss.2017.05.049",
      "Title": "Chord: Checkpoint-based scheduling using hybrid waiting list in shared clusters",
      "Abstract": "Cloud platforms supported by shared clusters are getting increasingly effective. Numerous tasks are submitted into clusters by a variety of users. Cloud platforms usually assign tasks with different priorities based on different Quality of Services (QoS) chosen by users. High-priority tasks can be executed primarily. As a consequence, preemption frequently occurs in almost all the commercial cloud platforms, such as Google and Amazon cluster. Although kill-based preemption is adopted as an optimal solution for high-priority tasks, it severely harms low-priority tasks. Especially, during the peak time, some low-priority tasks may be preempted and restarted repeatedly resulting in consuming much more precious resources including CPU cores, RAM and hard drives. Thanks to the checkpoint technology that provides an efficient solution to addressing the preemption issue. However, using checkpoint blindly will cause more resource waste. To address this issue, in this paper, we propose a concept of hybrid waiting list that holds all unfinished tasks and makes the resumption of tasks regularly. We leverage the checkpoint technology and design a novel approach based on the hybrid waiting list named Chord (Checkpoint with hybrid scheduling method) which effectively improves the performance of shared clusters. Specifically, by checking the occupancy of resources periodically and making checkpoints for certain tasks, our approach can effectively reduce unnecessary checkpoints and improve the performance of the whole cluster, especially for low-priority tasks. Extensive simulation experiments injecting tasks from the Google cloud trace logs were conducted to validate the superiority of our approach. Compared with the ordinary priority scheduling methods adopt by several commercial clouds, the improvement of response time gained by our Chord can reach 18.94%.",
      "Keywords": "Checkpoint | Occupancy of resources | Priority | Response time | Shared cluster | Utility",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Shao, Yiyang;Bao, Weidong;Zhu, Xiaomin;Xiao, Wenhua;Wang, Jian",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021138321",
      "Primary study DOI": "10.1016/j.jss.2017.06.019",
      "Title": "No silver brick: Opportunities and limitations of teaching Scrum with Lego workshops",
      "Abstract": "Education in Software Engineering has to both teach technical content such as databases and programming but also organisational skills such as team work and project management. While the former can be evaluated from a product perspective, the latter are usually embedded in a Software Engineering process and need to be assessed and adapted throughout their implementation. The in-action property of processes puts a strain on teachers since we cannot be present throughout the students’ work. To address this challenge we have adopted workshops to teach Scrum by building a Lego city in short sprints to focus on the methodological content. In this way we can be present throughout the process and coach the students. We have applied the exercise in six different courses, across five different educational programmes and observed more than 450 participating students. In this paper, we report on our experiences with this approach, based on quantitative data from the students and qualitative data from both students and teachers. We give recommendations for learning opportunities and best practices and discuss the limitations of these workshops in a classroom setting. We also report on how the students transferred their methodological knowledge to software development projects in an academic setting.",
      "Keywords": "Agile software engineering | Scrum | Software engineering education",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Steghöfer, Jan Philipp;Burden, Håkan;Alahyari, Hiva;Haneberg, Dominik",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85020444771",
      "Primary study DOI": "10.1016/j.jss.2017.05.080",
      "Title": "Dataclay: A distributed data store for effective inter-player data sharing",
      "Abstract": "In the Big Data era, both the academic community and industry agree that a crucial point to obtain the maximum benefits from the explosive data growth is integrating information from different sources, and also combining methodologies to analyze and process it. For this reason, sharing data so that third parties can build new applications or services based on it is nowadays a trend. Although most data sharing initiatives are based on public data, the ability to reuse data generated by private companies is starting to gain importance as some of them (such as Google, Twitter, BBC or New York Times) are providing access to part of their data. However, current solutions for sharing data with third parties are not fully convenient to either or both data owners and data consumers. Therefore we present dataClay, a distributed data store designed to share data with external players in a secure and flexible way based on the concepts of identity and encapsulation. We also prove that dataClay is comparable in terms of performance with trendy NoSQL technologies while providing extra functionality, and resolves impedance mismatch issues based on the Object Oriented paradigm for data representation.",
      "Keywords": "Data sharing | Distributed databases | NoSQL | Storage systems",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Martí, Jonathan;Queralt, Anna;Gasull, Daniel;Barceló, Alex;José Costa, Juan;Cortes, Toni",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85020376530",
      "Primary study DOI": "10.1016/j.jss.2017.05.128",
      "Title": "Deadlock detection in complex software systems specified through graph transformation using Bayesian optimization algorithm",
      "Abstract": "While developing concurrent systems, one of the important properties to be checked is deadlock freedom. Model checking is an accurate technique to detect errors, such as deadlocks. However, the problem of model checking in complex software systems is state space explosion in which all reachable states cannot be generated due to exponential memory usage. When a state space is too large to be explored exhaustively, using meta-heuristic and evolutionary approaches seems a proper solution to address this problem. Recently, a few methods using genetic algorithm, particle swarm optimization and similar approaches have been proposed to handle this problem. Even though the results of recent approaches are promising, the accuracy and convergence speed may still be a problem. In this paper, a novel method is proposed using Bayesian Optimization Algorithm (BOA) to detect deadlocks in systems specified formally through graph transformations. BOA is an Estimation of Distribution Algorithm in which a Bayesian network (as a probabilistic model) is learned from the population and then sampled to generate new solutions. Three different structures are considered for the Bayesian network to investigate deadlocks in the benchmark problems. To evaluate the efficiency of the proposed approach, it is implemented in GROOVE, an open source toolset for designing and model checking graph transformation systems. Experimental results show that the proposed approach is faster and more accurate than existing algorithms in discovering deadlock states in the most of case studies with large state spaces.",
      "Keywords": "Bayesian network | Bayesian optimization algorithm | Deadlock detection | Graph transformation system | State space explosion",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Pira, Einollah;Rafe, Vahid;Nikanjam, Amin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019738337",
      "Primary study DOI": "10.1016/j.jss.2017.05.079",
      "Title": "Rendex: A method for automated reviews of textual requirements",
      "Abstract": "Conducting requirements reviews before the start of software design is one of the central goals in requirements management. Fast and accurate reviews promise to facilitate software development process and mitigate technical risks of late design modifications. In large software development companies, however, it is difficult to conduct reviews as fast as needed, because the number of regularly incoming requirements is typically several thousand. Manually reviewing thousands of requirements is a time-consuming task and disrupts the process of continuous software development. As a consequence, software engineers review requirements in parallel with designing the software, thus partially accepting the technical risks. In this paper we present a measurement-based method for automating requirements reviews in large software development companies. The method, Rendex, is developed in an action research project in a large software development organization and evaluated in four large companies. The evaluation shows that the assessment results of Rendex have 73%-80% agreement with the manual assessment results of software engineers. Succeeding the evaluation, Rendex was integrated with the requirements management environment in two of the collaborating companies and is regularly used for proactive reviews of requirements.",
      "Keywords": "Complexity | Measure | Metric | Quality prediction | Requirements quality | Requirements review",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Antinyan, Vard;Staron, Miroslaw",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021142877",
      "Primary study DOI": "10.1016/j.jss.2017.06.022",
      "Title": "Obscured by the cloud: A resource allocation framework to model cloud outage events",
      "Abstract": "As Small Medium Enterprises (SMEs) adopt Cloud technologies to provide high value customer offerings, uptime is considered important. Cloud outages represent a challenge to SMEs and micro teams to maintain a services platform. If a Cloud platform suffers from downtime this can have a negative effect on business revenue. Additionally, outages can divert resources from product development/delivery tasks to reactive remediation. These challenges are immediate for SMEs or micro teams with a small levels of resources. In this paper we present a framework that can model the arrival of Cloud outage events. This framework can be used by DevOps teams to manage their scarce pool of resources to resolve outages, thereby minimising impact to service delivery. We analysed over 300 Cloud outage events from an enterprise data set. We modelled the inter-arrival and service times of each outage event and found a Pareto and a lognormal distribution to be a suitable fit. We used this result to produce a special case of the G/G/1 queue system to predict busy times of DevOps personnel. We also investigated dependence between overlapping outage events. Our predictive queuing model compared favourably with observed data, 72% precision was achieved using one million simulations.",
      "Keywords": "Cloud computing | Outage simulation | Queuing theory | Resource allocation model",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Dunne, Jonathan;Malone, David",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85003443942",
      "Primary study DOI": "10.1016/j.jss.2016.06.063",
      "Title": "Stepwise API usage assistance using n-gram language models",
      "Abstract": "Reusing software involves learning third-party APIs, a process that is often time-consuming and error-prone. Recommendation systems for API usage assistance based on statistical models built from source code corpora are capable of assisting API users through code completion mechanisms in IDEs. A valid sequence of API calls involving different types may be regarded as a well-formed sentence of tokens from the API vocabulary. In this article we describe an approach for recommending subsequent tokens to complete API sentences using n-gram language models built from source code corpora. The provided system was integrated in the code completion facilities of the Eclipse IDE, providing contextualized completion proposals for Java taking into account the nearest lines of code. The approach was evaluated against existing client code of four widely used APIs, revealing that in more than 90% of the cases the expected subsequent token is within the 10-top-most proposals of our models. The high score provides evidence that the recommendations could help on API learning and exploration, namely through the assistance on writing valid API sentences.",
      "Keywords": "API | Code completion | IDE | N-grams | Usability",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Santos, André L.;Prendi, Gonçalo;Sousa, Hugo;Ribeiro, Ricardo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84994226132",
      "Primary study DOI": "10.1016/j.jss.2016.07.035",
      "Title": "XTRAITJ: Traits for the Java platform",
      "Abstract": "Traits were proposed as a mechanism for fine-grained code reuse to overcome many limitations of class-based inheritance. A trait is a set of methods that is independent from any class hierarchy and can be flexibly used to build other traits or classes by means of a suite of composition operations. In this paper we present the new version of XTRAITJ, a trait-based programming language that features complete compatibility and interoperability with the JAVA platform. XTRAITJ is implemented in XTEXT and XBASE, and it provides a full Eclipse IDE that supports an incremental adoption of traits in existing JAVA projects. The new version of XTRAITJ allows traits to be accessed from any JAVA project or library, even if the original XTRAITJ source code is not available, since traits can be accessed in their byte-code format. This allows developers to create XTRAITJ libraries that can be provided in their binary only format. We detail the technique we used to achieve such an implementation; this technique can be reused in other languages implemented in XTEXT for the JAVA platform. We formalize our traits by means of flattening semantics and we provide some performance benchmarks that show that the runtime overhead introduced by our traits is acceptable.",
      "Keywords": "Eclipse | IDE | Implementation | Java | Trait",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Bettini, Lorenzo;Damiani, Ferruccio",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84997610070",
      "Primary study DOI": "10.1016/j.jss.2016.07.036",
      "Title": "Reusing business components and objects for modeling business systems: The influence of decomposition characteristics and analyst experience",
      "Abstract": "Component-based development (CBD) relies on the use of pre-fabricated business components to develop new application systems, rather than developing them from scratch. It provides an attractive alternative to more established development methods such as object-oriented analysis and design (OOAD). Given the growing demands for using agile methods for software development, we examine if systems analysts are more effective at modeling business systems by reusing business components than by reusing objects. We also examine whether the influence of component or object reuse on modeling performance is moderated by prior experience in systems analysis and design. We evaluate the representational constructs of the two based on a set of decomposition characteristics, and postulate hypotheses comparing the two based on theories in cognitive psychology and human factors. We find that models generated by reusing business components are of higher accuracy than those developed by reusing objects. An interesting finding of our study is that IT professionals who are less experienced in systems analysis and design perform on par with experienced professionals, when modeling business systems by reusing components. We argue that the decomposition characteristics of components—with respect to granularity, quality, and focus—enable less experienced analysts to perform on par with more experienced analysts.",
      "Keywords": "Analyst experience | Chunking | Cognitive fit | Conceptual modeling | Software reuse | Systems development",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Sinha, Atish P.;Jain, Hemant",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85002957064",
      "Primary study DOI": "10.1016/j.jss.2016.06.101",
      "Title": "Reverse engineering reusable software components from object-oriented APIs",
      "Abstract": "Object-oriented Application Programing Interfaces (APIs) support software reuse by providing pre-implemented functionalities. Due to the huge number of included classes, reusing and understanding large APIs is a complex task. Otherwise, software components are accepted to be more reusable and understandable entities than object-oriented ones. Thus, in this paper, we propose an approach for reengineering object-oriented APIs into component-based ones. We mine components as a group of classes based on the frequency they are used together and their ability to form a quality-centric component. To validate our approach, we experimented on 100 Java applications that used four APIs.",
      "Keywords": "API | Frequent usage pattern | Object-oriented | Reverse engineering | Software component | Software reuse",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Shatnawi, Anas;Seriai, Abdelhak Djamel;Sahraoui, Houari;Alshara, Zakarea",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019735323",
      "Primary study DOI": "10.1016/j.jss.2017.05.048",
      "Title": "Characterizing testing methods for context-aware software systems: Results from a quasi-systematic literature review",
      "Abstract": "Context-Aware Software Systems (CASS) use environmental information to provide better service to the systems’ actors to fulfill their goals. Testing of ubiquitous software systems can be challenging since it is unlikely that, while designing the test cases, the tester can identify all possible context variations. A quasi-Systematic Literature Review has been undertaken to characterize the methods usually used for testing CASS. The analysis and generation of knowledge in this work rely on classifying the extracted information. Established taxonomies of software testing and context-aware were used to characterize and interpret the findings. The results show that, although it is possible to observe the utilization of some software testing methods, few empirical studies are evaluating such methods when testing CASS. The selected technical literature conveys a lack of consensus on the understanding of context and CASS, and on the meaning of software testing. Furthermore, context variation in CASS has only been partially addressed by the identified approaches. They either rely on simulating context or in fixing the values of context variables during testing. We argue that the tests of context-aware software systems need to deal with the diversity of context instead of mitigating their effects.",
      "Keywords": "Context-aware | Software testing | Systematic literature review | Test case design",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Matalonga, Santiago;Rodrigues, Felyppe;Travassos, Guilherme Horta",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85020416606",
      "Primary study DOI": "10.1016/j.jss.2016.09.031",
      "Title": "A case study of black box fail-safe testing in web applications",
      "Abstract": "External failures like network changes can affect system operation negatively. Mitigation requirements try to prevent or reduce their effects. This paper presents a black box testing approach that tests fail-safe behavior in web applications. Failure mitigation tests are built based on a functional test suite. Mitigation requirements are used to build mitigation models and mitigation tests paths through them. A genetic algorithm is used to generate failure scenarios (failure mitigation test requirements). Weaving rules describe how mitigation test paths are combined with behavioral test paths to create failure mitigation test paths. These are then transformed into an executable test suite. The paper also evaluates the genetic algorithm used to generate test requirements with respect to efficiency and effectiveness. A large case study, a commercial mortgage lending system, is used to explore applicability, scalability, and effectiveness of the approach.",
      "Keywords": "Failure mitigation patterns | Genetic algorithm | Web testing",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Boukhris, Salah;Andrews, Anneliese;Alhaddad, Ahmed;Dewri, Rinku",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84997701970",
      "Primary study DOI": "10.1016/j.jss.2016.07.040",
      "Title": "Exploring quality measures for the evaluation of feature models: a case study",
      "Abstract": "Evaluating the quality of a feature model is essential to ensure that errors in the early stages do not spread throughout the Software Product Line (SPL). One way to evaluate the feature model is to use measures that could be associated with the feature model quality characteristics and their quality attributes. In this paper, we aim at investigating how measures can be applied to the quality assessment of SPL feature models. We performed an exploratory case study using the COfFEE maintainability measures catalog and the S.P.L.O.T. feature models repository. In order to support this case study, we built a dataset (denoted by MAcchiATO) containing the values of 32 measures from COfFEE for 218 software feature models, extracted from S.P.L.O.T. This research approach allowed us to explore three different data analysis techniques. First, we applied the Spearman's rank correlation coefficient in order to identify relationships between the measures. This analysis showed that not all 32 measures in COfFEE are necessary to reveal the quality of a feature model and just 15 measures could be used. Next, the 32 measures in COfFEE were grouped by applying the Principal Component Analysis and a set of 9 new grouped measures were defined. Finally, we used the Tolerance Interval technique to define statistical thresholds for these 9 new grouped measures. So, our findings suggest that measures can be effectively used to support the quality evaluation of SPL feature models.",
      "Keywords": "Feature models | Measures | Quality evaluation | Software product line",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Bezerra, Carla I.M.;Andrade, Rossana M.C.;Monteiro, Jose Maria",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85012878938",
      "Primary study DOI": "10.1016/j.jss.2017.01.031",
      "Title": "CollabRDL: A language to coordinate collaborative reuse",
      "Abstract": "Coordinating software reuse activities is a complex problem when considering collaborative software development. This is mainly motivated due to the difficulty in specifying how the artifacts and the knowledge produced in previous projects can be applied in future ones. In addition, modern software systems are developed in group working in separate geographical locations. Therefore, techniques to enrich collaboration on software development are important to improve quality and reduce costs. Unfortunately, the current literature fails to address this problem by overlooking existing reuse techniques. There are many reuse approaches proposed in academia and industry, including Framework Instantiation, Software Product Line, Transformation Chains, and Staged Configuration. But, the current approaches do not support the representation and implementation of collaborative instantiations that involve individual and group roles, the simultaneous performance of multiple activities, restrictions related to concurrency and synchronization of activities, and allocation of activities to reuse actors as a coordination mechanism. These limitations are the main reasons why the Reuse Description Language (RDL) is unable to promote collaborative reuse, i.e., those related to reuse activities in collaborative software development. To overcome these shortcomings, this work, therefore, proposes CollabRDL, a language to coordinate collaborative reuse by providing essential concepts and constructs for allowing group-based reuse activities. For this purpose, we extend RDL by introducing three new commands, including role, parallel, and doparallel. To evaluate CollabRDL we have conducted a case study in which developer groups performed reuse activities collaboratively to instantiate a mainstream Java framework. The results indicated that CollabRDL was able to represent critical workflow patterns, including parallel split pattern, synchronization pattern, multiple-choice pattern, role-based distribution pattern, and multiple instances with decision at runtime. Overall, we believe that the provision of a new language that supports group-based activities in framework instantiation can help enable software organizations to document their coordinated efforts and achieve the benefits of software mass customization with significantly less development time and effort.",
      "Keywords": "Collaboration | Framework | Language | Reuse process | Software reuse",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Lucas, Edson M.;Oliveira, Toacy C.;Farias, Kleinner;Alencar, Paulo S.C.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84999663630",
      "Primary study DOI": "10.1016/j.jss.2016.07.038",
      "Title": "Evaluating Lehman's Laws of software evolution within software product lines industrial projects",
      "Abstract": "The evolution of a single system is a task where we deal with the modification of a single product. Lehman's Laws of software evolution were broadly evaluated within this type of system and the results shown that these single systems evolve according to his stated laws over time. However, considering Software Product Lines (SPL), we need to deal with the modification of several products which include common, variable, and product specific assets. Because of the several assets within SPL, each stated law may have a different behavior for each asset kind. Nonetheless, we do not know if all of the stated laws are still valid for SPL since they were partially evaluated in this context. Thus, this paper details an empirical investigation where Lehman's Laws (LL) of Software Evolution were used in two SPL industrial projects to understand how the SPL assets evolve over time. These projects are related to an application in the medical domain and another in the financial domain, developed by medium-size companies in Brazil. They contain a total of 71 modules and a total of 71.442 bug requests in their tracking system, gathered along the total of more than 10 years. We employed two techniques - the KPSS Test and linear regression analysis, to assess the relationship between LL and SPL assets. Results showed that one law was completely supported (conservation of organizational stability) for all assets within both empirical studies. Two laws were partially supported for both studies depending on the asset type (continuous growth and conservation of familiarity). Finally, the remaining laws had differences among their results for all assets (continuous change, increasing complexity, and declining quality).",
      "Keywords": "Empirical study | Lehman's Laws of software evolution | Software evolution | Software product lines",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Oliveira, Raphael Pereira de;Santos, Alcemir Rodrigues;Almeida, Eduardo Santana de;Gomes, Gecynalda Soares da Silva",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85004178522",
      "Primary study DOI": "10.1016/j.jss.2016.06.058",
      "Title": "Scope-aided test prioritization, selection and minimization for software reuse",
      "Abstract": "Software reuse can improve productivity, but does not exempt developers from the need to test the reused code into the new context. For this purpose, we propose here specific approaches to white-box test prioritization, selection and minimization that take into account the reuse context when reordering or selecting test cases, by leveraging possible constraints delimiting the new input domain scope. Our scope-aided testing approach aims at detecting those faults that under such constraints would be more likely triggered in the new reuse context, and is proposed as a boost to existing approaches. Our empirical evaluation shows that in test suite prioritization we can improve the average rate of faults detected when considering faults that are in scope, while remaining competitive considering all faults; in test case selection and minimization we can considerably reduce the test suite size, with small to no extra impact on fault detection effectiveness considering both in-scope and all faults. Indeed, in minimization, we improve the in-scope fault detection effectiveness in all cases.",
      "Keywords": "In-scope entity | Test case prioritization | Test case selection | Test of reused code | Test suite minimization | Testing scope",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Miranda, Breno;Bertolino, Antonia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85020316281",
      "Primary study DOI": "10.1016/j.jss.2017.05.052",
      "Title": "Software product lines adoption in small organizations",
      "Abstract": "Context An increasing number of studies has demonstrated improvements in product quality, and time-to-market reductions when Software Product Line (SPL) engineering is introduced. However, despite the amount of successful stories about the use of SPL engineering, there is a lack of guidelines to support its adoption, especially to small-sized software organizations. Objective The aim of this study is to investigate SPL adoption in small organizations and to improve the generalization of evidence through the use of a multi-method approach. Method This paper reports on a multi-method study, where results from a mapping study, industrial case study and also expert opinion survey were considered to identify a set of findings. Results The study provides a better understanding of SPL adoption in the context of small to medium-sized organizations, by documenting evidence observed during the transition from single-system development to an SPL approach. This evidence is strengthened by the use of different research methods, which results in 22 findings regarding to the SPL adoption. Conclusion This research has synthesized the available evidence in SPL adoption and identifies gaps between required strategies, organizational structures, maturity level and existing adoption barriers. These findings are an important step to establish guidelines for SPL adoption.",
      "Keywords": "Adoption barriers | Case study | Mapping study | Multi-method approach | Software product lines | SPL adoption | Survey",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Bastos, Jonatas Ferreira;da Mota Silveira Neto, Paulo Anselmo;O'Leary, Pádraig;de Almeida, Eduardo Santana;de Lemos Meira, Silvio Romero",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84998705870",
      "Primary study DOI": "10.1016/j.jss.2016.07.039",
      "Title": "Recovering software product line architecture of a family of object-oriented product variants",
      "Abstract": "Software Product Line Engineering (SPLE) aims at applying a pre-planned systematic reuse of large-grained software artifacts to increase the software productivity and reduce the development cost. The idea of SPLE is to analyze the business domain of a family of products to identify the common and the variable parts between the products. However, it is common for companies to develop, in an ad-hoc manner (e.g. clone and own), a set of products that share common services and differ in terms of others. Thus, many recent research contributions are proposed to re-engineer existing product variants to a software product line. These contributions are mostly focused on managing the variability at the requirement level. Very few contributions address the variability at the architectural level despite its major importance. Starting from this observation, we propose an approach to reverse engineer the architecture of a set of product variants. Our goal is to identify the variability and dependencies among architectural-element variants. Our work relies on formal concept analysis to analyze the variability. To validate the proposed approach, we evaluated on two families of open-source product variants; Mobile Media and Health Watcher. The results of precision and recall metrics of the recovered architectural variability and dependencies are 81%, 91%, 67% and 100%, respectively.",
      "Keywords": "Formal concept analysis | Object-oriented product variants | Software architecture recovery | Software component | Software product line | Software reuse",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Shatnawi, Anas;Seriai, Abdelhak Djamel;Sahraoui, Houari",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019638976",
      "Primary study DOI": "10.1016/j.jss.2017.05.039",
      "Title": "Identification multi-level frequent usage patterns from APIs",
      "Abstract": "Software developers increasingly rely on application programming interfaces (APIs) of frameworks to increase productivity. An API method is generally used within code snippets along with other methods of the API of interest. When developers invoke API methods in a framework, they often encounter difficulty to determine which methods to call due to the huge number of included methods in that API. Developers usually exploit a source code search tool searching for code snippets that use the API methods of interest. However, the number of returned code snippets is very large which hinders the developer to locate useful ones. Moreover, co-usage relationships between API methods are often not documented. This article presents an approach to identify multi-level frequent usage patterns (IML-FUP) to help developers understand API usage and facilitate the development tasks when they use new APIs. An identified pattern represents a set of API methods that are frequently called together across interfering usage scenarios. In order to investigate the efficiency of the proposed approach, an experimental evaluation is conducted using four APIs and 89 client programs. For all studied APIs, the experimental results show that the proposed approach identifies usage patterns that are always strongly cohesive and highly consistent.",
      "Keywords": "API documentation | API usage | Formal concept analysis | Identification | Usage patterns",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Eyal Salman, Hamzeh",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85011965743",
      "Primary study DOI": "10.1016/j.jss.2017.01.030",
      "Title": "A domain-specific language for the control of self-adaptive component-based architecture",
      "Abstract": "Self-adaptive behaviours in the context of Component-based Architecture are generally designed based on past monitoring events, configurations (component assemblies) as well as behavioural programs defining the adaptation logics and invariant properties. Providing assurances on the navigation through the configuration space remains a challenge. That requires taking decisions on predictions on the possible futures of the system in order to avoid going into branches of the behavioural program leading to bad configurations. We propose the design of self-adaptive software components based on logical discrete control approaches, in which the self-adaptive behavioural models enrich component controllers with a knowledge not only on events, configurations and past history, but also with possible future configurations. This article provides the description, implementation and discussion of Ctrl-F, a Domain-specific Language whose objective is to provide high-level support for describing these control policies. Ctrl-Fis formally defined by translation to Finite State Automata models, which allow for the exploration of behavioural programs by verification or Discrete Controller Synthesis, i.e., by automatically generating a controller to enforce correct self-adaptive behaviours. We integrate Ctrl-F with FraSCAti, a Service Component Architecture middleware platform and we illustrate the use of Ctrl-Fby applying it to two case studies.",
      "Keywords": "Component-based architecture | Discrete control | Self-adaptation",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Alvares, Frederico;Rutten, Eric;Seinturier, Lionel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013212230",
      "Primary study DOI": "10.1016/j.jss.2017.01.028",
      "Title": "A hybrid and learning agent architecture for network intrusion detection",
      "Abstract": "Learning is an effective way for automating the adaptation of systems to their environment. This ability is especially relevant in dynamic environments as computer networks where new intrusions are constantly emerging, most of them having similarities and occurring frequently. Traditional intrusion detection systems still have limitations of adaptability because they are just able to detect intrusions previously set in system design. This paper proposes HyLAA a software agent architecture that combines case-based reasoning, reactive behavior and learning. Through its learning mechanism, HyLAA can adapt itself to its environment and identify new intrusions not previously specified in system design. This is done by learning new reactive rules by observing recurrent good solutions to the same perception from the case-based reasoning system, which will be stored in the agent knowledge base. The effectiveness of HyLAA to detect intrusions using case-based reasoning behavior, the accuracy of the classifier learned by the learning component and both the performance and effectiveness of HyLAA to detect intrusions using hybrid behavior with learning and without learning were evaluated, respectively, by conducting four experiments. In the first experiment, HyLAA exhibited good effectiveness to detect intrusions. In the second experiment the classifiers learned by the learning component presented high accuracy. Both the hybrid agent behavior with learning and without learning (third and fourth experiment, respectively) presented greater effectiveness and a balance between performance and effectiveness, but only the hybrid behavior showed better effectiveness and performance as long as the agent learns.",
      "Keywords": "Case-based reasoning | Hybrid agents | Information security | Intrusion detection systems | Learning agents | Ontologies",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Leite, Adriana;Girardi, Rosario",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019454236",
      "Primary study DOI": "10.1016/j.jss.2017.05.022",
      "Title": "Accurate modeling and efficient QoS analysis of scalable adaptive systems under bursty workload",
      "Abstract": "Fulfillment of QoS requirements for systems deployed in the Internet is becoming a must. A widespread characteristic of this kind of systems is that they are usually subject to highly variable and bursty workloads. The allocation of resources to fulfill QoS requirements during the peak workloads could entail a waste of computing resources. A solution is to provide the system with self-adaptive techniques that can allocate resources only when and where they are required. We pursue the QoS evaluation of workload-aware self-adaptive systems based on stochastic models. In particular, this work proposes an accurate modeling of the workload variability and burstiness phenomena based on previous approaches that use Markov Modulated Poisson Processes. We extend these approaches in order to accurately model the variations of the workload strongly influence the QoS of the self-adaptive system. Unfortunately, this stochastic modeling may lead to a non tractable QoS analysis. Consequently, this work also develops an efficient procedure for carrying out the QoS analysis.",
      "Keywords": "Adaptability | Markov models | Quality of service | Stochastic petri nets | Workload modeling",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Perez-Palacin, Diego;Mirandola, Raffaela;Merseguer, José",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85007565367",
      "Primary study DOI": "10.1016/j.jss.2016.11.034",
      "Title": "A search engine for finding and reusing architecturally significant code",
      "Abstract": "Architectural tactics are the building blocks of software architecture. They describe solutions for addressing specific quality concerns, and are prevalent across many software systems. Once a decision is made to utilize a tactic, the developer must generate a concrete plan for writing code and implementing the tactic. Unfortunately, this is a non-trivial task even for experienced developers. Often, developers resort to using search engines, crowd-sourcing websites, or discussion forums to find sample code snippets to implement a tactic. A fundamental problem of finding implementation for architectural tactics/patterns is the mismatch between the high-level intent reflected in the descriptions of these patterns and the low-level implementation details of them. To reduce this mismatch, we created a novel Tactic Search Engine called ArchEngine (ARCHitecture search ENGINE). ArchEngine can replace this manual internet-based search process and help developers find and reuse tactical code from a wide range of open source systems. ArchEngine helps developers find implementation examples of an architectural tactic for a given technical context. It uses information retrieval and program analysis techniques to retrieve applications that implement these design concepts. Furthermore, it lists and rank the code snippets where the patterns/tactics are located. Our case study with 21 graduate students (with experience level of junior software developers) shows that ArchEngine is more effective than other search engines (e.g., Krugle and Koders) in helping programmers to quickly find implementations of architectural tactics/patterns.",
      "Keywords": "Architecture | Information | Models | Tactics | Traceability",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Mujhid, Ibrahim Jameel;Joanna, Joanna C.;Gopalakrishnan, Raghuram;Mirakhorli, Mehdi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019184864",
      "Primary study DOI": "10.1016/j.jss.2017.04.018",
      "Title": "How developers micro-optimize Android apps",
      "Abstract": "Optimizing mobile apps early on in the development cycle is supposed to be a key strategy for obtaining higher user rankings, more downloads, and higher retention. In fact, mobile platform designers publish specific guidelines, and tools aimed at optimizing apps. However, little research has been done with respect to identifying and understanding actual optimization practices performed by developers. In this paper, we present the results of three empirical studies aimed at investigating practices of Android developers towards improving the performance of their apps, by means of micro-optimizations. We mined change histories of 3513 apps to identify the most frequent micro-optimization opportunities in 297K+ snapshots and to understand if (and when) developers implement these optimizations. Then, we performed an in-depth analysis into whether implementing micro-optimizations can help reduce memory/CPU usage. Finally, we conducted a survey with 389 open-source developers to understand how they use micro-optimizations to improve the performance of Android apps. Surprisingly, our results indicate that developers rarely implement micro-optimizations. Also, the impact of the analyzed micro-optimization on CPU/memory consumption is negligible in most of the cases. Finally, the results from the survey shed some light into why this happens as well as upon which practices developers rely upon.",
      "Keywords": "Android | Empirical studies | Measurement | Mining software repositories | Optimizations",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Linares-Vásquez, Mario;Vendome, Christopher;Tufano, Michele;Poshyvanyk, Denys",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019644895",
      "Primary study DOI": "10.1016/j.jss.2016.08.071",
      "Title": "Empirical study on refactoring large-scale industrial systems and its effects on maintainability",
      "Abstract": "Software evolves continuously, it gets modified, enhanced, and new requirements always arise. If we do not spend time occasionally on improving our source code, its maintainability will inevitably decrease. The literature tells us that we can improve the maintainability of a software system by regularly refactoring it. But does refactoring really increase software maintainability? Can it happen that refactoring decreases the maintainability? Empirical studies show contradicting answers to these questions and there have been only a few studies which were performed in a large-scale, industrial context. In our paper, we assess these questions in an in vivo context, where we analyzed the source code and measured the maintainability of 6 large-scale, proprietary software systems in their manual refactoring phase. We analyzed 2.5 million lines of code and studied the effects on maintainability of 315 refactoring commits which fixed 1273 coding issues. We found that single refactorings only make a very little difference (sometimes even decrease maintainability), but a whole refactoring period, in general, can significantly increase maintainability, which can result not only in the local, but also in the global improvement of the code.",
      "Keywords": "Antipatterns | Coding issues | ISO/IEC 25010 | Maintainability | Refactoring | Software quality",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Szőke, Gábor;Antal, Gábor;Nagy, Csaba;Ferenc, Rudolf;Gyimóthy, Tibor",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018165153",
      "Primary study DOI": "10.1016/j.jss.2017.04.015",
      "Title": "How to securely outsource the inversion modulo a large composite number",
      "Abstract": "Modular inversion is one of the most basic computations in algorithmic number theory. When it comes to cryptosystems, this computation is very time-consuming since the modulus is generally a large number. It is unrealistic for some devices with limited computation capability (e.g. mobile devices and IC cards) to conduct such a time-consuming computation. In this paper, we investigate how to securely outsource the inversion modulo a large composite number. Based on the Chinese Remainder Theorem (CRT), we design a secure outsourcing algorithm for inversion modulo a large composite number with two known prime factors for the client. Besides the privacy of the number and its modular inversion, our algorithm also protects the privacy of the modulus. We can verify the correctness of the result with probability 1. Traditionally, the complexity of modular inversion for a l-bit modulus is O(l3). By leveraging the cloud, our algorithm reduces the complexity to O(l2) on the client side. Also, we prove the security of our algorithm based on the one-malicious version of two untrusted program model (one-malicious model). We conduct several experiments to demonstrate the validity and the practicality of our proposed algorithm. In appendix, we show that our proposed algorithm can be extended and applied in the secret key generation of RSA algorithm on the resource-constrained devices.",
      "Keywords": "Chinese remainder theorem | Cloud computing | Modular inversion | Outsource-secure algorithms",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Su, Qianqian;Yu, Jia;Tian, Chengliang;Zhang, Hanlin;Hao, Rong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84967190374",
      "Primary study DOI": "10.1016/j.jss.2016.04.009",
      "Title": "Observational slicing based on visual semantics",
      "Abstract": "Program slicing has seen a plethora of applications and variations since its introduction over 35 years ago. The dominant method for computing slices involves significant complex source-code analysis to model the dependencies in the code. A recently introduced alternative, observation-based slicing, sidesteps this complexity by observing the behavior of candidate slices. Observation-based slicing has several other strengths, including the ability to easily slice multi-language systems. However, the initial implementation of observation-based slicing, ORBS, remains rooted in tradition as it captures semantics by comparing sequences of values. This raises the question of whether it is possible to extend slicing beyond its traditional semantic roots. A few existing projects have attempted this but the extension requires considerable effort. If it is possible to build on the ORBS platform to more easily generalize slicing to languages with non-traditional semantics, then there is the potential to vastly increase the range of programming languages to which slicing can be applied. ORBS supports this by reducing the problem to that of generalizing how semantics are captured. Taking Picture Description Languages as a case study, the challenges and effectiveness of such a generalization are considered. The results show that not only is it possible to generalize the ORBS implementation, but the resulting slicer is quite effective, removing from 8% to 98% of the original source code with an average of 83%. Finally a qualitative look at the slices finds the technique very effective, at times producing minimal slices.",
      "Keywords": "Non-traditional semantics | Observation",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Yoo, Shin;Binkley, David;Eastman, Roger",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018470279",
      "Primary study DOI": "10.1016/j.jss.2017.03.057",
      "Title": "Distributed analysis and filtering of application event streams",
      "Abstract": "Large information systems comprise different interconnected hardware and software components, that collectively generate large volumes of data. Furthermore, the run-time analysis of such data involves computationally expensive algorithms, and is pivotal to a number of software engineering activities such as, system understanding, diagnostics, and root cause analysis. In a quest to increase the performance of run-time analysis for large sets of logged data, we present an approach that allows for the real time reduction of one or more event streams by utilizing a set of filtering criteria. More specifically, the approach employs a similarity measure that is based on information theory principles, and is applied between the features of the incoming events, and the features of a set of retrieved or constructed events, that we refer to as beacons. The proposed approach is domain and event schema agnostic, can handle infinite data streams using a caching algorithm, and can be parallelized in order to tractably process high volume, high frequency, and high variability data. Experimental results obtained using the KDD’99 and CTU-13 labeled data sets, indicate that the approach is scalable, and can yield highly reduced sets with high recall values with respect to a use case.",
      "Keywords": "Dynamic analysis | Event stream filtering | Information theory | Software engineering | System understanding",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Kalamatianos, Theodoros;Kontogiannis, Kostas",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84979030646",
      "Primary study DOI": "10.1016/j.jss.2016.07.026",
      "Title": "API usage pattern recommendation for software development",
      "Abstract": "Application Programming Interfaces (APIs) facilitate pragmatic reuse and improve the productivity of software development. An API usage pattern documents a set of method calls from multiple API classes to achieve a reusable functionality. Existing approaches often use frequent-sequence mining to extract API usage patterns. However, as reported by earlier studies, frequent-sequence mining may not produce a complete set of usage patterns. In this paper, we explore the possibility of mining API usage patterns without relying on frequent-pattern mining. Our approach represents the source code as a network of object usages where an object usage is a set of method calls invoked on a single API class. We automatically extract usage patterns by clustering the data based on the co-existence relations between object usages. We conduct an empirical study using a corpus of 11,510 Android applications. The results demonstrate that our approach can effectively mine API usage patterns with high completeness and low redundancy. We observe 18% and 38% improvement on F-measure and response time respectively comparing to usage pattern extraction using frequent-sequence mining.",
      "Keywords": "Clustering | Object usage | Usage pattern",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Niu, Haoran;Keivanloo, Iman;Zou, Ying",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85007591252",
      "Primary study DOI": "10.1016/j.jss.2016.04.008",
      "Title": "Semantic versioning and impact of breaking changes in the Maven repository",
      "Abstract": "Systems that depend on third-party libraries may have to be updated when updates to these libraries become available in order to benefit from new functionality, security patches, bug fixes, or API improvements. However, often such changes come with changes to the existing interfaces of these libraries, possibly causing rework on the client system. In this paper, we investigate versioning practices in a set of more than 100,000 jar files from Maven Central, spanning over 7 years of history of more than 22,000 different libraries. We investigate to what degree versioning conventions are followed in this repository. Semantic versioning provides strict rules regarding major (breaking changes allowed), minor (no breaking changes allowed), and patch releases (only backward-compatible bug fixes allowed). We find that around one third of all releases introduce at least one breaking change. We perform an empirical study on potential rework caused by breaking changes in library releases and find that breaking changes have a significant impact on client libraries using the changed functionality. We find out that minor releases generally have larger release intervals than major releases. We also investigate the use of deprecation tags and find out that these tags are applied improperly in our dataset.",
      "Keywords": "Breaking changes | Semantic versioning | Software libraries",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Raemaekers, S.;van Deursen, A.;Visser, J.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018958478",
      "Primary study DOI": "10.1016/j.jss.2017.04.017",
      "Title": "A theoretical analysis on cloning the failed test cases to improve spectrum-based fault localization",
      "Abstract": "Fault localization is the activity to locate faults in programs. Spectrum-based fault localization (SBFL) is a class of techniques for it. It contrasts the code coverage achieved by passed runs and that by failed runs, and estimates program entities responsible for the latter. Although previous work has empirically shown that the effectiveness of typical SBFL techniques can be improved by incorporating more failed runs, debugging often takes place when there are very few of them. In this paper, we report a comprehensive study to investigate the impact of cloning the failed test cases on the effectiveness of SBFL techniques. We include 33 popular such techniques, and examine the accuracy of their formulas on twelve benchmark programs, using four accuracy metrics and in three scenarios. The empirical results show that on 22, 21, and 23 of them the fault-localization accuracy can be significantly improved, when the failed test cases are cloned in the single-fault, double-fault, and triple-fault scenarios, respectively. We also analytically show that on 19 of them the improvements are provable for an arbitrary program and an arbitrary test suite, in the single-fault scenario; and moreover, for ten of the rest formulas, their accuracy are proved unaffected in all scenarios.",
      "Keywords": "Class imbalance | Fault localization | Software debugging | Test suite cloning",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Zhang, Long;Yan, Lanfei;Zhang, Zhenyu;Zhang, Jian;Chan, W. K.;Zheng, Zheng",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84969506710",
      "Primary study DOI": "10.1016/j.jss.2016.05.015",
      "Title": "Topic-based software defect explanation",
      "Abstract": "Researchers continue to propose metrics using measurable aspects of software systems to understand software quality. However, these metrics largely ignore the functionality, i.e., the conceptual concerns, of software systems. Such concerns are the technical concepts that reflect the system's business logic. For instance, while lines of code may be a good general measure for defects, a large file responsible for simple I/O tasks is likely to have fewer defects than a small file responsible for complicated compiler implementation details. In this paper, we study the effect of concerns on software quality. We use a statistical topic modeling approach to approximate software concerns as topics (related words in source code). We propose various metrics using these topics to help explain the file defect-proneness. Case studies on multiple versions of Firefox, Eclipse, Mylyn, and NetBeans show that (i) some topics are more defect-prone than others; (ii) defect-prone topics tend to remain so over time; (iii) our topic-based metrics provide additional explanatory power for software quality over existing structural and historical metrics; and (iv) our topic-based cohesion metric outperforms state-of-the-art topic-based cohesion and coupling metrics in terms of defect explanatory power, while being simpler to implement and more intuitive to interpret.",
      "Keywords": "Code quality | Cohesion | Coupling | LDA | Metrics | Topic modeling",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Chen, Tse Hsun;Shang, Weiyi;Nagappan, Meiyappan;Hassan, Ahmed E.;Thomas, Stephen W.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85015694094",
      "Primary study DOI": "10.1016/j.jss.2017.03.003",
      "Title": "An efficient spark-based adaptive windowing for entity matching",
      "Abstract": "Entity Matching (EM), i.e., the task of identifying records that refer to the same entity, is a fundamental problem in every information integration and data cleansing system, e.g., to find similar product descriptions in databases. The EM task is known to be challenging when the datasets involved in the matching process have a high volume due to its pair-wise nature. For this reason, studies about challenges and possible solutions of how EM can benefit from modern parallel computing programming models, such as Apache Spark (Spark), have become an important demand nowadays (Christen, 2012a; Kolb et al., 2012b). The effectiveness and scalability of Spark-based implementations for EM depend on how well the workload distribution is balanced among all workers. In this article, we investigate how Spark can be used to perform efficiently (load balanced) parallel EM using a variation of the Sorted Neighborhood Method (SNM) that uses a varying (adaptive) window size. We propose Spark Duplicate Count Strategy (S-DCS++), a Spark-based approach for adaptive SNM, aiming to increase even more the performance of this method. The evaluation results, based on real-world datasets and cluster infrastructure, show that our approach increases the performance of parallel DCS++ regarding the EM execution time.",
      "Keywords": "Adaptive windowing | Entity matching | Load balancing | Spark",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Mestre, Demetrio Gomes;Pires, Carlos Eduardo Santos;Nascimento, Dimas Cassimiro;de Queiroz, Andreza Raquel Monteiro;Santos, Veruska Borges;Araujo, Tiago Brasileiro",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018458943",
      "Primary study DOI": "10.1016/j.jss.2016.11.033",
      "Title": "Using contexts similarity to predict relationships between tasks",
      "Abstract": "Developers’ tasks are often interrelated. A task might succeed, precede, block, or depend on another task. Or, two tasks might simply have a similar aim or require similar expertise. When working on tasks, developers interact with artifacts and tools, which constitute the contexts of the tasks. This work investigates the extent to which the similarity of the contexts predicts whether and how the respective tasks are related. The underlying assumption is simple: if during two tasks the same artifacts are touched or similar interactions are observed, the tasks might be interrelated. We define a task context as the set of all developer's interactions with the artifacts during the task. We then apply Jaccard index, a popular similarity measure to compare two contexts. Instead of only counting the artifacts in the intersection and union of the contexts as Jaccard does, we scale the artifacts with their relevance to the task. For this, we suggest a simple heuristic based on the Frequency, Duration, and Age of the interactions with the artifacts (FDA). Alternatively, artifact relevance can be estimated by the Degree-of-Interest (DOI) used in task-focused programming. To compare the accuracy of the context similarity models for predicting task relationships, we conducted a field study with professionals, analyzed data from the open source task repository Bugzilla, and ran an experiment with students. We studied two types of relationships useful for work coordination (dependsOn and blocks) and two types useful for personal work management (isNextTo and isSimilarTo). We found that context similarity models clearly outperform a random prediction for all studied task relationships. We also found evidence that, the more interrelated the tasks are, the more accurate the context similarity predictions are. Our results show that context similarity is roughly as accurate to predict task relationships as comparing the textual content of the task descriptions. Context and content similarity models might thus be complementary in practice, depending on the availability of text descriptions or context data. We discuss several use cases for this research, e.g. to assist developers choose the next task or to recommend other tasks they should be aware of.",
      "Keywords": "Developer productivity | Empirical study | Recommender systems | Task context | Task dependencies | Task management",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Maalej, Walid;Ellmann, Mathias;Robbes, Romain",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85016081845",
      "Primary study DOI": "10.1016/j.jss.2017.02.013",
      "Title": "Continuous Delivery: Overcoming adoption challenges",
      "Abstract": "Continuous Delivery (CD) is a relatively new software development approach. Companies that have adopted CD have reported significant benefits. Motivated by these benefits, many companies would like to adopt CD. However, adopting CD can be very challenging for a number of reasons, such as obtaining buy-in from a wide range of stakeholders whose goals may seemingly be different from—or even conflict with—our own; gaining sustained support in a dynamic complex enterprise environment; maintaining an application development team's momentum when their application's migration to CD requires an additional strenuous effort over a long period of time; and so on. To help overcome the adoption challenges, I present six strategies: (1) selling CD as a painkiller; (2) establishing a dedicated team with multi-disciplinary members; (3) continuous delivery of continuous delivery; (4) starting with the easy but important applications; (5) visual CD pipeline skeleton; (6) expert drop. These strategies were derived from four years of experience in implementing CD at a multi-billion-euro company. Additionally, our experience led to the identification of eight further challenges for research. The information contributes toward building a body of knowledge for CD adoption.",
      "Keywords": "Adoption | Agile Software Development | Continuous Delivery | Continuous Deployment | Continuous Software Engineering | DevOps",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Chen, Lianping",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028246809",
      "Primary study DOI": "10.1016/j.jss.2016.09.012",
      "Title": "A context model for IDE-based recommendation systems",
      "Abstract": "Context, as modeled through variables called contextual factors, can improve human-computer interaction. To date, in applications supporting software development, such as integrated development environments (IDEs) and recommendation systems for software engineering (RSSEs), contextual factors have generally been constrained to project artifacts, such as source code. In this paper, we present a context model that includes thirteen contextual factors, which capture various situations in which developers interact with an IDE. This context model can be used to support and enhance user interaction with an IDE or to improve the accuracy and timing of recommendations produced by RSSEs. To assess whether the proposed contextual factors are informative for a context model, we statistically evaluated the correlations between IDE command usage and different situations, as they are described by the factors. If a contextual factor correlates with the usage of a command this means that the user is using the command differently when the values of the contextual factor change. We discovered that different factors correlate with different commands and that all the factors correlate with some commands, hence, when a context change is detected, we can also expect a change in the interaction with an IDE.",
      "Keywords": "Commands | Context | Integrated development environment | Recommendation systems for software engineering | Software development | Usage",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Gasparic, Marko;Murphy, Gail C.;Ricci, Francesco",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85017006837",
      "Primary study DOI": "10.1016/j.jss.2017.03.015",
      "Title": "Simplification of UML/OCL schemas for efficient reasoning",
      "Abstract": "Ensuring the correctness of a conceptual schema is an essential task in order to avoid the propagation of errors during software development. The kind of reasoning required to perform such task is known to be exponential for UML class diagrams alone and even harder when considering OCL constraints. Motivated by this issue, we propose an innovative method aimed at removing constraints and other UML elements of the schema to obtain a simplified one that preserve the same reasoning outcomes. In this way, we can reason about the correctness of the initial artifact by reasoning on a simplified version of it. Thus, the efficiency of the reasoning process is significantly improved. In addition, since our method is independent from the reasoning engine used, any reasoning method may benefit from it.",
      "Keywords": "OCL | Reasoning | Simplification | UML",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Oriol, Xavier;Teniente, Ernest",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85017511513",
      "Primary study DOI": "10.1016/j.jss.2017.03.044",
      "Title": "Source code metrics: A systematic mapping study",
      "Abstract": "Context Source code metrics are essential components in the software measurement process. They are extracted from the source code of the software, and their values allow us to reach conclusions about the quality attributes measured by the metrics. Objectives This paper aims to collect source code metrics related studies, review them, and perform an analysis, while providing an overview on the current state of source code metrics and their current trends. Method A systematic mapping study was conducted. A total of 226 studies, published between the years 2010 and 2015, were selected and analyzed. Results Almost 300 source code metrics were found. Object oriented programming is the most commonly studied paradigm with the Chidamber and Kemerer metrics, lines of code, McCabe's cyclomatic complexity, and number of methods and attributes being the most used metrics. Research on aspect and feature oriented programming is growing, especially for the current interest in programming concerns and software product lines. Conclusions Object oriented metrics have gained much attention, but there is a current need for more studies on aspect and feature oriented metrics. Software fault prediction, complexity and quality assessment are recurrent topics, while concerns, big scale software and software product lines represent current trends.",
      "Keywords": "Aspect-oriented metrics | Feature-oriented metrics | Object-oriented metrics | Software metrics | Source code metrics | Systematic mapping study",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Nuñez-Varela, Alberto S.;Pérez-Gonzalez, Héctor G.;Martínez-Perez, Francisco E.;Soubervielle-Montalvo, Carlos",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85016037440",
      "Primary study DOI": "10.1016/j.jss.2017.03.006",
      "Title": "Experimentally assessing the combination of multiple visualization strategies for software evolution analysis",
      "Abstract": "Software engineers need to comprehend large amounts of data to maintain software. Software Visualization is an area that helps users to analyze software through the use of visual resources. It can be effectively used to understand the large amount of data produced during software evolution. A key challenge in the area is to create strategies to consistently visualize the many software attributes, modules and versions produced during its lifecycle. Most of the current visualization strategies seek to present data as a whole, including all available versions of the software in one visual scene. The area lacks strategies visualizing software in detail through the analysis of the evolution of specific software modules. Both strategies are useful, and should be selected according to the task at hand. This work focuses on combining software evolution visualization strategies, experimentally validating the benefits of the approach. Its goal was to build empirical evidence on the use of the combined multiple strategies for software evolution comprehension. It presents an experimental study that exploits the benefits of combining multiple visual strategies of software evolution analysis. The results show that combined visualization strategies perform better in terms of correctness and analysis time.",
      "Keywords": "Experimental evaluation | Software evolution visualization | Visual strategies",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Novais, Renato;Santos, José Amancio;Mendonça, Manoel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85017384564",
      "Primary study DOI": "10.1016/j.jss.2017.04.001",
      "Title": "A multivariate and quantitative model for predicting cross-application interference in virtual environments",
      "Abstract": "Cross-application interference can drastically affect performance of HPC applications executed in clouds. The problem is caused by concurrent access of co-located applications to shared resources such as cache and main memory. Several works of the related literature have considered general characteristics of HPC applications or the total amount of SLLC accesses to determine the cross-application interference. However, our experiments showed that the cross-application interference problem is related to the amount of simultaneous access to several shared resources, revealing its multivariate and quantitative nature. Thus, in this work we propose a multivariate and quantitative model able to predict cross-application interference level that considers the amount of concurrent accesses to SLLC, DRAM and virtual network, and the similarity between the amount of those accesses. An experimental analysis of our prediction model by using a real reservoir petroleum simulator and applications from a well-known HPC benchmark showed that our model could estimate the interference, reaching an average and maximum prediction errors around 4% and 12%, and achieving errors less than 10% in approximately 96% of all tested cases.",
      "Keywords": "Cloud computing | Cross-application interference | High Performance Computing | Virtual Machine Placement",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Melo Alves, Maicon;Drummond, Lúcia Maria de Assumpção",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85015261358",
      "Primary study DOI": "10.1016/j.jss.2017.03.005",
      "Title": "Automotive software engineering: A systematic mapping study",
      "Abstract": "The automotive industry is going through a fundamental change by moving from a mechanical to a software-intensive industry in which most innovation and competition rely on software engineering competence. Over the last few decades, the importance of software engineering in the automotive industry has increased significantly and has attracted much attention from both scholars and practitioners. A large body-of-knowledge on automotive software engineering has accumulated in several scientific publications, yet there is no systematic analysis of that knowledge. This systematic mapping study aims to classify and analyze the literature related to automotive software engineering in order to provide a structured body-of-knowledge, identify well-established topics and potential research gaps. The review includes 679 articles from multiple research sub-area, published between 1990 and 2015. The primary studies were analyzed and classified with respect to five different dimensions. Furthermore, potential research gaps and recommendations for future research are presented. Three areas, namely system/software architecture and design, qualification testing, and reuse were the most frequently addressed topics in the literature. There were fewer comparative and validation studies, and the literature lacks practitioner-oriented guidelines. Overall, research activity on automotive software engineering seems to have high industrial relevance but is relatively lower in its scientific rigor.",
      "Keywords": "Automotive software engineering | Automotive systems | Embedded systems | Literature survey | Software-intensive systems | Systematic mapping study",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Haghighatkhah, Alireza;Banijamali, Ahmad;Pakanen, Olli Pekka;Oivo, Markku;Kuvaja, Pasi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028275894",
      "Primary study DOI": "10.1016/j.jss.2016.07.016",
      "Title": "Using contextual information to predict co-changes",
      "Abstract": "Background: Co-change prediction makes developers aware of which artifacts will change together with the artifact they are working on. In the past, researchers relied on structural analysis to build prediction models. More recently, hybrid approaches relying on historical information and textual analysis have been proposed. Despite the advances in the area, software developers still do not use these approaches widely, presumably because of the number of false recommendations. We conjecture that the contextual information of software changes collected from issues, developers' communication, and commit metadata captures the change patterns of software artifacts and can improve the prediction models. Objective: Our goal is to develop more accurate co-change prediction models by using contextual information from software changes. Method: We selected pairs of files based on relevant association rules and built a prediction model for each pair relying on their associated contextual information. We evaluated our approach on two open source projects, namely Apache CXF and Derby. Besides calculating model accuracy metrics, we also performed a feature selection analysis to identify the best predictors when characterizing co-changes and to reduce overfitting. Results: Our models presented low rates of false negatives (∼8% average rate) and false positives (∼11% average rate). We obtained prediction models with AUC values ranging from 0.89 to 1.00 and our models outperformed association rules, our baseline model, when we compared their precision values. Commit-related metrics were the most frequently selected ones for both projects. On average, 6 out of 23 metrics were necessary to build the classifiers. Conclusions: Prediction models based on contextual information from software changes are accurate and, consequently, they can be used to support software maintenance and evolution, warning developers when they miss relevant artifacts while performing a software change.",
      "Keywords": "Change coupling | Change impact analysis | Change propagation | Co-change prediction | Contextual information | Software change context",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Wiese, Igor Scaliante;Ré, Reginaldo;Steinmacher, Igor;Kuroda, Rodrigo Takashi;Oliva, Gustavo Ansaldi;Treude, Christoph;Gerosa, Marco Aurélio",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85016268711",
      "Primary study DOI": "10.1016/j.jss.2017.03.009",
      "Title": "A tool to support the definition and enactment of model-driven migration processes",
      "Abstract": "One of the main challenges to achieve the industrial adoption of Model-Driven Engineering (MDE) paradigm is building tools able to support model-driven software processes. We present a tool for the definition and enactment of model-driven migration processes. We have created a SPEM-based language for defining Abstract Migration models that represent an MDE migration solution for a particular pair of source and target technologies. For each legacy application to be migrated, the Abstract Migration model is transformed into a Concrete Migration model which contains all the information needed for the enactment. Then, these models are enacted by means of a process interpreter which generates Trac tickets for executing automated tasks by means of Ant scripts and managing manual tasks with the Mylyn tool. Our work has therefore two main contributions: i) it proposes a novel solution for the enactment that integrates the execution of the automated tasks with the generation of tickets to support the manual tasks, and ii) it describes how MDE techniques can be used to implement process engineering tools, in particular migration processes. The article presents the approach and describes in detail the essential aspects of our tool.",
      "Keywords": "Model-driven engineering | Process enactment | Software migrations | Software processes",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Bermúdez Ruiz, Fco Javier;Sánchez Ramón, Óscar;García Molina, Jesús",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85015430911",
      "Primary study DOI": "10.1016/j.jss.2017.02.039",
      "Title": "Resemblance and mergence based indexing for high performance data deduplication",
      "Abstract": "Data deduplication, a data redundancy elimination technique, has been widely employed in many application environments to reduce data storage space. However, it is challenging to provide a fast and scalable key-value fingerprint index particularly for large datasets, while the index performance is critical to the overall deduplication performance. This paper proposes RMD, a resemblance and mergence based deduplication scheme, which aims to provide quick responses to fingerprint queries. The key idea of RMD is to leverage a bloom filter array and a data resemblance algorithm to dramatically reduce the query range. At data ingesting time, RMD uses a resemblance algorithm to detect resemble data segments and put resemblance segments in the same bin. As a result, at querying time, it only needs to search in the corresponding bin to detect duplicate content, which significantly speeds up the query process. Moreover, RMD uses a mergence strategy to accumulate resemblance segments to relevant bins, and exploits frequency-based fingerprint retention policy to cap the bin capacity to improve query throughput and data deduplication ratio. Extensive experimental results with real-world datasets have shown that RMD is able to achieve high query performance and outperforms several well-known deduplication schemes.",
      "Keywords": "Deduplication | Fast index | Fingerprint retrieval | Key value index | Resemblance mergence",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Zhang, Panfeng;Huang, Ping;He, Xubin;Wang, Hua;Zhou, Ke",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85016136266",
      "Primary study DOI": "10.1016/j.jss.2017.02.049",
      "Title": "A comprehensive framework for cloud computing migration using Meta-synthesis approach",
      "Abstract": "Migration to the cloud computing environment is a strategic organizational decision. Using a reliable framework for migration ensures managers to mitigate risks in the cloud computing technology. Therefore, organizations always search for cloud migration frameworks with dynamic nature as well as integrity beside their simplicity. In previous studies, these important features have received less attention and have not been achieved in an integrated and comprehensive way. The aim of this study is to use a meta-synthesis method for the first time for analysis and synthesis of previous published studies and suggests a comprehensive cloud migration framework. We review more than 657 papers from relevant journals and conference proceedings. The concepts which are extracted from these papers are classified to related sub-categories and categories. Then, our proposed framework based on these concepts and categories is developed. It includes seven main phases (categories) and fifteen sub-categories. To improve the migration process a maturity model called “ClM3” is introduced. Finally, proposed framework and maturity model is evaluated by forming different focus group meetings and taking advantages of the cloud experts’ opinion. The results of this research can help managers have a safe and effective migration to cloud computing environment.",
      "Keywords": "Cloud computing | Meta-synthesis | Migration framework | Process maturity model",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Bazi, Hamid reza;Hassanzadeh, Alireza;Moeini, Ali",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028242578",
      "Primary study DOI": "10.1016/j.jss.2016.03.030",
      "Title": "Eye gaze and interaction contexts for change tasks   Observations and potential",
      "Abstract": "The more we know about software developers’ detailed navigation behavior for change tasks, the better we are able to provide effective tool support. Currently, most empirical studies on developers performing change tasks are, however, limited to very small code snippets or limited by the granularity and detail of the data collected on developer's navigation behavior. In our research, we extend this work by combining user interaction monitoring to gather interaction context – the code elements a developer selects and edits – with eye-tracking to gather more detailed and fine-granular gaze context-code elements a developer looked at. In a study with 12 professional and 10 student developers we gathered interaction and gaze contexts from participants working on three change tasks of an open source system. Based on an analysis of the data we found, amongst other results, that gaze context captures different aspects than interaction context and that developers only read small portions of code elements. We further explore the potential of the more detailed and fine-granular data by examining the use of the captured change task context to predict perceived task difficulty and to provide better and more fine-grained navigation recommendations. We discuss our findings and their implications for better tool support.",
      "Keywords": "Empirical study | Eye-tracking | Interactions",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Kevic, K.;Walters, B. M.;Shaffer, T. R.;Sharif, B.;Shepherd, D. C.;Fritz, T.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85012982813",
      "Primary study DOI": "10.1016/j.jss.2017.02.002",
      "Title": "A value-oriented approach to business process specialization: Principles, proof-of-concept, and validation",
      "Abstract": "Organizations build information systems to support their business processes. Precise modeling of an organization's processes is a prerequisite for building information systems that support those processes. Our goal is to help business analysts produce detailed models of the business processes that best reflect the needs of their organizations. To this end, we propose to a) leverage the best practices in terms of a kernel of generic business processes, and b) provide analysts with tools to customize those processes by generating new process variants. We use business patterns from the Resource Event Agent ontology to identify variation points, and to codify the transformations inherent in the generation of the process variants. We developed a prototype process specialization tool using the Eclipse modeling ecosystem. We tested our approach on a set of processes from the Enterprise Resource Planning literature, and a set of variation points to assess the extent to which: 1) the identified variation points made sense, and 2) whether the generated variants made sense, from a business point of view. The results showed that 94.12% of the variation points made sense, and that 80.6% of the generated process variants corresponded to what the business process management specialists expected.",
      "Keywords": "Business ontology | Business pattern | Business process | Model transformation | Reuse | Specialization",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Leshob, Abderrahmane;Mili, Hafedh;Gonzalez-Huerta, Javier;Boubaker, Anis",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85012057884",
      "Primary study DOI": "10.1016/j.jss.2017.01.032",
      "Title": "Generating reusable, searchable and executable  architecture constraints as services ",
      "Abstract": "Architecture constraints are components of design documentation. They enable designers to enforce rules that architecture descriptions should respect. Many systems make it possible to associate constraints to models at design stage but very few enable their association to code at implementation stage. When possible, this is done manually, which is a tedious, error prone and time consuming task. Therefore, we propose in this work a process to automatically generate executable constraints associated to programs’ code from model-based constraints. First, the process translates the constraints specified at design-time into constraint-components described with an ADL, called CLACS. Then, it creates constraint-services which can be registered and later invoked to check their embedded constraints on component- and service-based applications. We chose to target components and services in order to make architecture constraints reusable, searchable in registries, customizable and checkable at the implementation stage. The generated constraint-services use the standard reflective (meta) layer provided by the programing language to introspect elements of the architecture. We experimented our work on a set of 15 architecture constraints and on a real-world system in order to evaluate the effectiveness of the process.",
      "Keywords": "Architecture constraint | Automatic translation | Constraint-component | Constraint-service | Introspection | OCL | OSGi",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Kallel, Sahar;Tramoni, Bastien;Tibermacine, Chouki;Dony, Christophe;Kacem, Ahmed Hadj",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85011396325",
      "Primary study DOI": "10.1016/j.jss.2017.01.016",
      "Title": "A similarity query system for road traffic data based on a NoSQL document store",
      "Abstract": "Advancements in sensing and communication technologies are enabling intelligent transportation systems (ITS) to easily acquire large volumes of road traffic big data. Querying road traffic data is a crucial task for providing citizens with more insightful information on traffic conditions. In this paper, we have developed a similarity query system for road traffic big data, called SigTrac, that runs on top of an existing MongoDB document store. The SigTrac system represents road traffic sensor data having spatio-temporal characteristics into traffic matrices and stores them into a MongoDB NoSQL document store by exploiting map-reduce operations of MongoDB. In addition, SigTrac efficiently processes similarity queries for traffic data with singular value decomposition (SVD)-based and incremental SVD-based algorithms. Our experimental studies with real traffic data demonstrate the efficiency of SiqTrac for similarity query processing for road traffic big data.",
      "Keywords": "MongoDB | NoSQL database | Road traffic data | Similarity queries | SVD-based query processing | Traffic matrices",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Damaiyanti, Titus Irma;Imawan, Ardi;Indikawati, Fitri Indra;Choi, Yoon Ho;Kwon, Joonho",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013224881",
      "Primary study DOI": "10.1016/j.jss.2017.02.003",
      "Title": "The continuity of continuous integration: Correlations and consequences",
      "Abstract": "The practice of continuous integration has firmly established itself in the mainstream of the software engineering industry, yet many questions surrounding it remain unanswered. Prominent among these is the issue of scalability: continuous integration has been reported to be possible to scale, but with difficulties. Understanding of the underlying mechanisms causing these difficulties is shallow, however: what is it about size that is problematic, which kind of size, and what aspect of continuous integration does it impede? Based on quantitative data from six industry cases encompassing close to 2000 engineers, complemented by interviews with engineers from five companies, this paper investigates the correlation between the continuity of continuous integration and size. It is found that not only is there indeed a correlation between the size and composition of a development organization and its tendency to integrate continuously; there is evidence that the size of the organization influences ways of working, which in turn correlate with the degree of continuity, raising the question of software manufacturability. It is further observed that developer behavior in ostensibly continuously integrating cases does not necessarily match expectations, and that frequent integration of the product itself does not automatically imply that each individual developer commits frequently.",
      "Keywords": "Agile | Complexity | Continuity | Continuous integration | Developability | Manufacturability | Modularity | Scale | Size",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Ståhl, Daniel;Mårtensson, Torvald;Bosch, Jan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013230578",
      "Primary study DOI": "10.1016/j.jss.2017.02.004",
      "Title": "Energy minimization for on-line real-time scheduling with reliability awareness",
      "Abstract": "Under current development of semiconductor technology, there is an exponential increase in transistor density on a single processing chip. This aggressive transistor integration significantly boosts the computing performance. However, it also results in a power explosion, which immediately decreases the system reliability. Moreover, some well-known power/energy reduction techniques, i.e. Dynamic Voltage and Frequency Scaling (DVFS), can cause adverse impact on system reliability. How to effectively manage the power/energy consumption, meanwhile keep the system reliability under control, is critical for the design of high performance computing systems. In this paper, we present an online power management approach to minimize the energy consumption for single processor real-time scheduling under reliability constraint. We formally prove that the proposed algorithm can guarantee the system reliability requirement. Our simulation results show that, by exploiting the run-time dynamics, the proposed approach can achieve more energy savings over previous work under reliability constraint.",
      "Keywords": "Energy minimization | Multi-core systems | Real-time scheduling | Reliability",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Fan, Ming;Han, Qiushi;Yang, Xiaokun",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84981719834",
      "Primary study DOI": "10.1016/j.jss.2016.08.037",
      "Title": "Proactive elasticity and energy awareness in data stream processing",
      "Abstract": "Data stream processing applications have a long running nature (24 hr/7 d) with workload conditions that may exhibit wide variations at run-time. Elasticity is the term coined to describe the capability of applications to change dynamically their resource usage in response to workload fluctuations. This paper focuses on strategies for elastic data stream processing targeting multicore systems. The key idea is to exploit Model Predictive Control, a control-theoretic method that takes into account the system behavior over a future time horizon in order to decide the best reconfiguration to execute. We design a set of energy-aware proactive strategies, optimized for throughput and latency QoS requirements, which regulate the number of used cores and the CPU frequency through the Dynamic Voltage and Frequency Scaling (DVFS) support offered by modern multicore CPUs. We evaluate our strategies in a high-frequency trading application fed by synthetic and real-world workload traces. We introduce specific properties to effectively compare different elastic approaches, and the results show that our strategies are able to achieve the best outcome.",
      "Keywords": "Data stream processing | Elasticity | Frequency scaling | Model predictive control",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "De Matteis, Tiziano;Mencagli, Gabriele",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85011930962",
      "Primary study DOI": "10.1016/j.jss.2017.01.029",
      "Title": "Designing and applying an approach to software architecting in agile projects in education",
      "Abstract": "Software architecting activities are not discussed in most agile software development methods. That is why, the combination of software architecting and agile methods has been in the focus of numerous publications. However, there is little literature on how to approach software architecting in agile projects in education. In this paper, we present our approach to the introduction of software architecting activities in an agile project course. The approach is based on literature sources and is tailored to fit our educational goals and context. The approach has been applied in two consecutive executions of the course. We observe improved understanding on the value of architecting activities and appreciation among students on the combination of architecting activities and agile development. We applied the approach predominantly in cases with an architecturally savvy Product Owner. Further research is required to understand how the approach performs in scenarios with architecturally unsavvy Product Owners and if it needs to be adapted for these scenarios. We also conclude that more research is needed on the challenges that architects face in agile projects in order to better prepare students for practice.",
      "Keywords": "Agile | Course | Project | Scrum | Software architecture | Software engineering education | Students | Teaching",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Angelov, S.;de Beer, P.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85006516228",
      "Primary study DOI": "10.1016/j.jss.2016.06.010",
      "Title": "Self-adaptive processing graph with operator fission for elastic stream processing",
      "Abstract": "Nowadays, information generated by the Internet interactions is growing exponentially, creating massive and continuous flows of events from the most diverse sources. These interactions contain valuable information for domains such as government, commerce, and banks, among others. Extracting information in near real-time from such data requires powerful processing tools to cope with the high-velocity and the high-volume stream of events. Specially designed distributed processing engines build a graph-based topology of a static number of processing operators creating bottlenecks and load balance problems when processing dynamic flows of events. In this work we propose a self-adaptive processing graph that provides elasticity and scalability by automatically increasing or decreasing the number of processing operators to improve performance and resource utilization of the system. Our solution uses a model that monitors, analyzes and changes the graph topology with a control algorithm that is both reactive and proactive to the flow of events. We have evaluated our solution with three stream processing applications and results show that our model can adapt the graph topology when receiving events at high rate with sudden peaks, producing very low costs of memory and CPU usage.",
      "Keywords": "Elastic processing | S4 | Scalable processing | Self-adaptable graph | Stream processing",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Hidalgo, Nicolas;Wladdimiro, Daniel;Rosas, Erika",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85011114814",
      "Primary study DOI": "10.1016/j.jss.2017.01.007",
      "Title": "Container-based virtual elastic clusters",
      "Abstract": "eScience demands large-scale computing clusters to support the efficient execution of resource-intensive scientific applications. Virtual Machines (VMs) have introduced the ability to provide customizable execution environments, at the expense of performance loss for applications. However, in recent years, containers have emerged as a light-weight virtualization technology compared to VMs. Indeed, the usage of containers for virtual clusters allows better performance for the applications and fast deployment of additional working nodes, for enhanced elasticity. This paper focuses on the deployment, configuration and management of Virtual Elastic computer Clusters (VEC) dedicated to process scientific workloads. The nodes of the scientific cluster are hosted in containers running on bare-metal machines. The open-source tool Elastic Cluster for Docker (EC4Docker) is introduced, integrated with Docker Swarm to create auto-scaled virtual computer clusters of containers across distributed deployments. We also discuss the benefits and limitations of this solution and analyse the performance of the developed tools under a real scenario by means of a scientific use case that demonstrates the feasibility of the proposed approach.",
      "Keywords": "Cluster computing | Computing | Containers | Elasticity",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "de Alfonso, Carlos;Calatrava, Amanda;Moltó, Germán",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85006469957",
      "Primary study DOI": "10.1016/j.jss.2016.06.009",
      "Title": "Data stream classification using random feature functions and novel method combinations",
      "Abstract": "Big Data streams are being generated in a faster, bigger, and more commonplace. In this scenario, Hoeffding Trees are an established method for classification. Several extensions exist, including high-performing ensemble setups such as online and leveraging bagging. Also, k-nearest neighbors is a popular choice, with most extensions dealing with the inherent performance limitations over a potentially-infinite stream. At the same time, gradient descent methods are becoming increasingly popular, owing in part to the successes of deep learning. Although deep neural networks can learn incrementally, they have so far proved too sensitive to hyper-parameter options and initial conditions to be considered an effective ‘off-the-shelf’ data-streams solution. In this work, we look at combinations of Hoeffding-trees, nearest neighbor, and gradient descent methods with a streaming preprocessing approach in the form of a random feature functions filter for additional predictive power. We further extend the investigation to implementing methods on GPUs, which we test on some large real-world datasets, and show the benefits of using GPUs for data-stream learning due to their high scalability. Our empirical evaluation yields positive results for the novel approaches that we experiment with, highlighting important issues, and shed light on promising future directions in approaches to data-stream classification.",
      "Keywords": "Big data | Classification | Data stream mining | GPUs",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Marrón, Diego;Read, Jesse;Bifet, Albert;Navarro, Nacho",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85002818117",
      "Primary study DOI": "10.1016/j.jss.2016.07.003",
      "Title": "The technology of processing intensive structured dataflow on a supercomputer",
      "Abstract": "Modern experimental setups generate prolonged and intense data streams. For example, non-contact measurement techniques PIV (Particle Image Velocimetry), based on continuous image processing, are widely used in the experimental aerodynamics and hydrodynamics. These experimental stereo or volumetric velocimetry 3D PIV setups are able to generate long-lasting and intensive flows of images by using two or more cameras. High computational complexity of the image processing algorithms is the main limiting factor in the conditions of the low computational performance of PIV setups itself. Removal of these limitations by moving image processing tasks on a remote supercomputer by using our proposed technology “Distributed PIV”, which will allow users to apply their new high-precision parallel algorithms in a real-time and implement feedback to the experimental setup. This paper describes the innovative technology for high-performance processing of the intensive flows of the structured data generated by an experimental setup and delivered through a high-speed DWDM backbone directly into computing nodes of a remote supercomputer. A high BDP (Bandwidth-Delay Product) problem case in the design of protocols such as TCP in respect of performance tuning to achieve maximum network throughput is solved by designed middleware.",
      "Keywords": "BDP | Experimental setup | Intense dataflow | Optical network | Queue manager | Supercomputer | TCP",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Shchapov, Vladislav A.;Masich, Aleksei G.;Masich, Grigorii F.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84994160270",
      "Primary study DOI": "10.1016/j.jss.2016.08.036",
      "Title": "Recursive prediction algorithm for non-stationary Gaussian Process",
      "Abstract": "Gaussian Process is a theoretically rigorous model for prediction problems. One of the deficiencies of this model is that its original exact inference algorithm is computationally intractable. Therefore, its applications are limited in the field of real-time online predictions. In this paper, a recursive prediction algorithm based on the Gaussian Process model is proposed. In recursive algorithms, the computational time of the next step can be greatly reduced by utilizing the intermediate results of the current step. The proposed recursive algorithm accelerates the prediction and avoids the loss of accuracy at the same time. Experiments are done on an ultra-short term electric load data set and the results are demonstrated to show the accuracy and efficiency of the new algorithm.",
      "Keywords": "",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Zhang, Yulai;Luo, Guiming",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85012069104",
      "Primary study DOI": "10.1016/j.jss.2017.01.027",
      "Title": "Bayesian network model for task effort estimation in agile software development",
      "Abstract": "Even though the use of agile methods in software development is increasing, the problem of effort estimation remains quite a challenge, mostly due to the lack of many standard metrics to be used for effort prediction in plan-driven software development. The Bayesian network model presented in this paper is suitable for effort prediction in any agile method. Simple and small, with inputs that can be easily gathered, the suggested model has no practical impact on agility. This model can be used as early as possible, during the planning stage. The structure of the proposed model is defined by the authors, while the parameter estimation is automatically learned from a dataset. The data are elicited from completed agile projects of a single software company. This paper describes various statistics used to assess the precision of the model: mean magnitude of relative error, prediction at level m, accuracy (the percentage of successfully predicted instances over the total number of instances), mean absolute error, root mean squared error, relative absolute error and root relative squared error. The obtained results indicate very good prediction accuracy.",
      "Keywords": "Agile software development | Bayesian network | Effort prediction",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Dragicevic, Srdjana;Celar, Stipe;Turic, Mili",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85008324741",
      "Primary study DOI": "10.1016/j.jss.2016.06.011",
      "Title": "Issues in complex event processing: Status and prospects in the Big Data era",
      "Abstract": "Many Big Data technologies were built to enable the processing of human generated data, setting aside the enormous amount of data generated from Machine-to-Machine (M2M) interactions and Internet-of-Things (IoT) platforms. Such interactions create real-time data streams that are much more structured, often in the form of series of event occurrences. In this paper, we provide an overview on the main research issues confronted by existing Complex Event Processing (CEP) techniques, with an emphasis on query optimization aspects. Our study expands on both deterministic and probabilistic event models and spans from centralized to distributed network settings. In that, we cover a wide range of approaches in the CEP domain and review the current status of techniques that tackle efficient query processing. These techniques serve as a starting point for developing Big Data oriented CEP applications. Therefore, we further study the issues that arise upon trying to apply those techniques over Big Data enabling technologies, as is the case with cloud platforms. Furthermore, we expand on the synergies among Predictive Analytics and CEP with an emphasis on scalability and elasticity considerations in cloud platforms with potentially dispersed resource pools.",
      "Keywords": "Cloud computing | Complex event processing | Predictive analytics",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Flouris, Ioannis;Giatrakos, Nikos;Deligiannakis, Antonios;Garofalakis, Minos;Kamp, Michael;Mock, Michael",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85011850905",
      "Primary study DOI": "10.1016/j.jss.2017.01.026",
      "Title": "A mapping study on design-time quality attributes and metrics",
      "Abstract": "Developing a plan for monitoring software quality is a non-trivial task, in the sense that it requires: (a) the selection of relevant quality attributes, based on application domain and development phase, and (b) the selection of appropriate metrics to quantify quality attributes. The metrics selection process is further complicated due to the availability of various metrics for each quality attribute, and the constraints that impact metric selection (e.g., development phase, metric validity, and available tools). In this paper, we shed light on the state-of-research of design-time quality attributes by conducting a mapping study. We have identified 154 papers that have been included as primary studies. The study led to the following outcomes: (a) low-level quality attributes (e.g., cohesion, coupling, etc.) are more frequently studied than high-level ones (e.g., maintainability, reusability, etc.), (b) maintainability is the most frequently examined high-level quality attribute, regardless of the application domain or the development phase, (c) assessment of quality attributes is usually performed by a single metric, rather than a combination of multiple metrics, and (d) metrics are mostly validated in an empirical setting. These outcomes are interpreted and discussed based on related work, offering useful implications to both researchers and practitioners.",
      "Keywords": "Design-time quality attributes | Mapping study | Measurement | Software quality",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Arvanitou, Elvira Maria;Ampatzoglou, Apostolos;Chatzigeorgiou, Alexander;Galster, Matthias;Avgeriou, Paris",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85008336840",
      "Primary study DOI": "10.1016/j.jss.2016.06.016",
      "Title": "Employing traditional machine learning algorithms for big data streams analysis: The case of object trajectory prediction",
      "Abstract": "In this paper, we model the trajectory of sea vessels and provide a service that predicts in near-real time the position of any given vessel in 4′, 10′, 20′ and 40′ time intervals. We explore the necessary tradeoffs between accuracy, performance and resource utilization is explored given the large volume and update rates of input data. We start with building models based on well-established machine learning algorithms using static datasets and multi-scan training approaches and identify the best candidate to be used in implementing a single-pass predictive approach, under real-time constraints. The results are measured in terms of accuracy and performance and are compared against the baseline kinematic equations. Results show that it is possible to efficiently model the trajectory of multiple vessels using a single model, which is trained and evaluated using an adequately large, static dataset, thus achieving a significant gain in terms of resource usage while not compromising accuracy.",
      "Keywords": "Data streams | Machine learning | Real-time query response | Trajectory prediction",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Valsamis, Angelos;Tserpes, Konstantinos;Zissis, Dimitrios;Anagnostopoulos, Dimosthenis;Varvarigou, Theodora",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85006357039",
      "Primary study DOI": "10.1016/j.jss.2016.06.008",
      "Title": "Repeating patterns as symbols for long time series representation",
      "Abstract": "Over the past years, many representations for time series were proposed with the main purpose of dimensionality reduction and as a support for various algorithms in the domain of time series data processing. However, most of the transformation algorithms are not directly applicable on streams of data but only on static collections of the data as they are iterative in their nature. In this work we propose a symbolic representation of time series along with a method for transformation of time series data into the proposed representation. As one of the basic requirements for applicable representation is the distance measure which would accurately reflect the true shape of the data, we propose a distance measure operating on the proposed representation and lower bounding the Euclidean distance on the original data. We evaluate properties of the proposed representation and the distance measure on the UCR collection of datasets. As we focus on stream data processing, we evaluate the properties and limitations of the proposed representation on very long time series from the domain of electricity consumption monitoring, simulating the processing of potentially unbound data stream.",
      "Keywords": "Lower bound | Stream processing | Symbolic representation | Time series representation",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Sevcech, Jakub;Bielikova, Maria",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85002530328",
      "Primary study DOI": "10.1016/j.jss.2016.07.004",
      "Title": "HB2DS: A behavior-driven high-bandwidth network mining system",
      "Abstract": "This paper proposes a behavior detection system, HB2DS, to address the behavior-detection challenges in high-bandwidth networks. In HB2DS, a summarization of network traffic is represented through some meta-events. The relationships amongst meta-events are used to mine end-user behaviors. HB2DS satisfies the main constraints exist in analyzing of high-bandwidth networks, namely online learning and outlier handling, as well as one-pass processing, delay, and memory limitations. Our evaluation indicates significant improvement in big data stream analyzing in terms of accuracy and efficiency.",
      "Keywords": "Behavior detection | big data stream | Data stream clustering | Network analysis",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Noferesti, Morteza;Jalili, Rasool",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85006368896",
      "Primary study DOI": "10.1016/j.jss.2016.06.012",
      "Title": "Large scale opinion mining for social, news and blog data",
      "Abstract": "Companies that collect and analyze data from social media, news and other data streams are faced with several challenges that concern storage and processing of huge amounts of data. When they want to serve the processed information to their customers and moreover, when they want to cover different information needs for each customer, they need solutions that process data in near real time in order to gain insights on the data in motion. The volume and volatility of opinionated data that is published in social media, in combination with the variety of data sources has created a demanding ecosystem for stream processing. Although, there are several solutions that can handle information of static nature and small volume quite efficiently, they usually do not scale up properly because of their high complexity. Moreover, such solutions have been designed to run once or to run in a fixed dataset and they are not sufficient for processing huge volumes of streamed data. To address this problem, a platform for real-time opinion mining is proposed. Based on prior research and real application services that have been developed, a new platform called “PaloPro” is presented to cover the needs for brand monitoring.",
      "Keywords": "News streams | Opinion mining | Social media",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Tsirakis, Nikos;Poulopoulos, Vasilis;Tsantilas, Panagiotis;Varlamis, Iraklis",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85002827336",
      "Primary study DOI": "10.1016/j.jss.2016.07.005",
      "Title": "A survey on feature drift adaptation: Definition, benchmark, challenges and future directions",
      "Abstract": "Data stream mining is a fast growing research topic due to the ubiquity of data in several real-world problems. Given their ephemeral nature, data stream sources are expected to undergo changes in data distribution, a phenomenon called concept drift. This paper focuses on one specific type of drift that has not yet been thoroughly studied, namely feature drift. Feature drift occurs whenever a subset of features becomes, or ceases to be, relevant to the learning task; thus, learners must detect and adapt to these changes accordingly. We survey existing work on feature drift adaptation with both explicit and implicit approaches. Additionally, we benchmark several algorithms and a naive feature drift detection approach using synthetic and real-world datasets. The results from our experiments indicate the need for future research in this area as even naive approaches produced gains in accuracy while reducing resources usage. Finally, we state current research topics, challenges and future directions for feature drift adaptation.",
      "Keywords": "Data stream mining | Feature drift | Feature selection",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Barddal, Jean Paul;Gomes, Heitor Murilo;Enembreck, Fabrício;Pfahringer, Bernhard",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85012296964",
      "Primary study DOI": "10.1016/j.jss.2017.01.005",
      "Title": "Profiling and accelerating commodity NFV service chains with SCC",
      "Abstract": "Recent approaches to network functions virtualization (NFV) have shown that commodity network stacks and drivers struggle to keep up with increasing hardware speed. Despite this, popular cloud networking services still rely on commodity operating systems (OSs) and device drivers. Taking into account the hardware underlying of commodity servers, we built an NFV profiler that tracks the movement of packets across the system's memory hierarchy by collecting key hardware and OS-level performance counters. Leveraging the profiler's data, our Service Chain Coordinator's (SCC) run-time accelerates user-space NFV service chains, based on commodity drivers. To do so, SCC combines multiplexing of system calls with scheduling strategies, taking time, priority, and processing load into account. By granting longer time quanta to chained network functions (NFs), combined with I/O multiplexing, SCC reduces unnecessary scheduling and I/O overheads, resulting in three-fold latency reduction due to cache and main memory utilization improvements. More importantly, SCC reduces the latency variance of NFV service chains by up to 40x compared to standard FastClick chains by making the average case for an NFV chain to perform as well as the best case. These improvements are possible because of our profiler's accuracy.",
      "Keywords": "I/O multiplexing | NFV | Profiler | Scheduling | Service chains",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Katsikas, Georgios P.;Maguire, Gerald Q.;Kostić, Dejan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85009383078",
      "Primary study DOI": "10.1016/j.jss.2017.01.003",
      "Title": "DOTS: An online and near-optimal trajectory simplification algorithm",
      "Abstract": "The last decade witnessed an increasing number of location acquisition equipments such as mobile phone, smart watch etc. Trajectory data is collected with such a high speed that the location based services (LBS) meet challenge to process and take advantage of that big data. Good trajectory simplification (TS) algorithm thus plays an important role for both LBS providers and users as it significantly reduces processing and response time by minimizing the trajectory size with acceptable precision loss. State of the art TS algorithms work in batch mode and are not suitable for streaming data. The online solutions, on the other hand, usually use some heuristics which won't hold the optimality. This paper proposed a Directed acyclic graph based Online Trajectory Simplification (DOTS) method which solves the problem by optimization. Time complexity of DOTS is O(N2/M). A cascaded version of DOTS with time complexity of O(N) is also proposed. To our best knowledge, this is the first time that an optimal and online TS method is proposed. We find both the normal and cascaded DOTS outperform current TS methods like Douglas–Peucker (Douglas and Peucker, 1973), SQUISH (Muckell et al., 2011) etc. with pretty big margin.",
      "Keywords": "Data management | Directed acyclic graph | GPS | Location based services | Priority queue | Trajectory simplification",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Cao, Weiquan;Li, Yunzhao",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85009982914",
      "Primary study DOI": "10.1016/j.jss.2017.01.004",
      "Title": "Development and use of a new task model for cyber-physical systems: A real-time scheduling perspective",
      "Abstract": "In a typical cyber-physical system (CPS), the cyber/computation subsystem controls the physical subsystem, and therefore the computer society has recently paid considerable attention to CPS research. To keep such a CPS stable, feedback control with periodic computation tasks has been widely used, and its theoretical guarantee of stability has been made with periodic real-time task models that enforce strict periodic control updates. However, some control update misses are usually allowed (e.g., via system over-design) in certain physical subsystem states (PSSes) without causing system instability, and the resources required for strict periodic control updates can thus be reduced or used for other purposes, achieving efficient controls for the entire CPS in terms of the operational cost, such as fuel consumption or tracking accuracy. In this paper, we propose a new periodic, fault-tolerant CPS task model, which not only expresses efficiency and stability of the underlying physical subsystem, but also generalizes existing periodic real-time task models, by capturing a tolerable number of control update misses in different PSSes. To demonstrate the utility of this model, we develop a new scheduling mechanism that prioritizes jobs (i.e., periodic invocations) of a set of tasks not only by the nature of each task, but also by the number of consecutive prior job deadline misses. Based on its analysis in terms of stability and efficiency, we also propose a priority-assignment policy that lowers the system operation cost without compromising stability. Our in-depth analysis and simulation results show that the scheduling mechanism and its analysis, as well as the priority-assignment policy under the proposed model not only generalize the existing periodic real-time task models, but also significantly lower the system operation cost without losing stability.",
      "Keywords": "Cyber-physical systems | Periodic real-time task model | Priority assignment | Real-time scheduling | Task-level fixed-priority scheduling",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Lee, Jinkyu;Shin, Kang G.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85009268752",
      "Primary study DOI": "10.1016/j.jss.2016.12.036",
      "Title": "Design annotations to improve API discoverability",
      "Abstract": "User studies have revealed that programmers face several obstacles when learning application programming interfaces (APIs). A considerable part of such difficulties relate to discovery of API elements and the relationships among them. To address discoverability problems, we show how to complement APIs with design annotations, which document design decisions in a program-processable form for types, methods, and parameters. The information provided by the annotations is consumed by the integrated development environment (IDE) in order to assist API users with useful code completion proposals regarding object creation and manipulation, which facilitate API exploration and learning. As a proof of concept, we developed Dacite, a tool which comprises a set of Java annotations and an accompanying plugin for the Eclipse IDE. A user study revealed that Dacite is usable and effective, and Dacite's proposals enable programmers to be more successful in solving programming tasks involving unfamiliar APIs.",
      "Keywords": "Annotations | API usability | Code completion | Eclipse | IDE",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Santos, André L.;Myers, Brad A.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85006086282",
      "Primary study DOI": "10.1016/j.jss.2016.09.043",
      "Title": "DRank: A semi-automated requirements prioritization method based on preferences and dependencies",
      "Abstract": "There are many types of dependencies between software requirements, such as the contributions dependencies (Make, Some+, Help, Break, Some-, Hurt) and business dependencies modeled in the i* framework. However, current approaches for prioritizing requirements seldom take these dependencies into consideration, because it is difficult for stakeholders to prioritize requirements considering their preferences as well as the dependencies between requirements. To make requirement prioritization more practical, a method called DRank is proposed. DRank has the following advantages: 1) a prioritization evaluation attributes tree is constructed to make the ranking criteria selection easier and more operable; 2) RankBoost is employed to calculate the subjective requirements prioritization according to stakeholder preferences, which reduces the difficulty of evaluating the prioritization; 3) an algorithm based on the weighted PageRank is proposed to analyze the dependencies between requirements, allowing the objective dependencies to be automatically transformed into partial order relations; and 4) an integrated requirements prioritization method is developed to amend the stakeholders’ subjective preferences with the objective requirements dependencies and make the process of prioritization more reasonable and applicable. A controlled experiment performed to validate the effectiveness of DRank based on comparisons with Case Based Ranking, Analytical Hierarchy Process, and EVOLVE. The results demonstrate that DRank is less time-consuming and more effective than alternative approaches. A simulation experiment demonstrates that taking requirement dependencies into consideration can improve the accuracy of the final prioritization sequence.",
      "Keywords": "Link analysis | Machine learning | Requirements dependency | Software requirements prioritization",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Shao, Fei;Peng, Rong;Lai, Han;Wang, Bangchao",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84963553383",
      "Primary study DOI": "10.1016/j.jss.2016.03.061",
      "Title": "How effectively can spreadsheet anomalies be detected: An empirical study",
      "Abstract": "While spreadsheets are widely used, they have been found to be error-prone. Various techniques have been proposed to detect anomalies in spreadsheets, with varying scopes and effectiveness. Nevertheless, there is no empirical study comparing these techniques’ practical usefulness and effectiveness. In this work, we conducted a large-scale empirical study of three state-of-the-art techniques on their effectiveness in detecting spreadsheet anomalies. Our study focused on the precision, recall rate, efficiency and scope. We found that one technique outperforms the other two in precision and recall rate of spreadsheet anomaly detection. Efficiency of the three techniques is acceptable for most spreadsheets, but they may not be scalable to large spreadsheets with complex formulas. Besides, they have different scopes for detecting different spreadsheet anomalies, thus complementing to each other. We also discussed limitations of these three techniques. Based on our findings, we give suggestions for future spreadsheet research.",
      "Keywords": "Anomaly detection | Empirical study | Spreadsheet",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Zhang, Ruiqing;Xu, Chang;Cheung, S. C.;Yu, Ping;Ma, Xiaoxing;Lu, Jian",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85002870388",
      "Primary study DOI": "10.1016/j.jss.2016.06.061",
      "Title": "Assignment strategies for ground truths in the crowdsourcing of labeling tasks",
      "Abstract": "It is expected that ground truths can result in many good labels in the crowdsourcing of labeling tasks. However, the use of ground truths has so far not been adequately addressed. In this paper, we develop algorithms that determine the number of ground truths that are necessary. We determine this number by iteratively calculating the expected quality of labels for tasks with various sets of ground truths, and then comparing the quality with the limit of the estimated label quality expected to be obtained by crowdsourcing. We assume that each worker has a different unknown labeling ability and performs a different number of tasks. Under this assumption, we develop assignment strategies for ground truths based on the estimated confidence intervals of the workers. Our algorithms can utilize different approaches based on the expectation maximization to estimate good-quality consensus labels. An experimental evaluation demonstrates that our algorithms work well in various situations.",
      "Keywords": "Condorcet jury theorem | Confidence interval | Expectation maximization algorithm | Human computation",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Kubota, Takuya;Aritsugi, Masayoshi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84966376822",
      "Primary study DOI": "10.1016/j.jss.2016.04.002",
      "Title": "A metamorphic testing approach for supporting program repair without the need for a test oracle",
      "Abstract": "Test suite based automated program repair (APR) relies on a test oracle to determine the execution result of individual test cases. The applicability of APR techniques, therefore, is limited by the fact that test oracles may not exist. Metamorphic Testing (MT) is a testing approach that, rather than checking the correctness of individual test outputs, checks testing results through verification of relations among multiple test cases and their outputs: MT can therefore be applied without test oracles. This paper presents an integration of MT with APR that enables application of APR without the need for a test oracle. Two important issues for this integration which have been thoroughly investigated and addressed are: (1) feasibility — which is addressed by proposing a framework to support the integration, and then presenting MT-GenProg, a tool incorporating MT with the popular APR technique GenProg; and (2) effectiveness — which is confirmed through an empirical study of GenProg and MT-GenProg on 1,143 program versions from the IntroClass benchmark suite, demonstrating MT-GenProg's comparable performance to GenProg, in terms of repair effectiveness. We conclude that the proposed integration is both practically feasible and effective, and thus successfully extends APR techniques to a broader application domain.",
      "Keywords": "Metamorphic testing | Test oracle | Test suite based automated program repair",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Jiang, Mingyue;Chen, Tsong Yueh;Kuo, Fei Ching;Towey, Dave;Ding, Zuohua",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84995569607",
      "Primary study DOI": "10.1016/j.jss.2016.09.015",
      "Title": "A survey of the use of crowdsourcing in software engineering",
      "Abstract": "The term ‘crowdsourcing’ was initially introduced in 2006 to describe an emerging distributed problem-solving model by online workers. Since then it has been widely studied and practiced to support software engineering. In this paper we provide a comprehensive survey of the use of crowdsourcing in software engineering, seeking to cover all literature on this topic. We first review the definitions of crowdsourcing and derive our definition of Crowdsourcing Software Engineering together with its taxonomy. Then we summarise industrial crowdsourcing practice in software engineering and corresponding case studies. We further analyse the software engineering domains, tasks and applications for crowdsourcing and the platforms and stakeholders involved in realising Crowdsourced Software Engineering solutions. We conclude by exposing trends, open issues and opportunities for future research on Crowdsourced Software Engineering.",
      "Keywords": "Crowdsourced software engineering | Crowdsourcing | Literature survey | Software crowdsourcing",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Mao, Ke;Capra, Licia;Harman, Mark;Jia, Yue",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85009186306",
      "Primary study DOI": "10.1016/j.jss.2017.01.001",
      "Title": "Understanding cloud-native applications after 10 years of cloud computing - A systematic mapping study",
      "Abstract": "It is common sense that cloud-native applications (CNA) are intentionally designed for the cloud. Although this understanding can be broadly used it does not guide and explain what a cloud-native application exactly is. The term “cloud-native” was used quite frequently in birthday times of cloud computing (2006) which seems somehow obvious nowadays. But the term disappeared almost completely. Suddenly and in the last years the term is used again more and more frequently and shows increasing momentum. This paper summarizes the outcomes of a systematic mapping study analyzing research papers covering “cloud-native” topics, research questions and engineering methodologies. We summarize research focuses and trends dealing with cloud-native application engineering approaches. Furthermore, we provide a definition for the term “cloud-native application” which takes all findings, insights of analyzed publications and already existing and well-defined terminology into account.",
      "Keywords": "Cloud-native application | CNA | Elastic platform | Microservice | Pattern | Self service | Softwareization | Systematic mapping study",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Kratzke, Nane;Quint, Peter Christian",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84979741017",
      "Primary study DOI": "10.1016/j.jss.2016.07.032",
      "Title": "ReSeer: Efficient search-based replay for multiprocessor virtual machines",
      "Abstract": "Efficient replay of virtual machines is important for software debugging, fault tolerance, and performance analysis. The current approaches of replaying virtual machines record the details of system execution at runtime. However, these approaches incur much overhead, which affects the system performance. Especially, in a multiprocessor system, recording the shared memory operations of multiple processors leads to a large amount of computing overhead and log files. To address the above issue, this paper proposes ReSeer—a search-based replay approach for multiprocessor virtual machines. ReSeer consists of three phases including record, search, and replay. In the record phase, we record only necessary non-deterministic events at runtime, and incrementally take memory checkpoints at a defined interval. In the search phase, we encode all the possible execution paths as binary strings, and use a genetic algorithm to search expected execution paths achieving the expected checkpoint. In the replay phase, we replay the system execution according to the searched execution paths and the logged non-deterministic events. Compared with current approaches, ReSeer significantly reduces performance overhead at runtime by searching expected execution paths instead of recording all the operations of accessing shared memory. We have implemented ReSeer, and then evaluated it with a series of typical benchmarks deployed on an open source virtual machine—Xen. The experimental results show that ReSeer can reduce the record overhead at runtime efficiently.",
      "Keywords": "Deterministic replay | Genetic algorithm | Memory checkpoint | Virtual machine | Xen",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Wang, Tao;Xu, Jiwei;Zhang, Wenbo;Zhang, Jianhua;Wei, Jun;Zhong, Hua",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85003666511",
      "Primary study DOI": "10.1016/j.jss.2016.06.060",
      "Title": "On scaling dynamic programming problems with a multithreaded tabling Prolog system",
      "Abstract": "Tabling is a powerful implementation technique that improves the declarativeness and expressiveness of traditional Prolog systems in dealing with recursion and redundant computations. It can be viewed as a natural tool to implement dynamic programming problems, where a general recursive strategy divides a problem in simple sub-problems that are often the same. When tabling is combined with multithreading, we have the best of both worlds, since we can exploit the combination of higher declarative semantics with higher procedural control. However, at the engine level, such combination for dynamic programming problems is very difficult to exploit in order to achieve execution scalability as we increase the number of running threads. In this work, we focus on two well-known dynamic programming problems, the Knapsack and the Longest Common Subsequence problems, and we discuss how we were able to scale their execution by using the multithreaded tabling engine of the Yap Prolog system. To the best of our knowledge, this is the first work showing a Prolog system to be able to scale the execution of multithreaded dynamic programming problems. Our experiments also show that our system can achieve comparable or even better speedup results than other parallel implementations of the same problems.",
      "Keywords": "Dynamic programming | Multithreading | Prolog | Scalability | Tabling",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Areias, Miguel;Rocha, Ricardo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85000786807",
      "Primary study DOI": "10.1016/j.jss.2016.11.036",
      "Title": "A new structure and access mechanism for secure and efficient XML data broadcast in mobile wireless networks",
      "Abstract": "Recently, the use of XML for data broadcasting in mobile wireless networks has gained many attentions. One of the most essential requirements for such networks is data confidentiality. In order to secure XML data broadcast in mobile wireless networks, mobile clients should obey a set of access authorizations specified on the original XML document. In such environments, mobile clients can only access authorized parts of encrypted XML stream based on their access authorizations. Several indexing methods have been proposed in order to have selective access to XML data over the XML stream. However, these indexing methods cannot be used for encrypted XML data. In this paper, we define a new structure for XML stream which supports data confidentiality of XML data over the wireless broadcast channel. We also define an access mechanism for our proposed structure to efficiently process XML queries over the encrypted XML stream. The experimental results demonstrate that the use of our proposed structure and access mechanism for XML data broadcast efficiently disseminates XML data in mobile wireless networks.",
      "Keywords": "Secure XML data broadcast | XML access control | XML query processing | XML stream",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Safabahar, Babak;Mirabi, Meghdad",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85007565914",
      "Primary study DOI": "10.1016/j.jss.2016.12.034",
      "Title": "A comparison framework for runtime monitoring approaches",
      "Abstract": "The full behavior of complex software systems often only emerges during operation. They thus need to be monitored at run time to check that they adhere to their requirements. Diverse runtime monitoring approaches have been developed in various domains and for different purposes. Their sheer number and heterogeneity, however, make it hard to find the right approach for a specific application or purpose. The aim of our research therefore was to develop a comparison framework for runtime monitoring approaches. Our framework is based on an analysis of the literature and existing taxonomies for monitoring languages and patterns. We use examples from existing monitoring approaches to explain the framework. We demonstrate its usefulness by applying it to 32 existing approaches and by comparing 3 selected approaches in the light of different monitoring scenarios. We also discuss perspectives for researchers.",
      "Keywords": "Comparison framework | Literature review | Runtime monitoring",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Rabiser, Rick;Guinea, Sam;Vierhauser, Michael;Baresi, Luciano;Grünbacher, Paul",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85000351129",
      "Primary study DOI": "10.1016/j.jss.2016.11.038",
      "Title": "Logical query optimization for Cloudera Impala system",
      "Abstract": "Cloudera Impala, an analytic database system for Apache Hadoop, has a severe problem with query plan generation: the system can only generate query plans in left-deep tree form, which restricts the ability of parallel execution. In this paper, we present a logical query optimization scheme for Impala system. First, an improved McCHyp (MinCutConservative Hypergraph) logical query plan generation algorithm is proposed for Impala system. It can reduce the plan generation time by introducing a pruning strategy. Second, a new cost model that takes the characteristics of Impala system into account is proposed. Finally, Impala system is extended to support query plans in bushy tree form by integrating the plan generation algorithm. We evaluated our scheme using TPC-DS test suit. Experimental results show that the extended Impala system generally performs better than the original system, and the improved plan generation algorithm has less execution time than McCHyp. In addition, our cost model fits better for Impala system, which supports query plans in bushy tree form.",
      "Keywords": "Bushy tree | Cloudera Impala system | Cost model | Logical query",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Ma, Jiaoyang;Chen, Ling;Lv, Mingqi;Yang, Yi;Zhao, Yuliang;Wu, Yong;Wang, Jingchang",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85002397974",
      "Primary study DOI": "10.1016/j.jss.2016.11.025",
      "Title": "Automatic verification and validation wizard in web-centred end-user software engineering",
      "Abstract": "This paper addresses one of the major web end-user software engineering (WEUSE) challenges, namely, how to verify and validate software products built using a life cycle enacted by end-user programmers. Few end-user development support tools implement an engineering life cycle adapted to the needs of end users. End users do not have the programming knowledge, training or experience to perform development tasks requiring creativity. Elsewhere we published a life cycle adapted to this challenge. With the support of a wizard, end-user programmers follow this life cycle and develop rich internet applications (RIA) to meet specific end-user requirements. However, end-user programmers regard verification and validation activities as being secondary or unnecessary for opportunistic programming tasks. Hence, although the solutions that they develop may satisfy specific requirements, it is impossible to guarantee the quality or the reusability of this software either for this user or for other developments by future end-user programmers. The challenge, then, is to find means of adopting a verification and validation workflow and adding verification and validation activities to the existing WEUSE life cycle. This should not involve users having to make substantial changes to the type of work that they do or to their priorities. In this paper, we set out a verification and validation life cycle supported by a wizard that walks the user through test case-based component, integration and acceptance testing. This wizard is well-aligned with WEUSE's characteristic informality, ambiguity and opportunisticity. Users applying this verification and validation process manage to find bugs and errors that they would otherwise be unable to identify. They also receive instructions for error correction. This assures that their composite applications are of better quality and can be reliably reused. We also report a user study in which users develop web software with and without a wizard to drive verification and validation. The aim of this user study is to confirm the applicability and effectiveness of our wizard in the verification and validation of a RIA.",
      "Keywords": "End-user programming | End-user software engineering | human-computer interaction | Reliability | visual programming | Web engineering",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Lizcano, David;Soriano, Javier;López, Genoveva;Gutiérrez, Javier J.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84958073935",
      "Primary study DOI": "10.1016/j.jss.2016.01.023",
      "Title": "Nebo: An efficient, parallel, and portable domain-specific language for numerically solving partial differential equations",
      "Abstract": "This paper presents Nebo, a declarative domain-specific language embedded in C++ for discretizing partial differential equations for transport phenomena on multiple architectures. Application programmers use Nebo to write code that appears sequential but can be run in parallel, without editing the code. Currently Nebo supports single-thread execution, multi-thread execution, and many-core (GPU-based) execution. With single-thread execution, Nebo performs on par with code written by domain experts. With multi-thread execution, Nebo can linearly scale (with roughly 90% efficiency) up to 12 cores, compared to its single-thread execution. Moreover, Nebo's many-core execution can be over 140x faster than its single-thread execution.",
      "Keywords": "Domain-specific language embedded in C++ | GPGPU",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Earl, Christopher;Might, Matthew;Bagusetty, Abhishek;Sutherland, James C.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84955312939",
      "Primary study DOI": "10.1016/j.jss.2015.12.050",
      "Title": "Critical-blame analysis for OpenMP 4.0 offloading on Intel Xeon Phi",
      "Abstract": "Recent supercomputers rated in the TOP 500 list increasingly utilize accelerator or co-processor devices to improve performance and energy efficiency. Since version 4.0 of the specification OpenMP addresses this heterogeneity in computing with the target directives, which enable programmers to offload portions of the code to massively-parallel target devices. Due to this new complexity in hardware and software design, performance optimization of large-scale parallel programs becomes more and more challenging. As manual performance analysis is getting infeasible for complex high performance computing (HPC) codes, we propose an approach to automatically detect bottlenecks such as load imbalances in heterogeneous OpenMP applications. We developed a method to perform critical-path and root-cause analysis for the OpenMP 4.0 offloading model and integrated it into the tool CASITA. The post-mortem analysis is based on execution traces that are generated with an implementation of the evolving OpenMP Tools Interface into the measurement system Score-P. To validate the implementation of our method we ported several existing codes to OpenMP 4.0 , executed them with an Intel Xeon Phi as the target device and analyzed the resulting trace files.",
      "Keywords": "Offloading | OpenMP | Performance analysis",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Dietrich, Robert;Schmitt, Felix;Grund, Alexander;Stolle, Jonas",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85008895600",
      "Primary study DOI": "10.1016/j.jss.2016.12.037",
      "Title": "Versatile workload-aware power management performability analysis of server virtualized systems",
      "Abstract": "The widespread integration of virtualization technologies in data centers has enabled in the last few years several benefits in terms of operating costs and flexibility. These benefits maybe boosted through join optimization of power management (PM) and dependability for virtualized systems. This indeed involves developing appropriate models to better understand their performability behavior whenever they are exposed to predictable (e.g. rejuvenation) and unpredictable breakdowns. We propose in this paper a performability analysis of server virtualized systems (SVSs) using a workload-aware PM mechanism based on non-Markovian Stochastic Reward Nets (SRNs) modeling approach. This analysis investigates interactions and correlations between several modules involving workload-aware PM mechanism, dynamic speed scaling processing, virtual machine (VM) and virtual machine monitor (VMM) both subject to software aging, failure and rejuvenation. We show through numerical results, using quantitative and qualitative metrics, how performance, power usage and efficiency are impacted by workload-aware PM mechanism. We show also how judicious choice of tunable attribute (i.e. Timeout) of the proposed PM mechanism with respect to workload can lead to a good power-performance trade-off.",
      "Keywords": "Performability | Power-performance trade-off | SRNs | SVSs | Virtualization | Workload-aware PM",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Escheikh, Mohamed;Barkaoui, Kamel;Jouini, Hana",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85002263399",
      "Primary study DOI": "10.1016/j.jss.2016.11.035",
      "Title": "Discovering partial periodic-frequent patterns in a transactional database",
      "Abstract": "Time and frequency are two important dimensions to determine the interestingness of a pattern in a database. Periodic-frequent patterns are an important class of regularities that exist in a database with respect to these two dimensions. Current studies on periodic-frequent pattern mining have focused on discovering full periodic-frequent patterns, i.e., finding all frequent patterns that have exhibited complete cyclic repetitions in a database. However, partial periodic-frequent patterns are more common due to the imperfect nature of real-world. This paper proposes a flexible and generic model to find partial periodic-frequent patterns. A new interesting measure, periodic-ratio, has been introduced to determine the periodic interestingness of a frequent pattern by taking into account its proportion of cyclic repetitions in a database. The proposed patterns do not satisfy the anti-monotonic property. A novel pruning technique has been introduced to reduce the search space effectively. A pattern-growth algorithm to find all partial periodic-frequent patterns has also been presented in this paper. Experimental results demonstrate that the proposed model can discover useful information, and the algorithm is efficient.",
      "Keywords": "Algorithms | Data mining | Knowledge discovery in databases | Partial periodicity | Pattern mining",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Kiran, R. Uday;Venkatesh, J. N.;Toyoda, Masashi;Kitsuregawa, Masaru;Reddy, P. Krishna",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85003875563",
      "Primary study DOI": "10.1016/j.jss.2016.11.027",
      "Title": "A systematic literature review: Opinion mining studies from mobile app store user reviews",
      "Abstract": "As mobile devices have overtaken fixed Internet access, mobile applications and distribution platforms have gained in importance. App stores enable users to search for, purchase and install mobile applications and then give feedback in the form of reviews and ratings. A review might contain information about the user's experience with the app and opinion of it, feature requests and bug reports. Hence, reviews are valuable not only to users who would like to find out what others think about an app, but also to developers and software companies interested in customer feedback. The rapid increase in the number of applications and total app store revenue has accelerated app store data mining and opinion aggregation studies. While development companies and app store regulators have pursued upfront opinion mining studies for business intelligence and marketing purposes, research interest into app ecosystem and user reviews is relatively new. In addition to studies examining online product reviews, there are now some academic studies focused on mobile app stores and user reviews. The objectives of this systematic literature review are to identify proposed solutions for mining online opinions in app store user reviews, challenges and unsolved problems in the domain, any new contributions to software requirements evolution and future research direction.",
      "Keywords": "App stores opinion mining | Mobile application | Requirements engineering | Systematic literature review",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Genc-Nayebi, Necmiye;Abran, Alain",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85007418329",
      "Primary study DOI": "10.1016/j.jss.2016.12.019",
      "Title": "Design pattern-based model transformation supported by QVT",
      "Abstract": "A design pattern helps to improve the quality of a software system by providing a proven solution for recurring design problems. However, the abstract and informal nature of prevailing pattern descriptions makes it difficult to use design patterns. There have been significant works on formalizing design patterns which found a base for systematic application of a design pattern. Pattern-based model transformation has emerged as an approach for incorporating pattern properties into a design model. However, the existing work mostly focuses on the solution domain of a pattern while leaving out the problem domain, structural pattern aspects with little attention to behavioral aspects, and general methodologies without concrete implementations. In this work, we present an approach for transforming an application model using both the structural and behavioral properties of a design pattern defined in terms of the problem and solution domain and its implementation using Query/View/Transformation (QVT). In the approach, we define pattern consistency for structural and behavioral pattern properties and pattern conformance for pattern applicability before transformation solution conformance after transformation. We demonstrate the approach using the Observer pattern applied to a graph application. Besides the Observer pattern, we also define transformation rules for the Visitor and Adapter patterns.",
      "Keywords": "Design pattern | Model transformation | QVT | UML",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Kim, Dae Kyoo;Lu, Lunjin;Lee, Byunghun",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85006804686",
      "Primary study DOI": "10.1016/j.jss.2016.12.004",
      "Title": "Impacts of organizational commitment, interpersonal closeness, and Confucian ethics on willingness to report bad news in software projects",
      "Abstract": "Individuals working on troubled software projects are often reluctant to report bad news concerning the project to senior management. This problem may be particularly acute when a subordinate must bypass their direct manager because the manager engages in a wrongful effort to suppress the bad news, which is the context for our study. In this research, we examine the impacts of organizational commitment, interpersonal closeness, and Confucian ethics on individuals’ willingness to report bad news that is deliberately hidden from upper management and we do this in the Chinese culture context. Based on data collected from 158 Chinese software engineers, we found that organizational commitment positively affects individuals’ willingness to report, while interpersonal closeness with the wrongdoer negatively affects willingness to report. With respect to the influence of Confucian ethics, our findings suggest that: (1) individuals’ ethical disposition toward loyalty between sovereign and subject (interpreted in this research as loyalty to one's organization) strengthens the positive effect of organizational commitment on willingness to report, and (2) individuals’ ethical disposition on trust between friends strengthens the negative effect of interpersonal closeness with the wrongdoer on willingness to report.",
      "Keywords": "Confucian ethics | Interpersonal closeness | IT project management | Organizational commitment | Whistleblowing",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Wang, Jijie;Keil, Mark;Oh, Lih bin;Shen, Yide",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85007247171",
      "Primary study DOI": "10.1016/j.jss.2016.12.007",
      "Title": "A study of value in agile software development organizations",
      "Abstract": "The Agile manifesto focuses on the delivery of valuable software. In Lean, the principles emphasise value, where every activity that does not add value is seen as waste. Despite the strong focus on value, and that the primary critical success factor for software intensive product development lies in the value domain, no empirical study has investigated specifically what value is. This paper presents an empirical study that investigates how value is interpreted and prioritised, and how value is assured and measured. Data was collected through semi-structured interviews with 23 participants from 14 agile software development organisations. The contribution of this study is fourfold. First, it examines how value is perceived amongst agile software development organisations. Second, it compares the perceptions and priorities of the perceived values by domains and roles. Third, it includes an examination of what practices are used to achieve value in industry, and what hinders the achievement of value. Fourth, it characterises what measurements are used to assure, and evaluate value-creation activities.",
      "Keywords": "Agile software development | Empirical | Value",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Alahyari, Hiva;Berntsson Svensson, Richard;Gorschek, Tony",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85002599816",
      "Primary study DOI": "10.1016/j.jss.2016.11.030",
      "Title": "The state of the art on design patterns: A systematic mapping of the literature",
      "Abstract": "Design patterns are widely used by software developers to build complex systems. Hence, they have been investigated by many researchers in recent decades. This leads to the emergence of various topics in the design patterns field. The objective of this paper is to present an overview of the research efforts on design patterns for those researchers who seek to enter this area. The main contributions are as follows: (a) identifying research topics in design patterns, (b) quantifying the research emphasis on each topic, and (c) describing the demographics of design patterns research. The last secondary study with similar goals in the design patterns field considers the Gang of Four design patterns only. However, the scope of the current study is all of the design patterns. Moreover, our review covers about six additional years and a larger number of publications and venues. In this systematic mapping study, a total of 2775 papers were identified as relevant, and 637 of them were included. According to the results, design patterns can be classified into six different research topics. As a consequence, it is concluded that Pattern Development, Pattern Mining, and Pattern Usage are the most active topics in the field of design patterns.",
      "Keywords": "Design patterns | Systematic mapping study | Systematic review",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Bafandeh Mayvan, B.;Rasoolzadegan, A.;Ghavidel Yazdi, Z.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85000774224",
      "Primary study DOI": "10.1016/j.jss.2016.11.029",
      "Title": "Analysis and selection of a regression model for the Use Case Points method using a stepwise approach",
      "Abstract": "This study investigates the significance of use case points (UCP) variables and the influence of the complexity of multiple linear regression models on software size estimation and accuracy. Stepwise multiple linear regression models and residual analysis were used to analyse the impact of model complexity. The impact of each variable was studied using correlation analysis. The estimated size of software depends mainly on the values of the weights of unadjusted UCP, which represent a number of use cases. Moreover, all other variables (unadjusted actors' weights, technical complexity factors, and environmental complexity factors) from the UCP method also have an impact on software size and therefore cannot be omitted from the regression model. The best performing model (Model D) contains an intercept, linear terms, and squared terms. The results of several evaluation measures show that this model's estimation ability is better than that of the other models tested. Model D also performs better when compared to the UCP model, whose Sum of Squared Error was 268,620 points on Dataset 1 and 87,055 on Dataset 2. Model D achieved a greater than 90% reduction in the Sum of Squared Errors compared to the Use Case Points method on Dataset 1 and a greater than 91% reduction on Dataset 2. The medians of the Sum of Squared Errors for both methods are significantly different at the 95% confidence level (p < 0.01), while the medians for Model D (312 and 37.26) are lower than Use Case Points (3134 and 3712) on Datasets 1 and 2, respectively.",
      "Keywords": "Dataset | Multiple linear regression | Software size estimation | Stepwise approach | Use Case Points | Variables analysis",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Silhavy, Radek;Silhavy, Petr;Prokopova, Zdenka",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85002678813",
      "Primary study DOI": "10.1016/j.jss.2016.07.025",
      "Title": "Hot spots profiling and dataflow analysis in custom dataflow computing SoftProcessors",
      "Abstract": "In the past decades, instruction set extension problem has been a key research area for state-of-the-art design automation of Very Large Scale Integration (VLSI) systems. Meanwhile, recently there is a renewed interest for hot spot profiling and dataflow analysis in custom instruction set processors. This paper proposes HOTISE, an architecture framework for adaptive reconfigurable instruction set processors (RISP) with dynamic profiling and dataflow analysis. A dynamic profiler is employed to obtain hot spots for each application at run-time. Then the selected hot spots will be considered as custom instructions and implemented in reconfigurable logic arrays. An instruction generator based on dataflow generation provides a mapping scheme from each selected instruction to hardware processing element in the array. To demonstrate the accuracy and feasibility of HOTISE, we have implemented a profiler prototype using simulator-based RTL codes. Experimental results show that the profiling results can cover more than 97% hot spots of MiBench and NetBench applications. In particular, the custom instruction of CRC and MD5 application proves the effectiveness of the mapping mechanism, the code sizes of CRC and MD5 could decrease to 32.5% and 37%, while achieving the speedup at 4.7x and 5.1x, respectively.",
      "Keywords": "Adaptive processor | Dynamic profiling | Hot spot | Instruction set extension",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Wang, Chao;Li, Xi;Zhang, Huizhen;Wang, Aili;Zhou, Xuehai",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85007102894",
      "Primary study DOI": "10.1016/j.jss.2016.12.006",
      "Title": "Adopters  trust in enterprise open source vendors: An empirical examination",
      "Abstract": "Although significant research attention has been directed at understanding open source software (OSS) adoption, very little attention has been paid to understanding what leads potential adopters to trust enterprise open source vendors. This study identifies organizational trust factors in enterprise open source vendors, namely vendors’ security, embracement of open standards, and support services. It also examines the impact of system trust on adopters’ attitudes and intentions. The study draws upon a total of 192 questionnaires collected from enterprise IT and project managers. Our results show that trust factors have a positive effect on system trust. We also found system trust to be effective in increasing adopters’ attitudes and intentions. Finally, our results provide several managerial implications for organizations as well as enterprise open source vendors.",
      "Keywords": "Adoption intent | Enterprise open source software vendor | Organizational adoption | System trust",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Roumani, Yaman;Nwankpa, Joseph K.;Roumani, Yazan F.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85001052447",
      "Primary study DOI": "10.1016/j.jss.2016.11.028",
      "Title": "Software component and the semantic Web: An in-depth content analysis and integration history",
      "Abstract": "With the advent of Component-based software engineering (CBSE), large software systems are being built by integrating pre-built software components. The Semantic Web in association with CBSE has shown to offer powerful representation facilities and reasoning techniques to enhance and support querying, reasoning, discovery, etc. of software components. The goal of this paper is to research the applicability of Semantic Web technologies in performing the various tasks of CBSE and review the experimental results of the same in an easy and effective manner. To the best of our knowledge, this is the first study which provides an extensive review of the application of Semantic Web in CBSE from different perspectives. A systematic literature review of the Semantic Web approaches, employed for use in CBSE, reported from 2001 until 2015, is conducted in this research article. Empirical results have been drawn through the question-answer based analysis of the research, which clearly tells the year wise trend of the research articles, with the possible justification of the usage of Semantic Web technology and tools for a particular phase of CBSE. To conclude, gaps in the current research and potential future prospects have been discussed.",
      "Keywords": "Component-based software engineering | Linked Data | Ontology | Reasoners | Semantic Web | Web services",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Kaur, Loveleen;Mishra, Ashutosh",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85011411870",
      "Primary study DOI": "10.1016/j.jss.2016.08.069",
      "Title": "A parallelization approach for resource-restricted embedded heterogeneous MPSoCs inspired by OpenMP",
      "Abstract": "Future low-end embedded systems will make an increased use of heterogeneous MPSoCs. To utilize these systems efficiently, methods and tools are required that support the extraction and implementation of parallelism typically found in embedded applications. Ideally, large amounts of existing legacy code should be reused and ported to these new systems. Existing parallelization infrastructures, however, mostly support parallelization according to the requirements of HPEC systems. For resource-restricted embedded systems, different parallelization strategies are necessary to achieve additional non-functional objectives such as the reduction of energy consumption. HPC-focused parallelization also assumes processor, memory and communication structures different from low-end embedded systems and therefore wastes optimization opportunities essential for improving the performance of resource-constrained embedded systems. This paper describes a new approach and infrastructure inspired by the OpenMP API to support the extraction and implementation of pipeline parallelism, which is commonly found in complex embedded applications. In addition, advanced techniques to extract parallelism from legacy applications requiring only minimal code modifications are presented. Further, the resulting toolflow combines advanced parallelization, mapping and communication optimization tools leading to a more efficient approach to exploit parallelism for typical embedded applications on heterogeneous MPSoCs running distributed real-time operating systems.",
      "Keywords": "Embedded systems | Heterogeneous multiprocessor system-on-chip | Parallelization",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Neugebauer, Olaf;Engel, Michael;Marwedel, Peter",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85000399255",
      "Primary study DOI": "10.1016/j.jss.2016.11.031",
      "Title": "Integration between requirements engineering and safety analysis: A systematic literature review",
      "Abstract": "Context: Safety-Critical Systems (SCS) require more sophisticated requirements engineering (RE) approaches as inadequate, incomplete or misunderstood requirements have been recognized as a major cause in many accidents and safety-related catastrophes. Objective: In order to cope with the complexity of specifying SCS by RE, we investigate the approaches proposed to improve the communication or integration between RE and safety engineering in SCS development. We analyze the activities that should be performed by RE during safety analysis, the hazard/safety techniques it could use, the relationships between safety information that it should specify, the tools to support safety analysis as well as integration benefits between these areas. Method: We use a Systematic Literature Review (SLR) as the basis for our work. Results: We developed four taxonomies to help RE during specification of SCS that classify: techniques used in (1) hazard analysis; (2) safety analysis; (3) safety-related information and (4) a detailed set of information regarding hazards specification. Conclusions: This paper is a step towards developing a body of knowledge in safety concerns necessary to RE in the specification of SCS that is derived from a large-scale SLR. We believe the results will benefit both researchers and practitioners.",
      "Keywords": "Communication | Integration | Requirements engineering | Safety analysis | Safety-critical systems | Systematic literature review",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Vilela, Jéssyka;Castro, Jaelson;Martins, Luiz Eduardo G.;Gorschek, Tony",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85002578907",
      "Primary study DOI": "10.1016/j.jss.2016.11.053",
      "Title": "Providing fair-share scheduling on multicore computing systems via progress balancing",
      "Abstract": "Performance isolation in a scalable multicore system is often attempted through periodic load balancing paired with per-core fair-share scheduling. Unfortunately, load balancing cannot guarantee the desired level of multicore fairness since it may produce unbounded differences in the progress of tasks. In reality, the balancing of load across cores is only indirectly related to multicore fairness. To address this limitation and ultimately achieve multicore fairness, we propose a new task migration policy we name progress balancing, and present an algorithm for its realization. Progress balancing periodically distributes tasks among cores to directly balance the progress of tasks by bounding their virtual runtime differences. In doing so, it partitions runnable tasks into task groups and allocates them onto cores such that tasks with larger virtual runtimes run on a core with a larger load and thus proceed more slowly. We formally prove the fairness property of our algorithm. To demonstrate its effectiveness, we implemented our algorithm into Linux kernel 3.10 and performed extensive experiments. In the target system, our algorithm yields the maximum virtual runtime difference of 1.07 s, regardless of the uptime of tasks, whereas the Linux CFS produces unbounded virtual runtime differences.",
      "Keywords": "Fair-share scheduling | Load balancing | Multicore scheduling | Resource management",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Huh, Sungju;Hong, Seongsoo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85008239831",
      "Primary study DOI": "10.1016/j.jss.2016.12.017",
      "Title": "A general framework for comparing automatic testing techniques of Android mobile apps",
      "Abstract": "As an increasing number of new techniques are developed for quality assurance of Android applications (apps), there is a need to evaluate and empirically compare them. Researchers as well as practitioners will be able to use the results of such comparative studies to answer questions such as, “What technique should I use to test my app?” Unfortunately, there is a severe lack of rigorous empirical studies on this subject. In this paper, for the first time, we present an empirical study comparing all existing fully automatic “online” testing techniques developed for the Android platform. We do so by first reformulating each technique within the context of a general framework. We recognize the commonalities between the techniques to develop the framework. We then use the salient features of each technique to develop parameters of the framework. The result is a general recasting of all existing approaches in a plug-in based formulation, allowing us to vary the parameters to create instances of each technique, and empirically evaluate them on a common set of subjects. Our results show that (1) the proposed general framework abstracts all the common characteristics of online testing techniques proposed in the literature, (2) it can be exploited to design experiments aimed at performing objective comparisons among different online testing approaches and (3) some parameters that we have identified influence the performance of the testing techniques.",
      "Keywords": "Android testing | Automated testing | Comparing android testing techniques | Event-based testing | Mobile apps testing | Random testing",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Amalfitano, Domenico;Amatucci, Nicola;Memon, Atif M.;Tramontana, Porfirio;Fasolino, Anna Rita",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85008352097",
      "Primary study DOI": "10.1016/j.jss.2016.12.016",
      "Title": "Software Systems Engineering programmes a capability approach",
      "Abstract": "This paper discusses third-level educational programmes that are intended to prepare their graduates for a career building systems in which software plays a major role. Such programmes are modelled on traditional Engineering programmes but have been tailored to applications that depend heavily on software. Rather than describe knowledge that should be taught, we describe capabilities that students should acquire in these programmes. The paper begins with some historical observations about the software development field.",
      "Keywords": "Education | Engineering | Information systems | Software design | Software development | Software documentation | Software education",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Landwehr, Carl;Ludewig, Jochen;Meersman, Robert;Parnas, David Lorge;Shoval, Peretz;Wand, Yair;Weiss, David;Weyuker, Elaine",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84964330300",
      "Primary study DOI": "10.1016/j.jss.2016.02.002",
      "Title": "An empirical study of data decomposition for software parallelization",
      "Abstract": "Context: Multi-core architectures are becoming increasingly ubiquitous and software professionals are seeking to leverage the capabilities of distributed-memory architectures. The process of parallelizing software applications can be very tedious and error-prone, in particular the task of data decomposition. Empirical studies investigating the complexity of data decomposition and communication are lacking. Objective: Our objective is threefold: (i) to gain an empirical-based understanding of the task of data decomposition as part of the parallelization of software applications; (ii) to identify key requirements for tools to assist developers in this task, and (iii) assess the current state-of-the-art. Methods: Our empirical investigation employed a multi-method approach, using an interview study, participant-observer case study, focus group study, and a sample survey. The empirical investigation involved collaborations with three industry partners: IBM's High Performance Computing Center, the Irish Centre for High-End Computing (ICHEC), and JBA Consulting. Results: This article presents data decomposition as one of the most prevalent tasks of parallelizing applications for multi-core architectures. Based on our studies, we identify ten key requirements for tool support to help HPC developers in this area. Our evaluation of the state-of-the-art shows that none of the extant tool support implements all 10 requirements. Conclusion: While there is a considerable body of research in the area of HPC, a few empirical studies exist which explicitly focus on the challenges faced by practitioners in this area; this research aims to address this gap. The empirical studies in this article provide insights that may help researchers and tool vendors to better understand the needs of parallel programmers.",
      "Keywords": "Data decomposition | Empirical studies | Parallelization",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Meade, Anne;Deeptimahanti, Deva Kumar;Buckley, Jim;Collins, J. J.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85007569676",
      "Primary study DOI": "10.1016/j.jss.2016.12.018",
      "Title": "Fine-grained access control system based on fully outsourced attribute-based encryption",
      "Abstract": "Attribute-based encryption (ABE) has potential to be applied in cloud computing applications to provide fine-grained access control over encrypted data. However, the computation cost of ABE is considerably expensive, because the pairing and exponentiation operations grow with the complexity of access formula. In this work, we propose a fully outsourced ciphertext-policy ABE scheme that for the first time achieves outsourced key generation, encryption and decryption simultaneously. In our scheme, heavy computations are outsourced to public cloud service providers, leaving no complex operations for the private key generator (PKG) and only one modular exponentiation for the sender or the receiver, and the communication cost of the PKG and users is optimized. Moreover, we give the security proof and implement our scheme in Charm, and the experimental results indicate that our scheme is efficient and practical.",
      "Keywords": "Access control | Attribute-based encryption | Low communication cost | Outsourced computation",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Zhang, Rui;Ma, Hui;Lu, Yao",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85003827397",
      "Primary study DOI": "10.1016/j.jss.2016.12.005",
      "Title": "Creating an invalid defect classification model using text mining on server development",
      "Abstract": "Invalid defects, which are often overlooked, reduce development productivity and efficiency. This study used exploratory study and text mining to answer three research questions related to invalid defects in two research stages. In the first stage, we filtered 231 invalid BIOS (basic input/output system) defects from the 3347 defects of three server projects. These defects were from numerous function areas owned by virtual teams located in Taiwan, China, and the United States. Results indicated that BIOS firmware demonstrates the maximum number of defects and invalid defects. This firmware accounted for 43.3% defects and 33% invalid defects in server development. Results determined that invalid defect classification that includes four types, namely, working as designed (WAD), user error, duplicate, and others. All of these types can be grouped under the term WUDO. WAD accounts for the maximum of 45% of invalid defects in the WUDO classification. In the second stage, this study determined a stable classification algorithm, namely, decision tree C4.5, to classify the invalid defect types. This study helps project teams for information technology products to classify the different invalid defect types that developers and testers face. Results can improve project team productivity and mitigate project risks in project management.",
      "Keywords": "BIOS | Classification | Invalid defect | Project management | Server development | Text mining",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Su, Yihsiung;Luarn, Pin;Lee, Yue Shi;Yen, Show Jane",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85001086191",
      "Primary study DOI": "10.1016/j.jss.2016.11.037",
      "Title": "Performance evaluation of cloud-based log file analysis with Apache Hadoop and Apache Spark",
      "Abstract": "Log files are generated in many different formats by a plethora of devices and software. The proper analysis of these files can lead to useful information about various aspects of each system. Cloud computing appears to be suitable for this type of analysis, as it is capable to manage the high production rate, the large size and the diversity of log files. In this paper we investigated log file analysis with the cloud computational frameworks Apache™Hadoop® and Apache Spark™. We developed realistic log file analysis applications in both frameworks and we performed SQL-type queries in real Apache Web Server log files. Various experiments were performed with different parameters in order to study and compare the performance of the two frameworks.",
      "Keywords": "Apache Hadoop | Apache Spark | Cloud | Log Analysis | Performance Evaluation",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Mavridis, Ilias;Karatza, Helen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85006760739",
      "Primary study DOI": "10.1016/j.jss.2016.12.009",
      "Title": "A systematic study of double auction mechanisms in cloud computing",
      "Abstract": "The cloud system is designed, implemented and conceptualized as a marketplace where resources are traded. This demands efficient allocation of resources to benefit both the cloud users and the cloud service providers. Accordingly, market based resource allocation models for cloud computing have been proposed. These models apply economy based approaches e.g. auction, negotiation etc. This work makes a detailed study of the double auction mechanisms and their applicability for the cloud markets. A framework for a future cloud market using double auction is also proposed. As most of the existing works in double auction confines only resource allocation, therefore, a Truthful Multi-Unit Double Auction mechanism (TMDA) is proposed that would help researchers to understand how a truthful double auction mechanism can be designed. TMDA is proven to be asymptotically efficient, individual rational, truthful and budget-balanced. TMDA would also encourage researchers to contribute in this emerging area. The performance of TMDA, which addresses the interests of both the cloud user and the provider, has been validated through simulation study. Various challenges in the realization of double auction mechanisms in cloud computing along-with the future possibilities are also presented.",
      "Keywords": "Cloud computing | Double auction | Mechanism design | Quality of Service (QoS) | Resource allocation",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Kumar, Dinesh;Baranwal, Gaurav;Raza, Zahid;Vidyarthi, Deo Prakash",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84996593342",
      "Primary study DOI": "10.1016/j.jss.2016.11.018",
      "Title": "Automated extraction of product comparison matrices from informal product descriptions",
      "Abstract": "Domain analysts, product managers, or customers aim to capture the important features and differences among a set of related products. A case-by-case reviewing of each product description is a laborious and time-consuming task that fails to deliver a condense view of a family of product. In this article, we investigate the use of automated techniques for synthesizing a product comparison matrix (PCM) from a set of product descriptions written in natural language. We describe a tool-supported process, based on term recognition, information extraction, clustering, and similarities, capable of identifying and organizing features and values in a PCM – despite the informality and absence of structure in the textual descriptions of products. We evaluate our proposal against numerous categories of products mined from BestBuy. Our empirical results show that the synthesized PCMs exhibit numerous quantitative, comparable information that can potentially complement or even refine technical descriptions of products. The user study shows that our automatic approach is capable of extracting a significant portion of correct features and correct values. This approach has been implemented in MatrixMiner a web environment with an interactive support for automatically synthesizing PCMs from informal product descriptions. MatrixMiner also maintains traceability with the original descriptions and the technical specifications for further refinement or maintenance by users.",
      "Keywords": "Feature mining | Product comparison matrices | Reverse engineering | Software product lines | Variability mining",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Nasr, Sana Ben;Bécan, Guillaume;Acher, Mathieu;Ferreira Filho, João Bosco;Sannier, Nicolas;Baudry, Benoit;Davril, Jean Marc",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84996563490",
      "Primary study DOI": "10.1016/j.jss.2016.11.024",
      "Title": "Group development and group maturity when building agile teams: A qualitative and quantitative investigation at eight large companies",
      "Abstract": "The agile approach to projects focuses more on close-knit teams than traditional waterfall projects, which means that aspects of group maturity become even more important. This psychological aspect is not much researched in connection to the building of an “agile team.” The purpose of this study is to investigate how building agile teams is connected to a group development model taken from social psychology. We conducted ten semi-structured interviews with coaches, Scrum Masters, and managers responsible for the agile process from seven different companies, and collected survey data from 66 group-members from four companies (a total of eight different companies). The survey included an agile measurement tool and the one part of the Group Development Questionnaire. The results show that the practitioners define group developmental aspects as key factors to a successful agile transition. Also, the quantitative measurement of agility was significantly correlated to the group maturity measurement. We conclude that adding these psychological aspects to the description of the “agile team” could increase the understanding of agility and partly help define an “agile team.” We propose that future work should develop specific guidelines for how software development teams at different maturity levels might adopt agile principles and practices differently.",
      "Keywords": "Agile processes | Empirical study | Group psychology | Maturity | Measurement",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Gren, Lucas;Torkar, Richard;Feldt, Robert",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84994877375",
      "Primary study DOI": "10.1016/j.jss.2016.10.018",
      "Title": "Identification and analysis of the elements required to manage technical debt by means of a systematic mapping study",
      "Abstract": "Technical debt, a metaphor for the long-term consequences of weak software development, must be managed to keep it under control. The main goal of this article is to identify and analyze the elements required to manage technical debt. The research method used to identify the elements is a systematic mapping, including a synthesis step to synthesize the elements definitions. Our perspective differs from previous literature reviews because it focused on the elements required to manage technical debt and not on the phenomenon of technical debt or the activities used in performing technical debt management. Additionally, the rigor and relevance for industry of the current techniques used to manage technical debt are studied. The elements were classified into three groups (basic decision-making factors, cost estimation techniques, practices and techniques for decision-making) and mapped according three stakeholders’ points of view (engineering, engineering management, and business-organizational management). The definitions, classification, and analysis of the elements provide a framework that can be deployed to help in the development of models that are adapted to the specific stakeholders’ interests to assist the decision-making required in technical debt management and to assess existing models and methods. The analysis indicated that technical debt management is context dependent.",
      "Keywords": "Basic decision-making factors | Business-organizational management | Cost estimation techniques | Decision making | Engineering | Engineering management | Framework | Practices and techniques for decision-making | Stakeholders’ points of view | Systematic mapping | Technical debt | Technical debt management",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Fernández-Sánchez, Carlos;Garbajosa, Juan;Yagüe, Agustín;Perez, Jennifer",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84955621963",
      "Primary study DOI": "10.1016/j.jss.2015.12.031",
      "Title": "A cybernetics Social Cloud",
      "Abstract": "This paper proposes a Social Cloud, which presents the system design, development and analysis. The technology is based on the BOINC open source software, our hybrid Cloud, Facebook Graph API and our development in a new Facebook API, SocialMedia. The creation of SocialMedia API with its four functions can ensure a smooth delivery of Big Data processing in the Social Cloud, with four selected examples provided. The proposed solution is focused on processing the contacts who click like or comment on the author's posts. Outputs result in visualization with their core syntax being demonstrated. Four functions in the SocialMedia API have evaluation test and each client-server API processing can be completed efficiently and effectively within 1.36 s. We demonstrate large scale simulations involved with 50,000 simulations and all the execution time can be completed within 70,000 s. Cybernetics functions are created to ensure that 100% job completion rate for Big Data processing. Results support our case for Big Data processing on Social Cloud with no costs involved. All the steps involved have closely followed system design, implementation, experiments and validation for Cybernetics to ensure a high quality of outputs and services at all times. This offers a unique contribution for Cybernetics to meet Big Data research challenges.",
      "Keywords": "Big Data cybernetics | Data visualization | SocialMedia API",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Chang, Victor",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84997171423",
      "Primary study DOI": "10.1016/j.jss.2016.10.031",
      "Title": "Platform design space exploration using architecture decision viewpoints A longitudinal study",
      "Abstract": "Design space exploration is the simultaneous analysis of problem and solution spaces for a specific domain or application scope. Performing this activity as part of the architectural design is beneficial, especially for software platforms, which are shared across organizations. Exploring the design space of software platforms in a multi-product and multi-domain context is not trivial, and only few methods exist to support this activity systematically. This paper reports on a longitudinal technical action research (TAR) study conducted to adapt and evaluate architecture decision viewpoints for supporting platform design space exploration. The study was conducted in the context of an ABB project, which was performed to explore the design space for a common software platform for mobile device support in several product-specific software platforms at ABB. The results indicate that the adapted decision viewpoints are well suitable for dealing with diverging stakeholder concerns, evaluating technological alternatives and uncovering relationships between decisions to be made.",
      "Keywords": "Architecture decision viewpoints | Design space exploration | Technical action research",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "van Heesch, U.;Jansen, A.;Pei-Breivold, H.;Avgeriou, P.;Manteuffel, C.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84994706966",
      "Primary study DOI": "10.1016/j.jss.2016.08.095",
      "Title": "Modern software cybernetics: New trends",
      "Abstract": "Software cybernetics research is to apply a variety of techniques from cybernetics research to software engineering research. For more than fifteen years since 2001, there has been a dramatic increase in work relating to software cybernetics. From cybernetics viewpoint, the work is mainly on the first-order level, namely, the software under observation and control. Beyond the first-order cybernetics, the software, developers/users, and running environments influence each other and thus create feedback to form more complicated systems. We classify software cybernetics as Software Cybernetics I based on the first-order cybernetics, and as Software Cybernetics II based on the higher order cybernetics. This paper provides a review of the literature on software cybernetics, particularly focusing on the transition from Software Cybernetics I to Software Cybernetics II. The results of the survey indicate that some new research areas such as Internet of Things, big data, cloud computing, cyber-physical systems, and even creative computing are related to Software Cybernetics II. The paper identifies the relationships between the techniques of Software Cybernetics II applied and the new research areas to which they have been applied, formulates research problems and challenges of software cybernetics with the application of principles of Phase II of software cybernetics; identifies and highlights new research trends of software cybernetic for further research.",
      "Keywords": "Artificial intelligence | Computer science | Control engineering | Software cybernetics | Software engineering",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-02-01",
      "Publication type": "Editorial",
      "Authors": "Yang, Hongji;Chen, Feng;Aliyu, Suleiman",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84995752387",
      "Primary study DOI": "10.1016/j.jss.2016.11.016",
      "Title": "On building a cloud-based mobile testing infrastructure service system",
      "Abstract": "With the rapid advance of mobile computing, cloud computing and wireless network, there is a significant increasing number of mobile subscriptions. This brings new business requirements and demands in mobile testing service, and causes new issues and challenges. In this paper, informative discussions about cloud-based mobile testing-as-a-service (mobile TaaS) are offered, including the essential concepts, focuses, test process, and the expected testing infrastructures. To address the need of infrastructure level service for mobile TaaS, this paper presents a developed system known as MTaaS to provide an infrastructure-as-a-service (IaaS) for mobile testing, in order to indicate the feasibility and effectiveness of cloud-based mobile testing service. In addition, the paper presents a comparison among cloud-based mobile TaaS approaches and several best practices in industry are discussed. Finally, the primary issues, challenges, and needs existed in current mobile TaaS are analyzed.",
      "Keywords": "Cloud-based infrastructure -as-a-service | Mobile application testing | Mobile testing as a service",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Tao, Chuanqi;Gao, Jerry",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84956868007",
      "Primary study DOI": "10.1016/j.jss.2015.12.030",
      "Title": "Requirements cybernetics: Elicitation based on user behavioral data",
      "Abstract": "Users’ behavioral data provides important cue for product improvement. Today's web based applications collect various kinds of service data, which is an ideal source of information for product designers to better understand users’ needs and behaviors. This paper first discusses the types of data collected so far, and then such data-driven requirements elicitation process is formulated as a feedback control system, where the classical requirements elicitation philosophy turns into a continuous optimization to user behavioral models. To this end, it is important to know how the data collection function reflects user behavior, and how specific data analysis approaches help making design decisions. This is an attempt to seek practical synergies between the two disciplines of requirements and cybernetics, to explore the possibilities of formulating problems in requirements with concepts and frameworks from cybernetics, and understand to what extent that known research results from cybernetics can be applied to address requirements problems. In particular, control frameworks for the user data driven requirements elicitation process are experimented, and potential control variables are discussed. We use two example cases to illustrate the proposed approach, an online dictionary service and a mobile music player service.",
      "Keywords": "Cybernetics | Data analysis | Requirements elicitation",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Liu, Lin;Zhou, Qing;Liu, Jilei;Cao, Zhanqiang",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84957402063",
      "Primary study DOI": "10.1016/j.jss.2016.01.031",
      "Title": "The verification of program relationships in the context of software cybernetics",
      "Abstract": "Software cybernetics aims at improving the reliability of software by introducing the control theory into software engineering domain systematically. A key issue in software verification is to improve the reliability of software by inspecting whether the software can achieve its expected behaviors. In this paper, the thought of software cybernetics is applied in the process of verification to address this issue and a nested control system is established. The proposed method verifies functional requirements in a dynamic environment with constantly changing user requirements, in which the program serves as a controlled object, and the verification strategy determined by software behavioral model (SBM) serves as a controller. The main contribution of this paper includes: (1) SBM is established in software design phase, and a concern-based construction approach is proposed, which starts from obtaining the software expected functionality extracted from a requirement text; (2) Program abstract-relationship model (PARM) is constructed basing on a process of gradual abstract to be a controlled object; (3) Feedback in a form of intermediate code is generated in the process of verification. The proposed method is validated by our case study.",
      "Keywords": "Control system | Software cybernetics | Software verification",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Liu, Huaxiao;Liu, Yuzhou;Liu, Lei",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84996606740",
      "Primary study DOI": "10.1016/j.jss.2016.11.010",
      "Title": "A supercomputing framework for the evaluation of real-time analysis and optimization techniques",
      "Abstract": "The evaluation of new approaches in the analysis and optimization of real-time systems usually relies on synthetic test systems. Therefore, the development of tools to create these test systems in an efficient way is highly desirable. It is usual for these evaluations to be constrained by the processing power of current personal computers. For example, in order to assess whether a specific technique generally performs better than others or whether the improvement observed is constrained to a limited set of circumstances, a vast set of examples must be tested, making the execution infeasible in a common PC. In this paper, we present a framework that defines the building blocks of a tool to enable the validation of real-time techniques, through the efficient execution of massive evaluations of event-driven synthetic distributed systems. Its main characteristic is that it can leverage the computing power of a supercomputer to perform large studies that otherwise could not be performed with a PC. The framework also defines different generation methods so that the generated systems can cover a wide range of characteristics that can be present in different application domains. As a case study, we also implement this framework based on a previously developed prototype.",
      "Keywords": "Distributed systems | Real-time systems | Supercomputer exploitation | Technique validation",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Rivas, Juan M.;Gutiérrez, J. Javier;González Harbour, Michael",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84977108920",
      "Primary study DOI": "10.1016/j.jss.2016.06.065",
      "Title": "Optimal control based regression test selection for service-oriented workflow applications",
      "Abstract": "Regression test selection, which is well known as an effective technology to ensure the quality of modified BPEL applications, is regarded as an optimal control issue. The BPEL applications under test serves as a controlled object and the regression test selection strategy functions as the corresponding controller. The performance index is to select fewest test cases to test modified BPEL applications. In addition, a promising controller (regression test selection approach) should be safe, which means that it can select all test cases in which faults might be exposed in modified versions under controlled regression testing from the original test suite. However, existing safe controllers may rerun some test cases without exposing fault. In addition, the unique features (e.g., dead path elimination semantics, communication mechanism, multi-assignment etc.) of BPEL applications also raise enormous problems in regression test selection. To address these issues, we present in this paper a safe optimal controller for BPEL applications. Firstly, to handle the unique features mentioned above, we transform BPEL applications and their modified versions into universal BPEL forms. Secondly, For our optimal controller, BPEL program dependence graphs corresponding to the two universal BPEL forms are established. Finally, guided by behavioral differences between the two versions, we construct an optimal controller and select test cases to be rerun. By contrast with the previous approaches, our approach can eliminate some unnecessary test cases to be selected. We conducted experiments with 8 BPEL applications to compare our approach with other typical approaches. Experimental results show that the test cases selected using our approach are fewer than other approaches.",
      "Keywords": "Behavioral difference | BPEL program dependence graph | Optimal control | Regression test selection | Safe | Service-oriented workflow applications | Software cybernetics",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Wang, Hongda;Xing, Jianchun;Yang, Qiliang;Wang, Ping;Zhang, Xuewei;Han, Deshuai",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84997693939",
      "Primary study DOI": "10.1016/j.jss.2016.11.026",
      "Title": "deExploit: Identifying misuses of input data to diagnose memory-corruption exploits at the binary level",
      "Abstract": "Memory-corruption exploits are one of the major threats to the Internet security. Once an exploit has been detected, exploit diagnosis techniques can be used to identify the unknown vulnerability and attack vector. In the security landscape, exploit diagnosis is always performed by third-party security experts who cannot access the source code. This makes binary-level exploit diagnosis a time-consuming and error-prone process. Despite considerable efforts to defend against exploits, automatic exploit diagnosis remains a significant challenge. In this paper, we propose a novel insight for detecting memory corruption at the binary level by identifying the misuses of input data and present an exploit diagnosis approach called deExploit. Our approach requires no knowledge of the source code or debugging information. For exploit diagnosis, deExploit is generic in terms of the detection of both control-flow-hijack and data-oriented exploits. In addition, deExploit automatically provides precise information regarding the corruption point, the memory operation that causes the corruption, and the key attack steps used to bypass existing defense mechanisms. We implement deExploit and perform it to diagnose multiple realistic exploits. The results show that deExploit is able to diagnose memory-corruption exploits.",
      "Keywords": "Exploit diagnosis | Memory corruption | Reverse engineering | Software vulnerability",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Wang, Run;Liu, Pei;Zhao, Lei;Cheng, Yueqiang;Wang, Lina",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84963795932",
      "Primary study DOI": "10.1016/j.jss.2016.03.010",
      "Title": "Self-adaptive architecture evolution with model checking: A software cybernetics approach",
      "Abstract": "The cloud computing era requires software architecture to be self-adaptive to the dynamic environment. This autonomous feature brings uncertainty and makes software behavior difficult to control. The uncontrollable behavior is caused by ill-defined architecture and might lead to system disruption. To address this problem, we propose a novel framework which applies software cybernetics to guide self-adaptive architecture evolution. In our framework, we formulate the architecture evolution process as a feedback control process. In the process, we take the self-adaptive architecture model and the model checking technique as the controlled object and controller, respectively. First, the self-adaptive architecture is specified by Breeze/ADL. Second, the framework leverages model checking to validate adaptive Breeze/ADL specifications. Third, a learning algorithm is designed to regulate validation results to generate feedback rules – Productions to guide the architecture evolution. A smart phone application example is chosen to demonstrate the feasibility of our framework. The results show that our framework facilitates architects to detect undesired states which are caused by error-prone adaptation rules.",
      "Keywords": "Architecture evolution | Model checking | Self-adaptive software architecture | Software cybernetics",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Chen, Luxi;Huang, Linpeng;Li, Chen;Wu, Xiwen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84961840046",
      "Primary study DOI": "10.1016/j.jss.2016.03.013",
      "Title": "Software cybernetics in BPM: Modeling software behavior as feedback for evolution by a novel discovery method based on augmented event logs",
      "Abstract": "Business Process Management (BPM) is a quickly developing management theory in recent years. The goal of BPM is to improve corporate performance by managing and optimizing the businesses process in and among enterprises. The goal is easier to achieve with the closed-loop feedback mechanism from business process execution to redesign in BPM life cycle, where the business process itself and the set of activities in BPM are viewed as a controlled object and a controller respectively. In this feedback control system, process mining plays an important role in generating feedback of process execution for redesign. However, the existing discovery methods cannot mine certain special structures from execution logs (e.g., implicit dependency, implicit place and short loops) correctly and their mining efficiencies cannot meet the requirements of online process mining. In this paper, we propose a novel discovery method to overcome these challenges based on a kind of augmented event log that will also bring new research directions for process discovery. A case study is presented for introducing how the mined model can be used in business process evolution. Results of experiments are described to show the improvements of the proposed algorithm compared with others.",
      "Keywords": "Petri nets | Process discovery | Software cybernetics",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Li, Chuanyi;Ge, Jidong;Huang, Liguo;Hu, Haiyang;Wu, Budan;Hu, Hao;Luo, Bin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84997112151",
      "Primary study DOI": "10.1016/j.jss.2016.11.001",
      "Title": "Nosv: A lightweight nested-virtualization VMM for hosting high performance computing on cloud",
      "Abstract": "Moving the high performance computing (HPC) to Cloud not only reduces the costs but also gives users the ability to customize their system. Besides, compared with the traditional HPC computing environments, such as grid and cluster which run HPC applications on bare-metal, cloud equipped with virtualization not only improves resource utilization but also reduces maintenance cost. While for some reasons, current virtualization-based cloud has limited performance for HPC. Such performance overhead could be caused by Virtual Machine Monitor (VMM) interceptions, virtualized I/O devices or cross Virtual Machine (VM) interference, etc. In order to guarantee the performance of HPC applications on Cloud, the VMM should interfere guest VMs as less as possible and allocate dedicated resources such as CPU cores, DRAM and devices to guest VMs running HPC applications. In this paper, we propose a novel cloud infrastructure to serve the HPC applications and normal applications concurrently. This novel infrastructure is based on a lightweight high performance VMM named nOSV. For HPC applications, nOSV constructs a strong isolated high performance guest VM with dedicated resources. At runtime, the high performance VM manages all resources by itself and is not interfered by nOSV. By supporting nested virtualization, nOSV can run HPC with commodity application and keep the flexibility of traditional Cloud. nOSV runs other virtualization environments, like Xen and Docker, as high performance guest VMs. All commodity Cloud applications are hosted in these virtualization environments and share hardware resources with each other. The prototype of nOSV shows that it provides a bare-metal like performance for HPC applications and has about 23% improvement compared to HPC applications running on Xen.",
      "Keywords": "Cloud computing | HPC | Multi-core | Nested-virtualization",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Ren, Jianbao;Qi, Yong;Dai, Yuehua;Xuan, Yu;Shi, Yi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84994005147",
      "Primary study DOI": "10.1016/j.jss.2016.07.006",
      "Title": "An improved genetic algorithm for task scheduling in the cloud environments using the priority queues: Formal verification, simulation, and statistical testing",
      "Abstract": "Cloud computing is a new platform to manage and provide services on the internet. Lately, researchers have paid attention a lot to this new subject. One of the reasons to have high performance in a cloud environment is the task scheduling. Since the task scheduling is an NP-Complete problem, in many cases, meta-heuristics scheduling algorithms are used. In this paper to optimize the task scheduling solutions, a powerful and improved genetic algorithm is proposed. The proposed algorithm uses the advantages of evolutionary genetic algorithm along with heuristic approaches. For analyzing the correctness of the proposed algorithm, we have presented a behavioral modeling approach based on model checking techniques. Then, the expected specifications of the proposed algorithm is extracted in the form of Linear Temporal Logic (LTL) formulas. To achieve the best performance in verification of the proposed algorithm, we use the Labeled Transition System (LTS) method. Also, the proposed behavioral models are verified using NuSMV and PAT model checkers. Then, the correctness of the proposed algorithm is analyzed according to the verification results in terms of some expected specifications, reachability, fairness, and deadlock-free. The simulation and statistical results revealed that the proposed algorithm outperformed the makespans of the three well-known heuristic algorithms and also the execution time of our recently meta-heuristics algorithm.",
      "Keywords": "Cloud computing | Directed acyclic graph | Formal verification | Genetic algorithm | Model checking | Task scheduling",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Keshanchi, Bahman;Souri, Alireza;Navimipour, Nima Jafari",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84961798862",
      "Primary study DOI": "10.1016/j.jss.2016.03.012",
      "Title": "A software cybernetics approach to self-tuning performance of on-line transaction processing systems",
      "Abstract": "Self-tuning performance of On-Line Transaction Processing (OLTP) Systems is a challenging and time-consuming task since multiple performance parameters are needed to be automatically configured in Database Management Systems (DBMSs). In this paper, we present a software cybernetics approach to self-tune the performance of DBMSs. A DBMS is designed with an adaptive control based on fuzzy logic such that it has the capability to control objects, i.e., the performance parameters, and update the controller itself, i.e., a set of fuzzy rules in our case. The principles and concepts in software cybernetics are applied to guide the synthesis of software controllers for monitoring and adapting system behaviors. Experimental results for On-Line Transaction Processing using TPC-C, a benchmark of the Transaction Processing Performance Council, show that the proposed method is feasible and effective.",
      "Keywords": "Database Management System (DBMS) | Fuzzy rules | Self-tuning",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Ding, Zuohua;Wei, Zhijie;Chen, Haibo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84993949496",
      "Primary study DOI": "10.1016/j.jss.2016.10.012",
      "Title": "Reconciling software architecture and source code in support of software evolution",
      "Abstract": "Even in the eighties, the need of managing software evolution has been detected as one of the most complex aspects of the software lifecycle. In this context, software architecture has been highlighted as an integral element of the software evolution process. However, no matter how much effort is put into the architecture, it must eventually be translated into source code. The potential misalignment between architecture and code can lead to failures in the evolution process in terms of economic impacts, failed expectations, and so on. In this article we report on a design science research study that we pursued to answer three research questions. First, we have studied whether and in how far it is possible to design an approach that both enforces the integration between software architecture and source code to avoid architectural erosion and architectural drift and, at the same time, provides automatic guidance to developers to carry out the required change tasks in each evolution steps. Second, we have studied whether this approach may be applied in realistic (open source) cases. Finally, we have analysed whether it is realizable at acceptable costs (in terms of development effort) in comparison to the overall development efforts roughly spent on the evolution of the projects in focus.",
      "Keywords": "ADLs | Architectural knowledge | Architecture reconstruction | Components | Evolution styles | M2M transformation",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Haitzer, Thomas;Navarro, Elena;Zdun, Uwe",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84993999933",
      "Primary study DOI": "10.1016/j.jss.2016.08.088",
      "Title": "Distributed architecture for developing mixed-criticality systems in multi-core platforms",
      "Abstract": "Partitioning is a widespread technique that enables the execution of mixed-criticality applications in the same hardware platform. New challenges for the next generation of partitioned systems include the use of multiprocessor architectures and distribution standards in order to open up this technique to a heterogeneous set of emerging scenarios (e.g., cyber-physical systems). This work describes a system architecture that enables the use of data-centric distribution middleware in partitioned real-time embedded systems based on a hypervisor for multi-core, and it focuses on the analysis of the available architectural configurations. We also present an application-case study to evaluate and identify the possible trade-offs among the different configurations.",
      "Keywords": "Application virtualization | Middleware | Multi-core | Real-time systems and embedded systems",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Pérez, Héctor;Gutiérrez, J. Javier;Peiró, Salva;Crespo, Alfons",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84991217529",
      "Primary study DOI": "10.1016/j.jss.2016.09.030",
      "Title": "Approaches to strategic alignment of software process improvement: A systematic literature review",
      "Abstract": "Context: Software process improvement (SPI) aims to increase the effectiveness of a software organization. Many studies indicate that the strategic alignment is a critical factor for the SPI success. However, little is known about practical approaches to achieving and maintaining such alignment. Objective: The goal of this study is to evaluate the validation evidence of the existing approaches to the strategic alignment of SPI. Method: We develop a search protocol that combines database search and snowballing to perform the systematic literature review and evaluate empirical studies by applying rigor and relevance criteria. To evaluate the efficiency of our protocol, we use a “quasi-gold standard” to compute the sensitivity and precision of the search. Result: We identified 30 studies (18 empirical) and 19 approaches to strategic alignment of SPI from 495 retrieved studies. Only three out of the 18 empirical studies were rated as high in the categories rigor and relevance, suggesting the need for a stronger validation of the approaches. Conclusion: We conclude that the lack of empirical validation indicates that the results of the existing approaches have not been adequately transferred to practitioners yet, calling for more rigorous studies on the subject.",
      "Keywords": "Business alignment | Software process improvement | Strategic alignment | Systematic literature review",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Vasconcellos, Francisco J.S.;Landre, Geraldo B.;Cunha, José Adson O.G.;Oliveira, Juliano L.;Ferreira, Ronaldo A.;Vincenzi, Auri M.R.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84963563079",
      "Primary study DOI": "10.1016/j.jss.2016.03.034",
      "Title": "The RIGHT model for Continuous Experimentation",
      "Abstract": "Context: Development of software-intensive products and services increasingly occurs by continuously deploying product or service increments, such as new features and enhancements, to customers. Product and service developers must continuously find out what customers want by direct customer feedback and usage behaviour observation. Objective: This paper examines the preconditions for setting up an experimentation system for continuous customer experiments. It describes the RIGHT model for Continuous Experimentation (Rapid Iterative value creation Gained through High-frequency Testing), illustrating the building blocks required for such a system. Method: An initial model for continuous experimentation is analytically derived from prior work. The model is matched against empirical case study findings from two startup companies and further developed. Results: Building blocks for a continuous experimentation system and infrastructure are presented. Conclusions: A suitable experimentation system requires at least the ability to release minimum viable products or features with suitable instrumentation, design and manage experiment plans, link experiment results with a product roadmap, and manage a flexible business strategy. The main challenges are proper, rapid design of experiments, advanced instrumentation of software to collect, analyse, and store relevant data, and the integration of experiment results in both the product development cycle and the software development process.",
      "Keywords": "Agile software development | Continuous experimentation | Lean software development | Product development | Software architecture | Software development process",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Fagerholm, Fabian;Sanchez Guinea, Alejandro;Mäenpää, Hanna;Münch, Jürgen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85039702464",
      "Primary study DOI": "10.1016/j.jss.2017.12.027",
      "Title": "Introduction to the special issue on software reliability engineering",
      "Abstract": "This Special Issue brings together novel research results in the Software Reliability Engineering area. This is the result of a collective effort from authors and reviewers and includes 23 manuscripts selected from 66 high quality submissions from 27 different countries.",
      "Keywords": "Diagnose and detection | Empirical studies | Formal analysis | Prediction | Reliability engineering | Testing and auditing",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2018-03-01",
      "Publication type": "Editorial",
      "Authors": "Vieira, Marco;Wolter, Katinka",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85025066854",
      "Primary study DOI": "10.1016/j.jss.2017.06.018",
      "Title": "Architecture-level hazard analysis using AADL",
      "Abstract": "Software systems are becoming increasingly important in safety-critical areas. Designing safe software requires a significant emphasis on hazards in the early design phase of software development. In this paper, we propose a hazard analysis approach based on Architecture Analysis and Design Language (AADL). First, to make up the deficiencies of Error Model Annex (EMV2), we create Hazard Model Annex (HMA) to specify the hazard sources, hazards, hazard trigger mechanisms, and mishaps. By using HMA, a safety model can be built by annotating an architecture model with the error model and hazard model. Then, an architecture-level hazard analysis approach is proposed to automatically generate the hazard analysis table. The approach contains the model transformation from a safety model to a Deterministic Stochastic Petri Nets (DSPNs) model for calculating the occurrence probability of hazards and mishaps. In addition, we present the formal semantics for each constituent part of the safety model, define the model mapping rules, and verify the semantic preservation of the transformation. Finally, HMA is implemented to build safety models and two Eclipse plug-ins of our methodology are also implemented. A case study on a flight control software system has been employed to demonstrate the feasibility of our proposed technique.",
      "Keywords": "AADL | Hazard analysis | Hazard model annex | Model transformation | Semantic preservation",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2018-03-01",
      "Publication type": "Article",
      "Authors": "Wei, Xiaomin;Dong, Yunwei;Li, Xuelin;Wong, W. Eric",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84993917401",
      "Primary study DOI": "10.1016/j.jss.2016.10.002",
      "Title": "Adapting collections and arrays: Another step towards the automated adaptation of object ensembles",
      "Abstract": "An important obstacle to reuse in object-oriented development is that objects or more generally components often cannot be plugged together directly due to interface mismatches. Consequently, automating the adaptation of software building blocks has been on the research agenda for quite a while. However, apart from various approaches based on (semi-)formal specifications, adaptation approaches based on test cases have only recently demonstrated that practically useable implementations of this idea are feasible. This article addresses the automated adaptation of arrays and collections in order to increase the applicability of existing test-based adaptation approaches.",
      "Keywords": "Object adaptation | Signature mismatches | Test-driven adaptation",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Seiffert, Dominic;Hummel, Oliver",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84959125153",
      "Primary study DOI": "10.1016/j.jss.2016.02.001",
      "Title": "Test coverage of impacted code elements for detecting refactoring faults: An exploratory study",
      "Abstract": "Refactoring validation by testing is critical for quality in agile development. However, this activity may be misleading when a test suite is insufficiently robust for revealing faults. Particularly, refactoring faults can be tricky and difficult to detect. Coverage analysis is a standard practice to evaluate fault detection capability of test suites. However, there is usually a low correlation between coverage and fault detection. In this paper, we present an exploratory study on the use of coverage data of mostly impacted code elements to identify shortcomings in a test suite. We consider three real open source projects and their original test suites. The results show that a test suite not directly calling the refactored method and/or its callers increases the chance of missing the fault. Additional analysis of branch coverage on test cases shows that there are higher chances of detecting a refactoring fault when branch coverage is high. These results give evidence that a combination of impact analysis with branch coverage could be highly effective in detecting faults introduced by refactoring edits. Furthermore, we propose a statistic model that evidences the correlation of coverage over certain code elements and the suite's capability of revealing refactoring faults.",
      "Keywords": "Coverage | Refactoring | Testing",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Alves, Everton L.G.;Massoni, Tiago;Machado, Patrícia Duarte de Lima",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84991236913",
      "Primary study DOI": "10.1016/j.jss.2016.09.047",
      "Title": "PHash: A memory-efficient, high-performance key-value store for large-scale data-intensive applications",
      "Abstract": "Large-scale data-intensive web services are evolving faster than ever, accelerating global growth in data usage and traffic at a rapid rate. This rapid growth is demanding the expansion of high-cost data infrastructures, which also underscores the industry's need for cost-effective, high-performance distributed key-value stores. Designing key-value stores often involves a trade-off between performance and memory usage. For example, many previous studies focusing on minimizing the memory usage have developed on-disk indexing schemes, leading to lower performance. An alternative design based on in-memory indexing allows better performance, but at the expense of greater memory usage. This paper proposes a novel key-value store called PHash (Packed Hash) based on an advanced design of index and data structures that ensures both high performance and small memory usage. These advantages make the proposed scheme a better fit for processing demanding workloads in large-scale data-intensive applications. Compared to the best-performing competitor, FAWN-DS, the proposed scheme significantly reduces the memory consumption (bytes per key-value) by 83% and improves the GET throughput by up to 27.3% while the PUT throughput decreases by 12.6%. In particular, the GET performance of the proposed scheme reaches up to 99.4% of the optimal performance of the raw SSD (Solid State Drive).",
      "Keywords": "Datastore | Distributed storage | Key-value store | NoSQL | Solid state drives",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Shim, Hyotaek",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84991810740",
      "Primary study DOI": "10.1016/j.jss.2016.10.001",
      "Title": "A mixed integer linear programming optimization approach for multi-cloud capacity allocation",
      "Abstract": "The large success of the Cloud computing, its strong impact on the ICT world and on everyday life testifies the maturity and effectiveness this paradigm achieved in the last few years. Presently, the Cloud market offers a multitude of heterogeneous solutions. However, despite the undeniable advantages, Cloud computing introduced new issues and challenges. In particular, the heterogeneity of the available Cloud services and their pricing models makes the identification of a configuration that minimizes the operating costs of a Cloud application, guaranteeing at the same time the Quality of Service, a challenging task. This situation requires new processes and models to design software architectures and predict costs and performance considering together the large variability in price models and the intrinsic dynamism and multi-tenancy of the Cloud environments. This work aims at providing a novel mathematical approach to this problem presenting a queuing theory based Mixed Integer Linear Program (MILP) to find a promising multi-cloud configuration for a given software architecture. The effectiveness of the proposed model has been favorably evaluated against first principle heuristics currently adopted by practitioners. Furthermore, the configuration returned by the model has been also used as initial solution for a local-search based optimization engine, which exploits more accurate but time-consuming performance models. This combined approach has been shown to improve the quality of the returned solutions by 37% on average and reducing the overall search time by 50% with respect to state-of-the-art heuristics based on tiers utilization thresholds.",
      "Keywords": "MILP | Multi-cloud capacity allocation | Optimization",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Ciavotta, Michele;Ardagna, Danilo;Gibilisco, Giovanni Paolo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85023770296",
      "Primary study DOI": "10.1016/j.jss.2017.06.037",
      "Title": "IoT-TEG: Test event generator system",
      "Abstract": "Internet of Things (IoT) has been paid increasingly attention by the government, academe and industry all over the world. One of the main drawbacks of the IoT systems is the amount of information they have to handle. This information arrives as events that need to be processed in real time in order to make correct decisions. Given that processing the data is crucial, testing the IoT systems that will manage that information is required. In order to test IoT systems, it is necessary to generate a huge number of events with specific structures and values to test the functionalities required by these systems. As this task is very hard and very prone to error if done by hand, this paper addresses the automated generation of appropriate events for testing. For this purpose, a general specification to define event types and its representation are proposed and an event generator is developed based on this definition. Thanks to the adaptability of the proposed specification, the event generator can generate events of an event type, or events which combine the relevant attributes of several event types. Results from experiments and real-world tests show that the developed system meets the demanded requirements.",
      "Keywords": "Complex event processing | Event generator | Event type definition | Internet of Things | Testing",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2018-03-01",
      "Publication type": "Article",
      "Authors": "Gutiérrez-Madroñal, L.;Medina-Bulo, I.;Domínguez-Jiménez, J. J.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84992420701",
      "Primary study DOI": "10.1016/j.jss.2016.09.044",
      "Title": "An assessment of extended finite state machine test selection criteria",
      "Abstract": "Extended finite state machines (EFSMs) provide a rigorous model for the derivation of functional tests for software systems and protocols. Various types of data-flow, control-flow, graph-based, and state machine based test selection criteria can be used for deriving tests from a given EFSM specification. Also, traditional types of state machine based notions of faults, such as transfer and output parameter faults, and common types of assignment faults can be used to describe the fault domains of EFSMs. We present an assessment of the most known types of EFSM test selection criteria such as test suites that cover single transfer faults, double transfer faults, single output parameter faults, and many types of single assignment faults of a given EFSM specification. Also, test suites that cover edge-pair, prime path, prime path with side trip, and all-uses criterion are derived from the graph and flow-graph representations of the specification. We also consider transition tour and random test suites. The assessment ranks the considered test suites in terms of their length and their coverage of single transfer, double transfer, and different type of single assignment faults. Dispersion of the obtained results is assessed and results are summarized.",
      "Keywords": "Extended finite state machines | Fault coverage assessment | Model based testing | Mutation testing | Test derivation",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "El-Fakih, Khaled;Simao, Adenilso;Jadoon, Noshad;Maldonado, Jose Carlos",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026383057",
      "Primary study DOI": "10.1016/j.jss.2017.07.007",
      "Title": "Introduction to the Special Issue on  International Conference on Software Reuse 2015 ",
      "Abstract": "The Special Issue of The Journal of Systems and Software presents a report on the International Conference on Software Reuse (ICSR) which took place in Miami (Florida/US), on 4-6 January, 2015. The first three papers deal with reuse in the context of software product lines. Anas Shatnawi and co-authors in Recovering Software Product Line Architecture of a Family of Object-Oriented Product Variants deal with re-engineering a software product line architecture from existing software variants. De Oliveira and co-authors in Evaluating Lehman’s Laws of Software Evolution within Software Product Lines Industrial Projects consider two real-world case studies to evaluate software product line evolution. The next two papers investigate reuse issues in the realm of open source software. In this context, reuse is privileged because of the easy access to an enormous quantity of source code. The next six papers consider reuse problems in the context of programming languages and environments. A Formal Approach to Implement Java Exceptions in Cooperative Systems, by Ana Cristina Vieira de Melo and Simone Hanazumi, proposes a reliable framework for the implementation of Java exceptions propagation and recovery in cooperative systems based on coordinated atomic action (CAA) concepts.",
      "Keywords": "",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Schaefer, Ina;Stamelos, Ioannis",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84944799971",
      "Primary study DOI": "10.1016/j.jss.2015.08.030",
      "Title": "Continuous performance evaluation and capacity planning using resource profiles for enterprise applications",
      "Abstract": "Continuous delivery (CD) is a software release process that helps to make features and bug fixes rapidly available in new enterprise application (EA) versions. Evaluating the performance of each EA version in a CD process requires a test environment comparable to a production system. Maintaining such systems is labor intensive and expensive. If multiple deployments of the same EA exist, it is often not feasible to maintain test instances for all of these systems. Furthermore, not all deployments are known at the time of a release (e.g., for off-the-shelf products). To address these challenges, this work proposes the use of resource profiles which describe the resource demand per transaction for each component of an EA and allow for performance predictions for different hardware environments and workloads without the need to own corresponding test environments. Within a CD process, resource profiles can be used to detect performance changes in EA versions. Once a version is released, resource profiles can be distributed along with the application binaries to support capacity planning for new deployments. Three integrated experiments for a representative EA provide validation for these capabilities.",
      "Keywords": "Capacity planning | Performance evaluation | Resource profile",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Brunnert, Andreas;Krcmar, Helmut",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84937109068",
      "Primary study DOI": "10.1016/j.jss.2015.06.063",
      "Title": "Continuous software engineering: A roadmap and agenda",
      "Abstract": "Throughout its short history, software development has been characterized by harmful disconnects between important activities such as planning, development and implementation. The problem is further exacerbated by the episodic and infrequent performance of activities such as planning, testing, integration and releases. Several emerging phenomena reflect attempts to address these problems. For example, Continuous Integration is a practice which has emerged to eliminate discontinuities between development and deployment. In a similar vein, the recent emphasis on DevOps recognizes that the integration between software development and its operational deployment needs to be a continuous one. We argue a similar continuity is required between business strategy and development, BizDev being the term we coin for this. These disconnects are even more problematic given the need for reliability and resilience in the complex and data-intensive systems being developed today. We identify a number of continuous activities which together we label as ‘Continuous *’ (i.e. Continuous Star) which we present as part of an overall roadmap for Continuous Software engineering. We argue for a continuous (but not necessarily rapid) software engineering delivery pipeline. We conclude the paper with a research agenda.",
      "Keywords": "Continuous software engineering | DevOps | Lean software development",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Fitzgerald, Brian;Stol, Klaas Jan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84994644415",
      "Primary study DOI": "10.1016/j.jss.2016.10.017",
      "Title": "Understanding the syntactic rule usage in java",
      "Abstract": "Context: Syntax is fundamental to any programming language: syntax defines valid programs. In the 1970s, computer scientists rigorously and empirically studied programming languages to guide and inform language design. Since then, language design has been artistic, driven by the aesthetic concerns and intuitions of language architects. Despite recent studies on small sets of selected language features, we lack a comprehensive, quantitative, empirical analysis of how modern, real-world source code exercises the syntax of its programming language. Objective: This study aims to understand how programming language syntax is employed in actual development and explore their potential applications based on the results of syntax usage analysis. Method: We present our results on the first such study on Java, a modern, mature, and widely-used programming language. Our corpus contains over 5000 open-source Java projects, totalling 150 million source lines of code (SLoC). We study both independent (i.e. applications of a single syntax rule) and dependent (i.e. applications of multiple syntax rules) rule usage, and quantify their impact over time and project size. Results: Our study provides detailed quantitative information and yields insight, particularly (i) confirming the conventional wisdom that the usage of syntax rules is Zipfian; (ii) showing that the adoption of new rules and their impact on the usage of pre-existing rules vary significantly over time; and (iii) showing that rule usage is highly contextual. Conclusions: Our findings suggest potential applications across language design, code suggestion and completion, automatic syntactic sugaring, and language restriction.",
      "Keywords": "Empirical study | Language syntax | Practical language usage",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Qiu, Dong;Li, Bixin;Barr, Earl T.;Su, Zhendong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026675790",
      "Primary study DOI": "10.1016/j.jss.2017.07.026",
      "Title": "PreX: A predictive model to prevent exceptions",
      "Abstract": "The exception handling mechanism has been one of the most used reliability tools in programming languages in the last decades. However, this model has not changed much with time, in spite of advances in programming languages, which include concurrent programming and a shift towards more reactive paradigms, the basic principle remains the same - an exception occurs, and the mechanism reacts. We propose a new paradigm, inspired by Online Failure Prediction (OFP), to predict exceptions and possibly avert them by triggering the execution of preventive actions. The proposed model - PreX - is, thus, proactive, operating in a much finer-grained level than any other form of online failure prediction. OFP has shown promising results in predicting failures at a higher level, but has never been available to the developer, being mainly a system level technique. Thus, PreX will offer developers a new range of revitalization strategies. In this work, we describe the model and evaluate its performance by applying it to a real e-commerce solution, demonstrating how it is capable of predicting and preventing exceptions at run-time. Furthermore, we also show that PreX increases the overall availability and performance of the system under the same conditions.",
      "Keywords": "Exception handling | Failure prediction | Predictive | Preventive | Proactive | Self-healing",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2018-03-01",
      "Publication type": "Article",
      "Authors": "Lourenço, João Ricardo;Cabral, Bruno;Bernardino, Jorge",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028026550",
      "Primary study DOI": "10.1016/j.jss.2017.08.010",
      "Title": "Time-space efficient regression testing for configurable systems",
      "Abstract": "Configurable systems are those that can be adapted from a set of input options, reflected in code in form of variations. Testing these systems is challenging because of the vast array of configuration possibilities where bugs can hide. In the context of evolution, testing becomes even more challenging — not only code but also the set of plausible configurations can change across versions. This paper proposes EvoSPLat, a regression testing technique for configurable systems that explores all dynamically reachable configurations from a test. EvoSPLat supports two important application scenarios of regression testing. In the RCS scenario EvoSPLat prunes configurations (not tests) that are not impacted by changes. In the RTS scenario EvoSPLat prunes tests (not configurations) which are not impacted by changes. To evaluate EvoSPLat under the RCS scenario we used a selection of configurable Java programs. Results indicate that EvoSPLat reduced time by ∼22% and reduced the number of configurations tested by ∼45%. To evaluate EvoSPLat under the RTS scenario we used GCC. Results indicate that EvoSPLat reduced time to run tests by ∼35%. Overall, results suggest that EvoSPLat is a promising technique to test configurable systems in the prevalent scenario of evolution.",
      "Keywords": "Configurable systems | Regression testing",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2018-03-01",
      "Publication type": "Article",
      "Authors": "Souto, Sabrina;d'Amorim, Marcelo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84992146690",
      "Primary study DOI": "10.1016/j.jss.2016.10.004",
      "Title": "An energy efficient and load balanced distributed routing scheme for wireless sensor networks with holes",
      "Abstract": "In this paper we present a new approach to route packets in the presence of routing holes. In our proposal, nodes cooperate to determine the approximate polygon of a specific hole and then exchange information about the approximate polygon. Based on the hole covering parallelogram and the hole view angle of a specific node, packets can be forwarded along an escape route that bends around the hole. We rigorously prove that the Euclidean stretch of an escape route is bounded. Simulation results show that the proposed scheme can save more than 16% of the energy consumption and 7% of the network lifetime in the comparison with existing routing algorithms. The average length of routing paths in our approach is less than 60% of other routing schemes.",
      "Keywords": "Angle of view | Hole | Routing | Wireless sensor",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Nguyen, Khanh Van;Nguyen, Phi Le;Vu, Quoc Huy;Do, Tien Van",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026630009",
      "Primary study DOI": "10.1016/j.jss.2017.07.042",
      "Title": "An empirical study of collaborative model and its security risk in Android",
      "Abstract": "Android provides a framework for the development of collaborative applications, which is considered as one of the reasons behind its success. Collaborative model provides flexibility to an application in utilizing services offered by other applications. This approach offers several advantages to developers, such as allowing them to dedicate all of their resources in developing only core functionalities of an application while leveraging services offered by other applications for its auxiliary functionalities. However, the collaborative model also has some disadvantages, such as opening of attack surfaces in an application during exposure of some of its components as it offers its services. Malicious actions can be performed through the exposed components of the application. Android provides permission-based security to protect the exposed components. However, developers must implement the security correctly. In this paper, we empirically evaluate the scale of the collaborative model adopted by Android applications. We also investigate various methods to achieve collaboration among applications. Furthermore, we evaluate the scale of security risk instigated by the collaborative model and perform several other empirical studies on 13,944 Android applications.",
      "Keywords": "Android applications | Collaborative application model | Inter-application communications | Permission-based security | Security risk assessment",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2018-03-01",
      "Publication type": "Article",
      "Authors": "Jha, Ajay Kumar;Lee, Woo Jin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84957818901",
      "Primary study DOI": "10.1016/j.jss.2015.12.015",
      "Title": "Continuous deployment of software intensive products and services: A systematic mapping study",
      "Abstract": "The software intensive industry is moving towards the adoption of a value-driven and adaptive real-time business paradigm. The traditional view of software as an item that evolves through releases every few months is being replaced by the continuous evolution of software functionality. This study aims to classify and analyse the literature related to continuous deployment in the software domain in order to scope the phenomenon, provide an overview of the state-of-the-art, investigate the scientific evidence in the reported results and identify areas suitable for further research. We conducted a systematic mapping study and classified the continuous deployment literature. The benefits and challenges related to continuous deployment were also analysed. RESULTS: The systematic mapping study includes 50 primary studies published between 2001 and 2014. An in-depth analysis of the primary studies revealed ten recurrent themes that characterize continuous deployment and provide researchers with directions for future work. In addition, a set of benefits and challenges of which practitioners may take advantage were identified. CONCLUSION: Overall, although the topic area is very promising, it is still in its infancy, thus offering a plethora of new opportunities for both researchers and software intensive companies.",
      "Keywords": "Continuous deployment | Software development | Systematic mapping study",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Rodríguez, Pilar;Haghighatkhah, Alireza;Lwakatare, Lucy Ellen;Teppola, Susanna;Suomalainen, Tanja;Eskeli, Juho;Karvonen, Teemu;Kuvaja, Pasi;Verner, June M.;Oivo, Markku",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84957895226",
      "Primary study DOI": "10.1016/j.jss.2016.01.024",
      "Title": "A license to kill   Improving UCSD in Agile development",
      "Abstract": "Agile development processes, such as Scrum, focus on communication, developer collaboration and delivery of working software early and continuously. User-centered systems design (UCSD) is a process emphasizing usability and the user experience throughout the system life cycle. It highlights the UCSD activities: understanding the context of use, iterative prototyping to explore the design space and active collaboration with users throughout the software development. Agile processes are by many assumed to address similar issues as UCSD, hence, by applying Agile processes the systems would become usable for the end-users and their user experience should improve. This paper discusses and interprets findings on UCSD activities in Agile projects in practice, that are analyzed according to the fundamental principles from the Agile manifesto. We show that Agile development has much to gain from integrating UCSD, and give guidance on how to integrate UCSD in Agile processes. User experience (UX) professionals need a more explicit role in the Agile projects and more authority. We recommend that they receive a “license to kill” design suggestions that are not perceived as useful by the users.",
      "Keywords": "Agile development | Scrum | User-centered system design",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Larusdottir, Marta;Gulliksen, Jan;Cajander, Åsa",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84991221546",
      "Primary study DOI": "10.1016/j.jss.2016.09.049",
      "Title": "A product-line model-driven engineering approach for generating feature-based mobile applications",
      "Abstract": "A significant challenge faced by the mobile application industry is developing and maintaining multiple native variants of mobile applications to support different mobile operating systems, devices and varying application functional requirements. The current industrial practice is to develop and maintain these variants separately. Any potential change has to be applied across variants manually, which is neither efficient nor scalable. We consider the problem of supporting multiple platforms as a ‘software product-line engineering’ problem. The paper proposes a novel application of product-line model-driven engineering to mobile application development and addresses the key challenges of feature-based native mobile application variants for multiple platforms. Specifically, we deal with three types of variations in mobile applications: variation due to operation systems and their versions, software and hardware capabilities of mobile devices, and functionalities offered by the mobile application. We develop a tool MOPPET that automates the proposed approach. Finally, the results of applying the approach on two industrial case studies show that the proposed approach is applicable to industrial mobile applications and have potential to significantly reduce the development effort and time.",
      "Keywords": "Feature model | Mobile applications | Software product-line engineering",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Usman, Muhammad;Iqbal, Muhammad Zohaib;Khan, Muhammad Uzair",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021174086",
      "Primary study DOI": "10.1016/j.jss.2017.05.047",
      "Title": "Analyzing software evolution and quality by extracting Asynchrony change patterns",
      "Abstract": "Change patterns describe two or more files were often changed together during the development or the maintenance of software systems. Several studies have been presented to detect change patterns and to analyze their types and their impact on software quality. In this context, we introduced the Asynchrony change pattern to describes a set of files that always change together in the same change periods, regardless developers who maintained them. In this paper, we investigate the impact of Asynchrony change pattern on design and code smells such as anti-patterns and code clones. Concretely, we conduct an empirical study by detecting Asynchrony change patterns, anti-patterns and code clones occurrences on 22 versions of four software systems and analyzing their fault-proneness. Results show that cloned files that follow the same Asynchrony change patterns have significantly increased fault-proneness with respect to other clones, and that anti-patterns following the same Asynchrony change pattern can be up to five times more risky in terms of fault-proneness as compared to other anti-patterns. Asynchrony change patterns thus seem to be strong indicators of fault-proneness for clones and anti-patterns.",
      "Keywords": "Anti-patterns | Change patterns | Clones | Fault-proneness | Software quality",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Jaafar, Fehmi;Lozano, Angela;Guéhéneuc, Yann Gaël;Mens, Kim",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028248184",
      "Primary study DOI": "10.1016/j.jss.2016.05.042",
      "Title": "On the use of developers  context for automatic refactoring of software anti-patterns",
      "Abstract": "Anti-patterns are poor solutions to design problems that make software systems hard to understand and extend. Entities involved in anti-patterns are reported to be consistently related to high change and fault rates. Refactorings, which are behavior preserving changes are often performed to remove anti-patterns from software systems. Developers are advised to interleave refactoring activities with their regular coding tasks to remove anti-patterns, and consequently improve software design quality. However, because the number of anti-patterns in a software system can be very large, and their interactions can require a solution in a set of conflicting objectives, the process of manual refactoring can be overwhelming. To automate this process, previous works have modeled anti-patterns refactoring as a batch process where a program provides a solution for the total number of classes in a system, and the developer has to examine a long list of refactorings, which is not feasible in most situations. Moreover, these proposed solutions often require that developers modify classes on which they never worked before (i.e., classes on which they have little or no knowledge). To improve on these limitations, this paper proposes an automated refactoring approach, ReCon (Refactoring approach based on task Context), that leverages information about a developer's task (i.e., the list of code entities relevant to the developer's task) and metaheuristics techniques to compute the best sequence of refactorings that affects only entities in the developer's context. We mine 1705 task contexts (collected using the Eclipse plug-in Mylyn) and 1013 code snapshots from three open-source software projects (Mylyn, PDE, Eclipse Platform) to assess the performance of our proposed approach. Results show that ReCon can remove more than 50% of anti-patterns in a software system, using fewer resources than the traditional approaches from the literature.",
      "Keywords": "Anti-patterns | Automatic refactoring | Interaction traces | Metaheuristics | Software maintenance | Task context",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Morales, Rodrigo;Soh, Zéphyrin;Khomh, Foutse;Antoniol, Giuliano;Chicano, Francisco",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85000352336",
      "Primary study DOI": "10.1016/j.jss.2016.11.032",
      "Title": "Exploiting traceability uncertainty between software architectural models and extra-functional results",
      "Abstract": "Deriving extra-functional properties (e.g., performance, security, reliability) from software architectural models is the cornerstone of software development as it supports the designers with quantitative predictions of system qualities. However, the problem of interpreting results from quantitative analysis of extra-functional properties is still challenging because it is hard to understand how the analysis results (e.g., response time, data confidentiality, mean time to failure) trace back to the architectural model elements (i.e., software components, interactions among components, deployment nodes). The goal of this paper is to automate the traceability between software architectural models and extra-functional results, such as performance and security, by investigating the uncertainty while bridging these two domains. Our approach makes use of extra-functional patterns and antipatterns, such as performance antipatterns and security patterns, to deduce the logical consequences between the architectural elements and analysis results and automatically build a graph of traces, thus to identify the most critical causes of extra-functional flaws. We developed a tool that jointly considers SOftware and Extra-Functional concepts (SoEfTraceAnalyzer), and it automatically builds model-to-results traceability links. This paper demonstrates the effectiveness of our automated and tool supported approach on three case studies, i.e., two academic research projects and one industrial system.",
      "Keywords": "Extra-functional results | Software modeling | Traceability | Uncertainty",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Trubiani, Catia;Ghabi, Achraf;Egyed, Alexander",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84961231169",
      "Primary study DOI": "10.1016/j.jss.2016.02.047",
      "Title": "Rapid quality assurance with Requirements Smells",
      "Abstract": "Bad requirements quality can cause expensive consequences during the software development lifecycle, especially if iterations are long and feedback comes late. We aim at a light-weight static requirements analysis approach that allows for rapid checks immediately when requirements are written down. We transfer the concept of code smells to requirements engineering as Requirements Smells. To evaluate the benefits and limitations, we define Requirements Smells, realize our concepts for a smell detection in a prototype called Smella and apply Smella in a series of cases provided by three industrial and a university context. The automatic detection yields an average precision of 59% at an average recall of 82% with high variation. The evaluation in practical environments indicates benefits such as an increase of the awareness of quality defects. Yet, some smells were not clearly distinguishable. Lightweight smell detection can uncover many practically relevant requirements defects in a reasonably precise way. Although some smells need to be defined more clearly, smell detection provides a helpful means to support quality assurance in requirements engineering, for instance, as a supplement to reviews.",
      "Keywords": "Automatic defect detection | Requirements engineering | Requirements Smells",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Femmer, Henning;Méndez Fernández, Daniel;Wagner, Stefan;Eder, Sebastian",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026505190",
      "Primary study DOI": "10.1109/MSR.2017.25",
      "Title": "On the Differences between Unit and Integration Testing in the TravisTorrent Dataset",
      "Abstract": "Already from the early days of testing, practitioners distinguish between unit tests and integration tests as a strategy to locate defects. Unfortunately, the mining software engineering community rarely distinguishes between these two strategies, mainly because it is not straightforward to separate them in the code repositories under study. In this paper we exploited the TravisTorrent dataset provided for the MSR 2017 mining challenge, separated unit tests from integration tests, and correlated these against the workflow as recorded in the corresponding issue reports. Further analysis confirmed that it is worthwhile to treat unit tests and integration tests differently: we discovered that unit tests cause more breaking builds, that fixing the defects exposed by unit tests takes longer and implies more coordination between team members.",
      "Keywords": "Integration testing | Software testing | Unit testting",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Orellana, Gerardo;Laghari, Gulsher;Murgia, Alessandro;Demeyer, Serge",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026500853",
      "Primary study DOI": "10.1109/MSR.2017.50",
      "Title": "Rediscovery datasets: Connecting duplicate reports",
      "Abstract": "The same defect can be rediscovered by multiple clients, causing unplanned outages and leading to reduced customer satisfaction. In the case of popular open source software, high volume of defects is reported on a regular basis. A large number of these reports are actually duplicates / rediscoveries of each other. Researchers have analyzed the factors related to the content of duplicate defect reports in the past. However, some of the other potentially important factors, such as the inter-relationships among duplicate defect reports, are not readily available in defect tracking systems such as Bugzilla. This information may speed up bug fixing, enable efficient triaging, improve customer profiles, etc. In this paper, we present three defect rediscovery datasets mined from Bugzilla. The datasets capture data for three groups of open source software projects: Apache, Eclipse, and KDE. The datasets contain information about approximately 914 thousands of defect reports over a period of 18 years (1999-2017) to capture the inter-relationships among duplicate defects. We believe that sharing these data with the community will help researchers and practitioners to better understand the nature of defect rediscovery and enhance the analysis of defect reports.",
      "Keywords": "",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Sadat, Mefta;Bener, Ayse Basar;Miranskyy, Andriy",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026553212",
      "Primary study DOI": "10.1109/MSR.2017.18",
      "Title": "A large-scale study of the impact of feature selection techniques on defect classification models",
      "Abstract": "The performance of a defect classification modeldepends on the features that are used to train it. Feature redundancy, correlation, and irrelevance can hinder the performance of a classification model. To mitigate this risk, researchers often use feature selection techniques, which transform or select a subset of the features in order to improve the performance of a classification model. Recent studies compare the impact of different feature selection techniques on the performance of defect classification models. However, these studies compare a limited number of classification techniques and have arrived at contradictory conclusions about the impact of feature selection techniques. To address this limitation, we study 30 feature selection techniques (11 filter-based ranking techniques, six filter based subset techniques, 12 wrapper-based subset techniques, and a no feature selection configuration) and 21 classification techniques when applied to 18 datasets from the NASA and PROMISE corpora. Our results show that a correlation-based filter-subset feature selection technique with a BestFirst search method outperforms other feature selection techniques across the studied datasets (it outperforms in 70%-87% of the PROMISE-NASA data sets) and across the studied classification techniques (it outperforms for 90% of the techniques). Hence, we recommend the application of such a selection technique when building defect classification models.",
      "Keywords": "Defect Classification Models | Feature Selection Techniques",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Ghotra, Baljinder;McIntosh, Shane;Hassan, Ahmed E.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026531580",
      "Primary study DOI": "10.1109/MSR.2017.47",
      "Title": "Bootstrapping a Lexicon for Emotional Arousal in Software Engineering",
      "Abstract": "Emotional arousal increases activation and performance but may also lead to burnout in software development. We present the first version of a Software Engineering Arousal lexicon (SEA) that is specifically designed to address the problem of emotional arousal in the software developer ecosystem. SEA is built using a bootstrapping approach that combines word embedding model trained on issue-Tracking data and manual scoring of items in the lexicon. We show that our lexicon is able to differentiate between issue priorities, which are a source of emotional activation and then act as a proxy for arousal. The best performance is obtained by combining SEA (428 words) with a previously created general purpose lexicon by Warriner et al. (13,915 words) and it achieves Cohen's d effect sizes up to 0.5.",
      "Keywords": "Emotional Arousal | Empirical Software Engineering | Issue Report | Lexicon | Sentiment Analysis",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Mantyla, Mika V.;Novielli, Nicole;Lanubile, Filippo;Claes, Maelick;Kuutila, Miikka",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026548340",
      "Primary study DOI": "10.1109/MSR.2017.53",
      "Title": "Predicting likelihood of requirement implementation within the planned iteration: An empirical study at IBM",
      "Abstract": "There has been a significant interest in the estimation of time and effort in fixing defects among both software practitioners and researchers over the past two decades. However, most of the focus has been on prediction of time and effort in resolving bugs, without much regard to predicting time needed to complete high-level requirements, a critical step in release planning. In this paper, we describe a mixed-method empirical study on three large IBM projects in which we developed and evaluated a process of training a predictive model constituting a set of 29 features in nine categories in order to predict if a requirement will be completed within its planned iteration. We conducted feature engineering through iterative interviews with IBM practitioners as well as analysis of large development repositories of these three projects. Using machine learning techniques, we were able to make predictions on completion time of requirements at four different stages of their lifetime. Using our industrial partner's interest in high precision over recall, we then adopted a cost sensitive learning method and maximized precision of predictions (ranging from 0.8 to 0.97) while maintaining an acceptable recall. We also ranked the features based on their relative importance to the optimized predictive model. We show that although satisfying predictions can be made at early stages, performance of predictions improves over time by taking advantage of requirements' progress data. Furthermore, feature importance ranking results show that although importance of features are highly dependent on project and prediction stage, there are certain features (e.g. requirement creator, time remained to the end of iteration, time since last requirement summary change and number of times requirement has been replanned for a new iteration) that emerge as important across most projects and stages, implying future worthwhile research directions for both researchers and practitioners.",
      "Keywords": "Completion Time Prediction | Machine Learning | Mining Software Repositories | Release Planning",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Dehghan, Ali;Neal, Adam;Blincoe, Kelly;Linaker, Johan;Damian, Daniela",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026514349",
      "Primary study DOI": "10.1109/MSR.2017.32",
      "Title": "How does contributors' involvement influence the build status of an open-source software project?",
      "Abstract": "The recent introduction of the pull-based development model promoted agile development practices such as Code Reviews and Continuous Integration (CI). CI, in particular, is currently a standard development practice in open-source software (OSS) projects. Although it is well-known that OSS contributors have different involvements (e.g., while some developers drive the project, there is a long tail of peripheral developers), little is known about how the contributor's degree of participation can influence the build status of an OSS project. Through TravisTorrent's dataset, we compare the success rates of builds made by casual and non-casual contributors and what factors on their contributions may influence the build result. Our results suggest that there is no representative difference between their build success (they are similar in 85% of the analyzed projects), meaning that being a casual contributor is not a strong indicator for creating failing builds. Also, factors like the size of their contributions and the number of project configurations (jobs) have the potential of impacting the build success.",
      "Keywords": "Casual Contributors | Continuous Integration | Open-Source Development",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Reboucas, Marcel;Santos, Renato O.;Pinto, Gustavo;Castor, Fernando",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026518861",
      "Primary study DOI": "10.1109/MSR.2017.4",
      "Title": "The impact of using regression models to build defect classifiers",
      "Abstract": "It is common practice to discretize continuous defect counts into defective and non-defective classes and use them as a target variable when building defect classifiers (discretized classifiers). However, this discretization of continuous defect counts leads to information loss that might affect the performance and interpretation of defect classifiers. Another possible approach to build defect classifiers is through the use of regression models then discretizing the predicted defect counts into defective and non-defective classes (regression-based classifiers). In this paper, we compare the performance and interpretation of defect classifiers that are built using both approaches (i.e., discretized classifiers and regression-based classifiers) across six commonly used machine learning classifiers (i.e., linear/logistic regression, random forest, KNN, SVM, CART, and neural networks) and 17 datasets. We find that: i) Random forest based classifiers outperform other classifiers (best AUC) for both classifier building approaches, ii) In contrast to common practice, building a defect classifier using discretized defect counts (i.e., discretized classifiers) does not always lead to better performance. Hence we suggest that future defect classification studies should consider building regression-based classifiers (in particular when the defective ratio of the modeled dataset is low). Moreover, we suggest that both approaches for building defect classifiers should be explored, so the best-performing classifier can be used when determining the most influential features.",
      "Keywords": "Bug prediction | Classification via regression | Discretization | Model Interpretation | Non-Discretization | Random forest",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Rajbahadur, Gopi Krishnan;Wang, Shaowei;Kamei, Yasutaka;Hassan, Ahmed E.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026557235",
      "Primary study DOI": "10.1109/MSR.2017.64",
      "Title": "How do apps evolve in their permission requests? a preliminary study",
      "Abstract": "We present a preliminary study to understand how apps evolve in their permission requests across different releases. We analyze over 14K releases of 227 Android apps, and we see how permission requests change and how they are used. We find that apps tend to request more permissions in their evolution, and many of the newly requested permissions are initially overprivileged. Our qualitative analysis, however, shows that the results that popular tools report on overprivileged apps may be biased by incomplete information or by other factors. Finally, we observe that when apps no longer request a permission, it does not necessarily mean that the new release offers less in terms of functionalities.",
      "Keywords": "",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Calciati, Paolo;Gorla, Alessandra",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026508847",
      "Primary study DOI": "10.1109/MSR.2017.6",
      "Title": "Analyzing program dependencies in Java EE applications",
      "Abstract": "Program dependency artifacts such as call graphs help support a number of software engineering tasks such as software mining, program understanding, debugging, feature location, software maintenance and evolution. Java Enterprise Edition (JEE) applications represent a significant part of the recent legacy applications, and we are interested in modernizing them. This modernization involves, among other things, analyzing dependencies between their various components/tiers. JEE applications tend to be multilanguage, rely on JEE container services, and make extensive use of late binding techniques-All of which makes finding such dependencies difficult. In this paper, we describe some of these difficulties and how we addressed them to build a dependency call graph. We developed our tool called DeJEE (Dependencies in JEE) as an Eclipse plug-in. We applied DeJEE on two open-source JEE applications: Java PetStore and JSP Blog. The results show that DeJEE is able to identify different types of JEE dependencies.",
      "Keywords": "Code Analysis | Container Services | Java EE application | Modernization | Program Dependency | Server Pages",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Shatnawi, Anas;Mili, Hafedh;El Boussaidi, Ghizlane;Boubaker, Anis;Gueheneuc, Yann Gael;Moha, Naouel;Privat, Jean;Abdellatif, Manel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026533835",
      "Primary study DOI": "10.1109/MSR.2017.28",
      "Title": "SpreadCluster: Recovering Versioned Spreadsheets through Similarity-Based Clustering",
      "Abstract": "Version information plays an important role in spreadsheet understanding, maintaining and quality improving. However, end users rarely use version control tools to document spreadsheets' version information. Thus, the spreadsheets' version information is missing, and different versions of a spreadsheet coexist as individual and similar spreadsheets. Existing approaches try to recover spreadsheet version information through clustering these similar spreadsheets based on spreadsheet filenames or related email conversation. However, the applicability and accuracy of existing clustering approaches are limited due to the necessary information (e.g., filenames and email conversation) is usually missing. We inspected the versioned spreadsheets in VEnron, which is extracted from the Enron Corporation. In VEnron, the different versions of a spreadsheet are clustered into an evolution group. We observed that the versioned spreadsheets in each evolution group exhibit certain common features (e.g., similar table headers and worksheet names). Based on this observation, we proposed an automatic clustering algorithm, SpreadCluster. SpreadCluster learns the criteria of features from the versioned spreadsheets in VEnron, and then automatically clusters spreadsheets with the similar features into the same evolution group. We applied SpreadCluster on all spreadsheets in the Enron corpus. The evaluation result shows that SpreadCluster could cluster spreadsheets with higher precision (78.5% vs. 59.8%) and recall rate (70.7% vs. 48.7%) than the filename-based approach used by VEnron. Based on the clustering result by SpreadCluster, we further created a new versioned spreadsheet corpus VEnron2, which is much bigger than VEnron (12,254 vs. 7,294 spreadsheets). We also applied SpreadCluster on the other two spreadsheet corpora FUSE and EUSES. The results show that SpreadCluster can cluster the versioned spreadsheets in these two corpora with high precision (91.0% and 79.8%).",
      "Keywords": "Clustering | Evolution | Spreadsheet | Version",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Xu, Liang;Dou, Wensheng;Gao, Chushu;Wang, Jie;Wei, Jun;Zhong, Hua;Huang, Tao",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026501092",
      "Primary study DOI": "10.1109/MSR.2017.17",
      "Title": "Predicting Usefulness of Code Review Comments Using Textual Features and Developer Experience",
      "Abstract": "Although peer code review is widely adopted in both commercial and open source development, existing studies suggest that such code reviews often contain a significant amount of non-useful review comments. Unfortunately, to date, no tools or techniques exist that can provide automatic support in improving those non-useful comments. In this paper, we first report a comparative study between useful and non-useful review comments where we contrast between them using their textual characteristics, and reviewers' experience. Then, based on the findings from the study, we develop RevHelper, a prediction model that can help the developers improve their code review comments through automatic prediction of their usefulnessduring review submission. Comparative study using 1,116 review comments suggested that useful comments share more vocabulary with the changed code, contain salient items like relevant code elements, and their reviewers are generally more experienced. Experiments using 1,482 review comments report that our model can predict comment usefulness with 66% prediction accuracy which is promising. Comparison with three variants of a baseline model using a case study validates our empirical findings and demonstrates the potential of our model.",
      "Keywords": "change triggering capability | code element | Code review quality | review comment usefulness | reviewing experience",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Rahman, Mohammad Masudur;Roy, Chanchal K.;Kula, Raula G.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026520340",
      "Primary study DOI": "10.1109/MSR.2017.15",
      "Title": "Some from Here, Some from There: Cross-Project Code Reuse in GitHub",
      "Abstract": "Code reuse has well-known benefits on code quality, coding efficiency, and maintenance. Open Source Software (OSS) programmers gladly share their own code and they happily reuse others'. Social programming platforms like GitHub have normalized code foraging via their common platforms, enabling code search and reuse across different projects. Removing project borders may facilitate more efficient code foraging, and consequently faster programming. But looking for code across projects takes longer and, once found, may be more challenging to tailor to one's needs. Learning how much code reuse goes on across projects, and identifying emerging patterns in past cross-project search behavior may help future foraging efforts. To understand cross-project code reuse, here we present an in-depth study of cloning in GitHub. Using Deckard, a clone finding tool, we identified copies of code fragments across projects, and investigate their prevalence and characteristics using statistical and network science approaches, and with multiple case studies. By triangulating findings from different methods, we find that cross-project cloning is prevalent in GitHub, ranging from cloning few lines of code to whole project repositories. Some of the projects serve as popular sources of clones, and others seem to contain more clones than their fair share. Moreover, we find that ecosystem cloning follows an onion model: most clones come from the same project, then from projects in the same application domain, and finally from projects in different domains. Our results show directions for new tools that can facilitate code foraging and sharing within GitHub.",
      "Keywords": "Code reuse | Cross-project clones | GitHub",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Gharehyazie, Mohammad;Ray, Baishakhi;Filkov, Vladimir",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026531932",
      "Primary study DOI": "10.1109/MSR.2017.22",
      "Title": "An Exploratory Study on Assessing the Impact of Environment Variations on the Results of Load Tests",
      "Abstract": "Large-scale software systems like Amazon and healthcare.gov are used by thousands or millions of people every day. To ensure the quality of these systems, load testing is a required testing procedure in addition to the conventional functional testing techniques like unit and system integration testing. One of the important requirements of load testing is to create a field-like test environment. Unfortunately, this task is often very challenging due to reasons like security and rapid field updates. In this paper, we have conducted an exploratory study on the impact of environment variations on the results of load tests. We have run over 110 hours load tests, which examine the system's behavior under load with various changes (e.g., installing an antivirus program) to the targeted deployment environment. We call such load tests as environment-variation-based load tests. Case studies in three open source systems have shown that there is a clear performance impact on the system's performance due to these environment changes. Different scenarios react differently to the changes in the underlying computing resources. When predicting the performance of the system under environment changes that are not previously load tested, our ensemble models out-perform (24%-94% better) the baseline models.",
      "Keywords": "load testing | mining performance counters | performance analysis | software analytics",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Gao, Ruoyu;Jiang, Zhen Ming",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026506794",
      "Primary study DOI": "10.1109/MSR.2017.42",
      "Title": "Choosing an NLP Library for Analyzing Software Documentation: A Systematic Literature Review and a Series of Experiments",
      "Abstract": "To uncover interesting and actionable information from natural language documents authored by software developers, many researchers rely on 'out-of-The-box' NLP libraries. However, software artifacts written in natural language are different from other textual documents due to the technical language used. In this paper, we first analyze the state of the art through a systematic literature review in which we find that only a small minority of papers justify their choice of an NLP library. We then report on a series of experiments in which we applied four state-of-The-Art NLP libraries to publicly available software artifacts from three different sources. Our results show low agreement between different libraries (only between 60% and 71% of tokens were assigned the same part-of-speech tag by all four libraries) as well as differences in accuracy depending on source: For example, spaCy achieved the best accuracy on Stack Overflow data with nearly 90% of tokens tagged correctly, while it was clearly outperformed by Google's SyntaxNet when parsing GitHub ReadMe files. Our work implies that researchers should make an informed decision about the particular NLP library they choose and that customizations to libraries might be necessary to achieve good results when analyzing software artifacts written in natural language.",
      "Keywords": "Natural language processing | NLP libraries | Part-of-Speech tagging | Software documentation",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Al Omran, Fouad Nasser A.;Treude, Christoph",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026517849",
      "Primary study DOI": "10.1109/MSR.2017.43",
      "Title": "Rationale in Development Chat Messages: An Exploratory Study",
      "Abstract": "Chat messages of development teams play an increasinglysignificant role in software development, having replacedemails in some cases. Chat messages contain informationabout discussed issues, considered alternatives and argumentationleading to the decisions made during software development. These elements, defined as rationale, are invaluable duringsoftware evolution for documenting and reusing developmentknowledge. Rationale is also essential for coping with changesand for effective maintenance of the software system. However, exploiting the rationale hidden in the chat messages is challengingdue to the high volume of unstructured messages covering a widerange of topics. This work presents the results of an exploratorystudy examining the frequency of rationale in chat messages, the completeness of the available rationale and the potential ofautomatic techniques for rationale extraction. For this purpose, we apply content analysis and machine learning techniques onmore than 8,700 chat messages from three software developmentprojects. Our results show that chat messages are a rich source ofrationale and that machine learning is a promising technique fordetecting rationale and identifying different rationale elements.",
      "Keywords": "Chat messages | Empirical Software Engineering | Rationale",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Alkadhi, Rana;Lata, Teodora;Guzmany, Emitza;Bruegge, Bernd",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026511853",
      "Primary study DOI": "10.1109/MSR.2017.36",
      "Title": "Built to last or built too fast? Evaluating prediction models for build times",
      "Abstract": "Automated builds are integral to the Continuous Integration (CI) software development practice. In CI, developers are encouraged to integrate early and often. However, long build times can be an issue when integrations are frequent. This research focuses on finding a balance between integrating often and keeping developers productive. We propose and analyze models that can predict the build time of a job. Such models can help developers to better manage their time and tasks. Also, project managers can explore different factors to determine the best setup for a build job that will keep the build wait time to an acceptable level. Software organizations transitioning to CI practices can use the predictive models to anticipate build times before CI is implemented. The research community can modify our predictive models to further understand the factors and relationships affecting build times.",
      "Keywords": "build time | builds | continuous integration | Machine learning",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Bisong, Ekaba;Tran, Eric;Baysal, Olga",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026533492",
      "Primary study DOI": "10.1109/MSR.2017.41",
      "Title": "Developer mistakes in writing android manifests: An empirical study of configuration errors",
      "Abstract": "Each Android app must have an Android manifest file. It is one of the most important configuration files manually written by developers. In addition to various configuration parameters required to run an app, it also contains configuration parameters which are used to implement security, compatibility, and accessibility of an app. Any mistakes in writing the manifest file can cause serious implications in terms of security, reliability, and availability of an app. In this paper, we study and report different types of mistakes committed by developers in writing Android manifest files. The study was performed on 13,483 real-world Android apps. We also present an open source rule-based static analysis tool which detects developer mistakes in the manifest file. The tool generates a warning message if it detects any misconfigurations in the manifest file. We used the tool to perform the empirical study and it generated total 59,547 configuration errors in 11,110 apps. Only 2,373 apps, among studied apps, do not have any configuration errors.",
      "Keywords": "Android Apps | Android Manifest | Configuration Errors | Rule-Based Error Detection",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Jha, Ajay Kumar;Lee, Sunghee;Lee, Woo Jin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026511687",
      "Primary study DOI": "10.1109/MSR.2017.23",
      "Title": "Understanding the origins of mobile app vulnerabilities: A large-scale measurement study of free and paid apps",
      "Abstract": "This paper reports a large-scale study that aims to understand how mobile application (app) vulnerabilities are associated with software libraries. We analyze both free and paid apps. Studying paid apps was quite meaningful because it helped us understand how differences in app development/maintenance affect the vulnerabilities associated with libraries. We analyzed 30k free and paid apps collected from the official Android marketplace. Our extensive analyses revealed that approximately 70%/50% of vulnerabilities of free/paid apps stem from software libraries, particularly from third-party libraries. Somewhat paradoxically, we found that more expensive/popular paid apps tend to have more vulnerabilities. This comes from the fact that more expensive/popular paid apps tend to have more functionality, i.e., more code and libraries, which increases the probability of vulnerabilities. Based on our findings, we provide suggestions to stakeholders of mobile app distribution ecosystems.",
      "Keywords": "Mobile App | Software Library | Vulnerability",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Watanabe, Takuya;Akiyama, Mitsuaki;Kanei, Fumihiro;Shioji, Eitaro;Takata, Yuta;Sun, Bo;Ishi, Yuta;Shibahara, Toshiki;Yagi, Takeshi;Mori, Tatsuya",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026546946",
      "Primary study DOI": "10.1109/MSR.2017.62",
      "Title": "Oops, My Tests Broke the Build: An Explorative Analysis of Travis CI with GitHub",
      "Abstract": "Continuous Integration (CI) has become a best practice of modern software development. Yet, at present, we have a shortfall of insight into the testing practices that are common in CI-based software development. In particular, we seek quantifiable evidence on how central testing is to the CI process, how strongly the project language influences testing, whether different integration environments are valuable and if testing on the CI can serve as a surrogate to local testing in the IDE. In an analysis of 2,640,825 Java and Ruby builds on Travis CI, we find that testing is the single most important reason why builds fail. Moreover, the programming language has a strong influence on both the number of executed tests, their run time, and proneness to fail. The use of multiple integration environments leads to 10% more failures being caught at build time. However, testing on Travis CI does not seem an adequate surrogate for running tests locally in the IDE. To further research on Travis CI with GitHub, we introduce TravisTorrent.",
      "Keywords": "",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Beller, Moritz;Gousios, Georgios;Zaidman, Andy",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026518178",
      "Primary study DOI": "10.1109/MSR.2017.13",
      "Title": "Stack Overflow in Github: Any Snippets There?",
      "Abstract": "When programmers look for how to achieve certain programming tasks, Stack Overflow is a popular destination in search engine results. Over the years, Stack Overflow has accumulated an impressive knowledge base of snippets of code that are amply documented. We are interested in studying how programmers use these snippets of code in their projects. Can we find Stack Overflow snippets in real projects? When snippets are used, is this copy literal or does it suffer adaptations? And are these adaptations specializations required by the idiosyncrasies of the target artifact, or are they motivated by specific requirements of the programmer? The large-scale study presented on this paper analyzes 909k non-fork Python projects hosted on Github, which contain 290M function definitions, and 1.9M Python snippets captured in Stack Overflow. Results are presented as quantitative analysis of block-level code cloning intra and inter Stack Overflow and GitHub, and as an analysis of programming behaviors through the qualitative analysis of our findings.",
      "Keywords": "Code clone | Code reuse | Large-scale analysis",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Yang, Di;Martins, Pedro;Saini, Vaibhav;Lopes, Cristina",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026507031",
      "Primary study DOI": "10.1109/MSR.2017.26",
      "Title": "Cost-Effective Build Outcome Prediction Using Cascaded Classifiers",
      "Abstract": "Software developers use continuous integration to find defects in the early stage and reduce risk. But this process can be resource and time consuming, which decreases the efficiency of development. In this work, we adopt cascaded classifiers to predict the build outcome and study what kinds of attributes are potentially useful for this process. We emphasize on the 'failed' instances which bring more cost. Our experiments reveal that our approach outperforms other commonly used classifiers. It reduces 51.7% of the waiting time and server workload while identifying 85.2% of the defective builds.",
      "Keywords": "continuous integration | cost-sensitive learning | ensemble learning | software engineering | software mining",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Ni, Ansong;Li, Ming",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026505018",
      "Primary study DOI": "10.1109/MSR.2017.20",
      "Title": "Concept-Based Classification of Software Defect Reports",
      "Abstract": "Automatic identification of the defect type from the textual description of a software defect can significantly speed-up as well as improve the software defect management life-cycle. This has been recognized in the research community and multiple solutions based on supervised learning approach have been proposed in the recent literature. However, these approaches need significant amount of labeled training data for use in real-life projects. In this paper, we propose to use Explicit Semantic Analysis (ESA) to carry out concept-based classification of software defect reports. We compute the 'semantic similarity' between the defect type labels and the defect report in a concept space spanned by Wikipedia articles and then, assign the defect type which has the highest similarity with the defect report. This approach helps us to circumvent the problem of dependence on labeled training data. Experimental results show that using concept-based classification is a promising approach for software defect classification to avoid the expensive process of creating labeled training data and yet get accuracy comparable to the traditional supervised learning approaches. To the best of our knowledge, this is the first use of Wikipedia and ESA for software defect classification problem.",
      "Keywords": "Explicit Semantic Analysis | Mining Software Respositories | Software Defect Classification | Text Data Mining",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Patil, Sangameshwar",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026515358",
      "Primary study DOI": "10.1109/MSR.2017.7",
      "Title": "Do Not Trust Build Results at Face Value-An Empirical Study of 30 Million CPAN Builds",
      "Abstract": "Continuous Integration (CI) is a cornerstone of modern quality assurance, providing on-demand builds (compilation and tests) of code changes or software releases. Despite the myriad of CI tools and frameworks, the basic activity of interpreting build results is not straightforward, due to not only the number of builds being performed but also, and especially, due to the phenomenon of build inflation, according to which one code change can be built on dozens of different operating systems, run-Time environments and hardware architectures. As existing work mostly ignored this inflation, this paper performs a large-scale empirical study of the impact of OS and run-Time environment on build failures on 30 million builds of the CPAN ecosystem's CI environment. We observe the evolution of build failures over time, and investigate the impact of OSes and environments on build failures. We show that distributions may fail differently on different OSes and environments and, thus, that the results of CI require careful filtering and selection to identify reliable failure data.",
      "Keywords": "Build | Build failure | Distribution | Environment | Module | Operating system",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Zolfagharinia, Mahdis;Adams, Bram;Guehenuc, Yann Gael",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026521412",
      "Primary study DOI": "10.1109/MSR.2017.29",
      "Title": "A Time Series Analysis of TravisTorrent Builds: To Everything There Is a Season",
      "Abstract": "We apply a seasonal decomposition time series analysisto TravisTorrent data in order to examine growth trendsand periodic behavior related to number of builds ina continuous integration environment. We apply our techniquesat the macro level using the full TravisTorrent repository consisting of 1,283 projects, and at the micro level considering the Apache Drill project. Our results demonstrate strong seasonal behavior at both the large and small scale using an additive time series model. In addition to being able to accurately capture trend and periodicity in builddata, our techniques are also able to accurately forecast the expected number of builds for a future time interval.",
      "Keywords": "Build Data | Data Seasonality | Time Series | TravisTorrent",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Atchison, Abigail;Berardi, Christina;Best, Natalie;Stevens, Elizabeth;Linstead, Erik",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026526919",
      "Primary study DOI": "10.1109/MSR.2017.44",
      "Title": "Software evolution and quality data from controlled, multiple, industrial case studies",
      "Abstract": "A main difficulty to study the evolution and quality of real-life software systems is the effect of moderator factors, such as: programming skill, type of maintenance task, and learning effect. Experimenters must account for moderator factors to identify the relationships between the variables of interest. In practice, controlling for moderator factors in realistic (industrial) settings is expensive and rather difficult. The data presented in this paper has two particularities: First, it involves six professional developers and four real-life, industrial systems. Second, it was obtained from controlled, multiple case studies where the moderator variables: programming skill, maintenance task, and learning effect were controlled for. This data set is relevant to experimenters studying evolution and quality of real-life systems, in particular those interested in studying industrial systems and replicating empirical studies.",
      "Keywords": "case study | code smells | empirical study | industrial data | moderator factors | replication | software defects | software evolution | software quality | software replicability",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Yamashita, Aiko;Abtahizadeh, S. Amirhossein;Khomh, Foutse;Gueheneuc, Yann Gael",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026513536",
      "Primary study DOI": "10.1109/MSR.2017.12",
      "Title": "Mining Change Histories for Unknown Systematic Edits",
      "Abstract": "Software developers often need to repeat similar modifications in multiple different locations of a system's source code. These repeated similar modifications, or systematic edits, can be both tedious and error-prone to perform manually. While there are tools that can be used to assist in automating systematic edits, it is not straightforward to find out where the occurrences of a systematic edit are located in an existing system. This knowledge is valuable to help decide whether refactoring is needed, or whether future occurrences of an existing systematic edit should be automated. In this paper, we tackle the problem of finding unknown systematic edits using a closed frequent itemset mining algorithm, operating on sets of distilled source code changes. This approach has been implemented for Java programs in a tool called SysEdMiner. To evaluate the tool's precision and scalability, we have applied it to an industrial use case.",
      "Keywords": "Change distilling | Frequent itemset mining | Systematic edits",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Molderez, Tim;Stevens, Reinout;De Roover, Coen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026499370",
      "Primary study DOI": "10.1109/MSR.2017.48",
      "Title": "An extensive dataset of UML models in GitHub",
      "Abstract": "The Unified Modeling Language (UML) is widely taught in academia and has good acceptance in industry. However, there is not an ample dataset of UML diagrams publicly available. Our aim is to offer a dataset of UML files, together with meta-data of the software projects where the UML files belong to. Therefore, we have systematically mined over 12 million GitHub projects to find UML files in them. We present a semi-Automated approach to collect UML stored in images,.xmi, and.uml files. We offer a dataset with over 93,000 UML diagrams from over 24,000 projects in GitHub.",
      "Keywords": "dataset | GitHub | mining software repositories | modeling | UML",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Robles, Gregorio;Ho-Quang, Truong;Hebig, Regina;Chaudron, Michel R.V.;Fernandez, Miguel Angel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026506925",
      "Primary study DOI": "10.1109/MSR.2017.56",
      "Title": "Candoia: A platform for building and sharing mining software repositories tools as apps",
      "Abstract": "We propose Candoia, a novel platform and ecosystemfor building and sharing Mining Software Repositories(MSR) tools. Using Candoia, MSR tools are built as apps, and Candoia ecosystem, acting as an appstore, allows effective sharing. Candoia platform provides, data extraction tools for curating custom datasets for user projects, and data abstractions for enabling uniform access to MSR artifacts from disparate sources, which makes apps portable and adoptable across diverse software project settings of MSR researchers and practitioners. The structured design of a Candoia app and the languages selected for building various components of a Candoia app promotes easy customization. To evaluate Candoia we have built over two dozen MSR apps for analyzing bugs, software evolution, project management aspects, and source code and programming practices showing the applicability of the platform for buildinga variety of MSR apps. For testing portability of apps acrossdiverse project settings, we tested the apps using ten popularproject repositories, such as Apache Tomcat, JUnit, Node.js, etc, and found that apps required no changes to be portable. We performed a user study to test customizability and we found that five of eight Candoia users found it very easy to customize an existing app. Candoia is available for download.",
      "Keywords": "Candioa ecosystem | Candoia | Candoia exchange | MSR | MSR apps | MSR data",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Tiwari, Nitin M.;Upadhyaya, Ganesha;Nguyen, Hoan Anh;Rajan, Hridesh",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026557187",
      "Primary study DOI": "10.1109/MSR.2017.57",
      "Title": "Euphony: Harmonious Unification of Cacophonous Anti-Virus Vendor Labels for Android Malware",
      "Abstract": "Android malware is now pervasive and evolving rapidly. Thousands of malware samples are discovered every day with new models of attacks. The growth of these threats has come hand in hand with the proliferation of collective repositories sharing the latest specimens. Having access to a large number of samples opens new research directions aiming at efficiently vetting apps. However, automatically inferring a reference ground-Truth from those repositories is not straightforward and can inadvertently lead to unforeseen misconceptions. On the one hand, samples are often mis-labeled as different parties use distinct naming schemes for the same sample. On the other hand, samples are frequently mis-classified due to conceptual errors made during labeling processes. In this paper, we analyze the associations between all labels given by different vendors and we propose a system called EUPHONY to systematically unify common samples into family groups. The key novelty of our approach is that no a-priori knowledge on malware families is needed. We evaluate our approach using reference datasets and more than 0.4 million additional samples outside of these datasets. Results show that EUPHONY provides competitive performance against the state-of-The-Art.",
      "Keywords": "android | datasets | ground-Truth | labeling | malware",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Hurier, Mederic;Suarez-Tangil, Guillermo;Dash, Santanu Kumar;Bissyande, Tegawende F.;Le Traon, Yves;Klein, Jacques;Cavallaro, Lorenzo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026515162",
      "Primary study DOI": "10.1109/MSR.2017.55",
      "Title": "Structure and evolution of package dependency networks",
      "Abstract": "Software developers often include available open-source software packages into their projects to minimize redundant effort. However, adding a package to a project can also introduce risks, which can propagate through multiple levels of dependencies. Currently, not much is known about the structure of open-source package ecosystems of popular programming languages and the extent to which transitive bug propagation is possible. This paper analyzes the dependency network structure and evolution of the JavaScript, Ruby, and Rust ecosystems. The reported results reveal significant differences across language ecosystems. The results indicate that the number of transitive dependencies for JavaScript has grown 60% over the last year, suggesting that developers should look more carefully into their dependencies to understand what exactly is included. The study also reveals that vulnerability to a removal of the most popular package is increasing, yet most other packages have a decreasing impact on vulnerability. The findings of this study can inform the development of dependency management tools.",
      "Keywords": "Dependency Management | Mining Software Repositories | Software Ecosystems | Software Evolution",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Kikas, Riivo;Gousios, Georgios;Dumas, Marlon;Pfahl, Dietmar",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026528397",
      "Primary study DOI": "10.1109/MSR.2017.16",
      "Title": "Mining social web service repositories for social relationships to aid service discovery",
      "Abstract": "The Service Oriented Computing (SOC) paradigm promotes building new applications by discovering and then invoking services, i.e., software components accessible through the Internet. Discovering services means inspecting registries where textual descriptions of services functional capabilities are stored. To automate this, existing approaches index descriptions and associate users' queries to relevant services. However, the massive adoption of Web-exposed API development practices, specially in large service ecosystems such as the IoT, is leading to evergrowing registries which challenge the accuracy and speed of such approaches. The recent notion of Social Web Services (SWS), where registries not only store service information but also sociallike relationships between users and services opens the door to new discovery schemes. We investigate an approach to discover SWSs that operates on graphs with user-service relationships and employs lightweight topological metrics to assess service similarity. Then, 'socially' similar services, which are determined exploiting explicit relationships and mining implicit relationships in the graph, are clustered via exemplar-based clustering to ultimately aid discovery. Experiments performed with the ProgrammableWeb.com registry, which is at present the largest SWS repository with over 15k services and 140k user-service relationships, show that pure topology-based clustering may represent a promising complement to content-based approaches, which in fact are more time-consuming due to text processing operations.",
      "Keywords": "Exemplar-based clustering | Service discovery | Social recommender systems | Social Web Service",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Corbellini, Alejandro;Godoy, Daniela;Mateos, Cristian;Zunino, Alejandro;Lizarralde, Ignacio",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026510205",
      "Primary study DOI": "10.1109/MSR.2017.39",
      "Title": "Impact of continuous integration on code reviews",
      "Abstract": "Peer code review and continuous integration often interleave with each other in the modern software quality management. Although several studies investigate how non-Technical factors (e.g., reviewer workload), developer participation and even patch size affect the code review process, the impact of continuous integration on code reviews is not yet properly understood. In this paper, we report an exploratory study using 578K automated build entries where we investigate the impact of automated builds on the code reviews. Our investigation suggests that successfully passed builds are more likely to encourage new code review participation in a pull request. Frequently built projects are found to be maintaining a steady level of reviewing activities over the years, which was quite missing from the rarely built projects. Experiments with 26,516 automated build entries reported that our proposed model can identify 64% of the builds that triggered new code reviews later.",
      "Keywords": "Automated build status | build frequency | code review quality | review participation",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Rahman, Mohammad Masudur;Roy, Chanchal K.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026547310",
      "Primary study DOI": "10.1109/MSR.2017.9",
      "Title": "Leveraging Automated Sentiment Analysis in Software Engineering",
      "Abstract": "Automated sentiment analysis in software engineering textual artifacts has long been suffering from inaccuracies in those few tools available for the purpose. We conduct an in-depth qualitative study to identify the difficulties responsible for such low accuracy. Majority of the exposed difficulties are then carefully addressed in developing SentiStrength-SE, a tool for improved sentiment analysis especially designed for application in the software engineering domain. Using a benchmark dataset consisting of 5,600 manually annotated JIRA issue comments, we carry out both quantitative and qualitative evaluations of our tool. SentiStrength-SE achieves 73.85% precision and 85% recall, which are significantly higher than a state-of-The-Art sentiment analysis tool we compare with.",
      "Keywords": "",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Islam, Md Rakibul;Zibran, Minhaz F.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026557876",
      "Primary study DOI": "10.1109/MSR.2017.24",
      "Title": "TravisTorrent: Synthesizing Travis CI and GitHub for Full-Stack Research on Continuous Integration",
      "Abstract": "Continuous Integration (CI) has become a best practice of modern software development. Thanks in part to its tight integration with GitHub, Travis CI has emerged as arguably the most widely used CI platform for Open-Source Software (OSS) development. However, despite its prominent role in Software Engineering in practice, the benefits, costs, and implications of doing CI are all but clear from an academic standpoint. Little research has been done, and even less was of quantitative nature. In order to lay the groundwork for data-driven research on CI, we built TravisTorrent, travistorrent.testroots.org, a freely available data set based on Travis CI and GitHub that provides easy access to hundreds of thousands of analyzed builds from more than 1,000 projects. Unique to TravisTorrent is that each of its 2,640,825 Travis builds is synthesized with meta data from Travis CI's API, the results of analyzing its textual build log, a link to the GitHub commit which triggered the build, and dynamically aggregated project data from the time of commit extracted through GHTorrent.",
      "Keywords": "",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Beller, Moritz;Gousios, Georgios;Zaidman, Andy",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026529844",
      "Primary study DOI": "10.1109/MSR.2017.54",
      "Title": "An Empirical Analysis of Build Failures in the Continuous Integration Workflows of Java-Based Open-Source Software",
      "Abstract": "Continuous Integration (CI) has become a common practice in both industrial and open-source software development. While CI has evidently improved aspects of the software development process, errors during CI builds pose a threat to development efficiency. As an increasing amount of time goes into fixing such errors, failing builds can significantly impair the development process and become very costly. We perform an indepth analysis of build failures in CI environments. Our approach links repository commits to data of corresponding CI builds. Using data from 14 open-source Java projects, we first identify 14 common error categories. Besides test failures, which are by far the most common error category (up to >80% per project), we also identify noisy build data, e.g., induced by transient Git interaction errors, or general infrastructure flakiness. Second, we analyze which factors impact the build results, taking into account general process and specific CI metrics. Our results indicate that process metrics have a significant impact on the build outcome in 8 of the 14 projects on average, but the strongest influencing factor across all projects is overall stability in the recent build history. For 10 projects, more than 50% (up to 80%) of all failed builds follow a previous build failure. Moreover, the fail ratio of the last k=10 builds has a significant impact on build results for all projects in our dataset.",
      "Keywords": "build errors | continuous integration | correlation analysis | mining software repositories",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Rausch, Thomas;Hummer, Waldemar;Leitner, Philipp;Schulte, Stefan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026515853",
      "Primary study DOI": "10.1109/MSR.2017.40",
      "Title": "Prevalence of botched code integrations",
      "Abstract": "Integrating code from different sources can be an error-prone and effort-intensive process. While an integration may appear statically sound, unexpected errors may still surface at run time. The industry practice of continuous integration aims to detect these and other run-Time errors through an extensive pipeline of successive tests. Using data from a continuous integration service, Travis CI, we look into the prevalence of integration errors. We find code integration causes failure less often than regular commits. Repairing is usually done the same day and takes less than ten lines of code, largely in the source code. These results indicate that applying proper practices mitigates many issues associated with code integration.",
      "Keywords": "code integration | empirical | GHTorrent | GitHub | merge commit | Mining Software Repositories | Travis CI | TravisTorrent",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Muylaert, Ward;De Roover, Coen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026525638",
      "Primary study DOI": "10.1109/MSR.2017.35",
      "Title": "Spencer: Interactive heap analysis for the masses",
      "Abstract": "Programming language-design and run-Time-implementation require detailed knowledge about the programs that users want to implement. Acquiring this knowledge is hard, and there is little tool support to effectively estimate whether a proposed tradeoff actually makes sense in the context of real world applications. Ideally, knowledge about behaviour of 'typical' programs is 1) easily obtainable, 2) easily reproducible, and 3) easily sharable. We present Spencer, an open source web service and APIframework for dynamic analysis of a continuously growing set of traces of standard program corpora. Users do not obtain traces on their own, but can instead send queries to the web service that will be executed on a set of program traces. Queries are built in terms of a set of query combinators that present a high level interface for working with trace data. Since the framework is high level, and there is a hosted collection of recorded traces, queries are easy to implement. Since the data sets are shared by the research community, results are reproducible. Since the actual queries run on one (or many) servers that provide analysis as a service, obtaining results is possible on commodity hardware. Data in Spencer is meant to be obtained once, and analysed often, making the overhead of data collection mostly irrelevant. This allows Spencer to collect more data than traditional tracing tools can afford within their performance budget. Results in Spencer are cached, making complicated analyses that build on cached primitive queries speedy.",
      "Keywords": "Dynamic Analysis | Heap Analysis | Tracing",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Brandauer, Stephan;Wrigstad, Tobias",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026507566",
      "Primary study DOI": "10.1109/MSR.2017.11",
      "Title": "Who you gonna call? analyzing web requests in android applications",
      "Abstract": "Relying on ubiquitous Internet connectivity, applications on mobile devices frequently perform web requests during their execution. They fetch data for users to interact with, invoke remote functionalities, or send user-generated content or meta-data. These requests collectively reveal common practices of mobile application development, like what external services are used and how, and they point to possible negative effects like security and privacy violations, or impacts on battery life. In this paper, we assess different ways to analyze what web requests Android applications make. We start by presenting dynamic data collected from running 20 randomly selected Android applications and observing their network activity. Next, we present a static analysis tool, Stringoid, that analyzes string concatenations in Android applications to estimate constructed URL strings. Using Stringoid, we extract URLs from 30, 000 Android applications, and compare the performance with a simpler constant extraction analysis. Finally, we present a discussion of the advantages and limitations of dynamic and static analyses when extracting URLs, as we compare the data extracted by Stringoid from the same 20 applications with the dynamically collected data.",
      "Keywords": "Dynamic Analysis | Mobile Applications | Static Analysis | Web Requests",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Rapoport, Marianna;Suter, Philippe;Wittern, Erik;Lhotak, Ondrej;Dolby, Julian",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026558022",
      "Primary study DOI": "10.1109/MSR.2017.27",
      "Title": "Sentiment Analysis of Travis CI Builds",
      "Abstract": "Human factors such as sentiments, emotions, mood, and stress along with their potential effect on software development are of paramount importance in software engineering, as we still strongly rely on human-To-human interaction for performing software development activities and driving results. With the advance of sentiment analysis tools, software engineering researchers have investigated the interplay between developers' sentiment and software engineering tasks such as issue fixing times. However, there is a lack of studies analyzing whether there is a relation between developers' sentiment and builds performed by continuous integration servers. Build breakage is not desired as it represents a signal that something went wrong in the software development activity and that extra work or rework should be done. In this paper, we report an empirical assessment over Travis CI builds and the corresponding commits in order to understand a potential association between developers' sentiment and build breakage. We found evidence that negative sentiment both affects and is affected by the result of the build process, although the influence seems to be small. Also, we found that developers tend to be more positive when writing about the CI server in commit messages.",
      "Keywords": "Continuous Integration | Human factors on Software Engineering | Sentiment Analysis",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Souza, Rodrigo;Silva, Bruno",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85025456145",
      "Primary study DOI": "10.1109/MSR.2017.5",
      "Title": "Using Q&A Websites as a Method for Assessing Systematic Reviews",
      "Abstract": "Questions and Answers (Q&A) websites maintain a long history of needs, problems, and challenges that software developers face. In contrast to Q&A websites, which are strongly tied to practitioners' needs, there are systematic reviews (SRs), which, according to recent studies, lack a connection with software engineering practice. In this paper, we investigate this claim by assessing to what extent systematic reviews help to solve questions posted on Q&A websites. To achieve this goal, we propose and evaluate a coverage method. We applied this method to a set of more than 600 questions related to agile software development. Results suggest that 12% of the related questions were covered. When considering specific agile methods, the majority of them have coverage below 50% or were not covered at all. We also identified 27 recurrent questions.",
      "Keywords": "",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Cartaxo, Bruno;Pinto, Gustavo;Ribeiro, Danilo;Kamei, Fernando;Santos, Ronnie E.S.;Da Silva, Fabio Q.B.;Soares, Sergio",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026503858",
      "Primary study DOI": "10.1109/MSR.2017.67",
      "Title": "An Empirical Analysis of the Docker Container Ecosystem on GitHub",
      "Abstract": "Docker allows packaging an application with its dependencies into a standardized, self-contained unit (a so-called container), which can be used for software development and to run the application on any system. Dockerfiles are declarative definitions of an environment that aim to enable reproducible builds of the container. They can often be found in source code repositories and enable the hosted software to come to life in its execution environment. We conduct an exploratory empirical study with the goal of characterizing the Docker ecosystem, prevalent quality issues, and the evolution of Dockerfiles. We base our study on a data set of over 70000 Dockerfiles, and contrast this general population with samplings that contain the Top-100 and Top-1000 most popular Docker-using projects. We find that most quality issues (28.6%) arise from missing version pinning (i.e., specifying a concrete version for dependencies). Further, we were not able to build 34% of Dockerfiles from a representative sample of 560 projects. Integrating quality checks, e.g., to issue version pinning warnings, into the container build process could result into more reproducible builds. The most popular projects change more often than the rest of the Docker population, with 5.81 revisions per year and 5 lines of code changed on average. Most changes deal with dependencies, that are currently stored in a rather unstructured manner. We propose to introduce an abstraction that, for instance, could deal with the intricacies of different package managers and could improve migration to more light-weight images.",
      "Keywords": "Docker | empirical software engineering | GitHub",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Cito, Jurgen;Schermann, Gerald;Wittern, John Erik;Leitner, Philipp;Zumberi, Sali;Gall, Harald C.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026514772",
      "Primary study DOI": "10.1109/MSR.2017.21",
      "Title": "Exception Evolution in Long-Lived Java Systems",
      "Abstract": "Exception handling allows developers to deal with abnormal situations that disrupt the execution flow of a program. There are mainly three types of exceptions: standard exceptions provided by the programming language itself, custom exceptions defined by the project developers, and third-party exceptions defined in external libraries. We conjecture that there are multiple factors that affect the use of these exception types. We perform an empirical study on long-lived Java projects to investigate these factors. In particular, we analyze how developers rely on the different types of exceptions in throw statements and exception handlers. We confirm that the domain, the type, and the development phase of a project affect the exception handling patterns. We observe that applications have significantly more error handling code than libraries and they increasingly rely on custom exceptions. Also, projects that belong to different domains have different preferences of exception types. For instance, content management systems rely more on custom exceptions than standard exceptions whereas the opposite is true in parsing frameworks.",
      "Keywords": "Empirical study | Exception handling | Software evolution",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Osman, Haidar;Chis, Andrei;Corrodi, Claudio;Ghafari, Mohammad;Nierstrasz, Oscar",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026547169",
      "Primary study DOI": "10.1109/MSR.2017.60",
      "Title": "An empirical study on android-related vulnerabilities",
      "Abstract": "Mobile devices are used more and more in everyday life. They are our cameras, wallets, and keys. Basically, they embed most of our private information in our pocket. For this and other reasons, mobile devices, and in particular the software that runs on them, are considered first-class citizens in the software-vulnerabilities landscape. Several studies investigated the software-vulnerabilities phenomenon in the context of mobile apps and, more in general, mobile devices. Most of these studies focused on vulnerabilities that could affect mobile apps, while just few investigated vulnerabilities affecting the underlying platform on which mobile apps run: The Operating System (OS). Also, these studies have been run on a very limited set of vulnerabilities. In this paper we present the largest study at date investigating Android-related vulnerabilities, with a specific focus on the ones affecting the Android OS. In particular, we (i) define a detailed taxonomy of the types of Android-related vulnerability, (ii) investigate the layers and subsystems from the Android OS affected by vulnerabilities, and (iii) study the survivability of vulnerabilities (i.e., the number of days between the vulnerability introduction and its fixing). Our findings could help OS and apps developers in focusing their verification & validation activities, and researchers in building vulnerability detection tools tailored for the mobile world.",
      "Keywords": "Android | Operating System | Taxonomy | Vulnerabilities",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Linares-Vasquez, Mario;Bavota, Gabriele;Escobar-Velasquez, Camilo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026557031",
      "Primary study DOI": "10.1109/MSR.2017.45",
      "Title": "A dataset of scratch programs: Scraped, shaped and scored",
      "Abstract": "Scratch is increasingly popular, both as an introductory programming language and as a research target in the computing education research field. In this paper, we present a dataset of 250K recent Scratch projects from 100K different authors scraped from the Scratch project repository. We processed the projects' source code and metadata to encode them into a database that facilitates querying and further analysis. We further evaluated the projects in terms of programming skills and mastery, and included the project scoring results. The dataset enables the analysis of the source code of Scratch projects, of their quality characteristics, and of the programming skills that their authors exhibit. The dataset can be used for empirical research in software engineering and computing education.",
      "Keywords": "computing education | dataset | Scratch",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Aivaloglou, Efthimia;Hermans, Felienne;Moreno-Leon, Jesus;Robles, Gregorio",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026537029",
      "Primary study DOI": "10.1109/MSR.2017.59",
      "Title": "Bug Characteristics in Blockchain Systems: A Large-Scale Empirical Study",
      "Abstract": "Bugs severely hurt blockchain system dependability. A thorough understanding of blockchain bug characteristics is required to design effective tools for preventing, detecting and mitigating bugs. We perform an empirical study on bug characteristics in eight representative open source blockchain systems. First, we manually examine 1,108 bug reports to understand the nature of the reported bugs. Second, we leverage card sorting to label the bug reports, and obtain ten bug categories in blockchain systems. We further investigate the frequency distribution of bug categories across projects and programming languages. Finally, we study the relationship between bug categories and bug fixing time. The findings include: (1) semantic bugs are the dominant runtime bug category, (2) frequency distributions of bug types show similar trends across different projects and programming languages, (3) security bugs take the longest median time to be fixed, (4) 35.71% performance bugs are fixed in more than one year, performance bugs take the longest average time to be fixed.",
      "Keywords": "blockchain | Bug characteristics | Empirical study",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Wan, Zhiyuan;Lo, David;Xia, Xin;Cai, Liang",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026546130",
      "Primary study DOI": "10.1109/MSR.2017.46",
      "Title": "Continuous defect prediction: The idea and a related dataset",
      "Abstract": "We would like to present the idea of our Continuous Defect Prediction (CDP) research and a related dataset that we created and share. Our dataset is currently a set of more than 11 million data rows, representing files involved in Continuous Integration (CI) builds, that synthesize the results of CI builds with data we mine from software repositories. Our dataset embraces 1265 software projects, 30,022 distinct commit authors and several software process metrics that in earlier research appeared to be useful in software defect prediction. In this particular dataset we use TravisTorrent as the source of CI data. TravisTorrent synthesizes commit level information from the Travis CI server and GitHub open-source projects repositories. We extend this data to a file change level and calculate the software process metrics that may be used, for example, as features to predict risky software changes that could break the build if committed to a repository with CI enabled.",
      "Keywords": "continuous defect prediction | defect prediction | mining software repositories | open science | software repository",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Madeyski, Lech;Kawalerowicz, Marcin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026503607",
      "Primary study DOI": "10.1109/MSR.2017.37",
      "Title": "The impact of the adoption of continuous integration on developer attraction and retention",
      "Abstract": "Open-source projects rely on attracting new and retaining old contributors for achieving sustainable success. One may suspect that adopting new development practices like Continuous Integration (CI) should improve the attractiveness of a project. However, little is known about the impact that adoption of CI has on developer attraction and retention. To bridge this gap, we study how the introduction of TRAVIS CI-a popular CI service provider-impacts developer attraction and retention in 217 GITHUB repositories. Surprisingly, we find that heuristics that estimate the developer attraction and retention of a project are higher in the year before adopting TRAVIS CI than they are in the year following TRAVIS CI adoption. Moreover, the results are statistically significant (Wilcoxon signed rank test, α = 0:05), with small but non-negligible effect sizes (Cliff's delta). Although we do not suspect a causal link, our results are worrisome. More work is needed to ascertain the relationship between CI and developer attraction and retention.",
      "Keywords": "",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Gupta, Yash;Khan, Yusaira;Gallaba, Keheliya;McIntosh, Shane",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026557767",
      "Primary study DOI": "10.1109/MSR.2017.14",
      "Title": "RefDiff: Detecting Refactorings in Version Histories",
      "Abstract": "Refactoring is a well-known technique that is widely adopted by software engineers to improve the design and enable the evolution of a system. Knowing which refactoring operations were applied in a code change is a valuable information to understand software evolution, adapt software components, merge code changes, and other applications. In this paper, we present RefDiff, an automated approach that identifies refactorings performed between two code revisions in a git repository. RefDiff employs a combination of heuristics based on static analysis and code similarity to detect 13 well-known refactoring types. In an evaluation using an oracle of 448 known refactoring operations, distributed across seven Java projects, our approach achieved precision of 100% and recall of 88%. Moreover, our evaluation suggests that RefDiff has superior precision and recall than existing state-of-The-Art approaches.",
      "Keywords": "Git | refactoring | Software evolution | Software repositories",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Silva, Danilo;Valente, Marco Tulio",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026521911",
      "Primary study DOI": "10.1109/MSR.2017.34",
      "Title": "Analyzing the impact of social attributes on commit integration success",
      "Abstract": "As the software development community makes it easier to contribute to open source projects, the number of commits and pull requests keep increasing. However, this exciting growthrenders it more difficult to only accept quality contributions. Recent research has found that both technical and social factors predictthe success of project contributions on GitHub. We take this question a step further, focusing on predicting continuousintegration build success based on technical andsocial factors involved in a commit. Specifically, we investigated if social factors (such as being acore member of the development team, having a large number of followers, or contributing a large number of commits) improve predictions of buildsuccess. We found that social factors cause a noticeable increasein predictive power (12%), core team members are more likely to pass the buildtests (10%), and users with 1000 or more followers are more likely topass the build tests (10%).",
      "Keywords": "GitHub | Predicting Integration Success | Social Attributes | Social Coding | Social Networks | Travis CI",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Soto, Mauricio;Coker, Zack;Le Goues, Claire",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026543643",
      "Primary study DOI": "10.1109/MSR.2017.66",
      "Title": "A study on the energy consumption of android app development approaches",
      "Abstract": "Mobile devices have become ubiquitous in the recent years, but the complaints about energy consumption are almost universal. On Android, the developer can choose among several different approaches to develop an app. In this paper, we investigate the impact of some of the most popular development approaches on the energy consumption of Android apps. Our study uses a testbed of 33 different benchmarks and 3 applications on 5 different devices to compare the energy efficiency and performance of the most commonly used approaches to develop apps on Android: Java, JavaScript, and C/C++ (through the NDK tools). In our experiments, Javascript was more energy-efficient in 75% of all benchmarks, while their Java counterparts consume up to 36.27x more energy (median of 1.97x). On the other hand, both Java and C++ outperformed JavaScript in most of the benchmarks. Based on these results, four Java applications were re-engineered to use a combination of Java and either JavaScript or C/C++ functions. For one of the apps, the hybrid solution using Java and C++ spent 10x less time and almost 100x less energy than a pure Java solution. The results were not uniform, however. For another app, when we restructured its implementation so as to minimize cross-language method invocations, the hybrid solution using Java and C++ took 8% longer to execute and consumed 11% more energy than a hybrid solution using Java and JavaScript. Since most Android apps are written solely in Java, the results of this study indicate that leveraging a combination of approaches may lead to non-negligible improvements in energy-efficiency and performance.",
      "Keywords": "Android | Benchmark Testing | Energy Consumption | Hybrid Apps | Performance | Smartphones",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Oliveira, Wellington;Oliveira, Renato;Castor, Fernando",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026548417",
      "Primary study DOI": "10.1109/MSR.2017.10",
      "Title": "Extracting code segments and their descriptions from research articles",
      "Abstract": "The availability of large corpora of online software-related documents today presents an opportunity to use machine learning to improve integrated development environments by first automatically collecting code examples along with associated descriptions. Digital libraries of computer science research and education conference and journal articles can be a rich source for code examples that are used to motivate or explain particular concepts or issues. Because they are used as examples in an article, these code examples are accompanied by descriptions of their functionality, properties, or other associated information expressed in natural language text. Identifying code segments in these documents is relatively straightforward, thus this paper tackles the problem of extracting the natural language text that is associated with each code segment in an article. We present and evaluate a set of heuristics that address the challenges of the text often not being colocated with the code segment as in developer communications such as online forums.",
      "Keywords": "Code Snippet Description | Information Extraction | Mining Software Repositories | Text Analysis",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Chatterjee, Preetha;Gause, Benjamin;Hedinger, Hunter;Pollock, Lori",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026534210",
      "Primary study DOI": "10.1109/MSR.2017.31",
      "Title": "An empirical study of the personnel overhead of continuous integration",
      "Abstract": "Continuous Integration (CI) is a software development practice where changes to the codebase are compiled and automatically checked for software quality issues. Like any software artifact (e.g., production code, build specifications), CI systems require an investment of development resources in order to keep them running smoothly. In this paper, we examine the human resources that are associated with developing and maintaining CI systems. Through the analysis of 1,279 GitHub repositories that adopt Travis CI (a popular CI service provider), we observe that: (i) there are 0 to 6 unique contributors to CI-related development in any 30-day period, regardless of project size, and (ii) the total number of CI developers has an upper bound of 15 for 99.2% of the studied projects, regardless of overall team size. These results indicate that service-based CI systems only require a small proportion of the development team to contribute. These costs are almost certainly outweighed by the reported benefits of CI (e.g., team communication and time-To-market for new content).",
      "Keywords": "Continuous Integration | Empirical Study | Software Engineering",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Manglaviti, Marco;Coronado-Montoya, Eduardo;Gallaba, Keheliya;McIntosh, Shane",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026499815",
      "Primary study DOI": "10.1109/MSR.2017.65",
      "Title": "Extracting Build Changes with BUILDDIFF",
      "Abstract": "Build systems are an essential part of modern software engineering projects. As software projects change continuously, it is crucial to understand how the build system changes because neglecting its maintenance can lead to expensive build breakage. Recent studies have investigated the (co-)evolution of build configurations and reasons for build breakage, but they did this only on a coarse grained level. In this paper, we present BUILDDIFF, an approach to extract detailed build changes from MAVEN build files and classify them into 95 change types. In a manual evaluation of 400 build changing commits, we show that BUILDDIFF can extract and classify build changes with an average precision and recall of 0.96 and 0.98, respectively. We then present two studies using the build changes extracted from 30 open source Java projects to study the frequency and time of build changes. The results show that the top 10 most frequent change types account for 73% of the build changes. Among them, changes to version numbers and changes to dependencies of the projects occur most frequently. Furthermore, our results show that build changes occur frequently around releases. With these results, we provide the basis for further research, such as for analyzing the (co-)evolution of build files with other artifacts or improving effort estimation approaches. Furthermore, our detailed change information enables improvements of refactoring approaches for build configurations and improvements of models to identify error-prone build files.",
      "Keywords": "Build Systems | Maintenance | Software Quality",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Macho, Christian;McIntosh, Shane;Pinzger, Martin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026523382",
      "Primary study DOI": "10.1109/MSR.2017.52",
      "Title": "A data set of OCL expressions on GitHub",
      "Abstract": "In model driven engineering (MDE), meta-models are the central artifacts. As a complement, the Object Constraint Language (OCL) is a language used to express constraints and operations on meta-models. The Eclipse Modeling Framework (EMF) provides an implementation of OCL, enabling OCL-Annotated meta-models. Existing empirical studies of the OCL have been conducted on small collections of data. To facilitate empirical research into the OCL on a larger scale, we present the first publicly available data set of OCL expressions. The data set contains 9188 OCL expressions originating from 504 EMF meta-models in 245 systematically selected GitHub repositories. Both the original meta-models and the generated abstract syntax trees are included, allowing for a variety of empirical studies of the OCL. To illustrate the applicability of this data set in practice, we performed three case studies.",
      "Keywords": "data set | GitHub | OCL",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Noten, Jeroen;Mengerink, Josh G.M.;Serebrenik, Alexander",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026510051",
      "Primary study DOI": "10.1109/MSR.2017.8",
      "Title": "A Large-Scale Study on the Usage of Testing Patterns That Address Maintainability Attributes: Patterns for Ease of Modification, Diagnoses, and Comprehension",
      "Abstract": "Test case maintainability is an important concern, especially in open source and distributed development environments where projects typically have high contributor turn-over with varying backgrounds and experience, and where code ownership changes often. Similar to design patterns, patterns for unit testing promote maintainability quality attributes such as ease of diagnoses, modifiability, and comprehension. In this paper, we report the results of a large-scale study on the usage of four xUnit testing patterns which can be used to satisfy these maintainability attributes. This is a first-of-its-kind study which developed automated techniques to investigate these issues across 82,447 open source projects, and the findings provide more insight into testing practices in open source projects. Our results indicate that only 17% of projects had test cases, and from the 251 testing frameworks we studied, 93 of them were being used. We found 24% of projects with test files implemented patterns that could help with maintainability, while the remaining did not use these patterns. Multiple qualitative analyses indicate that usage of patterns was an ad-hoc decision by individual developers, rather than motivated by the characteristics of the project, and that developers sometimes used alternative techniques to address maintainability concerns.",
      "Keywords": "Maintenance | Mining Software Repositories | Open Source | Unit Test Frameworks | Unit Test Patterns | Unit Testing",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Gonzalez, Danielle;Santos, Joanna C.S.;Popovich, Andrew;Mirakhorli, Mehdi;Nagappan, Mei",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026527817",
      "Primary study DOI": "10.1109/MSR.2017.38",
      "Title": "An empirical study of activity, popularity, size, testing, and stability in continuous integration",
      "Abstract": "A good understanding of the practices followed by software development projects can positively impact their success-particularly for attracting talent and on-boarding new members. In this paper, we perform a cluster analysis to classify software projects that follow continuous integration in terms of their activity, popularity, size, testing, and stability. Based on this analysis, we identify and discuss four different groups of repositories that have distinct characteristics that separates them from the other groups. With this new understanding, we encourage open source projects to acknowledge and advertise their preferences according to these defining characteristics, so that they can recruit developers who share similar values.",
      "Keywords": "characterizing | clustering | continuous integration | data mining",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Gautam, Aakash;Vishwasrao, Saket;Servant, Francisco",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026535951",
      "Primary study DOI": "10.1109/MSR.2017.19",
      "Title": "Source File Set Search for Clone-And-Own Reuse Analysis",
      "Abstract": "Clone-And-own approach is a natural way of source code reuse for software developers. To assess how known bugs and security vulnerabilities of a cloned component affect an application, developers and security analysts need to identify an original version of the component and understand how the cloned component is different from the original one. Although developers may record the original version information in a version control system and/or directory names, such information is often either unavailable or incomplete. In this research, we propose a code search method that takes as input a set of source files and extracts all the components including similar files from a software ecosystem (i.e., a collection of existing versions of software packages). Our method employs an efficient file similarity computation using b-bit minwise hashing technique. We use an aggregated file similarity for ranking components. To evaluate the effectiveness of this tool, we analyzed 75 cloned components in Firefox and Android source code. The tool took about two hours to report the original components from 10 million files in Debian GNU/Linux packages. Recall of the top-five components in the extracted lists is 0.907, while recall of a baseline using SHA-1 file hash is 0.773, according to the ground truth recorded in the source code repositories.",
      "Keywords": "File clone detection | Origin analysis | Software reuse | Source code search",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Ishio, Takashi;Sakaguchi, Yusuke;Ito, Kaoru;Inoue, Katsuro",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026506528",
      "Primary study DOI": "10.1109/MSR.2017.3",
      "Title": "Abnormal Working Hours: Effect of Rapid Releases and Implications to Work Content",
      "Abstract": "During the past years, overload at work leading to psychological diseases, such as burnouts, have drawn more public attention. This paper is a preliminary step toward an analysis of the work patterns and possible indicators of overload and time pressure on software developers with mining software repositories approach. We explore the working pattern of developers in the context of Mozilla Firefox, a large and long-lived open source project. To that end we investigate the impact of the move from traditional to rapid release cycle on work pattern. Moreover we compare Mozilla Firefox work pattern with another Mozilla product, Firefox OS, which has a different release cycle than Firefox. We find that both projects exhibit healthy working patterns, i.e. lower activity during the weekends and outside of office hours. Firefox experiences proportionally more activity on weekends than Firefox OS (Cohen's d = 0.94). We find that switching to rapid releases has reduced weekend work (Cohen's d = 1.43) and working during the night (Cohen's d = 0.45). This result holds even when we limit the analyzes on the hired resources, i.e. considering only individuals with Mozilla foundation email address, although, the effect sizes are smaller for weekends (Cohen's d = 0.64) and nights (Cohen's d = 0.23). Moreover, we use dissimilarity word clouds and find that work during the weekend is more technical while work during the week expresses more positive sentiment with words like 'good' and 'nice'. Our results suggest that moving to rapid releases have positive impact on the work health and work-life-balance of software engineers. However, caution is needed as our results are based on a limited set of quantitative data from a single organization.",
      "Keywords": "bugzilla | empirical software engineering | firefox | mozilla | release | time pressure | weekend | working pattern",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Claes, Maelick;Mantyla, Mika;Kuutila, Miikka;Adams, Bram",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026520371",
      "Primary study DOI": "10.1109/MSR.2017.61",
      "Title": "To Mock or Not to Mock? An Empirical Study on Mocking Practices",
      "Abstract": "When writing automated unit tests, developers often deal with software artifacts that have several dependencies. In these cases, one has the possibility of either instantiating the dependencies or using mock objects to simulate the dependencies' expected behavior. Even though recent quantitative studies showed that mock objects are widely used in OSS projects, scientific knowledge is still lacking on how and why practitioners use mocks. Such a knowledge is fundamental to guide further research on this widespread practice and inform the design of tools and processes to improve it. The objective of this paper is to increase our understanding of which test dependencies developers (do not) mock and why, as well as what challenges developers face with this practice. To this aim, we create MockExtractor, a tool to mine the usage of mock objects in testing code and employ it to collect data from three OSS projects and one industrial system. Sampling from this data, we manually analyze how more than 2,000 test dependencies are treated. Subsequently, we discuss our findings with developers from these systems, identifying practices, rationales, and challenges. These results are supported by a structured survey with more than 100 professionals. The study reveals that the usage of mocks is highly dependent on the responsibility and the architectural concern of the class. Developers report to frequently mock dependencies that make testing difficult and prefer to not mock classes that encapsulate domain concepts/rules of the system. Among the key challenges, developers report that maintaining the behavior of the mock compatible with the behavior of original class is hard and that mocking increases the coupling between the test and the production code.",
      "Keywords": "",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Spadini, Davide;Aniche, Mauricio;Bruntink, Magiel;Bacchelli, Alberto",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026557759",
      "Primary study DOI": "10.1109/MSR.2017.63",
      "Title": "Classifying Code Comments in Java Open-Source Software Systems",
      "Abstract": "Code comments are a key software component containing information about the underlying implementation. Several studies have shown that code comments enhance the readability of the code. Nevertheless, not all the comments have the same goal and target audience. In this paper, we investigate how six diverse Java OSS projects use code comments, with the aim of understanding their purpose. Through our analysis, we produce a taxonomy of source code comments, subsequently, we investigate how often each category occur by manually classifying more than 2,000 code comments from the aforementioned projects. In addition, we conduct an initial evaluation on how to automatically classify code comments at line level into our taxonomy using machine learning, initial results are promising and suggest that an accurate classification is within reach.",
      "Keywords": "comment taxonomy | software quality | source code comments",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Pascarella, Luca;Bacchelli, Alberto",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026554861",
      "Primary study DOI": "10.1109/MSR.2017.58",
      "Title": "Who Will Leave the Company?: A Large-Scale Industry Study of Developer Turnover by Mining Monthly Work Report",
      "Abstract": "Software developer turnover has become a big challenge for information technology (IT) companies. The departure of key software developers might cause big loss to an IT company since they also depart with important business knowledge and critical technical skills. Understanding developer turnover is very important for IT companies to retain talented developers and reduce the loss due to developers' departure. Previous studies mainly perform qualitative observations or simple statistical analysis of developers' activity data to understand developer turnover. In this paper, we investigate whether we can predict the turnover of software developers in non-open source companies by automatically analyzing monthly self-reports. The monthly work reports in our study are from two IT companies. Monthly reports in these two companies are used to report a developer's activities and working hours in a month. We would like to investigate whether a developer will leave the company after he/she enters company for one year based on his/her first six monthly reports. To perform our prediction, we extract many factors from monthly reports, which are grouped into 6 dimensions. We apply several classifiers including naive Bayes, SVM, decision tree, kNN and random forest. We conduct an experiment on about 6-years monthly reports from two companies, this data contains 3,638 developers over time. We find that random forest classifier achieves the best performance with an F1-measure of 0.86 for retained developers and an F1-measure of 0.65 for not-retained developers. We also investigate the relationship between our proposed factors and developers' departure, and the important factors that indicate a developer's departure. We find the content of task report in monthly reports, the standard deviation of working hours, and the standard deviation of working hours of project members in the first month are the top three important factors.",
      "Keywords": "Developer turnover | Mining software repositories | Prediction model",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Bao, Lingfeng;Xing, Zhenchang;Xia, Xin;Lo, David;Li, Shanping",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026534323",
      "Primary study DOI": "10.1109/MSR.2017.33",
      "Title": "On the interplay between non-functional requirements and builds on continuous integration",
      "Abstract": "Continuous Integration (CI) implies that a whole developer team works together on the mainline of a software project. CI systems automate the builds of a software. Sometimes a developer checks in code, which breaks the build. A broken build might not be a problem by itself, but it has the potential to disrupt co-workers, hence it affects the performance of the team. In this study, we investigate the interplay between non-functional requirements (NFRs) and builds statuses from 1,283 software projects. We found significant differences among NFRs related-builds statuses. Thus, tools can be proposed to improve CI with focus on new ways to prevent failures into CI, specially for efficiency and usability related builds. Also, the time required to put a broken build back on track indicates a bimodal distribution along all NFRs, with higher peaks within a day and lower peaks in six weeks. Our results suggest that more planned schedule for maintainability for Ruby, and for functionality and reliability for Java would decrease delays related to broken builds.",
      "Keywords": "Continuous integration | Non-functional requirements | Software repository mining | Topic models",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Paixao, Klerisson V.R.;Felicio, Cricia Z.;Delfim, Fernanda M.;Maia, Marcelo De A.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026496722",
      "Primary study DOI": "10.1109/MSR.2017.30",
      "Title": "Insights into Continuous Integration Build Failures",
      "Abstract": "Continuous integration is prevalently used in modern software engineering to build software systems automatically. Broken builds hinder developers' work and delay project progress. We must identify the factors causing build failures. This paper presents a large empirical study to identify the factors such as, complexity of a task, build strategy and contribution models (i.e., push and pull request), and projects level attributes (i.e., sizes of projects and teams), which potentially have impacts on the build results. We have studied 3.6 million builds over 1,090 open-source projects. The derived results add to our understanding of the role of those factors on build results, which can be used in minimizing build failures.",
      "Keywords": "",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Islam, Md Rakibul;Zibran, Minhaz F.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026504870",
      "Primary study DOI": "10.1109/MSR.2017.49",
      "Title": "A dataset for dynamic discovery of semantic changes in version controlled software histories",
      "Abstract": "Over the last few years, researchers proposed several semantic history slicing approaches that identify the set of semantically-related commits implementing a particular software functionality. However, there is no comprehensive benchmark for evaluating these approaches, making it difficult to assess their capabilities. This paper presents a dataset of 81 semantic change data collected from 8 real-world projects. The dataset is created for benchmarking semantic history slicing techniques. We provide details on the data collection process and the storage format. We also discuss usage and possible extensions of the dataset.",
      "Keywords": "benchmark | change history | Semantic history slicing",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Zhu, Chenguang;Li, Yi;Rubin, Julia;Chechik, Marsha",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026532196",
      "Primary study DOI": "10.1109/MSR.2017.2",
      "Title": "How Open Source Projects Use Static Code Analysis Tools in Continuous Integration Pipelines",
      "Abstract": "Static analysis tools are often used by software developers to entail early detection of potential faults, vulnerabilities, code smells, or to assess the source code adherence to coding standards and guidelines. Also, their adoption within Continuous Integration (CI) pipelines has been advocated by researchers and practitioners. This paper studies the usage of static analysis tools in 20 Java open source projects hosted on GitHub and using Travis CI as continuous integration infrastructure. Specifically, we investigate (i) which tools are being used and how they are configured for the CI, (ii) what types of issues make the build fail or raise warnings, and (iii) whether, how, and after how long are broken builds and warnings resolved. Results indicate that in the analyzed projects build breakages due to static analysis tools are mainly related to adherence to coding standards, and there is also some attention to missing licenses. Build failures related to tools identifying potential bugs or vulnerabilities occur less frequently, and in some cases such tools are activated in a 'softer' mode, without making the build fail. Also, the study reveals that build breakages due to static analysis tools are quickly fixed by actually solving the problem, rather than by disabling the warning, and are often properly documented.",
      "Keywords": "Continuous Integration | Empirical Study | Open Source Projects | Static Analysis Tools",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Zampetti, Fiorella;Scalabrino, Simone;Oliveto, Rocco;Canfora, Gerardo;Di Penta, Massimiliano",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85017214601",
      "Primary study DOI": "10.1002/smr.1860",
      "Title": "Coadapting multidimension process properties",
      "Abstract": "In the last decades, process verification has been intensively addressed and has become an essential activity to correct and to remove errors before process execution. Typical process verification ecosystems propose to express properties to be verified on the process. A property expresses a desired behavior that must hold or not in the process execution. Processes during their lifespan are continuously adapted for several purposes: enriching, correcting, and refactoring the process. When a process is adapted, the existing properties must naturally be rechecked to ensure that no errors have been introduced, ie, the properties still hold. However, the properties may become outdated and must be coadapted w.r.t. the adapted process before to be rechecked. Otherwise, the verification may raise false alarms or may not detect newly introduced errors. In this paper, we propose a coadaptation approach of properties while considering process adaptation for the different dimensions, namely, control flow, object flow, resources, and timing. We systematically studied process changes in the multiple dimensions to identify those that do impact properties and for which we propose resolution strategies. Our preliminary evaluation shows that our resolutions strategies allow to support users in correctly coadapting impacted properties.",
      "Keywords": "adaptation | coadaptation | control flow | object flow | process | properties | resources | timing",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-11-01",
      "Publication type": "Conference Paper",
      "Authors": "Khelladi, Djamel Eddine;Bendraou, Reda;Hebig, Regina;Gervais, Marie Pierre",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85016569647",
      "Primary study DOI": "10.1002/smr.1863",
      "Title": "The changing balance of technology and process: A case study on a combined setting of model-driven development and classical C coding",
      "Abstract": "The increasing flexibility in industry leads to an ecosystem of change, affecting the balance of processes and technology as well as the developers who have to cope with the change. Furthermore, the change itself might impact the ability to use quantitative methods to learn from previous experience. The goal of this study is to better understand the ecosystem of mutual impacts and changes of process and technologies as well as how developers perceive a technology setting and deal with its change. Therefore, we conducted a case study at Ericsson, performing a series of interviews among 6 employees (senior developers and architects). We identified a time line of changes that happened over 7 years. A set of observations about the relation between processes and tooling, and observations about developer's perceptions of the technology settings, and their strategy to deal with these changing technology settings. We discuss how the observed change impacts the ability to perform quantitative evaluations of technology and processes. The findings show that a bad choice of technologies can lead to unexpected impact on team dynamics. Furthermore, change happens so regular that it needs to be considered when collecting data for a quantitative evaluation of, eg, productivity.",
      "Keywords": "case study, development technologies, empirical investigation, software process",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-11-01",
      "Publication type": "Conference Paper",
      "Authors": "Hebig, Regina;Derehag, Jesper",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018403256",
      "Primary study DOI": "10.1002/smr.1865",
      "Title": "Comparing pre-commit reviews and post-commit reviews using process simulation",
      "Abstract": "Code review in practice is often performed change-based, ie, using the code changes belonging to a task to determine which code to review. Previous studies found that 2 variations of this process are used in industry: pre-commit review and post-commit review. The choice between these has implications not only for practitioners deciding on a code review process to use but also for the development of review tools and for experimentation with review processes. In some situations, a specific variant is clearly preferable due to the nature of the development process or team. In other situations, there are conflicting opinions. So we asked: Are there practically relevant performance differences between pre and post-commit reviews? How are these differences influenced by contextual factors? To assess these questions, we designed a parametric discrete event simulation model of certain agile development processes. We validated this model with practitioner's feedback and in part also with empirical data from industry. Our analysis indicates that the best choice does depend on the context but also that there are many situations with no practically relevant difference between both choices. We identified the main influencing factors and underlying effects and condensed our findings into heuristic rules.",
      "Keywords": "agile software development | code review | discrete event simulation | post-commit review | pre-commit review",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-11-01",
      "Publication type": "Conference Paper",
      "Authors": "Baum, Tobias;Kortum, Fabian;Schneider, Kurt;Brack, Arthur;Schauder, Jens",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029758369",
      "Primary study DOI": "10.1002/smr.1879",
      "Title": "Software process models vs descriptions: What do practitioners use and need?",
      "Abstract": "It is commonly known that software or system development processes are very important for getting a high-quality product. Such processes can be described in many different ways, from textual documents to existing modeling notations. Before dealing with the overall goal of improving the description of development processes, this paper evaluates the current state of practice regarding process descriptions and reasons for that. A major focus is on the degree of formality. Based on an interview guideline, a series of 12 interviews was conducted in German companies of different sizes. The results were analyzed to come up with the current usage of and need for software process models or descriptions. In general, our results confirm the literature statement that in industrial practice the majority of processes descriptions are detailed but rather focus on informal descriptions than formal notations or models. Nevertheless, they use semiformal process descriptions containing different graphical, table-based, or structured-text elements for representation. It is interesting that even if companies do not want to use formal models; they would like to have their advantages and even often have the detailed information necessary for formal models. Overall, our findings provide evidence on the state of practice regarding usage of process descriptions and models and show the need for future work in this area.",
      "Keywords": "development processes | empirical study | interview | practitioners | process descriptions | process documentation | process management | process modeling languages | process models | state of the practice",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-11-01",
      "Publication type": "Conference Paper",
      "Authors": "Diebold, P.;Scherr, S. A.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85016564285",
      "Primary study DOI": "10.1002/smr.1862",
      "Title": "Effort estimation in agile software development: Case study and improvement framework",
      "Abstract": "Effort estimation is more challenging in an agile context, as instead of exerting strict control over changes in requirements, dynamism is embraced. Current practice relies on expert judgment, where the accuracy of estimates is sensitive to the expertise of practitioners and prone to bias. To improve the effectiveness of the effort estimation process, the goal of this research is to investigate and understand the estimation process with respect to its accuracy in the context of agile software development from the perspective of agile development teams. Using case study research, 2 observations and eleven interviews were conducted with 3 agile development teams at SAP SE, a German multinational software corporation. The results reveal that factors such as the developer's knowledge, experience, and the complexity and impact of changes on the underlying system affect the magnitude as well as estimation accuracy. Furthermore, there is a need for a tool that incorporates expert knowledge, enables explicit consideration of cost drivers by experts and visualizes this information to improve the effectiveness of the effort estimation. On the basis of the findings of the case study, a framework, inspired by the quality improvement paradigm is proposed to improve effort estimation in agile development.",
      "Keywords": "agile development, change impact analysis, effort estimation, framework, QIP",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-11-01",
      "Publication type": "Conference Paper",
      "Authors": "Tanveer, Binish;Guzmán, Liliana;Engel, Ulf Martin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85032994724",
      "Primary study DOI": "10.1002/smr.1861",
      "Title": "Mobile medical app development with a focus on traceability",
      "Abstract": "Today, the growth of medical devices and mobile medical applications is increasing enormously, thanks to the efficiency and enhancement of new technology. When it comes to mobile medical apps, developers need to understand what is required when a mobile application fulfils the definition of a medical device. Such applications have to be developed in compliance with medical device regulations. This can be a challenge for mobile medical application developers, as medical device software is normally developed in a manner that will also ensure the production of regulatory documentation that is essential to market such devices. In this paper, we identify the need for a mobile medical application development framework, the key criteria for such a framework, and describe how the results were collected through performing a Medical Device Software Development workshop. Furthermore, we describe how MDevSPICE together with an agile software development approach can be tailored to support a mobile medical applications development framework. We detail one of the key criteria for mobile medical application development framework—traceability.",
      "Keywords": "medical device | medical device software | mobile application | mobile medical app | TAIF model | traceability",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-11-01",
      "Publication type": "Conference Paper",
      "Authors": "Trektere, Kitija;Regan, Gilbert;Caffery, Fergal Mc;Flood, Derek;Lepmets, Marion;Barry, Grainne",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85017406300",
      "Primary study DOI": "10.1002/smr.1866",
      "Title": "Continuous software engineering A microservices architecture perspective",
      "Abstract": "From its earliest days, software development has been beset with challenges in relation to timely delivery, appropriateness of features, and quality of deliverables. Many advances in software development processes have helped to address these concerns. For example, agile software development has helped to deliver working software more frequently, and capability maturity frameworks have brought about improved consistency in quality levels. However, the age-old challenge of better, cheaper, faster software has continued to beguile developers. In this paper, we discuss an emerging approach to software development, continuous software engineering (CSE), wherein software of operational quality may be delivered on a very frequent basis, up to many times in a given day. This approach uses a complex set of independent tools that together reduce the lead time in delivering commercial-grade software. Having examined the situational context of one industrial organization applying CSE, we conclude that the approach may not presently be appropriate to all manners of software development. Nonetheless, the authors are of the view that CSE represents a significant progression in software engineering and that software development settings of all types stand to gain from aspects of the CSE approach.",
      "Keywords": "agile | continuous software engineering | microservices | situational factors | software development process",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-11-01",
      "Publication type": "Conference Paper",
      "Authors": "O'Connor, Rory V.;Elger, Peter;Clarke, Paul M.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030985499",
      "Primary study DOI": "10.1002/smr.1914",
      "Title": "Corrigendum to: Empirical analysis of the relationship between CC and SLOC in a large corpus of Java methods and C functions: Empirical analysis of the relationship between CC and SLOC (Journal of Software: Evolution and Process, (2016), 28, 7, (589-618), 10.1002/smr.1760)",
      "Abstract": "INTRODUCTION During the preparation of the corresponding chapter in Davy Landman's PhD thesis, some minor graphical and statistical discrepancies were found in the paper “Empirical analysis of the relationship between CC and SLOC in a large corpus of Java methods and C functions.” To support future reproduction and use of this work, we prepared the current erratum, containing several updated figures, a diagnosis of the cause of the errors, and an explanation of the effect on the original paper. None of the issues reported in this erratum influence the conclusions of the original paper. ISSUES DISCOVERED The hexagonal scatter plots in Figure lack a more prominent line at CC = 0. This was caused by a bug *reported and confirmed: https://github.com/tidyverse/ggplot2/issues/2061in ggplot, which would filter out data around the limits. The R2 values in the Tables B and B of the C corpus were off by a maximum of 0.01 from the actual result. The cause was that this table was not re-calculated after fixing a bug in the “remove out-of-scope code” phase. Note that the impact of this error is scattered throughout the paper, as the correlations of Tables and are often repeated for clarity in the remaining sections (for example, the R2 of the linear model for all the C functions is 0.43 instead of 0.44). Our R code calculating the log-transformed linear fit contained an error. The dashed lines in Figures, and are impacted and the shape of the residual plot in 11. The biggest impact is in Figure, where the original fit seemed to miss the data almost entirely. We misinterpreted this phenomenon in the last sentence of the second paragraph of section 4.4.2; it is not caused by the skewness of the distributions of the two metrics, but rather by the current bug. The custom implementation of the log-scaled y-axis of the residual plots in Figure contained two errors: ∘ The labels on the y-axis were off by a factor 10 ∘For the negative side of the residual plot, we took the absolute, calculated the log10 value, and made it negative again. However, values between 0 and 1 (the values close to the linear fit) turn into a negative value (as log10(1) equals 0). This caused strange outliers in the original plots that were not scrutinized. The fixed residual plots do not have this outliers and look much more like the data in Figure. We republished the data sets related to the current paper on Zenodo to increase their availability: ∘ Landman, Davy. (2015). A Curated Corpus of Java Source Code based on Sourcerer (2015) [Data set]. Zenodo. http://doi.org/10.5281/zenodo.208213 ∘Landman, Davy. (2015). A Large Corpus of C Source Code based on Gentoo packages [Data set]. Zenodo. http://doi.org/10.5281/zenodo.208215 ∘Davy Landman. (2015, February 26). cwi-swat/jsep-sloc-versus-cc. Zenodo. http://doi.org/10.5281/zenodo.293795 NEW IMAGES The remaining part of this erratum contains updated tables and figures as replacements for the original paper. 8 (Figure presented.) Scatter plots of SLOC vs CC zoomed in on the bottom left quadrant. The solid and dashed lines are the linear regression before and after the log transform. The grayscale gradient of the hexagons is logarithmic 4 Correlations for part of the tail of the independent variable SLOC. All correlations have a high significance level p≤1×10−16.(b) C functions (Table presented.) 5 Correlations for part of the tail of the independent variable SLOC removed. All correlations have a high significance level p≤1×10−16.(b) C functions (Table presented.) 9 (Figure presented.) Scatter plots of SLOC vs CC on a log-log scale. The solid and dashed lines are the linear regression before and after the log transform. The grayscale gradient of the hexagons is logarithmic 11 (Figure presented.) Residual plot of the linear regressions after the log transform, both axis are on a log scale. The grayscale gradient of the hexagons is logarithmic 12 (Figure presented.) Scatter plots of SLOC vs CC for Java and C files. The solid and dashed lines are the linear regression before and after the log transform. The grayscale gradient of the hexagons is logarithmic.",
      "Keywords": "",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-10-01",
      "Publication type": "Erratum",
      "Authors": "Landman, Davy;Serebrenik, Alexander;Bouwers, Eric;Vinju, Jurgen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85031011796",
      "Primary study DOI": "10.1002/smr.1894",
      "Title": "An empirical assessment of technical debt practices in industry",
      "Abstract": "Context: Technical debt refers to the consequences of taking shortcuts when developing software. These consequences can impede the software growth and have financial implications. The software engineering research community needs to explore technical debt further from a practitioner standpoint. Objective: This study gathers insights from practitioners on key components of technical debt such as its definition, characterization, consequences, benefits, and how it is communicated. Method: We conducted semi-structured interviews with a convenience sample of 17 practitioners and a survey of 67 participants. Results: Despite the lack of consensus, we identified the most commonly accepted definition, method to measure technical debt (as person hours), and method to reduce debt (by allocating time in iterations to address the debt). Defects were also identified as type of debt and the cost of technical debt is more than the cost to make changes to source code. Three distinct company profiles emerged. Conclusion: Despite increasing research on technical debt, the field lacks consensus on its many facets. One interesting outcome of this study is how to assess the risks of technical debt by evaluating liabilities beyond costs of directly handling debt.",
      "Keywords": "clustering | semi-structured interviews | software practitioners | survey | technical debt",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Codabux, Zadia;Williams, Byron J.;Bradshaw, Gary L.;Cantor, Murray",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85031021306",
      "Primary study DOI": "10.1002/smr.1877",
      "Title": "On the interest of architectural technical debt: Uncovering the contagious debt phenomenon",
      "Abstract": "A known problem in large software companies is to balance the prioritization of short-term and long-term business goals. As an example, architecture suboptimality (Architectural Technical Debt), incurred to deliver fast, might hinder future feature development. However, some technical debt generates more interest to be paid than other. We conducted a multi-phase, multiple-case embedded case study comprehending 9 sites at 6 large international software companies. We have investigated which architectural technical debt items generate more interest, how the interest occurs during software development and which costly extra-activities are triggered as a result. We presented a taxonomy of the most dangerous items identified during the qualitative investigation and a model of their effects that can be used for prioritization, for further investigation and as a quality model for extracting more precise and context-specific metrics. We found that some architectural technical debt items are contagious, causing the interest to be not only fixed, but potentially compound, which leads to the hidden growth of interest (possibly exponential). We found important factors to be monitored to refactor the debt before it becomes too costly. Instances of these phenomena need to be identified and stopped before the development reaches a crises.",
      "Keywords": "agile software development | architectural technical debt | effort | multiple case study | qualitative model | sociotechnical phenomena",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Martini, Antonio;Bosch, Jan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021258815",
      "Primary study DOI": "10.1002/smr.1881",
      "Title": "An investigation of effort distribution among development phases: A four-stage progressive software cost estimation model",
      "Abstract": "Software cost estimation is a key process in project management. Estimations in the initial project phases are made with a lot of uncertainty that influences estimation accuracy which typically increases as the project progresses in time. Project data collected during the various project phases can be used in a progressive time-dependent fashion to train software cost estimation models. Our motivation is to reduce uncertainty and increase confidence based on the understanding of patterns of effort distributions in development phases of real-world projects. In this work, we study effort distributions and suggest a four-stage progressive software cost estimation model, adjusting the initial effort estimates during the development life-cycle based on newly available data. Initial estimates are reviewed on the basis of the experience gained as development progresses and as new information becomes available. The proposed model provides an early, a post-planning, a post-specifications, and a post-design estimate, while it uses industrial data from the ISBSG (R10) dataset. The results reveal emerging patterns of effort distributions and indicate that the model provides effective estimations and exhibits high explanatory value. Contributions in lessons learned and practical implications are also provided.",
      "Keywords": "four-stage progressive model | project management | software cost estimation",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-10-01",
      "Publication type": "Article",
      "Authors": "Papatheocharous, Efi;Bibi, Stamatia;Stamelos, Ioannis;Andreou, Andreas S.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028991133",
      "Primary study DOI": "10.1002/smr.1883",
      "Title": "A replicated empirical study to evaluate software testing methods",
      "Abstract": "Many empirical studies have been performed to evaluate software testing methods in past decades. However, we are still not able to generalize the results as most studies are not complete and differ significantly from one another. To contribute to the existing knowledge base of software testing methods, we performed an empirical study to evaluate 3 testing methods: (1) code reading by stepwise abstraction, (2) functional testing using equivalence partitioning and boundary value analysis, and (3) structural testing using 100% branch, multiple-condition, loop, and relational-operator coverage using a well-defined and standard schema. A controlled experiment is performed with 18 subjects who applied the 3 defect detection techniques to 3 C programs in a fractional factorial experimental design to observe failures and isolate faults. The experimental results show that (1) the techniques are equally effective in terms of observing failures and finding faults, (2) the effectiveness of the techniques depends on the nature of the program, (3) all the testing techniques are equally efficient in case of failure observation, (4) the techniques are different in their efficiency in terms of fault isolation where code reading performed better than that of structural and functional testing, and (5) with respect to the fault types, all the techniques were equally effective in observing failures and isolating faults except in case of cosmetic faults where functional testing performed better than the other 2 techniques The effectiveness and efficiency of testing techniques were significantly influenced by the type of program. The results presented in this paper contribute to an empirical knowledge base of testing methods and may be helpful for the software engineers to decide the appropriate techniques in improving the software testing process.",
      "Keywords": "code reading | comparison of testing methods | empirical study | functional testing | replication | structural testing",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Farooq, Sheikh Umar;Quadri, S. M.K.;Ahmad, Nesar",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026675381",
      "Primary study DOI": "10.1002/smr.1888",
      "Title": "Of software and change",
      "Abstract": "Change has been recognized as the distinguishing feature that makes software different from any other human-produced artifacts. Initial reflections on the urgent and unavoidable need to master change date back to the 1970s. However, despite the continuous progress that characterized software technology since, in practice, software change is still often handled as an afterthought, in an ad hoc and unprincipled manner. Agile development methods have been proposed and are now widely adopted to accommodate change during development. Recent extensions to also include operation—known as DevOps—are increasingly and successfully adopted by industry. Still, principled and rigorous foundations that can be taught, practiced, and replicated systematically are lacking. This paper argues that change has to become a first-class concept and that the development tools used by engineers and the runtime environment supporting software execution should be structured in a way that naturally accommodates change. It also provides a perspective along which several research approaches that were investigated by the community in the past decade might be integrated and extended to make this vision become true. Two main change categories are identified—evolution and adaptation—along with the forces that drive them. The paper discusses when and how the developed software can be designed in a way that it can self-adapt. It also discusses how the software itself can cooperate with humans in-the-loop to help them in their design and development efforts. Finally, it outlines a roadmap of future work needed to progress in the direction of supporting and automating software change that would lead to dependable adaptation and evolution.",
      "Keywords": "agile processes | change | development | DevOps | formal model | incrementality | operation | software | verification",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Ghezzi, C.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019108643",
      "Primary study DOI": "10.1002/smr.1870",
      "Title": "Toward automated quality-centric product line configuration using intentional variability",
      "Abstract": "Software product line engineering is a discipline that facilitates a systematic reuse-based approach by formally representing commonalities and variabilities between the applications of a target domain. As one of the main artifacts of the software product line, a feature model represents the possible configuration space and can be customized based on the stakeholders' needs. Considering the complexity of the variabilities represented by feature models and the diversity of the stakeholders' expectations, the configuration process can be viewed as a complex optimization problem. In previous research, researchers have bridged the gap between requirement and product line engineering by integrating feature models and goal models. In this paper, we propose an approach for the configuration process that seeks to satisfy the stakeholders' requirements as well as the feature models' structural and integrity constraints. We model stakeholders' functional and nonfunctional needs and their preferences using requirement engineering goal models. We formalize the structure of the feature model, the stakeholders' objectives, and their preferences in the form of an integer linear program to conduct a semi-automated feature model configuration process. Our experimental results show that the proposed configuration framework is scalable when considering both functional and nonfunctional requirements of stakeholders.",
      "Keywords": "configuration process | feature model | goal model | software product line",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Noorian, Mahdi;Bagheri, Ebrahim;Du, Weichang",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018924222",
      "Primary study DOI": "10.1002/smr.1882",
      "Title": "Analyzing the relationship between project productivity and environment factors in the use case points method",
      "Abstract": "Project productivity is a key factor for producing effort estimates from use case points (UCP), especially when the historical dataset is absent. The first versions of UCP effort estimation models used a fixed number or very limited numbers of productivity ratios for all new projects. These approaches have not been well examined over a large number of projects, so the validity of these studies was a matter for criticism. The newly available large software datasets allow us to perform further research on the usefulness of productivity for effort estimation of software development. Specifically, we studied the relationship between project productivity and UCP environmental factors, as they have a significant impact on the amount of productivity needed for a software project. Therefore, we designed 4 studies, using various classification and regression methods, to examine the usefulness of that relationship and its impact on UCP effort estimation. The results we obtained are encouraging and show potential improvement in effort estimation. Furthermore, the efficiency of that relationship is better over a dataset that comes from industry because of the quality of data collection. Our comment on the findings is that it is better to exclude environmental factors from calculating UCP and make them available only for computing productivity. The study also encourages project managers to understand how to better assess the environmental factors, as they do have a significant impact on productivity.",
      "Keywords": "environmental factors | software effort estimation | software productivity | use case points",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-09-01",
      "Publication type": "Article",
      "Authors": "Azzeh, Mohammad;Nassif, Ali Bou",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018653506",
      "Primary study DOI": "10.1002/smr.1872",
      "Title": "Motivators for adopting social computing in global software development: An empirical study",
      "Abstract": "Managing communication in collaborative global software development (GSD) projects is both critical and challenging. While social computing has received much attention from practitioners, social computing adoption is still an emerging research area in GSD. This research paper provides a review of the academic research in social computing and identifies motivators for adopting social computing in the GSD context. We applied the systematic literature review (SLR) and questionnaire survey with 35 software industry experts to address the research objective. Firstly, we implemented a formal SLR approach and identified an initial set of social computing adoption motivators. Secondly, a questionnaire survey was developed based on the SLR and was tested by means of a pilot study. The findings of this combined SLR and questionnaire survey indicate that real-time communication and coordination, knowledge acquisition, expert feedback, and information sharing are the key factors that motivate social computing adoption in GSD projects. The results of t test (ie, t =.558, P =.589) show that there is no significant difference between the findings of SLR and questionnaire. The results of this study suggest the need for developing social computing strategies and policies to guide the strategic adoption of social computing tools in GSD projects.",
      "Keywords": "empirical study | global software development | social computing | systematic literature review",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Niazi, Mahmood;Mahmood, Sajjad;Alshayeb, Mohammad;Baqais, Abdulrahman Ahmed Bobakr;Gill, Asif Qumer",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018786875",
      "Primary study DOI": "10.1002/smr.1873",
      "Title": "Automated support for reuse-based requirements engineering in global software engineering",
      "Abstract": "GlobalSoftware Engineering implies a paradigm shift towards globally-distributed development that can be advantageous, but at the cost of having to address the specific challenges that arise when the stakeholders are not colocated. Reusing assets during the initial processes of the software development life cycle could be beneficial, but automated support is essential if the expected benefits of requirements reuse are to be actually obtained. The main contribution of this paper is the specification of a collection of software features for a tool support for distributed, catalogue-based natural-language requirements reuse. Two additional contributions are also made: (1) an implementation of the requirements specifications previously mentioned using Drupal, a Content Management System; and (2) an empirical assessment of this tool support using distributed university students as subjects (n=57). According to our findings, the tool helps in making requirements reuse better than requirements specification from scratch and in managing traceability, is easy to use, useful, and easy to learn. In contrast, the tool is not particularly suitable for managing users and user roles.",
      "Keywords": "computer-aided requirements engineering tool | global software engineering | requirements engineering | reuse",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Carrillo de Gea, Juan Manuel;Nicolás, Joaquín;Fernández-Alemán, José L.;Toval, Ambrosio",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018645451",
      "Primary study DOI": "10.1002/smr.1875",
      "Title": "Software integration in global software development: Challenges for GSD vendors",
      "Abstract": "Context: The advances in information and communication technologies have revolutionized the software development environment from local to global software development (GSD). This revolution also created challenges for vendor organizations. Vendors face challenges in integrating the components developed independently by GSD teams into a final working product. Objective: The objectives of the current study is to find out those critical barriers/challenges that hinder the integration process at any stage in the GSD environment for different types and size of projects. Methodology: For achieving the objectives we initially conducted a comprehensive systematic literature review (SLR). We searched 6 digital libraries and also followed the snowballing technique. The data was extracted from a total of 88 finally selected papers. Findings of the SLR study were then empirically validated through a questionnaire survey in GSD industry. A total of 96 experts from 22 different countries participated in the survey. Results: We have found a total of 16 barriers/challenges among which ten barriers are ranked as critical barriers/challenges. Some of the top ranked barriers are “lack of communication,” “lack of proper documentation,” “lack of compatibility,” and “architecture mismatch.” The findings of our industrial survey are mostly coherent with the SLR findings. However, there is a difference in ranks of the various barriers/challenges across the 2 data sets (SLR and industrial survey). The identified challenges need to be properly addressed by software vendors to reduce the complexity of the integration process in GSD projects. Conclusion: We found that the severity of these barriers increases in large size projects. On the other hand, bespoke products are more affected by “lack of communication,” while off-the-shelf–based projects face integration problems due to “lack of compatibility,” “architecture mismatch,” and “wrong off the shelf product selection and customization”.",
      "Keywords": "barriers/challenges | empirical study | global software development | software integration | systematic literature review",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Ilyas, Muhammad;Khan, Siffat Ullah",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026836053",
      "Primary study DOI": "10.1002/smr.1864",
      "Title": "Identifying Classes in Legacy JavaScript Code",
      "Abstract": "JavaScript is the most popular programming language for the Web. Although the language is prototype-based, developers can emulate class-based abstractions in JavaScript to master the increasing complexity of their applications. Identifying classes in legacy JavaScript code can support these developers at least in the following activities: (1) program comprehension; (2) migration to the new JavaScript syntax that supports classes; and (3) implementation of supporting tools, including IDEs with class-based views and reverse engineering tools. In this paper, we propose a strategy to detect class-based abstractions in the source code of legacy JavaScript systems. We report on a large and in-depth study to understand how class emulation is employed, using a dataset of 918 JavaScript applications available on GitHub. We found that almost 70% of the JavaScript systems we study make some usage of classes. We also performed a field study with the main developers of 60 popular JavaScript systems to validate our findings. The overall results range from 97% to 100% for precision, from 70% to 89% for recall, and from 82% to 94% for F-score.",
      "Keywords": "JavaScript | Program comprehension | Reverse engineering",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-08-01",
      "Publication type": "Article",
      "Authors": "Silva, Leonardo Humberto;Valente, Marco Tulio;Bergel, Alexandre;Anquetil, Nicolas;Etien, Anne",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85015090645",
      "Primary study DOI": "10.1002/smr.1855",
      "Title": "The evolution of open-source mobile applications: An empirical study",
      "Abstract": "Now, mobile applications grow at an exponential speed and their evolution activities are very active, while there is little research on the evolution of mobile apps. To have a better understanding of the evolution of mobile apps and find similarities or patterns in their evolution process, we conduct an empirical study on long spans in the lifetime of 8 typical open-source mobile apps, which covers 348 official releases. First, we try to verify whether Lehman's laws still apply to mobile apps or not, extract a variety of metrics of the apps, and use statistical hypothesis testing to validate these laws. We find enough data that support a subset of Lehman's laws, while the rest do not. Second, we make some novel observations, eg, the growth of mobile apps is nonsmooth, and some versions of the apps have a great growth in their evolution. Enough data confirming that software instability increases great with the addition of third-party method invocations, and automatic build and manage tool based on contract is introduced into project as apps continue evolving.",
      "Keywords": "empirical study | Lehman's laws | mobile software engineering | software evolution",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Li, Deguang;Guo, Bing;Shen, Yan;Li, Junke;Huang, Yanhui",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85016594110",
      "Primary study DOI": "10.1002/smr.1859",
      "Title": "Using discriminative feature in software entities for relevance identification of code changes",
      "Abstract": "Developers often bundle unrelated changes (eg, bug fix and feature addition) in a single commit and then submit a “poor cohesive” commit to version control system. Such a commit consists of multiple independent code changes and makes review of code changes harder. If the code changes before commit can be identified as related and unrelated ones, the “cohesiveness” of a commit can be guaranteed. Inspired by the effectiveness of machine learning techniques in classification field, we model the relevance identification of code changes as a binary classification problem (ie, related and unrelated changes) and propose discriminative feature in software entities to characterize the relevance of code changes. In particular, to quantify the discriminative feature, 21 coupling rules and 4 cochanged type relationships are elaborately extracted from software entities to construct related changes vector (RCV). Twenty-one coupling rules at granularities of class, attribute, and method can capture the relevance of code changes from structural coupling dimension, and 4 cochanged type relationships are defined to capture the change type combinations of software entities that may cause related changes. Based on RCV, machine learning algorithms are applied to identify the relevance of code changes. The experiment results show that probabilistic neural network and general regression neural network provide statistically significant improvements in accuracy of relevance identification of code changes over the other 4 machine learning algorithms. Related changes vector with 72 dimensions (RCV72) outperforms other 2RCVs with less dimensions.",
      "Keywords": "cochanged types | coupling rules | discriminative feature | relevance of code changes",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Huang, Yuan;Chen, Xiangping;Liu, Zhiyong;Luo, Xiaonan;Zheng, Zibin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021712461",
      "Primary study DOI": "10.1002/smr.1876",
      "Title": "From ad hoc to strategic ecosystem management: the  Three-Layer Ecosystem Strategy Model  (TeLESM)",
      "Abstract": "Recently, business ecosystems have been recognized as one of the most interesting phenomenon in software engineering research. Companies experience a paradigm shift where product development and innovation is moving outside the boundaries of the firm and where networks of stakeholders join forces to co-create value. While there is prominent research focusing on the managerial perspective of business ecosystems, few studies provide strategic guidance for how to intentionally manage the different ecosystems that companies operate in. Therefore, and on the basis of multicase study research, we provide empirical evidence on the challenges that software-intensive companies experience in relation to the different types of business ecosystems they operate in. We conduct a “state-of-the-art” literature review to identify strategies that are used to manage ecosystem engagements, and we develop a conceptual model in which we identify strategies for managing the innovation ecosystem, the differentiating ecosystem, and the commoditizing ecosystem. By categorising the different strategies in relation to the different types of ecosystems for which they are valid, the “three-layer ecosystem strategy model” provides comprehensive support for strategy selection. We validate the use of the identified strategies in 6 software-intensive case companies, and we provide empirical insights on the “relevance” and the “desired use” of these strategies as experienced by the case companies.",
      "Keywords": "business ecosystems | commoditizing ecosystem | differentiating ecosystem | ecosystem challenges | ecosystem strategies | innovation ecosystem",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Holmström Olsson, Helena;Bosch, Jan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85017497395",
      "Primary study DOI": "10.1002/smr.1867",
      "Title": "Analysis of task allocation in distributed software development through a hybrid methodology of verbal decision analysis",
      "Abstract": "The management of distributed software development projects presents many challenges. One of them happens right at the start of the project and consists of the allocation of tasks between remote teams. When allocating a task to a site, the project manager takes into account several factors such as technical knowledge of staff and proximity to the client. The project manager usually takes this decision in a subjective way. The verbal decision analysis is an approach based on solving problems through multicriteria qualitative analysis, which means it considers the analysis of subjective criteria. This paper describes the application of verbal decision analysis methods ORdinal CLASSification and ZAPROS III-i to classify and rank the most relevant factors that the project managers should take into account when allocating tasks in projects of distributed software development.",
      "Keywords": "distributed software development | multi-criteria decision analysis | ORCLASS Method | task allocation | verbal decision analysis | ZAPROS III-i method",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Simão Filho, Marum;Pinheiro, Plácido Rogério;Albuquerque, Adriano Bessa",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84979082226",
      "Primary study DOI": "10.1002/smr.1794",
      "Title": "Cluster-based test cases prioritization and selection technique for agile regression testing",
      "Abstract": "Regression testing repeatedly executes test cases of previous builds to validate that the original features are not affected with any new changes. In recent years, regression testing has seen a remarkable progress with the increasing popularity of agile methods, which stress the central role of regression testing in maintaining software quality. The optimum case for regression testing in agile context is to run regression set at the end of each sprint and release, which requires a lot of cost and time. In this paper, we present an automated agile regression testing approach on both the sprints and release levels. The proposed approach addresses both weighted sprint test cases prioritization technique, which prioritizes test cases based on several parameters having real practical weight for testers, and Cluster-based Release Test cases Selection technique that clusters user stories based on the similarity of covered modules to solve the scalability issue. Test cases are then selected based on issues logged for failed test cases using text mining techniques. The proposed approach achieves enhancement for both the prioritization and selection of test cases for agile regression testing. Copyright © 2016 John Wiley & Sons, Ltd.",
      "Keywords": "agile testing | clustering | regression testing | test prioritization | test selection | text mining",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-06-01",
      "Publication type": "Conference Paper",
      "Authors": "Kandil, Passant;Moussa, Sherin;Badr, Nagwa",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85005950370",
      "Primary study DOI": "10.1002/smr.1835",
      "Title": "Assisting the continuous improvement of Scrum projects using metrics and Bayesian networks",
      "Abstract": "Scrum is a simple process to understand, but hard to adopt. Therefore, there is a need for resources to assist on its adoption. In this paper, we present the process followed to build a Bayesian network to assist on the assessment of the quality of the software process in the context of Scrum projects. The model provides data to help Scrum Masters lead the improvement of business value delivery of Scrum teams. The process is divided into 2 phases. In the first phase, we built the Bayesian network based on expert knowledge extracted from the literature and experts. We used a top-down approach and reasoning to define the key metrics necessary to build the models and their relationships. In the second phase, we updated the Bayesian network based on limitations of the first version. We validated the Bayesian network inferences with 10 simulated scenarios. Comparing both versions, for all scenarios, we improved the accuracy of the inferences. Therefore, we concluded that the Bayesian networks adequately represent Scrum projects from the viewpoint of the Scrum Master. Finally, the model built is in conformance with agile methods tailoring and can be adapted to any Scrum team.",
      "Keywords": "agile methods | Bayesian networks | continuous improvement | KaizenScrum",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-06-01",
      "Publication type": "Conference Paper",
      "Authors": "Perkusich, Mirko;Gorgônio, Kyller Costa;Almeida, Hyggo;Perkusich, Angelo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84996486863",
      "Primary study DOI": "10.1002/smr.1834",
      "Title": "An empirical study of portfolio management and Kanban in agile and lean software companies",
      "Abstract": "Traditional portfolio management tools and methods are challenging for software companies that use agile and lean approaches, because of the complex pre-planning activities required. In this paper, traditional portfolio management tools and methods that have been conventionally used in industry and presented in literature are reviewed. Findings concerning their use in agile and lean software companies are investigated, on the basis of the literature review and semi-structured interviews conducted in 7 agile and lean software companies in Finland. The findings indicate that the studied companies do not use traditional portfolio management tools and methods in their entirety. The software companies adapted suitable components from traditional portfolio management tools and methods into their practices. They also apply new tools and methods to manage their portfolios of offerings, to cope with the dynamics required by agile and lean methods. The companies' emphasis is on using tools to facilitate immediate feedback from customers, to assess the value of offerings in their portfolio. The results showed a growing interest in using Kanban at the portfolio level to provide a visual overview of offerings.",
      "Keywords": "agile | Kanban | lean | portfolio | software product management",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-06-01",
      "Publication type": "Conference Paper",
      "Authors": "Ahmad, Muhammad Ovais;Lwakatare, Lucy Ellen;Kuvaja, Pasi;Oivo, Markku;Markkula, Jouni",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84978959115",
      "Primary study DOI": "10.1002/smr.1796",
      "Title": "Assessing the adoption level of scaled agile development: a maturity model for Scaled Agile Framework",
      "Abstract": "Although the agile software development approaches have gained wide acceptance in practice, the concerns regarding the scalability and integration of agile practices in traditional large-scale system development projects are prevailing. Scaled Agile Framework (SAFe) has emerged as a solution to address some of these concerns. Despite few encouraging results, case studies indicate several challenges of SAFe adoption. Currently, there is a lack of a well-structured gradual approach for establishing SAFe. Before and during SAFe adoption, organizations can benefit greatly from a uniform model for assessing the current progress, and establishing a roadmap for the initiative. To address this need, we developed a maturity model that provides guidance for software developing organizations in defining a roadmap for adopting SAFe. The model can also be used to assess the level of SAFe adoption. We took an existing agile maturity model as a basis for agile practices and extended it with practices that are key to SAFe. The model was developed and refined with industry experts using the Delphi technique. A case study was conducted in a large organization where we evaluated the model by applying it to assess the level of SAFe adoption. © 2016 The Authors. Journal of Software: Evolution and Process Published by John Wiley & Sons Ltd.",
      "Keywords": "agile development in large-scale | agile software development | Delphi study | maturity model | Scaled Agile Framework",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-06-01",
      "Publication type": "Conference Paper",
      "Authors": "Turetken, Oktay;Stojanov, Igor;Trienekens, Jos J.M.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85021358087",
      "Primary study DOI": "10.1002/smr.1885",
      "Title": "A qualitative study of DevOps usage in practice",
      "Abstract": "Organizations are introducing agile and lean software development techniques in operations to increase the pace of their software development process and to improve the quality of their software. They use the term DevOps, a portmanteau of development and operations, as an umbrella term to describe their efforts. In this paper, we describe the ways in which organizations implement DevOps and the outcomes they experience. We first summarize the results of a systematic literature review that we performed to discover what researchers have written about DevOps. We then describe the results of an exploratory interview-based study involving 6 organizations of various sizes that are active in various industries. As part of our findings, we observed that all organizations were positive about their experiences and only minor problems were encountered while adopting DevOps.",
      "Keywords": "agile software development | DevOps | empirical study | qualitative interviews | software development life cycle | systematic literature review",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-06-01",
      "Publication type": "Conference Paper",
      "Authors": "Erich, F. M.A.;Amrit, C.;Daneva, M.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013471809",
      "Primary study DOI": "10.1002/smr.1857",
      "Title": "Enabling code transformations with FermaT on simplified bytecode",
      "Abstract": "This paper presents a new approach for working with Java bytecode that uses existing tools (FermaT and the wide spectrum language) that have been industrially tested and proven in transforming legacy assembly code. The first step has been a successful translator that works with a subset language (MicroJava) and enables the usage of formal transformations in FermaT to restructure the code from a machine level to human readable high level structures, both automatically and manually. It mostly relies on the existing transformations, but some new ones were introduced in this work, and some old ones were modified.",
      "Keywords": "bytecode | FermaT | transformation | translation | WSL",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Pracner, Doni;Budimac, Zoran",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85014656546",
      "Primary study DOI": "10.1002/smr.1843",
      "Title": "MORE: A multi-objective refactoring recommendation approach to introducing design patterns and fixing code smells",
      "Abstract": "Refactoring is widely recognized as a crucial technique applied when evolving object-oriented software systems. If applied well, refactoring can improve different aspects of software quality including readability, maintainability, and extendibility. However, despite its importance and benefits, recent studies report that automated refactoring tools are underused much of the time by software developers. This paper introduces an automated approach for refactoring recommendation, called MORE, driven by 3 objectives: (1) to improve design quality (as defined by software quality metrics), (2) to fix code smells, and (3) to introduce design patterns. To this end, we adopt the recent nondominated sorting genetic algorithm, NSGA-III, to find the best trade-off between these 3 objectives. We evaluated the efficacy of our approach using a benchmark of 7 medium and large open-source systems, 7 commonly occurring code smells (god class, feature envy, data class, spaghetti code, shotgun surgery, lazy class, and long parameter list), and 4 common design pattern types (visitor, factory method, singleton, and strategy). Our approach is empirically evaluated through a quantitative and qualitative study to compare it against 3 different state-of-the art approaches, 2 popular multiobjective search algorithms, and random search. The statistical analysis of the results confirms the efficacy of our approach in improving the quality of the studied systems while successfully fixing 84% of code smells and introducing an average of 6 design patterns. In addition, the qualitative evaluation shows that most of the suggested refactorings (an average of 69%) are considered by developers to be relevant and meaningful.",
      "Keywords": "code smells | design patterns | refactoring | search-based software engineering | software quality",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Ouni, Ali;Kessentini, Marouane;Ó Cinnéide, Mel;Sahraoui, Houari;Deb, Kalyanmoy;Inoue, Katsuro",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013498173",
      "Primary study DOI": "10.1002/smr.1858",
      "Title": "A comprehensive framework for modeling requirements of CSCW systems",
      "Abstract": "As in any other system, an accurate requirements specification is essential to developing a collaborative system, which has special kinds of requirements that are hard to specify by means of current Requirements Engineering techniques. The Collaborative Systems Requirement Modeling Framework (CSRMF) was developed to address this problem; the original Collaborative Systems Requirement Modeling Language was extended and modeled to properly deal with collaboration and awareness requirements. The developed CSRMF framework consists of 3 components: an Requirements Engineering modeling language able to represent collaboration among users as well as awareness needs, a set of design guidelines that drive Computer Supported Cooperative Work system specification by means of 5 different types of diagrams, and a supporting computer-aided software engineering tool to specify and validate Computer Supported Cooperative Work system requirements. CSRMF provides Requirements Engineers with a complete solution to the specification of awareness-demanding collaborative systems, as they can now take advantage of a language and a set of guidelines supported by a tool to guide them in specifying system requirements.",
      "Keywords": "awareness | CASE tool | CSCW | goal-oriented | requirements engineering process",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Teruel, Miguel A.;Navarro, Elena;López-Jaquero, Víctor;Montero, Francisco;González, Pascual",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013230742",
      "Primary study DOI": "10.1002/smr.1847",
      "Title": "The long-term growth rate of evolving software: Empirical results and implications",
      "Abstract": "The amount of code in evolving software-intensive systems appears to be growing relentlessly, affecting products and entire businesses. Objective figures quantifying the software code growth rate bounds in systems over a large time scale can be used as a reliable predictive basis for the size of software assets. We analyze a reference base of over 404 million lines of open source and closed software systems to provide accurate bounds on source code growth rates. We find that software source code in systems doubles about every 42 months on average, corresponding to a median compound annual growth rate of 1.21 ± 0.01. Software product and development managers can use our findings to bound estimates, to assess the trustworthiness of road maps, to recognise unsustainable growth, to judge the health of a software development project, and to predict a system's hardware footprint.",
      "Keywords": "CAGR | code growth rate | compound annual growth rate | empirical study | software evolution",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Hatton, Les;Spinellis, Diomidis;van Genuchten, Michiel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84995584073",
      "Primary study DOI": "10.1002/smr.1830",
      "Title": "A method of requirements elicitation and analysis for Global Software Development",
      "Abstract": "To perform requirements elicitation and analysis, effective communication and collaboration between stakeholders are necessary. Global Software Development (GSD), where software teams are located in different parts of the world, has become increasingly popular. However, geographical distance, cultural diversity, differences in time zones, and language barriers create difficulties for GSD stakeholders in engaging in effective communication. Taking into consideration the factors involved in GSD, previous research showed that the ways by which requirements are gathered and analyzed for collocated software development cannot be used effectively for GSD. Thus, in this paper, we present a method of requirements elicitation and analysis for GSD. The method consists of 4 stages: (1) data collection; (2) educating stakeholders about GSD issues; (3) post-education assessment; and (4) requirements elicitation and analysis. Past researchers used student groups in a university environment to play the roles of stakeholders in experiments in GSD studies. Likewise, we preliminarily validate our method by applying it to a case study of an online shopping system, where the roles of client, requirements engineer, project analyst, and designers were played by a group of students.",
      "Keywords": "distributed teams | Global Software Development | requirements analysis | requirements elicitation",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Ali, Naveed;Lai, Richard",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85012964590",
      "Primary study DOI": "10.1002/smr.1842",
      "Title": "The relationship between evolutionary coupling and defects in large industrial software",
      "Abstract": "Evolutionary coupling (EC) is defined as the implicit relationship between 2 or more software artifacts that are frequently changed together. Changing software is widely reported to be defect-prone. In this study, we investigate the effect of EC on the defect proneness of large industrial software systems and explain why the effects vary. We analysed 2 large industrial systems: a legacy financial system and a modern telecommunications system. We collected historical data for 7 years from 5 different software repositories containing 176 thousand files. We applied correlation and regression analysis to explore the relationship between EC and software defects, and we analysed defect types, size, and process metrics to explain different effects of EC on defects through correlation. Our results indicate that there is generally a positive correlation between EC and defects, but the correlation strength varies. Evolutionary coupling is less likely to have a relationship to software defects for parts of the software with fewer files and where fewer developers contributed. Evolutionary coupling measures showed higher correlation with some types of defects (based on root causes) such as code implementation and acceptance criteria. Although EC measures may be useful to explain defects, the explanatory power of such measures depends on defect types, size, and process metrics.",
      "Keywords": "evolutionary coupling | industrial software | legacy software | measurement | mining software repositories | software defects",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Kirbas, Serkan;Caglayan, Bora;Hall, Tracy;Counsell, Steve;Bowes, David;Sen, Alper;Bener, Ayse",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85007240960",
      "Primary study DOI": "10.1002/smr.1838",
      "Title": "There and back again: Can you compile that snapshot?",
      "Abstract": "A broken snapshot represents a snapshot from a project's change history that cannot be compiled. Broken snapshots can have significant implications for researchers, as they could hinder any analysis of the past project history that requires code to be compiled. Noticeably, while some broken snapshots may be observable in change history repositories (e.g., no longer available dependencies), some of them may not necessarily happen during the actual development. In this paper, we systematically study the compilability of broken snapshots in 219 395 snapshots belonging to 100 Java projects from the Apache Software Foundation, all relying on Maven as an automated build tool. We investigated broken snapshots from 2 different perspectives: (1) how frequently they happen and (2) likely causes behind them. The empirical results indicate that broken snapshots occur in most (96%) of the projects we studied and that they are mainly due to problems related to the resolution of dependencies. On average, only 38% of the change history of the analyzed systems is currently successfully compilable.",
      "Keywords": "broken snapshots | empirical studies | mining software repositories | software quality",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Tufano, Michele;Palomba, Fabio;Bavota, Gabriele;Di Penta, Massimiliano;Oliveto, Rocco;De Lucia, Andrea;Poshyvanyk, Denys",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84990888758",
      "Primary study DOI": "10.1002/smr.1820",
      "Title": "A meta-model of software development project states behavioral study of software projects",
      "Abstract": "Software development project during its lifecycle passes through various states. These states describe the condition, status, and behavior of software projects. In the present study, these states are defined based on the various activities performed during project lifecycle like initial environment setup, requirement analysis, coding, testing, problem resolution, and completion. The taxonomy of these states and substates is defined, and a project states meta-model is designed. The meta-model is composed of states and substates of the software projects. Detailed case studies of real projects have been conducted to validate the states of the meta-model. Evidences are collected; and events and observations are recorded about existence of the states, execution flow, duration, and behavior. The evidences, events, and observations are presented in the sequence to translate them into results. Results show that project states exist in all projects such that each software project passes through these states serially and in particular cases, a few states may exist in parallel. Project states show the status and progress of the software projects. It is found that issues in software projects can effectively be resolved by performing micro project management activities of the projects states. Project states meta-model provides basic structure for deriving new models.",
      "Keywords": "lifecycle | meta-model | micromanagement | project progress | project states | risks",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Akbar, Rehan;Hassan, Mohd Fadzil;Abdullah, Azrai",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85017101718",
      "Primary study DOI": "10.1002/smr.1831",
      "Title": "Simplifying the construction of source code transformations via automatic syntactic restructurings",
      "Abstract": "A set of restructurings to systematically normalize selective syntax in C++ is presented. The objective is to convert variations in syntax of specific portions of code into a single form to simplify the construction of large, complex program transformation rules. Current approaches to constructing transformations require developers to account for a large number of syntactic cases, many of which are syntactically different but semantically equivalent. The work identifies classes of such syntactic variations and presents normalizing restructurings to simplify each variation to a single, consistent syntactic form. The normalizing restructurings for C++ are presented and applied to two open source systems for evaluation. The evaluation uses the system's test cases to validate that the normalizing restructurings do not affect the systems' tested behavior. In addition, a set of example transformations that benefit from the prior application of normalizing restructurings are presented along with a small survey to assess the effect of the readability of the resultant code.",
      "Keywords": "adaptive maintenance | restructuring | syntactic isomorphism | transformation",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Newman, Christian D.;Bartman, Brian;Collard, Michael L.;Maletic, Jonathan I.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84994850635",
      "Primary study DOI": "10.1002/smr.1806",
      "Title": "A model for assessing and re-assessing the value of software reuse",
      "Abstract": "Background: Software reuse is often seen as a cost avoidance rather than a gained value. This results in a rather one-sided debate where issues such a resource control, release schedule, quality, or reuse in more than one release are neglected. Aims: We propose a reuse value assessment framework, intended to provide a more nuanced view of the value and costs associated with different reuse candidates. Method: This framework is constructed based on findings from an interview study at a large software development company. Results: The framework considers the functionality, compliance to standards, provided quality, and provided support of a reuse candidate, thus enabling an informed comparison between different reuse candidates. Furthermore, the framework provides means for tracking the value of the reused asset throughout subsequent releases. Conclusions: The reuse value assessment framework is a tool to assist in the selection between different reuse candidates. The framework also provides a means to assess the current value of a reusable asset in a product, which can be used to indicate where maintenance efforts would increase the utilized potential of the reusable asset.",
      "Keywords": "assessment | software reuse | value",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-04-01",
      "Publication type": "Article",
      "Authors": "Svahnberg, Mikael;Gorschek, Tony",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85011664439",
      "Primary study DOI": "10.1002/smr.1839",
      "Title": "Work fragmentation in developer interaction data",
      "Abstract": "Information workers and software developers are exposed to work fragmentation, an interleaving of activities and interruptions during their normal work day. Small-scale observational studies have shown that this can be detrimental to their work. In this paper, we perform a large-scale study of this phenomenon for the particular case of software developers performing software evolution tasks. Our study is based on several thousands interaction traces collected by Mylyn and the Eclipse Usage Data Collector. We observe that work fragmentation is correlated to lower observed productivity at both the macro level (for entire sessions) and at the micro level (around markers of work fragmentation); further, longer activity switches seem to strengthen the effect, and different activities seem to be affected differently. These observations give ground for subsequent studies investigating the phenomenon of work fragmentation.",
      "Keywords": "interaction data | interruptions | work fragmentation",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-03-01",
      "Publication type": "Conference Paper",
      "Authors": "Cruz, Luis C.;Sanchez, Heider;González, Víctor M.;Robbes, Romain",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84995582865",
      "Primary study DOI": "10.1002/smr.1821",
      "Title": "Detecting duplicate bug reports with software engineering domain knowledge",
      "Abstract": "Bug deduplication, ie, recognizing bug reports that refer to the same problem, is a challenging task in the software-engineering life cycle. Researchers have proposed several methods primarily relying on information-retrieval techniques. Our work motivated by the intuition that domain knowledge can provide the relevant context to enhance effectiveness, attempts to improve the use of information retrieval by augmenting with software-engineering knowledge. In our previous work, we proposed the software-literature-context method for using software-engineering literature as a source of contextual information to detect duplicates. If bug reports relate to similar subjects, they have a better chance of being duplicates. Our method, being largely automated, has a potential to substantially decrease the level of manual effort involved in conventional techniques with a minor trade-off in accuracy. In this study, we extend our work by demonstrating that domain-specific features can be applied across projects than project-specific features demonstrated previously while still maintaining performance. We also introduce a hierarchy-of-context to capture the software-engineering knowledge in the realms of contextual space to produce performance gains. We also highlight the importance of domain-specific contextual features through cross-domain contexts: adding context improved accuracy; Kappa scores improved by at least 3.8% to 10.8% per project.",
      "Keywords": "deduplication | documentation | duplicate bug reports | information retrieval | machine learning | software engineering textbooks | software literature",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-03-01",
      "Publication type": "Conference Paper",
      "Authors": "Aggarwal, Karan;Timbers, Finbarr;Rutgers, Tanner;Hindle, Abram;Stroulia, Eleni;Greiner, Russell",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85015706775",
      "Primary study DOI": "10.1002/smr.1840",
      "Title": "Understanding systematic and collaborative code changes by mining evolutionary trajectory patterns",
      "Abstract": "The life cycle of a large-scale software system can undergo many releases. Each release often involves hundreds or thousands of revisions committed by many developers over time. Many code changes are made in a systematic and collaborative way. However, such systematic and collaborative code changes are often undocumented and hidden in the evolution history of a software system. It is desirable to recover commonalities and associations among dispersed code changes in the evolutionary trajectory of a software system. In this paper, we present Summarizing Evolutionary Trajectory by Grouping and Aggregation (SETGA), an approach to summarizing historical commit records as trajectory patterns by grouping and aggregating relevant code changes committed over time. The SETGA extracts change operations from a series of commit records from version control systems. It then groups extracted change operations by their common properties from different dimensions such as change operation types, developers, and change locations. After that, SETGA aggregates relevant change operation groups by mining various associations among them. We implement SETGA and conduct an empirical study with 3 open-source systems. We investigate underlying evolution rules and problems that can be revealed by the identified patterns and analyze the evolution of trajectory patterns in different periods. The results show that SETGA can identify various types of trajectory patterns that are useful for software evolution management and quality assurance.",
      "Keywords": "code change | evolution | mining | pattern | version control system",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-03-01",
      "Publication type": "Conference Paper",
      "Authors": "Jiang, Qingtao;Peng, Xin;Wang, Hai;Xing, Zhenchang;Zhao, Wenyun",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84970003079",
      "Primary study DOI": "10.1002/smr.1789",
      "Title": "A bug reproduction approach based on directed model checking and crash traces",
      "Abstract": "Reproducing a bug that caused a system to crash is an important task for uncovering the causes of the crash and providing appropriate fixes. In this paper, we propose a novel crash reproduction approach that combines directed model checking and backward slicing to identify the program statements needed to reproduce a crash. Our approach, named JCHARMING (Java CrasH Automatic Reproduction by directed Model checkING), uses information found in crash traces combined with static program slices to guide a model checking engine in an optimal way. We show that JCHARMING is efficient in reproducing bugs from 10 different open source systems. Overall, JCHARMING is able to reproduce 80% of the bugs used in this study in an average time of 19 min. Copyright © 2016 John Wiley & Sons, Ltd.",
      "Keywords": "automatic bug reproduction | dynamic analysis | model checking | software maintenance",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-03-01",
      "Publication type": "Conference Paper",
      "Authors": "Nayrolles, Mathieu;Hamou-Lhadj, Abdelwahab;Tahar, Sofiène;Larsson, Alf",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013469369",
      "Primary study DOI": "10.1002/smr.1853",
      "Title": "Uncovering sustainability concerns in software product lines",
      "Abstract": "Sustainable living, ie, living within the bounds of the available environmental, social, and economic resources, is the focus of many present-day social and scientific discussions. But what does sustainability mean within the context of software engineering? In this paper, we undertake a comprehensive analysis of 8 case studies to address this question within the context of a specific software engineering approach, software product line engineering (SPLE). We identify the sustainability-related characteristics that arise in present-day studies that apply SPLE. We conclude that technical and economic sustainability are in prime focus on the present SPLE practice, with social sustainability issues, where they relate to organisations, also addressed to a good degree. On the other hand, the issues related to the personal sustainability are less prominent, and environmental considerations are nearly completely amiss. We present feature models and cross-relations that result from our analysis as a starting point for sustainability engineering through SPLE, suggesting that any new development should consider how these models would be instantiated and expanded for the intended sociotechnical system. The good representation of sustainability features in these models is also validated with 2 additional case studies.",
      "Keywords": "case study analysis, software product line engineering, sustainability, qualitative text analysis",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-02-01",
      "Publication type": "Conference Paper",
      "Authors": "Chitchyan, Ruzanna;Groher, Iris;Noppen, Joost",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85011634412",
      "Primary study DOI": "10.1002/smr.1851",
      "Title": "Investigating the effect of design patterns on energy consumption",
      "Abstract": "Gang of Four (GoF) patterns are well-known best practices for the design of object-oriented systems. In this paper, we aim at empirically assessing their relationship to energy consumption, ie, a performance indicator that has recently attracted the attention of both researchers and practitioners. To achieve this goal, we investigate pattern-participating methods (ie, those that play a role within the pattern) and compare their energy consumption to the consumption of functionally equivalent alternative (nonpattern) solutions. We obtained the alternative solution by refactoring the pattern instances using well-known transformations (eg, replace polymorphism with conditional statements). The comparison is performed on 169 methods of 2 GoF patterns (namely, State/Strategy and Template Method), retrieved from 2 well-known open source projects. The results suggest that for the majority of cases the alternative design excels in terms of energy consumption. However, in some cases (eg, when the method is large in size or invokes many methods) the pattern solution presents similar or lower energy consumption. The outcome of our study can be useful to both researchers and practitioners, because we: (1) provide evidence on a possible negative effect of GoF patterns, and (2) can provide guidance on which cases the use of the pattern is not hurting energy consumption.",
      "Keywords": "design patterns | energy efficiency | GoF patterns | state pattern | strategy pattern | template method pattern",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-02-01",
      "Publication type": "Conference Paper",
      "Authors": "Feitosa, Daniel;Alders, Rutger;Ampatzoglou, Apostolos;Avgeriou, Paris;Nakagawa, Elisa Yumi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85010950739",
      "Primary study DOI": "10.1002/smr.1848",
      "Title": "BEFTIGRE: Behaviour-driven full-tier green evaluation of mobile cloud applications",
      "Abstract": "With the resource-constrained nature of mobile devices and the resource-abundant offerings of the cloud, several promising optimisation techniques have been proposed by the green computing research community. Prominent techniques and unique methods have been developed to offload resource intensive tasks from mobile devices to the cloud. Although these schemes address similar questions within the same domain of mobile cloud application (MCA) optimisation, evaluation is tailored to the scheme and also solely mobile focused, thus making it difficult to clearly compare with other existing counterparts. In this work, we first analyse the existing/commonly adopted evaluation technique, then with the aim to fill the above gap, we propose the behaviour-driven full-tier green evaluation approach, which adopts the behaviour-driven concept for evaluating MCA performance and energy usage—ie, green metrics. To automate the evaluation process, we also present and evaluate the effectiveness of a resultant application program interface and tool driven by the behaviour-driven full-tier green evaluation approach. The application program interface is based on Android and has been validated with Elastic Compute Cloud instance. Experiments show that Beftigre is capable of providing a more distinctive, comparable, and reliable green test results for MCAs.",
      "Keywords": "BDD for MCA | behaviour-driven evaluation | green metrics evaluation | green mobile cloud | mobile cloud evaluation | mobile offloading comparison",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-02-01",
      "Publication type": "Conference Paper",
      "Authors": "Chinenyeze, Samuel J.;Liu, Xiaodong;Al-Dubai, Ahmed",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85011883133",
      "Primary study DOI": "10.1002/smr.1850",
      "Title": "GreCo: Green code of ethics",
      "Abstract": "Background: Codes of ethics (CoE) are widely adopted in several professional areas, including that of Software Engineering. However, contemporary CoE do not pay sufficient attention to one of the most important trends to have appeared in the last years environmental issues. Aim: The aim of this study is to establish a Green CoE for software engineering and Professional Practices (GreCo). Our intention is to cover a wide range of aspects related to sustainability, such as economic, environmental, social, and technical features. We are additionally interested in encouraging software engineers to adopt these aspects. Methods: The Green CoE presented is the result of the interaction of several experts in the area. A first version of GreCo, whose starting point was a discussion at the GInSEng workshop, was created by identifying key principles and adapting them to the Green area. Next, various important CoE were reviewed so as to gather the existing references to sustainability or to detect new ones. These elements would then possibly be incorporated into the new code or stimulate the creation of new sustainable principles. Results: The final result is the GreCo code, which has been produced by modifying existing principles or by the introduction of new ones.",
      "Keywords": "code of ethics | green software engineering | sustainability",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-02-01",
      "Publication type": "Conference Paper",
      "Authors": "Moraga, Ma Ángeles;García-Rodríguez de Guzmán, Ignacio;Calero, Coral;Johann, Timo;Me, Gianantonio;Münzel, Harald;Kindelsberger, Julia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85012950011",
      "Primary study DOI": "10.1002/smr.1852",
      "Title": "Energy efficiency on the product roadmap: An empirical study across releases of a software product",
      "Abstract": "In the quest for energy efficient Information and Communication Technology, research has mostly focused on the role of hardware. However, the impact of software on energy consumption has been acknowledged as significant by researchers in software engineering. In spite of that, due to cost and time constraints, many software producing organizations are unable to effectively measure software energy consumption preventing them to include energy efficiency in the product roadmap. In this paper, we apply a software energy profiling method to reliably compare the energy consumed by a commercial software product across 2 consecutive releases. We demonstrate how the method can be applied and provide an in-depth analysis of energy consumption of software components. Additionally, we investigate the added value of these measurement for multiple stakeholders in a software producing organization, by means of semistructured interviews. Our results show how the introduction of an encryption module caused a noticeable increase in the energy consumption of the product. Such results were deemed valuable by the stakeholders and provided insights on how specific software changes might affect energy consumption. In addition, our interviews show that such a quantification of software energy consumption helps to create awareness and eventually consider energy efficiency aspects when planning software releases.",
      "Keywords": "energy efficiency | product roadmap | profiling | software product",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-02-01",
      "Publication type": "Conference Paper",
      "Authors": "Jagroep, Erik;Procaccianti, Giuseppe;van der Werf, Jan Martijn;Brinkkemper, Sjaak;Blom, Leen;van Vliet, Rob",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85012993383",
      "Primary study DOI": "10.1002/smr.1849",
      "Title": "Cloud architecture continuity: Change models and change rules for sustainable cloud software architectures",
      "Abstract": "Cloud systems provide elastic execution environments of resources that link application and infrastructure/platform components, which are both exposed to uncertainties and change. Change appears in 2 forms: the evolution of architectural components under changing requirements and the adaptation of the infrastructure running applications. Cloud architecture continuity refers to the ability of a cloud system to change its architecture and maintain the validity of the goals that determine the architecture. Goal validity implies the satisfaction of goals in adapting or evolving systems. Architecture continuity aids technical sustainability, that is, the longevity of information, systems, and infrastructure and their adequate evolution with changing conditions. In a cloud setting that requires both steady alignment with technological evolution and availability, architecture continuity directly impacts economic sustainability. We investigate change models and change rules for managing change to support cloud architecture continuity. These models and rules define transformations of architectures to maintain system goals: Evolution is about unanticipated change of structural aspects of architectures, and adaptation is about anticipated change of architecture configurations. Both are driven by quality and cost, and both represent multidimensional decision problems under uncertainty. We have applied the models and rules for adaptation and evolution in research and industry consultancy projects.",
      "Keywords": "adaptation | change | change models | cloud systems | evolution | software architecture",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-02-01",
      "Publication type": "Conference Paper",
      "Authors": "Pahl, Claus;Jamshidi, Pooyan;Weyns, Danny",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029295409",
      "Primary study DOI": "10.1002/smr.1891",
      "Title": "Prioritizing key success factors of software projects using fuzzy AHP",
      "Abstract": "The main purpose of most software projects is to build an effective and efficient product that satisfies the user's requirements under constraints of basic available resources. Thus, recognizing the critical features that may affect developing the application could be imperative. In this study, we apply the fuzzy analytic hierarchy process (AHP) to the issue of ordering key success factors (KSFs) for software development projects. To do this, we first simplified the constrained fuzzy AHP method, and then from systematic literature reviews, a preliminary list of potential KSFs that influences software development projects was identified and compiled. Subsequently, on the basis of the criteria of the frequency of citation in previous studies, closely related and similar functionality, the preliminary list was consolidated into a final set of 15 possible KSFs and considered at 4 perspectives of project management, development team, product, and processes. Pairwise comparisons, required by fuzzy AHP method, were done by 22 experienced professional internal stakeholders to determine the preference weights of factors and perspectives in terms of natural language expressions. The results reveal that project management has the highest importance in comparison with other 3 perspectives, and the factor of efficient project management skills/methodologies is the major. The procedure used in this paper is simple and accurate to implement and concludes both relative weights and rankings of alternatives in a fuzzy environment.",
      "Keywords": "constrained fuzzy AHP | key success factors | project management | software development",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2018-01-01",
      "Publication type": "Article",
      "Authors": "Yaghoobi, Tahere",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84994317541",
      "Primary study DOI": "10.1002/smr.1797",
      "Title": "An empirical study on developer-related factors characterizing fix-inducing commits",
      "Abstract": "This paper analyzes developer-related factors that could influence the likelihood for a commit to induce a fix. Specifically, we focus on factors that could potentially hinder developers' ability to correctly understand the code components involved in the change to be committed as follows: (i) the coherence of the commit (i.e., how much it is focused on a specific topic); (ii) the experience level of the developer on the files involved in the commit; and (iii) the interfering changes performed by other developers on the files involved in past commits. The results of our study indicate that ‘fix-inducing’ commits (i.e., commits that induced a fix) are significantly less coherent than ‘clean’ commits (i.e., commits that did not induce a fix). Surprisingly, ‘fix-inducing’ commits are performed by more experienced developers; yet, those are the developers performing more complex changes in the system. Finally, ‘fix-inducing’ commits have a higher number of past interfering changes as compared with ‘clean’ commits. Our empirical study sheds light on previously unexplored factors and presents significant results that can be used to improve approaches for defect prediction. Copyright © 2016 John Wiley & Sons, Ltd.",
      "Keywords": "bug introduction | commits | empirical study",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Tufano, Michele;Bavota, Gabriele;Poshyvanyk, Denys;Di Penta, Massimiliano;Oliveto, Rocco;De Lucia, Andrea",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85028298064",
      "Primary study DOI": "10.1002/smr.1889",
      "Title": "Purity analysis for JavaScript through abstract interpretation",
      "Abstract": "We present a static analysis for determining whether and to what extent functions in JavaScript programs are pure. To this end, the analysis classifies functions as pure functions, observers, or procedures. A function is pure if none of its executions generate or depend upon externally observable side effects. A function is an observer as soon as one of its executions depends on an external side effect, but none of its executions generate observable side effects. Otherwise, the function is classified as a procedure. Function executions and associated callers are found by traversing all reachable function execution contexts on the call stack at the point where an effect occurs. Our approach is based on a flow analysis that, in addition to computing traditional control and value flow, keeps track of read and write effects. To increase the precision of our purity analysis, we combine it with an intraprocedural analysis that determines freshness of variables and objects. We formalize the core aspects of our technique and discuss its implementation and results on common JavaScript benchmarks. Results show that our approach is capable of determining function purity in the presence of higher-order functions, dynamic property expressions, and prototypal inheritance. When compared with existing purity analyses, we find that our approach is as precise or more precise than the existing analyses.",
      "Keywords": "abstract interpretation | freshness analysis | JavaScript | purity analysis | side effect analysis",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Nicolay, Jens;Stiévenart, Quentin;De Meuter, Wolfgang;De Roover, Coen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85034265555",
      "Primary study DOI": "10.1002/smr.1918",
      "Title": "Internal quality assurance for external contributions in GitHub: An empirical investigation",
      "Abstract": "For popular open-source software projects, there are always a large number of worldwide developers who have been glued to making code contributions, while most of these developers play the role of casual contributors because of their very limited code commits. The frequent turnover of such a group of developers and the wide variations in their coding experiences challenge the project management on code and quality. This paper aims to investigate the status quo of internal quality assurance for external contributions in social coding sites. We first conducted a case study of 21 popular GitHub projects to estimate the code quality of the casual contributors. The quantitative results show that the casual contributors introduced greater quantity and severity of code quality issues than the main contributors; the developers who contribute to different projects as main and casual contributors did not perform significantly differently in terms of their code quality. On the basis of these findings, we further conducted a survey of 81 developers on GitHub to understand their practices on internal quality assurance. The qualitative results expose some limitations of present internal quality control for external contributions in GitHub. Finally, we discuss an alternative quality management paradigm: Continuous Inspection for industrial practices.",
      "Keywords": "casual contributor | GitHub | internal quality",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2018-04-01",
      "Publication type": "Article",
      "Authors": "Lu, Yao;Mao, Xinjun;Li, Zude;Zhang, Yang;Wang, Tao;Yin, Gang",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84990218876",
      "Primary study DOI": "10.1002/smr.1803",
      "Title": "The formal semantics of program slicing for nonterminating computations",
      "Abstract": "Since the original development of program slicing in 1979 there have been many attempts to define a suitable semantics, which will precisely define the meaning of a slice. Particular issues include handling termination and nontermination, slicing nonterminating programs, and slicing nondeterministic programs. In this paper we review and critique the main attempts to construct a semantics for slicing and present a new operational semantics, which correctly handles slicing for nonterminating and nondeterministic programs. We also present a modified denotational semantics, which we prove to be equivalent to the operational semantics. This provides programmers with 2 different methods to prove the correctness of a slice or a slicing algorithm and means that the program transformation theory and FermaT transformation system, developed last 25 years of research, and which has proved so successful in analyzing terminating programs, can now be applied to nonterminating interactive programs.",
      "Keywords": "denotational semantics | nondeterminism | nontermination | program analysis | program slicing | wide spectrum language",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Ward, Martin;Zedan, Hussein",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85032890385",
      "Primary study DOI": "10.1002/smr.1921",
      "Title": "Onboarding software developers and teams in three globally distributed legacy projects: A multi-case study",
      "Abstract": "Onboarding is the process of supporting new employees regarding their social and performance adjustment to their new job. Software companies have faced challenges with recruitment and onboarding of new team members, and there is no study that investigates it in a holistic way. In this paper, we conducted a multi-case study to investigate the onboarding of software developers/teams, associated challenges, and areas for further improvement in 3 globally distributed legacy projects. We employed Bauer's model for onboarding to identify the current state of the onboarding strategies employed in each case. We learned that the employed strategies are semi-formalized. Besides, in projects with multiple sites, some functions are executed locally, and the onboarding outcomes may be hard to control. We also learned that onboarding in legacy projects is especially challenging and that decisions to distribute such projects across multiple locations shall be approached carefully. In our cases, the challenges to learn legacy code were further amplified by the project scale and the distance to the original sources of knowledge. Finally, we identified practices that can be used by companies to increase the chances of being successful when onboarding software developers and teams in globally distributed legacy projects.",
      "Keywords": "global software development | global software engineering | legacy | onboarding",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2018-04-01",
      "Publication type": "Article",
      "Authors": "Britto, Ricardo;Cruzes, Daniela S.;Smite, Darja;Sablis, Aivars",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013387683",
      "Primary study DOI": "10.1002/smr.1845",
      "Title": "TMAP: Discovering relevant API methods through text mining of API documentation",
      "Abstract": "Developers often migrate their applications to support various platform/programming-language application programming interfaces (APIs) to retain existing users and to attract new users. To migrate an application written using 1 API (source) to another API (target), a developer must know how the methods in the source API map to the methods in the target API. Given that a typical platform or language exposes a large number of API methods, manually discovering API mappings is prohibitively resource-intensive and may be error prone. The goal of this research is to support software developers in migrating an application from a source API to a target API by automatically discovering relevant method mappings across APIs using text mining on the natural language API method descriptions. This paper proposes text mining based approach (TMAP) to discover relevant API mappings. To evaluate our approach, we used TMAP to discover API mappings for 15 classes across (1) Java and C# API; and (2) Java ME and Android API. We compared the discovered mappings with state-of-the-art source code analysis-based approaches: Rosetta and StaMiner. Our results indicate that TMAP on average found relevant mappings for 56% and 57% more methods compared to the Rosetta and the StaMiner approaches, respectively.",
      "Keywords": "API documents | API mappings | text mining",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Pandita, Rahul;Jetley, Raoul;Sudarsan, Sithu;Menzies, Timothy;Williams, Laurie",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030026901",
      "Primary study DOI": "10.1002/smr.1897",
      "Title": "Cloud-based performance management of community care services",
      "Abstract": "Triple Aim for connected care is about improving patient experience, improving population health, and lowering cost of care. One of the key challenges to operationalizing Triple Aim is our ability to measure improvements at all levels of the health care ecosystem especially with respect to connected care. With a lack of interoperability between stakeholders in the health care system, our ability to measure quality of care goals is limited. Cloud computing offers the potential to improve data sharing and interoperability, thereby providing a good framework for operationalizing Triple Aim. In this paper, we evaluate our systematic framework for performance management of community care services based on a successful implementation for a large urban health region in support of Triple Aim. Three critical roadblocks to interoperability in a cloud computing context are identified (infrastructure, common data model, and a compliance framework), and alternative approaches to addressing them are evaluated and discussed.",
      "Keywords": "cloud computing | community care | data sharing | health care interoperability | performance management | Triple Aim",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2018-07-01",
      "Publication type": "Conference Paper",
      "Authors": "Eze, Benjamin;Kuziemsky, Craig;Peyton, Liam",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84991736250",
      "Primary study DOI": "10.1002/smr.1832",
      "Title": "Dynamics of task allocation in global software development",
      "Abstract": "Context: Global software development (GSD) promises high-quality software at low cost. GSD enables around-the-clock development to achieve maximum production in a short period of time by using expertise around the globe. This development is only possible if tasks are effectively distributed among sites to ensure smooth development. Therefore, one of the key challenges of GSD is to design a task allocation strategy. Objective: The objective of this study is to identify various factors that influence task allocation decisions in GSD and to assess their relative importance. We also aim to determine the interrelationship between the factors along with role played by product architecture and communication and coordination needs during task allocation. Methods: We used multiple methods to collect data about the task allocation factors and process. A web-based survey of 54 GSD practitioners from around the globe was conducted to identify the factors and their relative importance for task allocation decision. The selection of the sample was performed via the snowball sampling technique. To increase the sample size, the survey was also posted on social media, that is, Facebook, LinkedIn, and Twitter. Nonparametric statistical tests were applied on the response data to identify correlations and significance. Interviews were conducted from 11 project managers having 10 to 30 years GSD experience to gain insight into the dynamics of task allocation process. Results: The survey results highlight “expertise,” “site characteristics,” and “task site dependency” as the most important factors for a task allocation decision. The interview study has highlighted the importance of situation-specific decision making during task allocation. The significance of factors varies with the characteristics of task, characteristics of organization, type of GSD, and objective of doing GSD. The culture and time differences between distributed sites have been assigned a low priority by the majority of the practitioners. The most common way of distributing task is functional area of expertise and phase-based division, where detailed architecture is not considered. Interdependent modules are not allocated to distributed sites because of communication and coordination overhead. Our results also demonstrate a correlation between various factors and support Conway's law. Conclusions: We have interesting results in which certain factors are ranked differently from the prevalent views in the GSD literature. The survey results have also confirmed the application of Conway's law in practice for task allocation, where interdependent modules are not allocated to distributed sites. The significance of factors varies with characteristics of task, characteristics of organization, type of GSD, and objective of GSD, which require trade-off between factors. The need of a well-defined situation-specific task allocation framework is evident from the results of survey and interview study. The outline of a task allocation framework for GSD is presented.",
      "Keywords": "framework | global software development | interview study | practitioners view | survey | task allocation",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Imtiaz, Salma;Ikram, Naveed",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85039169585",
      "Primary study DOI": "10.1002/smr.1933",
      "Title": "More effective sprint retrospective with statistical analysis",
      "Abstract": "Scrum teams aim to deliver products productively with highest possible value and quality, so they try to deliver high priority and high value product backlog items in earlier sprints. Making size estimation of product backlog items correctly is one of the most prominent factors for effective sprint planning. Retrospective meetings are an opportunity for teams to improve product quality, their productivity, and estimation capability. Enhancing in those areas requires empiricism as agility requires; hence, measureable indicators should be inspected and adapted at regular intervals. In this study, we assessed how and what kind of historical data is required to be collected for monitoring, and how statistical analysis can be investigated for inspection and adaptation in retrospective meetings. We experimented that statistics of “Correlation between Story Point and Actual Effort” and “Consistency of Relative Estimation” were convenient for inspection and adaptation of estimation capability of teams. Past retrospective meetings also showed that statistics of “Team's Actual Effort on Product,” “Team Velocity,” “Actual Effort for One Story Point,” “Innovation Rate,” and “Velocity vs Unplanned Effort Rate” were helpful to control and increase the productivity of teams. “Actual Effort Rate of Quality Activities” and “Subcomponent Defect Density” statistical results helped to enhance product quality.",
      "Keywords": "process improvement | product quality | productivity | Scrum | sprint retrospective | statistical analysis",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2018-05-01",
      "Publication type": "Conference Paper",
      "Authors": "Erdoğan, Onur;Pekkaya, Muhammed Emre;Gök, Halime",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85037660038",
      "Primary study DOI": "10.1002/smr.1926",
      "Title": "Context-dependent reconfiguration of autonomous vehicles in mixed traffic",
      "Abstract": "Human drivers naturally adapt their behaviour depending on the traffic conditions, such as the current weather and road type. Autonomous vehicles need to do the same, in a way that is both safe and efficient in traffic composed of both conventional and autonomous vehicles. In this paper, we demonstrate the applicability of a reconfigurable vehicle controller agent for autonomous vehicles that adapts the parameters of a used car-following model at runtime, so as to maintain a high degree of traffic quality (efficiency and safety) under different weather conditions. We follow a dynamic software product line approach to model the variability of the car-following model parameters, context changes and traffic quality, and generate specific configurations for each particular context. Under realistic conditions, autonomous vehicles have only a very local knowledge of other vehicles' variables. We investigate a distributed model predictive controller agent for autonomous vehicles to estimate their behavioural parameters at runtime, based on their available knowledge of the system. We show that autonomous vehicles with the proposed reconfigurable controller agent lead to behaviour similar to that achieved by human drivers, depending on the context.",
      "Keywords": "autonomous vehicles | car-following model | dynamic software product line | reconfiguration | traffic quality",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2018-04-01",
      "Publication type": "Article",
      "Authors": "Horcas, José Miguel;Monteil, Julien;Bouroche, Mélanie;Pinto, Mónica;Fuentes, Lidia;Clarke, Siobhán",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85031123712",
      "Primary study DOI": "10.1002/smr.1911",
      "Title": "Software quality model for a research-driven organization-An experience report",
      "Abstract": "In the paper, we present a measurement framework for evaluating quality in software products developed within the research and innovation framework project GÉANT. The proposed framework is based on the quality models by Boehm and McCall, but also addresses the presence and point of view of a third stakeholder: an external funding agency (EU), which has started and is temporally financing the project, but aims at making it self-financing in the future. We also provide results of evaluation of 2 projects from the GÉANT ecosystem and one open-source system with this framework.",
      "Keywords": "quality models | software measurement | software process improvement | software quality",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2018-05-01",
      "Publication type": "Conference Paper",
      "Authors": "Wolski, Marcin;Walter, Bartosz;Kupiński, Szymon;Chojnacki, Jakub",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029385454",
      "Primary study DOI": "10.1002/smr.1893",
      "Title": "Identifying unusual commits on GitHub",
      "Abstract": "Transparent environments and social-coding platforms as GitHub help developers to stay abreast of changes during the development and maintenance phase of a project. Especially, notification feeds can help developers to learn about relevant changes in other projects. Unfortunately, transparent environments can quickly overwhelm developers with too many notifications, such that they lose the important ones in a sea of noise. Complementing existing prioritization and filtering strategies based on binary compatibility and code ownership, we develop an anomaly detection mechanism to identify unusual commits in a repository, which stand out with respect to other changes in the same repository or by the same developer. Among others, we detect exceptionally large commits, commits at unusual times, and commits touching rarely changed file types given the characteristics of a particular repository or developer. We automatically flag unusual commits on GitHub through a browser plug-in. In an interactive survey with 173 active GitHub users, rating commits in a project of their interest, we found that, although our unusual score is only a weak predictor of whether developers want to be notified about a commit, information about unusual characteristics of a commit changes how developers regard commits. Our anomaly detection mechanism is a building block for scaling transparent environments.",
      "Keywords": "anomaly detection | information overload | notification feeds | software ecosystems | transparent environments",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2018-01-01",
      "Publication type": "Article",
      "Authors": "Goyal, Raman;Ferreira, Gabriel;Kästner, Christian;Herbsleb, James",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85034047583",
      "Primary study DOI": "10.1002/smr.1915",
      "Title": "Database engines: Evolution of greenness",
      "Abstract": "Information technology consumes up to 10% of the world's electricity generation, contributing to CO2 emissions and high energy costs. Data centers, particularly databases, use up to 23% of this energy. Therefore, building an energy-efficient (green) database engine could reduce energy consumption and CO2 emissions. The goal of this study is to understand the factors driving databases' energy consumption and execution time throughout their evolution. We conducted an empirical case study of energy consumption by 2 MySQL database engines, InnoDB and MyISAM, across 40 releases. We examined the relationships of 4 software metrics to energy consumption and execution time to determine which metrics reflect the greenness and performance of a database. Our analysis shows that database engines' energy consumption and execution time increase as databases evolve. Moreover, the lines of code (LOC) metric is correlated moderately to strongly with energy consumption and execution time in 88% of cases. Our findings provide insights to practitioners and researchers. Database administrators may use them to select a fast, green release of the MySQL database engine. MySQL developers may use LOC to assess products' greenness and performance. Researchers may use our findings to further develop new hypotheses or build models predicting greenness and performance of databases.",
      "Keywords": "database | energy consumption | green computing | MySQL | software metrics",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2018-04-01",
      "Publication type": "Article",
      "Authors": "Miranskyy, A. V.;Al-zanbouri, Z.;Godwin, D.;Bener, A. B.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84986286537",
      "Primary study DOI": "10.1002/smr.1805",
      "Title": "Exploiting spatial code proximity and order for improved source code retrieval for bug localization",
      "Abstract": "Practically all information retrieval based approaches developed to date for automatic bug localization are based on the bag-of-words assumption that ignores any positional and ordering relationships between the terms in a query. In this paper, we argue that bug reports are ill-served by this assumption because such reports frequently contain various types of structural information whose terms must obey certain positional and ordering constraints. It therefore stands to reason that the quality of retrieval for bug localization would improve if these constraints could be taken into account when searching for the most relevant files. In this paper, we demonstrate that such is indeed the case. We show how the well-known Markov Random Field based retrieval framework can be used for taking into account the term-term proximity and ordering relationships in a query vis-à-vis the same relationships in the files of a source-code library to greatly improve the quality of retrieval of the most relevant source files. We have carried out our experimental evaluations on popular large software projects using over 4000 bug reports. The results we present demonstrate unequivocally that the new proposed approach is far superior to the widely used bag-of-words based approaches. Copyright © 2016 John Wiley & Sons, Ltd.",
      "Keywords": "bug localization | Markov Random Fields | source code search | term proximity",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2017-01-01",
      "Publication type": "Article",
      "Authors": "Sisman, Bunyamin;Akbar, Shayan A.;Kak, Avinash C.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85039172997",
      "Primary study DOI": "10.1002/smr.1929",
      "Title": "A hybrid assessment approach for medical device software development companies",
      "Abstract": "Medical device software development organizations are bound by regulatory requirements and constraints to ensure that developed medical devices will not harm patients. Medical devices have to be treated as complete systems and be evaluated in this manner. Instead of manufacturers having to ensure compliance to various regulatory standards individually, the authors previously developed a medical device software process assessment framework called MDevSPICE® that integrates the regulatory requirements from all the relevant medical device software standards. The MDevSPICE® was developed in a manner that suits plan-driven software development. To improve the usability of MDevSPICE® in agile settings, we extended the assessment approach. The hybrid assessment approach described here combines the MDevSPICE®-based process assessment method with steps for prioritization of improvement needs through value stream mapping and enabling process improvement through the use of KATA technique. This approach integrates agile methods into the medical device software development process while adhering to the requirements of the regulatory standards. This paper describes the implementation of the approach within 4 organizations that develop software in line with medical device regulations.",
      "Keywords": "agile software development | MDevSPICE ® | medical device software development | medical standards | process assessment",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2018-07-01",
      "Publication type": "Conference Paper",
      "Authors": "Özcan-Top, Özden;McCaffery, Fergal",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018416465",
      "Primary study DOI": "10.1109/SANER.2017.7884619",
      "Title": "Extracting executable transformations from distilled code changes",
      "Abstract": "Change distilling algorithms compute a sequence of fine-grained changes that, when executed in order, transform a given source AST into a given target AST. The resulting change sequences are used in the field of mining software repositories to study source code evolution. Unfortunately, detecting and specifying source code evolutions in such a change sequence is cumbersome. We therefore introduce a tool-supported approach that identifies minimal executable subsequences in a sequence of distilled changes that implement a particular evolution pattern, specified in terms of intermediate states of the AST that undergoes each change. This enables users to describe the effect of multiple changes, irrespective of their execution order, while ensuring that different change sequences that implement the same code evolution are recalled. Correspondingly, our evaluation is two-fold. Using examples, we demonstrate the expressiveness of specifying source code evolutions through intermediate ASTs. We also show that our approach is able to recall different implementation variants of the same source code evolution in open-source histories.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Stevens, Reinout;De Roover, Coen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018395714",
      "Primary study DOI": "10.1109/SANER.2017.7884675",
      "Title": "Self-adaptive systems framework based on agent and search-based optimization",
      "Abstract": "Future-generation SASs need to have the adaptive abilities to efficiently handle changes from different sources and to mitigate conflicts caused by multiple simultaneous changes. However, existing methods cannot simultaneously make Future-generation SASs have the above abilities. This paper proposes an adaptive system framework based on agent technology and search-based software engineering technology (SBSE) for developing future-generation SASs with above-mentioned abilities. The framework integrates a hybrid adaptation logic based on agents to deal with various software changes from different layers, and an adaptation planning method with search-based optimization mechanism to mitigate conflicts caused by multiple simultaneous changes.",
      "Keywords": "Adaptation logic | Multi-agent systems | Search-based software engineering | Self-adaptive systems",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "He, Liu;Li, Qingshan;Wang, Lu;Wan, Jiewen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018373240",
      "Primary study DOI": "10.1109/SANER.2017.7884676",
      "Title": "The importance of program Design Patterns training",
      "Abstract": "Design Patterns for Object Oriented Systems constitute an important tool for improving software quality by providing reusable design. Many academic institutions believe in their relevance, and do courses accordingly. This paper explores practitioners' perception of the relevance their patterns knowledge has for their work. The paper also assesses how managers' perception of pattern knowledge conforms with practitioners' perceptions. We found convincing evidence for practitioners' confidence in pattern knowledge and its positive influence on their coding abilities. Based on our findings we claim that training of design patterns is important for practitioners.",
      "Keywords": "Design Patterns | empirical assessment | managers | practitioners | training",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Holmstedt, Viggo;Mengiste, Shegaw A.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018374658",
      "Primary study DOI": "10.1109/SANER.2017.7884679",
      "Title": "Modernizing domain-specific languages with XMLText and IntellEdit",
      "Abstract": "The necessity of software evolution caused by novel requirements is often triggered alongside the advancement of underlying languages and tools. Although modern language workbenches decrease the opportunity cost of creating new language implementations, they do not offer automated and complete integration of existing languages. Moreover, they still require complex language engineering skills and extensive manual implementation effort to suit the expectations of domain experts, e.g., in terms of editor capabilities. In this work we present XMLIntellEdit - a framework for evolving domain-specific languages by automating the generation of modernized languages offering advanced editing capabilities, such as extended validation, content-assist, and quick fix solutions. Our approach builds on techniques from Model-Driven Engineering and Search-based Software Engineering research. Initial results indicate that XML Schema definitions containing restrictions can be applied for the automated generation of advanced editing facilities.",
      "Keywords": "Advanced Editor Support | Domain Specific Languages | Model Driven Engineering | Search-based Software Engineering | XML",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Neubauer, Patrick;Bill, Robert;Wimmer, Manuel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018444029",
      "Primary study DOI": "10.1109/SANER.2017.7884608",
      "Title": "StiCProb: A novel feature mining approach using conditional probability",
      "Abstract": "Software Product Line Engineering is a key approach to construct applications with systematical reuse of architecture, documents and other relevant components. To migrate legacy software into a product line system, it is essential to identify the code segments that should be constructed as features from the source base. However, this could be an error-prone and complicated task, as it involves exploring a complex structure and extracting the relations between different components within a system. And normally, representing structural information of a program in a mathematical way should be a promising direction to investigate. We improve this situation by proposing a probability-based approach named StiCProb to capture source code fragments for feature concerned, which inherently provides a conditional probability to describe the closeness between two programming elements. In the case study, we conduct feature mining on several legacy systems, to compare our approach with other related approaches. As demonstrated in our experiment, our approach could support developers to locate features within legacy successfully with a better performance of 83% for precision and 41% for recall.",
      "Keywords": "feature mining | program slicing | Software product line | variability",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Tang, Yutian;Leung, Hareton",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018373833",
      "Primary study DOI": "10.1109/SANER.2017.7884655",
      "Title": "SrcQL: A syntax-aware query language for source code",
      "Abstract": "A tool and domain specific language for querying source code is introduced and demonstrated. The tool, srcQL, allows for the querying of source code using the syntax of the language to identify patterns within source code documents. srcQL is built upon srcML, a widely used XML representation of source code, to identify the syntactic contexts being queried. srcML inserts XML tags into the source code to mark syntactic constructs. srcQL uses a combination of XPath on srcML, regular expressions, and syntactic patterns within a query. The syntactic patterns are snippets of source code that supports the use of logical variables which are unified during the query process. This allows for very complex patterns to be easily formulated and queried. The tool is implemented (in C++) and a number of queries are presented to demonstrate the approach. srcQL currently supports C++ and scales to large systems.",
      "Keywords": "Source code querying | srcML | syntactic search",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Bartman, Brian;Newman, Christian D.;Collard, Michael L.;Maletic, Jonathan I.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018436001",
      "Primary study DOI": "10.1109/SANER.2017.7884635",
      "Title": "Dynamic patch generation for null pointer exceptions using metaprogramming",
      "Abstract": "Null pointer exceptions (NPE) are the number one cause of uncaught crashing exceptions in production. In this paper, we aim at exploring the search space of possible patches for null pointer exceptions with metaprogramming. Our idea is to transform the program under repair with automated code transformation, so as to obtain a metaprogram. This metaprogram contains automatically injected hooks, that can be activated to emulate a null pointer exception patch. This enables us to perform a fine-grain analysis of the runtime context of null pointer exceptions. We set up an experiment with 16 real null pointer exceptions that have happened in the field. We compare the effectiveness of our metaprogramming approach against simple templates for repairing null pointer exceptions.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Durieux, Thomas;Cornu, Benoit;Seinturier, Lionel;Monperrus, Martin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018436574",
      "Primary study DOI": "10.1109/SANER.2017.7884631",
      "Title": "Antipatterns causing memory bloat: A case study",
      "Abstract": "Java is one of the languages that are popular for high abstraction and automatic memory management. As in other object-oriented languages, Java's objects can easily represent a domain model of an application. While it has a positive impact on the design, implementation and maintenance of applications, there are drawbacks as well. One of them is a relatively high memory overhead to manage objects. In this work, we show our experience with searching for this problem in an application that we refactored to use less memory. Although the application was relatively well designed with no memory leaks, it required such a big amount of memory that for large data the application was not usable in reality. We did three relatively simple improvements: we reduced the usage of Java Collections, removed unnecessary object instances, and simplified the domain model, which reduced memory needs up to 88% and made the application better usable and even faster. This work is a case-study reporting results. Moreover, the employed ideas are formulated as a set of antipatterns, which may be used for other applications.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Jezek, Kamil;Lipka, Richard",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018374157",
      "Primary study DOI": "",
      "Title": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Abstract": "The proceedings contain 82 papers. The topics discussed include: an empirical comparison of dependency issues in OSS packaging ecosystems; detecting similar repositories on GitHub; socio-technical evolution of the ruby ecosystem in GitHub; StiCProb: a novel feature mining approach using conditional probability; HDSKG: harvesting domain specific knowledge graph from content of webpages; analyzing closeness of code dependencies for improving IR-based traceability recovery; analyzing reviews and code of mobile apps for better release planning; software-based energy profiling of android apps: simple, efficient and reliable?; automated generation of consistency-achieving model editors; reducing redundancies in multi-revision code analysis; recommending source code locations for system specific transformations; extracting executable transformations from distilled code changes; automatically generating natural language descriptions for object-related statement sequences; computing counter-examples for privilege protection losses using security models; and improving fault localization for Simulink models using search-based testing and prediction models.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018422479",
      "Primary study DOI": "10.1109/SANER.2017.7884649",
      "Title": "Cloud-based parallel concolic execution",
      "Abstract": "Path explosion is one of the biggest challenges hindering the wide application of concolic execution. Although several parallel approaches have been proposed to accelerate concolic execution, they neither scale well nor properly handle resource fluctuations and node failures, which often happen in practice. In this paper, we propose a novel approach, named PACCI, which parallelizes concolic execution and adapts to the drastic changes of computing resources by leveraging cloud infrastructures. PACCI tailors concolic execution to the MapReduce programming model and takes into account the features of cloud infrastructures. In particular, we tackle several challenging issues, such as making the exploration of different program paths independently and constructing an extensible path exploration module to support the prioritization of test inputs from a global perspective. Preliminary experimental results show that PACCI is scalable (e.g., gaining about 20× speedup using 24 nodes) and its efficiency declines slightly about 5% and 6.1% under resource fluctuations and node failures, respectively.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Chen, Ting;Feng, Youzheng;Luo, Xiapu;Lin, Xiaodong;Zhang, Xiaosong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018436698",
      "Primary study DOI": "10.1109/SANER.2017.7884658",
      "Title": "C-JRefRec: Change-based identification of Move Method refactoring opportunities",
      "Abstract": "We propose, in this paper, a lightweight refactoring recommendation tool, namely c-JRefRec, to identify Move Method refactoring opportunities based on four heuristics using static and semantic program analysis. Our tool aims at identiying refactoring opportunities before a code change is committed to the codebase based on current code changes whenever the developer saves/compiles his code. We evaluate the efficiency of our approach in detecting Feature Envy smells and recommending Move Method refactorings to fix them on three Java open-source systems and 30 code changes. Results show that our approach achieves an average precision of 0.48 and 0.73 of recall and outperforms a state-of-the-art approach namely JDeodorant.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Ujihara, Naoya;Ouni, Ali;Ishio, Takashi;Inoue, Katsuro",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018376376",
      "Primary study DOI": "10.1109/SANER.2017.7884618",
      "Title": "Recommending source code locations for system specific transformations",
      "Abstract": "From time to time, developers perform sequences of code transformations in a systematic and repetitive way. This may happen, for example, when introducing a design pattern in a legacy system: similar classes have to be introduced, containing similar methods that are called in a similar way. Automation of these sequences of transformations has been proposed in the literature to avoid errors due to their repetitive nature. However, developers still need support to identify all the relevant code locations that are candidate for transformation. Past research showed that these kinds of transformation can lag for years with forgotten instances popping out from time to time as other evolutions bring them into light. In this paper, we evaluate three distinct code search approaches ('structural', based on Information Retrieval, and AST based algorithm) to find code locations that would require similar transformations. We validate the resulting candidate locations from these approaches on real cases identified previously in literature. The results show that looking for code with similar roles, e.g., classes in the same hierarchy, provides interesting results with an average recall of 87% and in some cases the precision up to 70%.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Santos, Gustavo;Paixao, Klerisson V.R.;Anquetil, Nicolas;Etien, Anne;De Almeida Maia, Marcelo;Ducasse, Stephane",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018444461",
      "Primary study DOI": "10.1109/SANER.2017.7884666",
      "Title": "Bringing back-in-time debugging down to the database",
      "Abstract": "With back-in-time debuggers, developers can explore what happened before observable failures by following infection chains back to their root causes. While there are several such debuggers for object-oriented programming languages, we do not know of any back-in-time capabilities at the database-level. Thus, if failures are caused by SQL scripts or stored procedures, developers have difficulties in understanding their unexpected behavior. In this paper, we present an approach for bringing back-in-time debugging down to the SAP HANA in-memory database. Our TARDISP debugger allows developers to step queries backwards and inspecting the database at previous and arbitrary points in time. With the help of a SQL extension, we can express queries covering a period of execution time within a debugging session and handle large amounts of data with low overhead on performance and memory. The entire approach has been evaluated within a development project at SAP and shows promising results with respect to the gathered developer feedback.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Treffer, Arian;Perscheid, Michael;Uflacker, Matthias",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018376563",
      "Primary study DOI": "10.1109/SANER.2017.7884624",
      "Title": "Lexical categories for source code identifiers",
      "Abstract": "A set of lexical categories, analogous to part-of-speech categories for English prose, is defined for source-code identifiers. The lexical category for an identifier is determined from its declaration in the source code, syntactic meaning in the programming language, and static program analysis. Current techniques for assigning lexical categories to identifiers use natural-language part-of-speech taggers. However, these NLP approaches assign lexical tags based on how terms are used in English prose. The approach taken here differs in that it uses only source code to determine the lexical category. The approach assigns a lexical category to each identifier and stores this information along with each declaration. srcML is used as the infrastructure to implement the approach and so the lexical information is stored directly in the srcML markup as an additional XML element for each identifier. These lexical-category annotations can then be later used by tools that automatically generate such things as code summarization or documentation. The approach is applied to 50 open source projects and the soundness of the defined lexical categories evaluated. The evaluation shows that at every level of minimum support tested, categorization is consistent at least 79% of the time with an overall consistency (across all supports) of at least 88%. The categories reveal a correlation between how an identifier is named and how it is declared. This provides a syntax-oriented view (as opposed to English part-of-speech view) of developer intent of identifiers.",
      "Keywords": "identifier analysis | Natural Language Processing | part-of-speech tagging | program comprehension",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Newman, Christian D.;Alsuhaibani, Reem S.;Collard, Michael L.;Maletic, Jonathan I.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018369850",
      "Primary study DOI": "10.1109/SANER.2017.7884654",
      "Title": "XCORE: Support for developing program analysis tools",
      "Abstract": "Building program analysis tools is hard. A recurring development task is the implementation of the meta-model around which a tool is usually constructed. The XCORE prototype supports this task by generating the implementation of the meta-model. For this purpose, developers will add directly into the source code of the tool under construction some meta-information describing the desired meta-model. Our demo presents some internal details of XCORE and emphasizes the advantages of our tool by describing the construction of a basic analysis instrument.",
      "Keywords": "meta-model | program analysis tool",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Stefanica, Alexandru;Mihancea, Petru Florin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018393622",
      "Primary study DOI": "10.1109/SANER.2017.7884673",
      "Title": "Towards reverse engineering of intermediate code for documentation generators",
      "Abstract": "We describe the motivation, approach and first experience from reverse engineering Common Intermediate Language (CIL) for the purpose of documentation generation. Instead of parsing source code implemented in different programming languages, we reverse engineer CIL code and thereby enable documentation generation for all programming languages that can be compiled into CIL code. Initial results show that we are able to generate documents in the same quality as compared to directly analyzing source code. To overcome initial shortcomings we introduce additional preprocessing in form of AST refactoring which is not required when analyzing source code.",
      "Keywords": "documentation generator | intermediate language | reverse engineering | static analysis",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Moser, Michael;Pfeiffer, Michael;Pichler, Josef",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018379245",
      "Primary study DOI": "10.1109/SANER.2017.7884632",
      "Title": "Variant-preserving refactorings for migrating cloned products to a product line",
      "Abstract": "A common and simple way to create custom product variants is to copy and adapt existing software (a. k. a. the clone-and-own approach). Clone-and-own promises low initial costs for creating a new variant as existing code is easily reused. However, clone-and-own also comes with major drawbacks for maintenance and evolution since changes, such as bug fixes, need to be synchronized among several product variants. Software product lines (SPLs) provide solutions to these problems because commonalities are implemented only once. Thus, in an SPL, changes also need to be applied only once. Therefore, the migration of cloned product variants to an SPL would be beneficial. The main tasks of migration are the identification and extraction of commonalities from existing products. However, these tasks are challenging and currently not well-supported. In this paper, we propose a step-wise and semi-automated process to migrate cloned product variants to a feature-oriented SPL. Our process relies on clone detection to identify code that is common to multiple variants and novel, variant-preserving refactorings to extract such common code. We evaluated our approach on five cloned product variants, reducing code clones by 25 %. Moreover, we provide qualitative insights into possible limitations and potentials for removing even more redundant code. We argue that our approach can effectively decrease synchronization effort compared to clone-and-own development and thus reduce the long-term costs for maintenance and evolution.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Fenske, Wolfram;Meinicke, Jens;Schulze, Sandro;Schulze, Steffen;Saake, Gunter",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85016217853",
      "Primary study DOI": "10.1109/SANER.2017.7884642",
      "Title": "Towards continuous software release planning",
      "Abstract": "Continuous software engineering is a new trend that is gaining increasing attention of the research community in the last years. The main idea behind this trend is to tighten the connection between the software engineering lifecycle activities (e.g., development, planning, integration, testing, etc.). While the connection between development and integration (i.e., continuous integration) has been subject of research and is applied in industrial settings, the connection between other activities is still in a very early stage. We are contributing to this research topic by proposing our ideas towards connecting the software development and software release planning activities (i.e., continuous software release planning). In this paper we present our initial findings on this topic, how we envision to address the continuous software release planning, and a research agenda to fulfil our objectives.",
      "Keywords": "Continuous Software Engineering | Project Management | Software Release Planning",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Ameller, David;Farre, Carles;Franch, Xavier;Valerio, Danilo;Cassarino, Antonino",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018455595",
      "Primary study DOI": "10.1109/SANER.2017.7884685",
      "Title": "Towards understanding an open-source bounty: Analysis of Bountysource",
      "Abstract": "When developing and maintaining a software project, many issues about bug fixing or feature addition are reported on the Bug Tracking System (BTS) and the Issue Tracking System (ITS). Bountysource is a web founding platform that awards developers who have solved issues on the BTS/ITS. Users can post a bounty for the issues, and a developer who solves the issue can get that bounty. This research analyzes Bountysource to clarify how bounties act in open source software projects and discusses further research topics in open-source bounties.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Kanda, Tetsuya;Guo, Mingyu;Hata, Hideaki;Matsumoto, Kenichi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018369980",
      "Primary study DOI": "10.1109/SANER.2017.7884620",
      "Title": "Automatic generation of opaque constants based on the k-clique problem for resilient data obfuscation",
      "Abstract": "Data obfuscations are program transformations used to complicate program understanding and conceal actual values of program variables. The possibility to hide constant values is a basic building block of several obfuscation techniques. For example, in XOR Masking a constant mask is used to encode data, but this mask must be hidden too, in order to keep the obfuscation resilient to attacks. In this paper, we present a novel technique based on the k-clique problem, which is known to be NP-complete, to generate opaque constants, i.e. values that are difficult to guess by static analysis. In our experimental assessment we show that our opaque constants are computationally cheap to generate, both at obfuscation time and at runtime. Moreover, due to the NP-completeness of the k-clique problem, our opaque constants can be proven to be hard to attack with state-of-the-art static analysis tools.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Tiella, Roberto;Ceccato, Mariano",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018421262",
      "Primary study DOI": "10.1109/SANER.2017.7884648",
      "Title": "CodeCritics applied to database schema: Challenges and first results",
      "Abstract": "Relational databases (DB) play a critical role in many information systems. For different reasons, their schemas gather not only tables and columns but also views, triggers or stored functions (i.e., fragments of code describing treatments). As for any other code-related artefact, software quality in a DB schema helps avoiding future bugs. However, few tools exist to analyse DB quality and prevent the introduction of technical debt. Moreover, these tools suffer from limitations like the difficulty to deal with some entities (e.g., functions) or dependencies between entities. This paper presents research issues related to assessing the software quality of a DB schema by adapting existing source code analysis research to database schemas. We present preliminary results that have been validated through the implementation of DBCritics, a prototype tool to perform static analysis on the SQL source code of a database schema. DBCritics addresses the limitations of existing DB quality tools based on an internal representation considering all entities of the database and their relationships.",
      "Keywords": "database critics | database software quality | design smells | quality assessment",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Delplanque, Julien;Etien, Anne;Auverlot, Olivier;Mens, Tom;Anquetil, Nicolas;Ducasse, Stephane",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018390307",
      "Primary study DOI": "10.1109/SANER.2017.7884609",
      "Title": "HDSKG: Harvesting domain specific knowledge graph from content of webpages",
      "Abstract": "Knowledge graph is useful for many different domains like search result ranking, recommendation, exploratory search, etc. It integrates structural information of concepts across multiple information sources, and links these concepts together. The extraction of domain specific relation triples (subject, verb phrase, object) is one of the important techniques for domain specific knowledge graph construction. In this research, an automatic method named HDSKG is proposed to discover domain specific concepts and their relation triples from the content of webpages. We incorporate the dependency parser with rule-based method to chunk the relations triple candidates, then we extract advanced features of these candidate relation triples to estimate the domain relevance by a machine learning algorithm. For the evaluation of our method, we apply HDSKG to Stack Overflow (a Q&A website about computer programming). As a result, we construct a knowledge graph of software engineering domain with 35279 relation triples, 44800 concepts, and 9660 unique verb phrases. The experimental results show that both the precision and recall of HDSKG (0.78 and 0.7 respectively) is much higher than the openIE (0.11 and 0.6 respectively). The performance is particularly efficient in the case of complex sentences. Further more, with the self-training technique we used in the classifier, HDSKG can be applied to other domain easily with less training data.",
      "Keywords": "Dependency Parse | Knowledge Graph | openIE | Stack Overflow | Structural Information Extraction",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Zhao, Xuejiao;Xing, Zhenchang;Kabir, Muhammad Ashad;Sawada, Naoya;Li, Jing;Lin, Shang Wei",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018427771",
      "Primary study DOI": "10.1109/SANER.2017.7884684",
      "Title": "Log generation for coding behavior analysis: For focusing on how kids are coding not what they are coding",
      "Abstract": "Block programming lowers the barrier for programming learners and it is used in many software education program. Based on our observation, we realized that there are differences in way of learning and time of finishing goals even in under same instructors. To know the cause of this difference we propose a logging function to see the coding behavior of programmers. In this work we have developed library for generating log of developer's behavior in the process of block programming and defined required common items in creating block log process. In addition, we present the coding characteristics from the log, available information for deriving coding characteristics and detail criteria for deriving each characteristic. The contribution of this work is in development of framework generating logs of block programming process. This work will contribute to understand the programming learners' behaviors and enable instructors to design the learning courses properly.",
      "Keywords": "coding behavior | Entry platform | Log generation",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Moon, Ra Jeong;Shim, Kyu Min;Lee, Hae Young;Kim, Hyung Jong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018442292",
      "Primary study DOI": "10.1109/SANER.2017.7884682",
      "Title": "Which review feedback did long-term contributors get on OSS projects?",
      "Abstract": "Open Source Software (OSS) cannot exist without contributions from the community. In particular, long-term contributors (LTCs) (e.g., committer), defined as contributors who spend at least one year on OSS projects, play crucial role in a project success because they would have permission to add (commit) code changes to a project's version control system, and to become a mentor for a beginner in OSS projects. However, contributors often leave a project before becoming a LTC because most contributors are volunteers. If contributors are motivated in their work in OSS projects, they might not leave the projects. In this study, we examine the phenomena involved in becoming a LTC in terms of motivation to continue in OSS projects. In particular, our target motivation is to understand what is involved in long-term contribution with other expert contributors. We study classifier to identify a LTC who will contribute patch submissions for more than one year based on collaboration in terms of the code review process. In detail, we analyze what review feedbacks encourage a contributor to continue with OSS project. Using a Qt project dataset, we understand review feedback which affected contribution period of the developer.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Norikane, Takuto;Ihara, Akinori;Matsumoto, Kenichi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018378749",
      "Primary study DOI": "10.1109/SANER.2017.7884663",
      "Title": "Columbo: High perfomance unpacking",
      "Abstract": "Columbo is a tool for unpacking malware. A key feature is the ability to uncompress and/or decrypt areas of memory quickly using algorithms focused on basic block compression and loop optimizations.",
      "Keywords": "Compression | Decryption | malware | Reverse Engineering | Unpacking",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Raber, Jason",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018389539",
      "Primary study DOI": "10.1109/SANER.2017.7884674",
      "Title": "A framework for classifying and comparing source code recommendation systems",
      "Abstract": "The use of Application Programming Interfaces (APIs) is pervasive in software systems; it makes the development of new software much easier, but remembering large APIs with sophisticated usage protocol is arduous for software developers. Code recommendation systems alleviate this burden by providing developers with a ranked list of API usages that are estimated to be most useful to their development tasks. The promise of these systems has motivated researchers to invest considerable effort to develop many of them over the past decade, yet the achievements are not evident. To assess the state of the art in code recommendation, we propose a framework for classifying and comparing these systems. We hope the framework will help the community to conduct a systematic study to gain insight into how much code recommendation has so far achieved, in both research and practice.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Ghafari, Mohammad;Moradi, Hamidreza",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018381196",
      "Primary study DOI": "10.1109/SANER.2017.7884623",
      "Title": "Shorter identifier names take longer to comprehend",
      "Abstract": "Developers spend the majority of their time comprehending code, a process in which identifier names play a key role. Although many identifier naming styles exist, they often lack an empirical basis and it is not quite clear whether short or long identifier names facilitate comprehension. In this paper, we investigate the effect of different identifier naming styles (letters, abbreviations, words) on program comprehension, and whether these effects arise because of their length or their semantics. We conducted an experimental study with 72 professional C# developers, who looked for defects in source-code snippets. We used a within-subjects design, such that each developer saw all three versions of identifier naming styles and we measured the time it took them to find a defect. We found that words lead to, on average, 19% faster comprehension speed compared to letters and abbreviations, but we did not find a significant difference in speed between letters and abbreviations. The results of our study suggest that defects in code are more difficult to detect when code contains only letters and abbreviations. Words as identifier names facilitate program comprehension and can help to save costs and improve software quality.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Hofmeister, Johannes;Siegmund, Janet;Holt, Daniel V.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018391309",
      "Primary study DOI": "10.1109/SANER.2017.7884661",
      "Title": "DynStruct: An automatic reverse engineering tool for structure recovery and memory use analysis",
      "Abstract": "dynStruct is an open source structure recovery tool for ×86 binaries. It uses dynamic binary instrumentation to record information about memory accesses, which is then processed off-line to recover structures created and used by the binary. It provides a powerful web interface which not only displays the raw data and the recovered structures but also allows this information to be explored and manually edited. dynStruct is an effective tool for analyzing programs as complex as emacs. A demonstration video is available at: http://bit.ly/2gQu26e.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Mercier, Daniel;Chawdhary, Aziem;Jones, Richard",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018412386",
      "Primary study DOI": "10.1109/SANER.2017.7884668",
      "Title": "Business process recovery based on system log and information of organizational structure",
      "Abstract": "In most current cases of enterprise system development, the requirement specifications should follow those of an existing legacy system. However, it is difficult to identify high-level specifications, such as business process steps, from legacy and undocumented systems. In this paper, we propose a method to recover an abstract business process by using system logs and the organizational information of the operators using an existing legacy system. Our method provides a hierarchical view based on a clustering technique to find abstract activities that consist of a series of operations. We also propose a method to extract the main operation in a cluster. We evaluated the effectiveness of our method through experiments on a real system.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Mibe, Ryota;Tanaka, Tadashi;Kobayashi, Takashi;Kobayashi, Shingo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018430718",
      "Primary study DOI": "10.1109/SANER.2017.7884604",
      "Title": "An empirical comparison of dependency issues in OSS packaging ecosystems",
      "Abstract": "Nearly every popular programming language comes with one or more open source software packaging ecosystem(s), containing a large collection of interdependent software packages developed in that programming language. Such packaging ecosystems are extremely useful for their respective software development community. We present an empirical analysis of how the dependency graphs of three large packaging ecosystems (npm, CRAN and RubyGems) evolve over time. We study how the existing package dependencies impact the resilience of the three ecosystems over time and to which extent these ecosystems suffer from issues related to package dependency updates. We analyse specific solutions that each ecosystem has put into place and argue that none of these solutions is perfect, motivating the need for better tools to deal with package dependency update problems.",
      "Keywords": "package dependency management | software distribution | software ecosystem | software evolution | software repository mining",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Decan, Alexandre;Mens, Tom;Claes, Maelick",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018406802",
      "Primary study DOI": "10.1109/SANER.2017.7884613",
      "Title": "Software-based energy profiling of Android apps: Simple, efficient and reliable?",
      "Abstract": "Modeling the power profile of mobile applications is a crucial activity to identify the causes behind energy leaks. To this aim, researchers have proposed hardware-based tools as well as model-based and software-based techniques to approximate the actual energy profile. However, all these solutions present their own advantages and disadvantages. Hardware-based tools are highly precise, but at the same time their use is bound to the acquisition of costly hardware components. Model-based tools require the calibration of parameters needed to correctly create a model on a specific hardware device. Software-based approaches do not need any hardware components, but they rely on battery measurements and, thus, they are hardware-assisted. These tools are cheaper and easier to use than hardware-based tools, but they are believed to be less precise. In this paper, we take a deeper look at the pros and cons of software-based solutions investigating to what extent their measurements depart from hardware-based solutions. To this aim, we propose a software-based tool named PETRA that we compare with the hardware-based MONSOON toolkit on 54 Android apps. The results show that PETRA performs similarly to MONSOON despite not using any sophisticated hardware components. In fact, in all the apps the mean relative error with respect to MONSOON is lower than 0.05. Moreover, for 95% of the analyzed methods the estimation error is within 5% of the actual values measured using the hardware-based toolkit.",
      "Keywords": "Energy Consumption | Estimation | Mobile Apps",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Di Nucci, Dario;Palomba, Fabio;Prota, Antonio;Panichella, Annibale;Zaidman, Andy;De Lucia, Andrea",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018430760",
      "Primary study DOI": "10.1109/SANER.2017.7884664",
      "Title": "Hindsight: Revealing the evolution of dependencies to developers",
      "Abstract": "Software systems are inherently complex and this is because of the interactions between their constituent entities. These affect refactoring efforts and therefore numerous tools that reveal dependencies between software artefacts have been proposed. However, existing tools only take into account the current version of a system, while the evolution of dependencies can hold clues that can help developers with their refactoring decisions.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Ganea, George",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018435832",
      "Primary study DOI": "10.1109/SANER.2017.7884643",
      "Title": "An exploratory study on library aging by monitoring client usage in a software ecosystem",
      "Abstract": "In recent times, use of third-party libraries has become prevalent practice in contemporary software development. Much like other code components, unmaintained libraries are a cause for concern, especially when it risks code degradation over time. Therefore, awareness of when a library should be updated is important. With the emergence of large libraries hosting repositories such as Maven Central, we can leverage the dynamics of these ecosystems to understand and estimate when a library is due for an update. In this paper, based on the concepts of software aging, we empirically explore library usage as a means to describe its age. The study covers about 1,500 libraries belonging to the Maven software ecosystem. Results show that library usage changes are not random, with 81.7% of the popular libraries fitting typical polynomial models. Further analysis show that ecosystem factors such as emerging rivals has an effect on aging characteristics. Our preliminary findings demonstrate that awareness of library aging and its characteristics is a promising step towards aiding client systems in the maintenance of their libraries.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Kula, Raula Gaikovina;German, Daniel M.;Ishio, Takashi;Ouni, Ali;Inoue, Katsuro",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018372386",
      "Primary study DOI": "10.1109/SANER.2017.7884669",
      "Title": "Multi-language re-documentation to support a COBOL to Java migration project",
      "Abstract": "Software migration projects need precise and up-to-date documentation of the software system to be migrated. Missing or outdated documentation hampers the migration process and compromises the overall quality of the resulting new software system. Moreover, if documentation is missing in the first place and no additional effort is undertaken to document the new software system, future maintenance and evolution tasks are burdened right from the beginning. Therefore, we apply an automatic re-documentation approach that uses a single tool chain to generate documentation for the software to be migrated and the transformed software system. By this, we not only support an ongoing COBOL to Java migration project at one of our industry partners but as well create the foundations to continuously generate up-to-date documentation for the new software system.",
      "Keywords": "documentation generator | re-documentation | reverse engineering | static program analysis",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Dorninger, Bernhard;Moser, Michael;Pichler, Josef",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018384049",
      "Primary study DOI": "10.1109/SANER.2017.7884681",
      "Title": "Does the release cycle of a library project influence when it is adopted by a client project?",
      "Abstract": "A key goal of this research is to understand the relationship between adoption of software library versions and its release cycle. In detail, we conducted an empirical study of the release cycle of 23 libraries and how they were adopted by 415 Apache Software Foundation (ASF) client projects. Our preliminary findings show that software projects are quicker to update earlier rapid-release libraries compared to library projects with a longer release cycle.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Fujibayashi, Daiki;Ihara, Akinori;Suwa, Hirohiko;Kula, Raula Gaikovina;Matsumoto, Kenichi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018409737",
      "Primary study DOI": "10.1109/SANER.2017.7884644",
      "Title": "Trends on empty exception handlers for Java open source libraries",
      "Abstract": "Exception-handling structures provide a means to recover from unexpected or undesired flows that occur during software execution, allowing the developer to put the program in a valid state. Still, the application of proper exception-handling strategies is at the bottom of priorities for a great number of developers. Studies have already discussed this subject pinpointing that, frequently, the implementation of exception-handling mechanisms is enforced by compilers. As a consequence, several anti-patterns about Exception-handling are already identified in literature. In this study, we have picked several releases from different Java programs and we investigated one of the most well-known anti-patterns: the empty catch handlers. We have analysed how the empty handlers evolved through several releases of a software product. We have observed some common approaches in terms of empty catches' evolution. For instance, often an empty catch is transformed into a empty catch with a comment. Moreover, for the majority of the programs, the percentage of empty handlers has decreased when comparing the first and last releases. Future work includes the automation of the analysis allowing the inclusion of data collected from other software artefacts: test suites and data from issue tracking systems.",
      "Keywords": "Exception-Handling | Open-Source | Software Evolution",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Nogueira, Ana Filipa;Ribeiro, Jose C.B.;Zenha-Rela, Mario A.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018444565",
      "Primary study DOI": "10.1109/SANER.2017.7884630",
      "Title": "An empirical study of code smells in JavaScript projects",
      "Abstract": "JavaScript is a powerful scripting programming language that has gained a lot of attention this past decade. Initially used exclusively for client-side web development, it has evolved to become one of the most popular programming languages, with developers now using it for both client-side and server-side application development. Similar to applications written in other programming languages, JavaScript applications contain code smells, which are poor design choices that can negatively impact the quality of an application. In this paper, we investigate code smells in JavaScript server-side applications with the aim to understand how they impact the fault-proneness of applications. We detect 12 types of code smells in 537 releases of five popular JavaScript applications (i.e., express, grunt, bower, less.js, and request) and perform survival analysis, comparing the time until a fault occurrence, in files containing code smells and files without code smells. Results show that (1) on average, files without code smells have hazard rates 65% lower than files with code smells. (2) Among the studied smells, 'Variable Re-assign' and 'Assignment In Conditional statements' code smells have the highest hazard rates. Additionally, we conduct a survey with 1,484 JavaScript developers, to understand the perception of developers towards our studied code smells. We found that developers consider 'Nested Callbacks', 'Variable Re-assign' and 'Long Parameter List' code smells to be serious design problems that hinder the maintainability and reliability of applications. This assessment is in line with the findings of our quantitative analysis. Overall, code smells affect negatively the quality of JavaScript applications and developers should consider tracking and removing them early on before the release of applications to the public.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Saboury, Amir;Musavi, Pooya;Khomh, Foutse;Antoniol, Giulio",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018443363",
      "Primary study DOI": "10.1109/SANER.2017.7884627",
      "Title": "How to gamify software engineering",
      "Abstract": "Software development, like any prolonged and intellectually demanding activity, can negatively affect the motivation of developers. This is especially true in specific areas of software engineering, such as requirements engineering, test-driven development, bug reporting and fixing, where the creative aspects of programming fall short. The developers' engagement might progressively degrade, potentially impacting their work's quality.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Dal Sasso, Tommaso;Mocci, Andrea;Lanza, Michele;Mastrodicasa, Ebrisa",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018392680",
      "Primary study DOI": "10.1109/SANER.2017.7884670",
      "Title": "Proactive reviews of textual requirements",
      "Abstract": "In large software development products the number of textual requirements can reach tens of thousands. When such a large number of requirements is delivered to software developers, there is a risk that vague or complex requirements remain undetected until late in the design process. In order to detect such requirements, companies conduct manual reviews of requirements. Manual reviews, however, take substantial amount of effort, and the efficiency is low. The goal of this paper is to present the application of a method for proactive requirements reviews. The method, that was developed and evaluated in a previous study, is now used in three companies. We show how the method evolved from an isolated scripted use to a fully integrated use in the three companies. The results showed that software engineers in the three companies use the method as a help in their job for continuous improvements of requirements.",
      "Keywords": "measure | metric | requirement review | technical risk",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Antinyan, Vard;Staron, Miroslaw",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018428699",
      "Primary study DOI": "10.1109/SANER.2017.7884622",
      "Title": "Automatically generating natural language descriptions for object-related statement sequences",
      "Abstract": "Current source code analyses driving software maintenance tools treat methods as either a single unit or a set of individual statements or words. They often leverage method names and any existing internal comments. However, internal comments are rare, and method names do not typically capture the method's multiple high-level algorithmic steps that are too small to be a single method, but require more than one statement to implement. Previous work demonstrated feasibility of identifying high level actions automatically for loops; however, many high level actions remain unaddressed and undocumented, particularly sequences of consecutive statements that are associated with each other primarily by object references. We call these object-related action units. In this paper, we present an approach to automatically generate natural language descriptions of object-related action units within methods. We leverage the available, large source of high-quality open source projects to learn the templates of object-related actions, identify the statement that can represent the main action, and generate natural language descriptions for these actions. Our evaluation study of a set of 100 object-related statement sequences showed promise of our approach to automatically identify the action and arguments and generate natural language descriptions.",
      "Keywords": "abstraction | documentation generation | mining code patterns",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Wang, Xiaoran;Pollock, Lori;Vijay-Shanker, K.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018435389",
      "Primary study DOI": "10.1109/SANER.2017.7884662",
      "Title": "InsDal: A safe and extensible instrumentation tool on Dalvik byte-code for Android applications",
      "Abstract": "Program instrumentation is a widely used technique in dynamic analysis and testing, which makes use of probe code inserted to the target program to monitor its behaviors, or log runtime information for off-line analysis. There are a number of automatic tools for instrumentation on the source or byte code of Java programs. However, few works address this issue on the register-based Dalvik byte-code of ever-increasing Android apps. This paper presents a lightweight tool, InsDal, for inserting instructions to specific points of the Dalvik byte-code according to the requirements of users. It carefully manages the registers to protect the behavior of original code from illegal manipulation, and optimizes the inserted code to avoid memory waste and unnecessary overhead. This tool is easy to use and has been applied to several scenarios (e.g. energy analysis, code coverage analysis).",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Liu, Jierui;Wu, Tianyong;Deng, Xi;Yan, Jun;Zhang, Jian",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018395855",
      "Primary study DOI": "10.1109/SANER.2017.7884621",
      "Title": "The dark side of event sourcing: Managing data conversion",
      "Abstract": "Evolving software systems includes data schema changes, and because of those schema changes data has to be converted. Converting data between two different schemas while continuing the operation of the system is a challenge when that system is expected to be available always. Data conversion in event sourced systems introduces new challenges, because of the relative novelty of the event sourcing architectural pattern, because of the lack of standardized tools for data conversion, and because of the large amount of data that is stored in typical event stores. This paper addresses the challenge of schema evolution and the resulting data conversion for event sourced systems. First of all a set of event store upgrade operations is proposed that can be used to convert data between two versions of a data schema. Second, a set of techniques and strategies that execute the data conversion while continuing the operation of the system is discussed. The final contribution is an event store upgrade framework that identifies which techniques and strategies can be combined to execute the event store upgrade operations while continuing operation of the system. Two utilizations of the framework are given, the first being as decision support in upfront design of an upgrade system for event sourced systems. The framework can also be utilized as the description of an automated upgrade system that can be used for continuous deployment. The event store upgrade framework is evaluated in interviews with three renowned experts in the domain and has been found to be a comprehensive overview that can be utilized in the design and implementation of an upgrade system. The automated upgrade system has been implemented partially and applied in experiments.",
      "Keywords": "CQRS | Data Conversion | Data Transformation | Deployment Strategy | Event Driven Architecture | Event Sourcing | Schema Evolution | Schema Versioning | Software Evolution",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Overeem, Michiel;Spoor, Marten;Jansen, Slinger",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018410501",
      "Primary study DOI": "10.1109/SANER.2017.7884672",
      "Title": "An empirical study of clone density evolution and developer cloning tendency",
      "Abstract": "Code clones commonly occur during software evolution. They impact the effort of software development and maintenance, and therefore they need to be monitored. We present a large-scale empirical study (237 open-source Java projects maintained by 500 individuals) that investigates how the number of clones changes throughout software evolution, as well as the tendency of individual developers to introduce clones. Our results will set a point-of-reference against which development teams can compare and, if needed, adjust.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Van Bladel, Brent;Murgia, Alessandro;Demeyer, Serge",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018371090",
      "Primary study DOI": "10.1109/SANER.2017.7884680",
      "Title": "Lost comments support program comprehension",
      "Abstract": "Source code comments are valuable to keep developers' explanations of code fragments. Proper comments help code readers understand the source code quickly and precisely. However, developers sometimes delete valuable comments since they do not know about the readers' knowledge and think the written comments are redundant. This paper describes a study of lost comments based on edit operation histories of source code. The experimental result shows that developers sometimes delete comments although their associated code fragments are not changed. Lost comments contain valuable descriptions that can be utilized as new data sources to support program comprehension.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Omori, Takayuki",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018454407",
      "Primary study DOI": "10.1109/SANER.2017.7884660",
      "Title": "Scat: Learning from a single execution of a binary",
      "Abstract": "Retrieving information from a binary code is required in several application domains such as system integration or security analysis. Providing tools to help engineers in this task is therefore an important need. We present in this paper scat, an open-source toolbox, relying on lightweight runtime instrumentation to infer source-level and behavioral information from a binary code, like function prototypes or data-flow relations. We explain the functioning principle of this toolbox, and we give some results obtained on real examples to show its effectiveness.",
      "Keywords": "binary code | data-flow | dynamic analysis | memory allocation | Reverse-engineering",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "De Goer, Franck;Ferreira, Christopher;Mounier, Laurent",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018456155",
      "Primary study DOI": "10.1109/SANER.2017.7884652",
      "Title": "Query Construction Patterns in PHP",
      "Abstract": "Most PHP applications use databases, with developers including both static queries, given directly in the code, and dynamic queries, which are based on a mixture of static text, computed values, and user input. In this paper, we focus specifically on how developers create queries that are then used with the original MySQL API library. Based on a collection of open-source PHP applications, our initial results show that many of these queries are created according to a small collection of query construction patterns. We believe that identifying these patterns provides a solid base for program analysis, comprehension, and transformation tools that need to reason about database queries, including tools to support renovating existing PHP code to support safer, more modern database access APIs.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Anderson, David;Hills, Mark",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018383572",
      "Primary study DOI": "10.1109/SANER.2017.7884637",
      "Title": "An empirical investigation into the cost-effectiveness of test effort allocation strategies for finding faults",
      "Abstract": "In recent years, it has been shown that fault prediction models could effectively guide test effort allocation in finding faults if they have a high enough fault prediction accuracy (Norm(Popt) > 0.78). However, it is often difficult to achieve such a high fault prediction accuracy in practice. As a result, fault-prediction-model-guided allocation (FPA) methods may be not applicable in real development environments. To attack this problem, in this paper, we propose a new type of test effort allocation strategy: reliability-growth-model-guided allocation (RGA) method. For a given project release V, RGA attempts to predict the optimal test effort allocation for V by learning the fault distribution information from the previous releases. Based on three open-source projects, we empirically investigate the cost-effectiveness of three test effort allocation strategies for finding faults: RGA, FPA, and structural-complexity-guided allocation (SCA) method. The experimental results show that RGA shows a promising performance in finding faults when compared with SCA and FPA.",
      "Keywords": "effort allocation | fault prediction model | reliability growth model | structural complexity | Test",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Feng, Yiyang;Ma, Wanwangying;Yang, Yibiao;Lu, Hongmin;Zhou, Yuming;Xu, Baowen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018428488",
      "Primary study DOI": "10.1109/SANER.2017.7884671",
      "Title": "Data access visualization for legacy application maintenance",
      "Abstract": "Software clustering techniques have been studied and applied to analyze and visualize the actual structure of legacy applications, which have used program information, e.g., dependencies, as input. However, business data also play an important role in a business system. Revealing which programs actually use data in the current system can give us a key insight when analyzing a long-lived complicated system. In this paper, we calculate indexes indicating how a data entity is used, making use of software clustering, which can be used to detect problematic or characteristic parts of the system. The developed technique can reveal the characteristics of a data entity; i.e., it is used like master data. We applied this technique to two business systems used for many years and found that our technique can help us understand the systems in terms of business data usage. Through case studies, we evaluated the validity of the indexes and showed that software visualization with the indexes can be used to investigate a system in an exploratory way.",
      "Keywords": "database | program comprehension | Software clustering | software visualization",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Yano, Keisuke;Matsuo, Akihiko",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018422607",
      "Primary study DOI": "10.1109/SANER.2017.7884638",
      "Title": "What information about code snippets is available in different software-related documents? An exploratory study",
      "Abstract": "A large corpora of software-related documents is available on the Web, and these documents offer the unique opportunity to learn from what developers are saying or asking about the code snippets that they are discussing. For example, the natural language in a bug report provides information about what is not functioning properly in a particular code snippet. Previous research has mined information about code snippets from bug reports, emails, and Q&A forums. This paper describes an exploratory study into the kinds of information that is embedded in different software-related documents. The goal of the study is to gain insight into the potential value and difficulty of mining the natural language text associated with the code snippets found in a variety of software-related documents, including blog posts, API documentation, code reviews, and public chats.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Chatterjee, Preetha;Nishi, Manziba Akanda;Damevski, Kostadin;Augustine, Vinay;Pollock, Lori;Kraft, Nicholas A.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018372756",
      "Primary study DOI": "10.1109/SANER.2017.7884629",
      "Title": "Stack Overflow: A code laundering platform?",
      "Abstract": "Developers use Question and Answer (Q&A) websites to exchange knowledge and expertise. Stack Overflow is a popular Q&A website where developers discuss coding problems and share code examples. Although all Stack Overflow posts are free to access, code examples on Stack Overflow are governed by the Creative Commons Attribute-ShareAlike 3.0 Unported license that developers should obey when reusing code from Stack Overflow or posting code to Stack Overflow. In this paper, we conduct a case study with 399 Android apps, to investigate whether developers respect license terms when reusing code from Stack Overflow posts (and the other way around). We found 232 code snippets in 62 Android apps from our dataset that were potentially reused from Stack Overflow, and 1,226 Stack Overflow posts containing code examples that are clones of code released in 68 Android apps, suggesting that developers may have copied the code of these apps to answer Stack Overflow questions. We investigated the licenses of these pieces of code and observed 1,279 cases of potential license violations (related to code posting to Stack overflow or code reuse from Stack overflow). This paper aims to raise the awareness of the software engineering community about potential unethical code reuse activities taking place on Q&A websites like Stack Overflow.",
      "Keywords": "Knowledge repository | Mining software repositories | Q&A website | Software licenses | Stack Overflow",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "An, Le;Mlouki, Ons;Khomh, Foutse;Antoniol, Giuliano",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018386636",
      "Primary study DOI": "10.1109/SANER.2017.7884612",
      "Title": "Analyzing reviews and code of mobile apps for better release planning",
      "Abstract": "The mobile applications industry experiences an unprecedented high growth, developers working in this context face a fierce competition in acquiring and retaining users. They have to quickly implement new features and fix bugs, or risks losing their users to the competition. To achieve this goal they must closely monitor and analyze the user feedback they receive in form of reviews. However, successful apps can receive up to several thousands of reviews per day, manually analysing each of them is a time consuming task. To help developers deal with the large amount of available data, we manually analyzed the text of 1566 user reviews and defined a high and low level taxonomy containing mobile specific categories (e.g. performance, resources, battery, memory, etc.) highly relevant for developers during the planning of maintenance and evolution activities. Then we built the User Request Referencer (URR) prototype, using Machine Learning and Information Retrieval techniques, to automatically classify reviews according to our taxonomy and recommend for a particular review what are the source code files that need to be modified to handle the issue described in the user review. We evaluated our approach through an empirical study involving the reviews and code of 39 mobile applications. Our results show a high precision and recall of URR in organising reviews according to the defined taxonomy.",
      "Keywords": "Code Localization | Mobile Applications | Text Classification | User Reviews",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Ciurumelea, Adelina;Schaufelbuhl, Andreas;Panichella, Sebastiano;Gall, Harald C.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018445107",
      "Primary study DOI": "10.1109/SANER.2017.7884614",
      "Title": "Investigating the energy impact of Android smells",
      "Abstract": "Android code smells are bad implementation practices within Android applications (or apps) that may lead to poor software quality. These code smells are known to degrade the performance of apps and to have an impact on energy consumption. However, few studies have assessed the positive impact on energy consumption when correcting code smells. In this paper, we therefore propose a tooled and reproducible approach, called HOT-PEPPER, to automatically correct code smells and evaluate their impact on energy consumption. Currently, HOT-PEPPER is able to automatically correct three types of Android-specific code smells: Internal Getter/Setter, Member Ignoring Method, and HashMap Usage. HOT-PEPPER derives four versions of the apps by correcting each detected smell independently, and all of them at once. HOT-PEPPER is able to report on the energy consumption of each app version with a single user scenario test. Our empirical study on five open-source Android apps shows that correcting the three aforementioned Android code smells effectively and significantly reduces the energy consumption of apps. In particular, we observed a global reduction in energy consumption by 4,83% in one app when the three code smells are corrected. We also take advantage of the flexibility of HOT-PEPPER to investigate the impact of three picture smells (bad picture format, compression, and bitmap format) in sample apps. We observed that the usage of optimised JPG pictures with the Android default bitmap format is the most energy efficient combination in Android apps. We believe that developers can benefit from our approach and results to guide their refactoring, and thus improve the energy consumption of their mobile apps.",
      "Keywords": "Android | code smells | energy consumption | picture",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Carette, Antonin;Younes, Mehdi Adel Ait;Hecht, Geoffrey;Moha, Naouel;Rouvoy, Romain",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018430951",
      "Primary study DOI": "10.1109/SANER.2017.7884656",
      "Title": "UAV: Warnings from multiple Automated Static Analysis Tools at a glance",
      "Abstract": "Automated Static Analysis Tools (ASATs) are an integral part of today's software quality assurance practices. At present, a plethora of ASATs exist, each with different strengths. However, there is little guidance for developers on which of these ASATs to choose and combine for a project. As a result, many projects still only employ one ASAT with practically no customization. With UAV, the Unified ASAT Visualizer, we created an intuitive visualization that enables developers, researchers, and tool creators to compare the complementary strengths and overlaps of different Java ASATs. UAV's enriched treemap and source code views provide its users with a seamless exploration of the warning distribution from a high-level overview down to the source code. We have evaluated our UAV prototype in a user study with ten second-year Computer Science (CS) students, a visualization expert and tested it on large Java repositories with several thousands of PMD, FindBugs, and Checkstyle warnings. Project Website: https://clintoncao.github.io/uav/",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Buckers, Tim;Cao, Clinton;Doesburg, Michiel;Gong, Boning;Wang, Sunwei;Beller, Moritz;Zaidman, Andy",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018455025",
      "Primary study DOI": "10.1109/SANER.2017.7884636",
      "Title": "Improving fault localization for Simulink models using search-based testing and prediction models",
      "Abstract": "One promising way to improve the accuracy of fault localization based on statistical debugging is to increase diversity among test cases in the underlying test suite. In many practical situations, adding test cases is not a cost-free option because test oracles are developed manually or running test cases is expensive. Hence, we require to have test suites that are both diverse and small to improve debugging. In this paper, we focus on improving fault localization of Simulink models by generating test cases. We identify three test objectives that aim to increase test suite diversity. We use these objectives in a search-based algorithm to generate diversified but small test suites. To further minimize test suite sizes, we develop a prediction model to stop test generation when adding test cases is unlikely to improve fault localization. We evaluate our approach using three industrial subjects. Our results show (1) the three selected test objectives are able to significantly improve the accuracy of fault localization for small test suite sizes, and (2) our prediction model is able to maintain almost the same fault localization accuracy while reducing the average number of newly generated test cases by more than half.",
      "Keywords": "Fault localization | search-based testing | Simulink models | supervised learning | test suite diversity",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Liu, Bing;Lucia, ;Nejati, Shiva;Briand, Lionel C.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018385623",
      "Primary study DOI": "10.1109/SANER.2017.7884657",
      "Title": "The Spartanizer: Massive automatic refactoring",
      "Abstract": "The Spartanizer is an eclipse plugin featuring over one hundred and fifty refactoring techniques, all aimed at reducing various size complexity of the code, without changing its design, i.e., inheritance relations, modular structure, etc. Typical use case of the Spartanizer is in an automatic mode: refactoring operations are successively selected and applied by the tool, until the code is reshaped in spartan style (a frugal coding style minimizing the use of characters, variables, tokens, etc.). The Spartanizer demonstrates the potential of automatic refactoring: tens of thousands of transformations are applied in matter of seconds, chains of dependent applications of transformations with tens of operations in them, significant impact on code size, and extent reaching almost every line of code, even of professional libraries.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Gil, Yossi;Orru, Matteo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018415398",
      "Primary study DOI": "10.1109/SANER.2017.7884653",
      "Title": "Supporting schema evolution in schema-less NoSQL data stores",
      "Abstract": "NoSQL data stores are becoming popular due to their schema-less nature. They offer a high level of flexibility, since they do not require to declare a global schema. Thus, the data model is maintained within the application source code. However, due to this flexibility, developers have to struggle with a growing data structure entropy and to manage legacy data. Moreover, support to schema evolution is lacking, which may lead to runtime errors or irretrievable data loss, if not properly handled. This paper presents an approach to support the evolution of a schema-less NoSQL data store by analyzing the application source code and its history. We motivate this approach on a subject system and explain how useful it is to understand the present database structure and facilitate future developments.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Meurice, Loup;Cleve, Anthony",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018449902",
      "Primary study DOI": "10.1109/SANER.2017.7884606",
      "Title": "Code of conduct in open source projects",
      "Abstract": "Open source projects rely on collaboration of members from all around the world using web technologies like GitHub and Gerrit. This mixture of people with a wide range of backgrounds including minorities like women, ethnic minorities, and people with disabilities may increase the risk of offensive and destroying behaviours in the community, potentially leading affected project members to leave towards a more welcoming and friendly environment. To counter these effects, open source projects increasingly are turning to codes of conduct, in an attempt to promote their expectations and standards of ethical behaviour. In this first of its kind empirical study of codes of conduct in open source software projects, we investigated the role, scope and influence of codes of conduct through a mixture of quantitative and qualitative analysis, supported by interviews with practitioners. We found that the top codes of conduct are adopted by hundreds to thousands of projects, while all of them share 5 common dimensions.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Tourani, Parastou;Adams, Bram;Serebrenik, Alexander",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018390328",
      "Primary study DOI": "10.1109/SANER.2017.7884615",
      "Title": "Automated generation of consistency-achieving model editors",
      "Abstract": "The advances of domain-specific modeling languages (DSMLs) and their editors created with modern language work-benches, have convinced domain experts of applying them as important and powerful means in their daily endeavors. Despite the fact that such editors are proficient in retaining syntactical model correctness, they present major shortages in mastering the preservation of consistency in models with elaborated language-specific constraints which require language engineers to manually implement sophisticated editing capabilities. Consequently, there is a demand for automating procedures to support editor users in both comprehending as well as resolving consistency violations. In this paper, we present an approach to automate the generation of advanced editing support for DSMLs offering automated validation, content-assist, and quick fix capabilities beyond those created by state-of-the-art language workbenches that help domain experts in retaining and achieving the consistency of models. For validation, we show potential error causes for violated constraints, instead of only the context in which constraints are violated. The state-space explosion problem is mitigated by our approach resolving constraint violations by increasing the neighborhood scope in a three-stage process, seeking constraint repair solutions presented as quick fixes to the editor user. We illustrate and provide an initial evaluation of our approach based on an Xtext-based DSML for modeling service clusters.",
      "Keywords": "Advanced Editor Support | Domain Specific Modeling Languages | Model Driven Engineering | Search-based Software Engineering",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Neubauer, Patrick;Bill, Robert;Mayerhofer, Tanja;Wimmer, Manuel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018457428",
      "Primary study DOI": "10.1109/SANER.2017.7884683",
      "Title": "Frame-based behavior preservation in refactoring",
      "Abstract": "Behavior preservation often bothers programmers in refactoring. This poster paper proposes a new approach that tames the behavior preservation by introducing the concept of a frame. A frame in refactoring defines stakeholder's individual concerns about the refactored code. Frame-based refactoring preserves the observable behavior within a particular frame. Therefore, it helps programmers distinguish the behavioral changes that they should observe from those that they can ignore.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Maruyama, Katsuhisa;Hayashi, Shinpei;Yoshida, Norihiro;Choi, Eunjong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018430122",
      "Primary study DOI": "10.1109/SANER.2017.7884647",
      "Title": "Statically identifying class dependencies in legacy JavaScript systems: First results",
      "Abstract": "Identifying dependencies between classes is an essential activity when maintaining and evolving software applications. It is also known that JavaScript developers often use classes to structure their projects. This happens even in legacy code, i.e., code implemented in JavaScript versions that do not provide syntactical support to classes. However, identifying associations and other dependencies between classes remain a challenge due to the lack of static type annotations. This paper investigates the use of type inference to identify relations between classes in legacy JavaScript code. To this purpose, we rely on Flow, a state-of-the-art type checker and inferencer tool for JavaScript. We perform a study using code with and without annotating the class import statements in two modular applications. The results show that precision is 100% in both systems, and that the annotated version improves the recall, ranging from 37% to 51% for dependencies in general and from 54% to 85% for associations. Therefore, we hypothesize that these tools should also depend on dynamic analysis to cover all possible dependencies in JavaScript code.",
      "Keywords": "Class dependencies | JavaScript | Reverse engineering",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Silva, Leonardo Humberto;Valente, Marco Tulio;Bergel, Alexandre",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018455943",
      "Primary study DOI": "10.1109/SANER.2017.7884610",
      "Title": "Analyzing closeness of code dependencies for improving IR-based Traceability Recovery",
      "Abstract": "Information Retrieval (IR) identifies trace links based on textual similarities among software artifacts. However, the vocabulary mismatch problem between different artifacts hinders the performance of IR-based approaches. A growing body of work addresses this issue by combining IR techniques with code dependency analysis such as method calls. However, so far the performance of combined approaches is highly dependent to the correctness of IR techniques and does not take full advantage of the code dependency analysis. In this paper, we combine IR techniques with closeness analysis to improve IR-based traceability recovery. Specifically, we quantify and utilize the 'closeness' for each call and data dependency between two classes to improve rankings of traceability candidate lists. An empirical evaluation based on three real-world systems suggests that our approach outperforms three baseline approaches.",
      "Keywords": "Call Dependencies | Closeness Analysis | Data Dependencies | Information Retrieval | Traceability Recovery",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Kuang, Hongyu;Nie, Jia;Hu, Hao;Rempel, Patrick;Lu, Jian;Egyed, Alexander;Mader, Patrick",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018436207",
      "Primary study DOI": "10.1109/SANER.2017.7884634",
      "Title": "Spreadsheet testing in practice",
      "Abstract": "Despite being popular end-user tools, spreadsheets suffer from the vulnerability of error-proneness. In software engineering, testing has been proposed as a way to address errors. It is important therefore to know whether spreadsheet users also test, or how do they test and to what extent, especially since most spreadsheet users do not have the training, or experience, of software engineering principles. Towards this end, we conduct a two-phase mixed methods study. First, a qualitative phase, in which we interview 12 spreadsheet users, and second, a quantitative phase, in which we conduct an online survey completed by 72 users. The outcome of the interviews, organized into four different categories, consists of an overview of test practices, perceptions of spreadsheet users about testing, a set of preventive measures for avoiding errors, and an overview of maintenance practices for ensuring correctness of spreadsheets over time. The survey adds to the findings by providing quantitative estimates indicating that ensuring correctness is an important concern, and a major fraction of users do test their spreadsheets. However, their techniques are largely manual and lack formalism. Tools and automated supports are rarely used.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Roy, Sohon;Hermans, Felienne;Van Deursen, Arie",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018447159",
      "Primary study DOI": "10.1109/SANER.2017.7884633",
      "Title": "Efficient method extraction for automatic elimination of type-3 clones",
      "Abstract": "A semantics-preserving transformation by Komondoor and Horwitz has been shown to be most effective in the elimination of type-3 clones. The two original algorithms for realizing this transformation, however, are not as efficient as the related (slice-based) transformations. We present an asymptotically-faster algorithm that implements the same transformation via bidirectional reachability on a program dependence graph, and we prove its equivalence to the original formulation.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Ettinger, Ran;Tyszberowicz, Shmuel;Menaia, Shay",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018422603",
      "Primary study DOI": "10.1109/SANER.2017.7884667",
      "Title": "Performance tuning for automotive Software Fault Prediction",
      "Abstract": "Fault prediction on high quality industry grade software often suffers from strong imbalanced class distribution due to a low bug rate. Previous work reports on low predictive performance, thus tuning parameters is required. As the State of the Art recommends sampling methods for imbalanced learning, we analyse effects when under- and oversampling the training data evaluated on seven different classification algorithms. Our results demonstrate settings to achieve higher performance values but the various classifiers are influenced in different ways. Furthermore, not all performance reports can be tuned at the same time.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Altinger, Harald;Herbold, Steffen;Schneemann, Friederike;Grabowski, Jens;Wotawa, Franz",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018378901",
      "Primary study DOI": "10.1109/SANER.2017.7884646",
      "Title": "On the evolution of exception usage in Java projects",
      "Abstract": "Programming languages use exceptions to handle abnormal situations during the execution of a program. While programming languages often provide a set of standard exceptions, developers can further create custom exceptions to capture relevant data about project- and domain-specific errors. We hypothesize that, given their usefulness, custom exceptions are used increasingly as software systems mature. To assess this claim, we empirically analyze the evolution of exceptions and exception-handling code within four, popular and long-lived Java systems. We observe that indeed the amount of error-handling code, together with the number of custom exceptions and their usage in catch handlers and throw statements increase as projects evolve. However, we find that the usage of standard exceptions increases more than the usage of custom exceptions in both catch handlers and throw statements. A preliminary manual analysis of throw statements reveals that developers encode the domain information into the standard Java exceptions as custom string error messages instead of relying on custom exception classes.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Osman, Haidar;Chis, Andrei;Schaerer, Jakob;Ghafari, Mohammad;Nierstrasz, Oscar",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018411983",
      "Primary study DOI": "10.1109/SANER.2017.7884645",
      "Title": "Analyzing the evolution of testing library usage in open source Java projects",
      "Abstract": "Software development projects frequently rely on testing-related libraries to test the functionality of the software product automatically and efficiently. Many such libraries are available for Java, and developers face a hard time deciding which libraries are most appropriate for their project, or when to migrate to a competing library. We empirically analysed the usage of eight testing-related libraries in 4,532 open source Java projects hosted on GitHub. We studied how frequently specific (pairs of) libraries are used over time. We also identified if and when library usages are replaced by competing ones during a project's lifetime. We found that some libraries are considerably more popular than their competitors, while some libraries become more popular over time. We observed that many projects tend to use multiple libraries together. We also observed permanent and temporary migrations between competing libraries. These findings may pave the way for recommendation tools that allow project developers to choose the most appropriate library for their needs, and to be informed of better alternatives.",
      "Keywords": "Empirical Analysis | Java | Library Migration | Library Usage | Open Source | Software Evolution | Testing",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Zerouali, Ahmed;Mens, Tom",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018429620",
      "Primary study DOI": "10.1109/SANER.2017.7884617",
      "Title": "Reducing redundancies in multi-revision code analysis",
      "Abstract": "Software engineering research often requires analyzing multiple revisions of several software projects, be it to make and test predictions or to observe and identify patterns in how software evolves. However, code analysis tools are almost exclusively designed for the analysis of one specific version of the code, and the time and resources requirements grow linearly with each additional revision to be analyzed. Thus, code studies often observe a relatively small number of revisions and projects. Furthermore, each programming ecosystem provides dedicated tools, hence researchers typically only analyze code of one language, even when researching topics that should generalize to other ecosystems. To alleviate these issues, frameworks and models have been developed to combine analysis tools or automate the analysis of multiple revisions, but little research has gone into actually removing redundancies in multi-revision, multi-language code analysis. We present a novel end-to-end approach that systematically avoids redundancies every step of the way: when reading sources from version control, during parsing, in the internal code representation, and during the actual analysis. We evaluate our open-source implementation, LISA, on the full history of 300 projects, written in 3 different programming languages, computing basic code metrics for over 1.1 million program revisions. When analyzing many revisions, LISA requires less than a second on average to compute basic code metrics for all files in a single revision, even for projects consisting of millions of lines of code.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Alexandru, Carol V.;Panichella, Sebastiano;Gall, Harald C.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018441752",
      "Primary study DOI": "10.1109/SANER.2017.7884639",
      "Title": "Harnessing Twitter to support serendipitous learning of developers",
      "Abstract": "Developers often rely on various online resources, such as blogs, to keep themselves up-to-date with the fast pace at which software technologies are evolving. Singer et al. found that developers tend to use channels such as Twitter to keep themselves updated and support learning, often in an undirected or serendipitous way, coming across things that they may not apply presently, but which should be helpful in supporting their developer activities in future. However, identifying relevant and useful articles among the millions of pieces of information shared on Twitter is a non-trivial task. In this work to support serendipitous discovery of relevant and informative resources to support developer learning, we propose an unsupervised and a supervised approach to find and rank URLs (which point to web resources) harvested from Twitter based on their informativeness and relevance to a domain of interest. We propose 14 features to characterize each URL by considering contents of webpage pointed by it, contents and popularity of tweets mentioning it, and the popularity of users who shared the URL on Twitter. The results of our experiments on tweets generated by a set of 85,171 users over a one-month period highlight that our proposed unsupervised and supervised approaches can achieve a reasonably high Normalized Discounted Cumulative Gain (NDCG) score of 0.719 and 0.832 respectively.",
      "Keywords": "Online Resources | Recommendation System | Social Media for Software Engineering",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Sharma, Abhishek;Tian, Yuan;Sulistya, Agus;Lo, David;Yamashita, Aiko Fallas",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018449691",
      "Primary study DOI": "10.1109/SANER.2017.7884626",
      "Title": "Enriching in-IDE process information with fine-grained source code history",
      "Abstract": "Current studies on software development either focus on the change history of source code from version-control systems or on an analysis of simplistic in-IDE events without context information. Each of these approaches contains valuable information that is unavailable in the other case. Our work proposes enriched event streams, a solution that combines the best of both worlds and provides a holistic view on the software development process. Enriched event streams not only capture developer activities in the IDE, but also specialized context information, such as source-code snapshots for change events. To enable the storage of such code snapshots in an analyzable format, we introduce a new intermediate representation called Simplified Syntax Trees (SSTs) and build CA□RET, a platform that offers reusable components to conveniently work with enriched event streams. We implement FEEDBAG++, an instrumentation for Visual Studio that collects enriched event streams with code snapshots in the form of SSTs. We share a dataset of enriched event streams captured from 58 users and representing 915 days of work. Additionally, to demonstrate usefulness, we present three research applications that have already made use of CA□RET and FEEDBAG++.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Proksch, Sebastian;Nadi, Sarah;Amann, Sven;Mezini, Mira",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018397079",
      "Primary study DOI": "10.1109/SANER.2017.7884611",
      "Title": "STRICT: Information retrieval based search term identification for concept location",
      "Abstract": "During maintenance, software developers deal with numerous change requests that are written in an unstructured fashion using natural language. Such natural language texts illustrate the change requirement involving various domain related concepts. Software developers need to find appropriate search terms from those concepts so that they could locate the possible locations in the source code using a search technique. Once such locations are identified, they can implement the requested changes there. Studies suggest that developers often perform poorly in coming up with good search terms for a change task. In this paper, we propose a novel technique-STRICT-that automatically identifies suitable search terms for a software change task by analyzing its task description using two information retrieval (IR) techniques-TextRank and POSRank. These IR techniques determine a term's importance based on not only its co-occurrences with other important terms but also its syntactic relationships with them. Experiments using 1,939 change requests from eight subject systems report that STRICT can identify better quality search terms than baseline terms from 52%-62% of the requests with 30%-57% Top-10 retrieval accuracy which are promising. Comparison with two state-of-the-art techniques not only validates our empirical findings and but also demonstrates the superiority of our technique.",
      "Keywords": "Concept location | information retrieval | POSRank | search term identification | TextRank",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Rahman, Mohammad Masudur;Roy, Chanchal K.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018418614",
      "Primary study DOI": "10.1109/SANER.2017.7884616",
      "Title": "Historical and impact analysis of API breaking changes: A large-scale study",
      "Abstract": "Change is a routine in software development. Like any system, libraries also evolve over time. As a consequence, clients are compelled to update and, thus, benefit from the available API improvements. However, some of these API changes may break contracts previously established, resulting in compilation errors and behavioral changes. In this paper, we study a set of questions regarding API breaking changes. Our goal is to measure the amount of breaking changes on real-world libraries and its impact on clients at a large-scale level. We assess (i) the frequency of breaking changes, (ii) the behavior of these changes over time, (iii) the impact on clients, and (iv) the characteristics of libraries with high frequency of breaking changes. Our large-scale analysis on 317 real-world Java libraries, 9K releases, and 260K client applications shows that (i) 14.78% of the API changes break compatibility with previous versions, (ii) the frequency of breaking changes increases over time, (iii) 2.54% of their clients are impacted, and (iv) systems with higher frequency of breaking changes are larger, more popular, and more active. Based on these results, we provide a set of lessons to better support library and client developers in their maintenance tasks.",
      "Keywords": "API Stability | API Usage | Backwards Compatibility | Software Evolution",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Xavier, Laerte;Brito, Aline;Hora, Andre;Valente, Marco Tulio",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018453756",
      "Primary study DOI": "10.1109/SANER.2017.7884640",
      "Title": "Why do we break APIs? First answers from developers",
      "Abstract": "Breaking contracts have a major impact on API clients. Despite this fact, recent studies show that libraries are often backward incompatible and that the rate of breaking changes increase over time. However, the specific reasons that motivate library developers to break contracts with their clients are still unclear. In this paper, we describe a qualitative study with library developers and real instance of API breaking changes. Our goal is to (i) elicit the reasons why developers introduce breaking changes; and (ii) check if they are aware about the risks of such changes. Our survey with the top contributors of popular Java libraries contributes to reveal a list of five reasons why developers break API contracts. Moreover, it also shows that most of developers are aware of these risks and, in some cases, adopt strategies to mitigate them. We conclude by prospecting a future study to strengthen our current findings. With this study, we expect to contribute on delineating tools to better assess the risks and impacts of API breaking changes.",
      "Keywords": "API Stability | Backwards Compatibility | Qualitative Study | Software Evolution",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Xavier, Laerte;Hora, Andre;Valente, Marco Tulio",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018415478",
      "Primary study DOI": "10.1109/SANER.2017.7884678",
      "Title": "Two improvements to detect duplicates in Stack Overflow",
      "Abstract": "Stack Overflow is one of the most popular question-and-answer sites for programmers. However, there are a great number of duplicate questions that are expected to be detected automatically in a short time. In this paper, we introduce two approaches to improve the detection accuracy: splitting body into different types of data and using word-embedding to treat word ambiguities that are not contained in the general corpuses. The evaluation shows that these approaches improve the accuracy compared with the traditional method.",
      "Keywords": "duplicate question | information retrieval | machine learning | Stack Overflow | word-embedding",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Mizobuchi, Yuji;Takayama, Kuniharu",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85016229266",
      "Primary study DOI": "10.1109/SANER.2017.7884665",
      "Title": "Replan: A Release Planning Tool",
      "Abstract": "Software release planning is the activity of deciding what is to be implemented, when and by who. It can be divided into two tasks: strategic planning (i.e., the what) and operational (i.e., the when and the who). Replan, the tool that we present in this demo, handles both tasks in an integrated and flexible way, allowing its users (typically software product managers and developer team leaders) to (re)plan the releases dynamically by assigning new features and/or modifying the available resources allocated at each release. A recorded video demo of Replan is available at https://youtu.be/PNK5EUTdqEg.",
      "Keywords": "Feature Scheduling | Resource Allocation | Software Release Planning",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Ameller, David;Farre, Carles;Franch, Xavier;Cassarino, Antonino;Valerio, Danilo;Elvassore, Valentin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018403590",
      "Primary study DOI": "10.1109/SANER.2017.7884625",
      "Title": "Computing counter-examples for privilege protection losses using security models",
      "Abstract": "Role-Based Access Control (RBAC) is commonly used in web applications to protect information and restrict operations. Code changes may affect the security of the application and need to be validated, in order to avoid security vulnerabilities, which is a major undertaking. A statement suffers from privilege protection loss in a release pair when it was definitely protected on all execution paths in the previous release and is now reachable by some execution paths with an inferior privilege protection. Because the code change and the resulting privilege protection loss may be distant (e.g. in different functions or files), developers may find it difficult to diagnose and correct the issue. We use Pattern Traversal Flow Analysis (PTFA) to statically analyze code-derived formal models. Our analysis automatically computes counter-examples of definite protection properties and privilege protection losses. We computed privilege protections and their changes for 147 release pairs of WordPress. We computed counter-examples for a total of 14,116 privilege protection losses we found spread in 31 release pairs.We present the distribution of counter-examples' lengths, as well as their spread across function and file boundaries. Our results show that counter-examples are typically short and localized. The median example spans 88 statements, crosses a single function boundary, and is contained in the same file. The 90th centile example measures 174 statements and spans 3 function boundaries over 3 files. We believe that the privilege protection counter-examples' characteristics would be helpful to focus developers' attention for security reviews. These counter-examples are also a first step toward explanations.",
      "Keywords": "Access Control | Evolution | Model Checking | Software Maintenance | Static Analysis",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Laverdiere, Marc Andre;Merlo, Ettore",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018417236",
      "Primary study DOI": "10.1109/SANER.2017.7884650",
      "Title": "Under-optimized smart contracts devour your money",
      "Abstract": "Smart contracts are full-fledged programs that run on blockchains (e.g., Ethereum, one of the most popular blockchains). In Ethereum, gas (in Ether, a cryptographic currency like Bitcoin) is the execution fee compensating the computing resources of miners for running smart contracts. However, we find that under-optimized smart contracts cost more gas than necessary, and therefore the creators or users will be overcharged. In this work, we conduct the first investigation on Solidity, the recommended compiler, and reveal that it fails to optimize gas-costly programming patterns. In particular, we identify 7 gas-costly patterns and group them to 2 categories. Then, we propose and develop GASPER, a new tool for automatically locating gas-costly patterns by analyzing smart contracts' bytecodes. The preliminary results on discovering 3 representative patterns from 4,240 real smart contracts show that 93.5%, 90.1% and 80% contracts suffer from these 3 patterns, respectively.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Chen, Ting;Li, Xiaoqi;Luo, Xiapu;Zhang, Xiaosong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018431396",
      "Primary study DOI": "10.1109/SANER.2017.7884628",
      "Title": "Scalable tag recommendation for software information sites",
      "Abstract": "Software developers can search, share and learn development experience, solutions, bug fixes and open source projects in software information sites such as StackOverflow and Freecode. Many software information sites rely on tags to classify their contents, i.e. software objects, in order to improve the performance and accuracy of various operations on the sites. The quality of tags thus has a significant impact on the usefulness of these sites. High quality tags are expected to be concise and can describe the most important features of the software objects.",
      "Keywords": "Multi-Classification | Software Information Site | Software Object | Tag Recommendation",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Zhou, Pingyi;Liu, Jin;Yang, Zijiang;Zhou, Guangyou",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018375328",
      "Primary study DOI": "10.1109/SANER.2017.7884677",
      "Title": "Conventionality analysis of array objects in JavaScript",
      "Abstract": "In JavaScript, arrays are objects with a property named length that is automatically updated. An index is a property that is a string representation of an integer between 0 and 232 - 2. A conventional array is an array that does not have any properties other than indices and length. Do JavaScript programmers use arrays conventionally just like in other object oriented languages? Do they use arrays like regular objects? This paper proposes a static conventionality analysis for JavaScript to help us to find answers to these questions. Conventionality analysis can be useful for program understanding and optimization.",
      "Keywords": "abstract interpretation | abstract string domain | JavaScript | static analysis",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Younang, Astrid;Lu, Lunjin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018430664",
      "Primary study DOI": "10.1109/SANER.2017.7884607",
      "Title": "Socio-technical evolution of the Ruby ecosystem in GitHub",
      "Abstract": "The evolution dynamics of a software ecosystem depend on the activity of the developer community contributing to projects within it. Both social and technical changes affect an ecosystem's evolution and the research community has been investigating the impact of these modifications over the last few years. Existing studies mainly focus on temporary modifications, often ignoring the effect of permanent changes on the software ecosystem. We present an empirical study of the magnitude and effect of permanent modifications in both the social and technical parts of a software ecosystem. More precisely, we measure permanent changes with regard to the ecosystem's projects, contributors and source code files and present our findings concerning the effect of these modifications. We study the Ruby ecosystem in GitHub over a nine-year period by carrying out a socio-technical analysis of the co-evolution of a large number of base projects and their forks. This analysis involves both the source code developed for these projects as well as the developers having contributed to them. We discuss our findings with respect to the ecosystem evolution according to three different viewpoints: (1) the base projects, (2) the forks and (3) the entire ecosystem containing both the base projects and forks. Our findings show an increased growth in both the technical and social aspects of the Ruby ecosystem until early 2014, followed by an increased contributor and project abandonment rate. We show the effect of permanent modifications in the ecosystem evolution and provide preliminary evidence of contributors migrating to other ecosystems when leaving the Ruby ecosystem.",
      "Keywords": "GitHub | Ruby | Socio-Technical Analysis | Software Ecosystem | Software Evolution",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Constantinou, Eleni;Mens, Tom",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018380936",
      "Primary study DOI": "10.1109/SANER.2017.7884651",
      "Title": "Pluggable Controllers and Nano-Patterns",
      "Abstract": "This paper raises the idea of giving end users the ability to modify and extend the control flow constructs (if, while, etc.) of the underlying programming language, just as they can modify and extend the library standard implementation of function printf and class String. Pluggable Controllers are means for modular design of control constructors, e.g., if, while, do, switch, and operators such as short circuit conjunction (&&) and the '?.' operator of the Swift programming language. We propose a modular, pluggable controllers based, design of a language. In this design there are control constructors which are core, augmented by a standard library of control constructors, which just like all standard libraries, is extensible and replaceable. The control constructors standard library can then follow a course of evolution that is less coupled with that of the main language, where a library release does not mandate new language release. At the same time, the library could be extended by individuals, corporate and communities to implement more or less idiosyncratic Nano-Patterns. We demonstrate the imposition of pluggable control constructors on Java by employing Lola - a Turing-complete and programming language independent code preprocessor.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Gil, Yossi;Marcovitch, Ori;Orru, Matteo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018437178",
      "Primary study DOI": "10.1109/SANER.2017.7884605",
      "Title": "Detecting similar repositories on GitHub",
      "Abstract": "GitHub contains millions of repositories among which many are similar with one another (i.e., having similar source codes or implementing similar functionalities). Finding similar repositories on GitHub can be helpful for software engineers as it can help them reuse source code, build prototypes, identify alternative implementations, explore related projects, find projects to contribute to, and discover code theft and plagiarism. Previous studies have proposed techniques to detect similar applications by analyzing API usage patterns and software tags. However, these prior studies either only make use of a limited source of information or use information not available for projects on GitHub. In this paper, we propose a novel approach that can effectively detect similar repositories on GitHub. Our approach is designed based on three heuristics leveraging two data sources (i.e., GitHub stars and readme files) which are not considered in previous works. The three heuristics are: repositories whose readme files contain similar contents are likely to be similar with one another, repositories starred by users of similar interests are likely to be similar, and repositories starred together within a short period of time by the same user are likely to be similar. Based on these three heuristics, we compute three relevance scores (i.e., readme-based relevance, stargazer-based relevance, and time-based relevance) to assess the similarity between two repositories. By integrating the three relevance scores, we build a recommendation system called RepoPal to detect similar repositories. We compare RepoPal to a prior state-of-the-art approach CLAN using one thousand Java repositories on GitHub. Our empirical evaluation demonstrates that RepoPal achieves a higher success rate, precision and confidence over CLAN.",
      "Keywords": "GitHub | Information Retrieval | Recommendation System | Similar Repositories",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Zhang, Yun;Lo, David;Kochhar, Pavneet Singh;Xia, Xin;Li, Quanlai;Sun, Jianling",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018413794",
      "Primary study DOI": "10.1109/SANER.2017.7884659",
      "Title": "Lightweight detection of Android-specific code smells: The aDoctor project",
      "Abstract": "Code smells are symptoms of poor design solutions applied by programmers during the development of software systems. While the research community devoted a lot of effort to studying and devising approaches for detecting the traditional code smells defined by Fowler, little knowledge and support is available for an emerging category of Mobile app code smells. Recently, Reimann et al. proposed a new catalogue of Android-specific code smells that may be a threat for the maintainability and the efficiency of Android applications. However, current tools working in the context of Mobile apps provide limited support and, more importantly, are not available for developers interested in monitoring the quality of their apps. To overcome these limitations, we propose a fully automated tool, coined ADOCTOR, able to identify 15 Android-specific code smells from the catalogue by Reimann et al. An empirical study conducted on the source code of 18 Android applications reveals that the proposed tool reaches, on average, 98% of precision and 98% of recall. We made ADOCTOR publicly available.",
      "Keywords": "Android-specific Code Smells | Detection Tool | Empirical Study",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Palomba, Fabio;Di Nucci, Dario;Panichella, Annibale;Zaidman, Andy;De Lucia, Andrea",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018402234",
      "Primary study DOI": "10.1109/SANER.2017.7884641",
      "Title": "An arc-based approach for visualization of code smells",
      "Abstract": "Code smells are indicators of design flaws that may have negative effects on software comprehensibility and changeability. In recent years several detection tools have been developed that are supposed to help in revealing code smells in large size software systems. However, usually a subset of the detected code smells are suitable for refactorings only. Previous studies on software clones have shown that visualization of findings may assist developers in identifying relevant refactoring opportunities by highlighting peculiarities and, thus, is useful to enhance a software's maintainability. Nevertheless, techniques to visualize code smells in general are rare, though, being an interesting field of research to bridge the gap between code smell detection and code smell refactoring. This paper presents a visualization approach that is supposed to help in assessing the dispersion and extent of arbitrary code smells by combining different existing techniques. The core of our approach consists of several Treemaps that are arranged on a circle in order to obtain a better integration of additional visualizations. Furthermore, the presented technique provides various interaction mechanisms that allow users to adjust the visualization to target elements of interest.",
      "Keywords": "",
      "Publication venue": "SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2017-03-21",
      "Publication type": "Conference Paper",
      "Authors": "Steinbeck, Marcel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85032478596",
      "Primary study DOI": "10.5267/j.dsl.2017.8.003",
      "Title": "Prioritizing the refactoring need for critical component using combined approach",
      "Abstract": "One of the most promising strategies that will smooth out the maintainability issues of the software is refactoring. Due to lack of proper design approach, the code often inherits some bad smells which may lead to improper functioning of the code, especially when it is subject to change and requires some maintenance. A lot of studies have been performed to optimize the refactoring strategy which is also a very expensive process. In this paper, a component based system is considered, and a Fuzzy Multi Criteria Decision Making (FMCDM) model is proposed by combining subjective and objective weights to rank the components as per their urgency of refactoring. Jdeodorant tool is used to detect the code smells from the individual components of a software system. The objective method uses the Entropy approach to rank the component having the code smell. The subjective method uses the Fuzzy TOPSIS approach based on decision makers’ judgement, to identify the critically and dependency of these code smells on the overall software. The suggested approach is implemented on component-based software having 15 components. The constitute components are ranked based on refactoring requirements.",
      "Keywords": "Code smell | Entropy | FMCDM analysis | Fuzzy TOPSIS | Refactoring",
      "Publication venue": "Decision Science Letters",
      "Publication date": "2018-07-01",
      "Publication type": "Article",
      "Authors": "Sehgal, Rajni;Mehrotra, Deepti;Bala, Manju",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85041469974",
      "Primary study DOI": "10.1016/j.infsof.2017.11.010",
      "Title": "An empirical study to improve software security through the application of code refactoring",
      "Abstract": "Context: Code bad smells indicate design flaws that can degrade the quality of software and can potentially lead to the introduction of faults. They can be eradicated by applying refactoring techniques. Code bad smells that impact the security perspective of software should be detected and removed from their code base. However, the existing literature is insufficient to support this claim and there are few studies that empirically investigate bad smells and refactoring opportunities from a security perspective. Objective: In this paper, we investigate how refactoring can improve the security of an application by removing code bad smell. Method: We analyzed three different code bad smells in five software systems. First, the identified code bad smells are filtered against security attributes. Next, the object-oriented design and security metrics are calculated for the five investigated systems. Later, refactoring is applied to remove security-related code bad smells. The correctness of detection and refactoring of investigated code smells are then validated. Finally, both traditional object-oriented and security metrics are again calculated after removing bad smells to assess its impact on the design and security attributes of systems. Results: We found ‘feature envy’ to be the most abundant security bad smell in investigated projects. The ‘move method’ and ‘move field’ are commonly applied refactoring techniques because of the abundance of feature envy. Conclusion: The results of security metrics indicate that refactoring helps improve the security of an application without compromising the overall quality of software systems.",
      "Keywords": "Code bad smells | Empirical study | Refactoring | Secured software",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2018-04-01",
      "Publication type": "Article",
      "Authors": "Mumtaz, Haris;Alshayeb, Mohammad;Mahmood, Sajjad;Niazi, Mahmood",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85039428979",
      "Primary study DOI": "10.1016/j.jss.2017.12.013",
      "Title": "Smells in software test code: A survey of knowledge in industry and academia",
      "Abstract": "As a type of anti-pattern, test smells are defined as poorly designed tests and their presence may negatively affect the quality of test suites and production code. Test smells are the subject of active discussions among practitioners and researchers, and various guidelines to handle smells are constantly offered for smell prevention, smell detection, and smell correction. Since there is a vast grey literature as well as a large body of research studies in this domain, it is not practical for practitioners and researchers to locate and synthesize such a large literature. Motivated by the above need and to find out what we, as the community, know about smells in test code, we conducted a ‘multivocal’ literature mapping (classification) on both the scientific literature and also practitioners’ grey literature. By surveying all the sources on test smells in both industry (120 sources) and academia (46 sources), 166 sources in total, our review presents the largest catalogue of test smells, along with the summary of guidelines/techniques and the tools to deal with those smells. This article aims to benefit the readers (both practitioners and researchers) by serving as an “index” to the vast body of knowledge in this important area, and by helping them develop high-quality test scripts, and minimize occurrences of test smells and their negative consequences in large test automation projects.",
      "Keywords": "Automated testing | Multivocal literature mapping | Software testing | Survey | Systematic mapping | Test anti-patterns | Test automation | Test scripts | Test smells",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2018-04-01",
      "Publication type": "Article",
      "Authors": "Garousi, Vahid;Küçük, Barış",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85039063985",
      "Primary study DOI": "10.1016/j.infsof.2017.11.016",
      "Title": "Exploiting load testing and profiling for Performance Antipattern Detection",
      "Abstract": "Context: The performance assessment of complex software systems is not a trivial task since it depends on the design, code, and execution environment. All these factors may affect the system quality and generate negative consequences, such as delays and system failures. The identification of bad practices leading to performance flaws is of key relevance to avoid expensive rework in redesign, reimplementation, and redeployment. Objective: The goal of this manuscript is to provide a systematic process, based on load testing and profiling data, to identify performance issues with runtime data. These performance issues represent an important source of knowledge as they are used to trigger the software refactoring process. Software characteristics and performance measurements are matched with well-known performance antipatterns to document common performance issues and their solutions. Method: We execute load testing based on the characteristics of collected operational profile, thus to produce representative workloads. Performance data from the system under test is collected using a profiler tool to create profiler snapshots and get performance hotspot reports. From such data, performance issues are identified and matched with the specification of antipatterns. Software refactorings are then applied to solve these performance antipatterns. Results: The approach has been applied to a real-world industrial case study and to a representative laboratory study. Experimental results demonstrate the effectiveness of our tool-supported approach that is able to automatically detect two performance antipatterns by exploiting the knowledge of domain experts. In addition, the software refactoring process achieves a significant performance gain at the operational stage in both case studies. Conclusion: Performance antipatterns can be used to effectively support the identification of performance issues from load testing and profiling data. The detection process triggers an antipattern-based software refactoring that in our two case studies results in a substantial performance improvement.",
      "Keywords": "Empirical data | Load testing and profiling | Software performance antipatterns | Software performance engineering",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2018-03-01",
      "Publication type": "Article",
      "Authors": "Trubiani, Catia;Bran, Alexander;van Hoorn, André;Avritzer, Alberto;Knoche, Holger",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029760730",
      "Primary study DOI": "10.1016/j.infsof.2017.09.006",
      "Title": "Performance-driven software model refactoring",
      "Abstract": "Context: Software refactoring is a common practice aimed at addressing requirements or fixing bugs during the software development. While refactoring related to functional requirements has been widely studied in the last few years, non-functional-driven refactoring is still critical, mostly because non-functional characteristics of software are hard to assess and appropriate refactoring actions can be difficult to identify. In the context of performance, which is the focus of this paper, antipatterns represent effective instruments to tackle this issue, because they document common mistakes leading to performance problems as well as their solutions. Objective: In order to effectively reuse the knowledge beyond performance antipatterns, automation is required to detect and remove them. In this paper we introduce a framework that enables, in an unique tool context, the refactoring of software models driven by performance antipattern detection and removal. Method: We have implemented, within the EPSILON platform, detection rules and refactoring actions on UML models for a set of well-known performance antipatterns. By exploiting the EPSILON languages to check properties and apply refactoring on models, we enable three types of refactoring sessions. Results: We experiment our framework on a Botanical Garden Management System to show, on one side, that antipatterns can effectively drive software refactoring towards models that satisfy performance requirements and, on the other side, that the automation introduced by EPSILON-based sessions enables to inspect multiple paths and to propose a variety of solutions. Conclusion: This work demonstrates that automation in performance-driven software model refactoring can be beneficial, and that performance antipatterns can be powerful instruments in the hands of software engineers for detecting (and solving) performance problems usually hidden to traditional bottleneck analysis. This work also opens the road to the integration of well-known techniques for software refactoring driven by functional requirements with novel techniques addressing non-functional requirements like performance.",
      "Keywords": "Model-driven engineering | Performance antipatterns | Performance engineering | Software refactoring | UML",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2018-03-01",
      "Publication type": "Article",
      "Authors": "Arcelli, Davide;Cortellessa, Vittorio;Di Pompeo, Daniele",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84930531514",
      "Primary study DOI": "10.1007/s10664-015-9378-4",
      "Title": "Comparing and experimenting machine learning techniques for code smell detection",
      "Abstract": "Several code smell detection tools have been developed providing different results, because smells can be subjectively interpreted, and hence detected, in different ways. In this paper, we perform the largest experiment of applying machine learning algorithms to code smells to the best of our knowledge. We experiment 16 different machine-learning algorithms on four code smells (Data Class, Large Class, Feature Envy, Long Method) and 74 software systems, with 1986 manually validated code smell samples. We found that all algorithms achieved high performances in the cross-validation data set, yet the highest performances were obtained by J48 and Random Forest, while the worst performance were achieved by support vector machines. However, the lower prevalence of code smells, i.e., imbalanced data, in the entire data set caused varying performances that need to be addressed in the future studies. We conclude that the application of machine learning to the detection of these code smells can provide high accuracy (>96 %), and only a hundred training examples are needed to reach at least 95 % accuracy.",
      "Keywords": "Benchmark for code smell detection | Code smells detection | Machine learning techniques",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2016-06-01",
      "Publication type": "Article",
      "Authors": "Arcelli Fontana, Francesca;Mäntylä, Mika V.;Zanoni, Marco;Marino, Alessandro",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85031772226",
      "Primary study DOI": "10.1016/j.csi.2017.09.010",
      "Title": "Spotting and Removing WSDL Anti-pattern Root Causes in Code-first Web Services Using NLP Techniques: A Thorough Validation of Impact on Service Discoverability",
      "Abstract": "To expose software as Web-accesible services, Web Service technologies demand developers to implement certain sofware artifacts, such as the service description using WSDL. Therefore, developers usually use automatic tools to perform this task, which take as input a code written in a programming language –e.g. Java– and generate the necessary artifacts for invoking it remotely. However, as a result of tool flaws and some bad coding practices, the description of the resulting Web Services might contain anti-patterns that difficult their discovery and use. In earlier work we proposed a tool-supported, code-first approach named Gapidt to develop Web Services in Java while early reducing the presence of anti-patterns in their descriptions through code refactorings. Bad coding practices, which potentially reduce the textual and structural information of generated WSDL documents, are automatically detected and informed to the developer by means of the GAnalyzer module so he/she can fix the service code. Moreover, developer provided information, such as service parameter names and operation comments, as well as re-utilization of data-type definitions, are exploited by the GMapper module upon generating WSDL documents. This paper focuses on a comprehensive experimental evaluation of the approach oriented at prospective users to assess expected discoverability gains and usage considerations taking into account various relevant service publishing technologies. In addition, we introduce a detailed comparison of Gapidt with a similar approach from the literature. The results show that Gapidt outperforms its competitor in terms of discoverability while improves Web Service description quality (better documentation and data-models). The Web Service discoverability levels of Gapidt outperforms that of third-party tools, either when using the GAnalyzer plus the GMapper, or only the GMapper.",
      "Keywords": "Automatic detection | Code-first | Service discovery | Web services | WSDL anti-patterns",
      "Publication venue": "Computer Standards and Interfaces",
      "Publication date": "2018-02-01",
      "Publication type": "Article",
      "Authors": "Hirsch, Matías;Rodriguez, Ana;Rodriguez, Juan Manuel;Mateos, Cristian;Zunino, Alejandro",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85041003076",
      "Primary study DOI": "10.1109/TSE.2018.2797899",
      "Title": "Design Rule Spaces: A New Model for Representing and Analyzing Software Architecture",
      "Abstract": "In this paper, we propose an architecture model called Design Rule Space (DRSpace). We model the architecture of a software system as multiple overlapping DRSpaces, reflecting the fact that any complex software system must contain multiple aspects, features, patterns, etc. We show that this model provides new ways to analyze software quality. In particular, we introduce an Architecture Root detection algorithm that captures DRSpaces containing large numbers of a project's bug-prone files, which are called Architecture Roots (ArchRoots). After investigating ArchRoots calculated from 15 open source projects, the following observations become clear: from 35 to 91 percent of a project's most bug-prone files can be captured by just 5 ArchRoots, meaning that bug-prone files are likely to be architecturally connected. Furthermore, these ArchRoots tend to live in the system for significant periods of time, serving as the major source of bug-proneness and high maintainability costs. Moreover, each ArchRoot reveals multiple architectural flaws that propagate bugs among files and this will incur high maintenance costs over time. The implication of our study is that the quality, in terms of bug-proneness, of a large, complex software project cannot be fundamentally improved without first fixing its architectural flaws.",
      "Keywords": "bug localization | code smells | defect prediction | reverse-engineering | Software architecture | technical debt",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-07-01",
      "Publication type": "Article",
      "Authors": "Cai, Yuanfang;Xiao, Lu;Kazman, Rick;Mo, Ran;Feng, Qiong",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85039433296",
      "Primary study DOI": "10.1007/978-981-10-5547-8_56",
      "Title": "Software fault prediction using machine-learning techniques",
      "Abstract": "Machine-learning techniques are used to find the defect, fault, ambiguity, and bad smell to accomplish quality, maintainability, and reusability in software. Software fault prediction techniques are used to predict software faults by using statistical techniques. However, Machine-learning techniques are also valuable in detecting software fault. This paper presents an overview of software fault prediction using machine-learning techniques to predict the occurrence of faults. This paper also presents the conventional techniques. It aims at describing the problem of fault proneness.",
      "Keywords": "Machine learning techniques | Regression | Software fault prediction | Software faults | Software quality",
      "Publication venue": "Smart Innovation, Systems and Technologies",
      "Publication date": "2018-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Sharma, Deepak;Chandra, Pravin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029407321",
      "Primary study DOI": "10.1016/j.infsof.2017.09.002",
      "Title": "Are you smelling it? Investigating how similar developers detect code smells",
      "Abstract": "Context A code smell indicates a poor implementation choice that often worsens software quality. Thus, code smell detection is an elementary technique to identify refactoring opportunities in software systems. Unfortunately, there is limited knowledge on how similar two or more developers detect smells in code. In particular, few studies have investigated if developers agree or disagree when recognizing a smell and which factors can influence on such (dis)agreement. Objective We perform a broader study to investigate how similar the developers detect code smells. We also analyze whether certain factors related to the developers’ profiles concerning background and experience may influence such (dis)agreement. Moreover, we analyze if the heuristics adopted by developers on detecting code smells may influence on their (dis)agreement. Method We conducted an empirical study with 75 developers who evaluated instances of 15 different code smell types. For each smell type, we analyzed the agreement among the developers and we assessed the influence of 6 different factors on the developers’ evaluations. Altogether more than 2700 evaluations were collected, resulting in substantial quantitative and qualitative analyses. Results The results indicate that the developers presented a low agreement on detecting all 15 smell types analyzed in our study. The results also suggest that factors related to background and experience did not have a consistent influence on the agreement among the developers. On the other hand, the results show that the agreement was consistently influenced by specific heuristics employed by developers. Conclusions Our findings reveal that the developers detect code smells in significantly different ways. As a consequence, these findings introduce some questions concerning the results of previous studies that did not consider the different perceptions of developers on detecting code smells. Moreover, our findings shed light towards improving state-of-the-art techniques for accurate, customized detection of code smells.",
      "Keywords": "Code smell | Detection | Empirical study | Software maintenance",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2018-01-01",
      "Publication type": "Article",
      "Authors": "Hozano, Mário;Garcia, Alessandro;Fonseca, Baldoino;Costa, Evandro",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85032694896",
      "Primary study DOI": "10.1007/978-3-319-69341-5_23",
      "Title": "Towards detecting MVC architectural smells",
      "Abstract": "The term “bad smell” denotes a symptom of poor design or implementation that negatively impacts a software system’s properties. The research community has been actively identifying the characteristics of bad smells bad smells as well as developing approaches for detecting and fixing them. However, most of these efforts focus on smells that occur at code level: little consideration is given to smells that occur at higher levels of abstraction. This paper presents an initial effort to fill this gap by contributing to (i) the characterization of bad smells that are relevant to the Model-View-Controller architectural style and (ii) assessing the feasibility of their automatic detection using text analysis techniques in five systems, implemented with the Yii Framework. The obtained results show that the defined smells exist in practice and give some insight into which of them tend to occur more frequently. Regarding the automatic detection method, results show that it exhibits good performance and accuracy.",
      "Keywords": "Bad smells | MVC | Software architecture | Static analysis | Text analysis | Yii",
      "Publication venue": "Advances in Intelligent Systems and Computing",
      "Publication date": "2018-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Velasco-Elizondo, Perla;Castañeda-Calvillo, Lucero;García-Fernandez, Alejandro;Vazquez-Reyes, Sodel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85043277879",
      "Primary study DOI": "10.1109/CHILECON.2017.8229526",
      "Title": "Do developers care about code smells? A replicated study from Chile",
      "Abstract": "Code smells are a metaphor to describe symptoms in the code that may provoke maintainability problems. Code smell detection and remove have a closer relationship to refactoring which has been a well-known and disseminated practice for improving software maintenance and reuse. In 2013 a study for knowing about the position and knowledge of developers was conducted from Norway. This study covered developers from more than 25 different countries and got interesting results about code smells. In this paper we have replicated this study by interviewing exclusively Chilean participants. The results indicate few differences on poor comprehension about object orientation and a better position of the current Chilean concerns on code smells and refactoring.",
      "Keywords": "Code smells | Maintainability | Refactoring | Replicated study",
      "Publication venue": "2017 CHILEAN Conference on Electrical, Electronics Engineering, Information and Communication Technologies, CHILECON 2017 - Proceedings",
      "Publication date": "2017-12-19",
      "Publication type": "Conference Paper",
      "Authors": "Ancán, Oscar;Cares, Carlos",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85042390923",
      "Primary study DOI": "10.1109/ESEM.2017.12",
      "Title": "An Empirical Examination of the Relationship between Code Smells and Merge Conflicts",
      "Abstract": "Background: Merge conflicts are a common occurrence in software development. Researchers have shown the negative impact of conflicts on the resulting code quality and the development workflow. Thus far, no one has investigated the effect of bad design (code smells) on merge conflicts. Aims: We posit that entities that exhibit certain types of code smells are more likely to be involved in a merge conflict. We also postulate that code elements that are both 'smelly' and involved in a merge conflict are associated with other undesirable effects (more likely to be buggy). Method: We mined 143 repositories from GitHub and recreated 6,979 merge conflicts to obtain metrics about code changes and conflicts. We categorized conflicts into semantic or non-semantic, based on whether changes affected the Abstract Syntax Tree. For each conflicting change, we calculate the number of code smells and the number of future bug-fixes associated with the affected lines of code. Results: We found that entities that are smelly are three times more likely to be involved in merge conflicts. Method-level code smells (Blob Operation and Internal Duplication) are highly correlated with semantic conflicts. We also found that code that is smelly and experiences merge conflicts is more likely to be buggy. Conclusion: Bad code design not only impacts maintainability, it also impacts the day to day operations of a project, such as merging contributions, and negatively impacts the quality of the resulting code. Our findings indicate that research is needed to identify better ways to support merge conflict resolution to minimize its effect on code quality.",
      "Keywords": "Code Smell | Empirical Analysis | Machine Learning | Merge Conflict",
      "Publication venue": "International Symposium on Empirical Software Engineering and Measurement",
      "Publication date": "2017-12-07",
      "Publication type": "Conference Paper",
      "Authors": "Ahmed, Iftekhar;Brindescu, Caius;Mannan, Umme Ayda;Jensen, Carlos;Sarma, Anita",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85042350972",
      "Primary study DOI": "10.1109/ESEM.2017.14",
      "Title": "What if i Had No Smells?",
      "Abstract": "What would have happened if I did not have any code smell? This is an interesting question that no previous study, to the best of our knowledge, has tried to answer. In this paper, we present a method for implementing a what-if scenario analysis estimating the number of defective files in the absence of smells. Our industrial case study shows that 20% of the total defective files were likely avoidable by avoiding smells. Such estimation needs to be used with the due care though as it is based on a hypothetical history (i.e., zero number of smells and same process and product change characteristics). Specifically, the number of defective files could even increase for some types of smells. In addition, we note that in some circumstances, accepting code with smells might still be a good option for a company.",
      "Keywords": "code smells | machine learning | software estimation | technical debt",
      "Publication venue": "International Symposium on Empirical Software Engineering and Measurement",
      "Publication date": "2017-12-07",
      "Publication type": "Conference Paper",
      "Authors": "Falessi, Davide;Russo, Barbara;Mullen, Kathleen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018955129",
      "Primary study DOI": "10.1016/j.knosys.2017.04.014",
      "Title": "Code smell severity classification using machine learning techniques",
      "Abstract": "Several code smells detection tools have been developed providing different results, because smells can be subjectively interpreted and hence detected in different ways. Machine learning techniques have been used for different topics in software engineering, e.g., design pattern detection, code smell detection, bug prediction, recommending systems. In this paper, we focus our attention on the classification of code smell severity through the use of machine learning techniques in different experiments. The severity of code smells is an important factor to take into consideration when reporting code smell detection results, since it allows the prioritization of refactoring efforts. In fact, code smells with high severity can be particularly large and complex, and create larger issues to the maintainability of software a system. In our experiments, we apply several machine learning models, spanning from multinomial classification to regression, plus a method to apply binary classifiers for ordinal classification. In fact, we model code smell severity as an ordinal variable. We take the baseline models from previous work, where we applied binary classification models for code smell detection with good results. We report and compare the performance of the models according to their accuracy and four different performance measures used for the evaluation of ordinal classification techniques. From our results, while the accuracy of the classification of severity is not high as in the binary classification of absence or presence of code smells, the ranking correlation of the actual and predicted severity for the best models reaches 0.88–0.96, measured through Spearman's ρ.",
      "Keywords": "Code smell severity | Code smells detection | Machine learning | Ordinal classification | Refactoring prioritization",
      "Publication venue": "Knowledge-Based Systems",
      "Publication date": "2017-07-15",
      "Publication type": "Article",
      "Authors": "Arcelli Fontana, Francesca;Zanoni, Marco",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85042633185",
      "Primary study DOI": "10.1109/ICACCI.2017.8126033",
      "Title": "Evaluation of sampling techniques in software fault prediction using metrics and code smells",
      "Abstract": "The highly imbalanced nature of software fault datasets results in poor performance of machine leaning techniques used for software fault prediction. The objective of this paper is to evaluate sampling techniques and Meta-Cost learning in software fault prediction to alleviate problem of imbalanced data. We evaluate four sampling techniques in metrics as well as code smells based fault prediction on fault data sets of two open source systems ANT and POI. Our results indicate that Resample technique is best for metrics based fault prediction whereas Synthetic Minority Oversampling is best suited for code smells based fault prediction. The results are presented in terms of accuracy measures like G-Mean, Fmeasure and area under ROC curve. We also evaluate Meta-Cost learning and found that all sampling techniques outperform Meta-Cost learning. Our results also indicate that software metrics are better predictor of software faults than code smells.",
      "Keywords": "Code smells | Machine learning algorithms | Meta cost learning | Performance measures | Software fault prediction | Static code metrics",
      "Publication venue": "2017 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2017",
      "Publication date": "2017-11-30",
      "Publication type": "Conference Paper",
      "Authors": "Kaur, Kamaldeep;Kaur, Parmeet",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85042638100",
      "Primary study DOI": "10.1109/ICACCI.2017.8126075",
      "Title": "Determination of optimum refactoring sequence using A? algorithm after prioritization of classes",
      "Abstract": "Bad smells are the surface indication of deeper problem into source code; therefore, they need to be identified as early as possible without compromising on the quality of the software. This lead towards the requirement of refactoring that is the process used in improving the internal attributes like maintainability of the software without affecting its external attributes. Hence, to enhance quality in terms of maintainability refactoring should be done in a controlled and iterative manner. In this study, we have proposed a method that will help researchers and developers to generate a refactoring sequence in advance with the help of heuristic search A∗ algorithm. We have chosen one class of an open source project with the help of prioritization technique to illustrate the generation of the sequence. A∗ algorithm helps in finding an appropriate sequence which has maximum value of maintainability by choosing a path of minimum metrics value. We have identified ten bad smells and used nine refactoring techniques to remove them. With the help of this technique, software developers and maintainers team would be able to figure out refactoring sequence in advance and hence will help them in completing their work within time and budget constraints.",
      "Keywords": "A∗ algorithm | Maintainability | Metrics | Quality | Refactoring sequence",
      "Publication venue": "2017 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2017",
      "Publication date": "2017-11-30",
      "Publication type": "Conference Paper",
      "Authors": "Chug, Anuradha;Tarwani, Sandhya",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85043475991",
      "Primary study DOI": "10.1109/SATE.2017.9",
      "Title": "An empirical study of the impact of bad designs on defect proneness",
      "Abstract": "To reduce loss from software defects, in the past decades, a number of software engineering researchers have proposed many software defect prediction techniques, which mainly focus on predicting the defect prone software modules, source code files, or code changes. Prior research have identified software design has significant impacts on software quality, especially the bad designs, e.g., anti-patterns, high dependency design, and large source code files, have made various software engineering tasks more difficult. Given these prior works, various bad designs indicators have been widely considered as the fundamental defect prediction metrics in various defect prediction models. Even though the performance of these techniques have been investigated empirically, researchers have not yet gained a clear understanding of correlation between these design metrics and defects proneness. To bridge this gap, in this paper, we investigate the impact of the three kinds of bad design indicators on software defect proneness by conducting a comprehensive empirical study on 18 release versions of the Apache Commons series. In details, we discuss the defect proneness on the file level of three kinds of bad designs, corresponding to seven defect proneness metrics, including various types of well defined code smells, high method dependency, and the files of large size. Furthermore, we investigate the performance of each defect proneness metrics and the overlap between the file sets involved in the bad designs. The experiment results indicate that the three types of bad designs do have impact on defect proneness, the files participating in some special code smell, the large number of code calls to other modules and the large number of lines of code are significantly more likely to be faulty. Moreover, the overlaps of three types of bad designs are relatively small, which means that each group of defect proneness metrics is independent of each other.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2017 Annual Conference on Software Analysis, Testing and Evolution, SATE 2017",
      "Publication date": "2017-11-22",
      "Publication type": "Conference Paper",
      "Authors": "Zhang, Xiaofang;Zhou, Yida;Zhu, Can",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85040775890",
      "Primary study DOI": "10.1109/ASE.2017.8115717",
      "Title": "Kobold: Web usability as a service",
      "Abstract": "While Web applications have become pervasive in today's business, social interaction and information exchange, their usability is often deficient, even being a key factor for a website success. Usability problems repeat across websites, and many of them have been catalogued, but usability evaluation and repair still remains expensive. There are efforts from both the academy and industry to automate usability testing or to provide automatic statistics, but they rarely offer concrete solutions. These solutions appear as guidelines or patterns that developers can follow manually. This paper presents Kobold, a tool that detects usability problems from real user interaction (UI) events and repairs them automatically when possible, at least suggesting concrete solutions. By using the refactoring technique and its associated concept of bad smell, Kobold mines UI events to detect usability smells and applies usability refactorings on the client to correct them. The purpose of Kobold is to deliver usability advice and solutions as a service (SaaS) for developers, allowing them to respond to feedback of the real use of their applications and improve usability incrementally, even when there are no usability experts on the team. Kobold is available at: http://autorefactoring.lifia.info.unlp.edu.ar. A screencast is available at https://youtu.be/c-myYPMUh0Q.",
      "Keywords": "Software as a Service | Usability Refactoring | Web Usability",
      "Publication venue": "ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering",
      "Publication date": "2017-11-20",
      "Publication type": "Conference Paper",
      "Authors": "Grigera, Julian;Garrido, Alejandra;Rossi, Gustavo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85037032662",
      "Primary study DOI": "10.1145/3141517.3141856",
      "Title": "An annotation-based API for supporting runtime code annotation reading",
      "Abstract": "Code annotations are the core of the main APIs and frameworks for enterprise development, and are widely used on several applications. However, despite these APIs and frameworks made advanced uses of annotations, the language API for annotation reading is far from their needs. In particular, annotation reading is still a relatively complex task, that can consume a lot of development time and that can couple the framework internal structure to its annotations. This paper proposes an annotation-based API to retrieve metadata from code annotations and populate an instance with meta-information ready to be used by the framework. The proposed API is based on best practices and approaches for metadata definition documented on patterns, and has been implemented by a framework named Esfinge Metadata. We evaluated the approach by refactoring an existing framework to use it through Esfinge Metadata. The original and the refactored versions are compared using several code assessment techniques, such as software metrics, and bad smells detection, followed by a qualitative analysis based on source code inspection. As a result, the case study revealed that the usage of the proposed API can reduce the coupling between the metadata reading code and the annotations.",
      "Keywords": "Code annotation | Framework development | Metadata",
      "Publication venue": "Meta 2017 - Proceedings of the 2nd ACM SIGPLAN International Workshop on Meta-Programming Techniques and Reflection, co-located with SPLASH 2017",
      "Publication date": "2017-10-22",
      "Publication type": "Conference Paper",
      "Authors": "Lima, Phyllipe;Guerra, Eduardo;Nardes, Marco;Mocci, Andrea;Bavota, Gabriele;Lanza, Michele",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85037130198",
      "Primary study DOI": "10.1109/ICISC.2017.8068719",
      "Title": "Systematic exhortation of code smell detection using JSmell for Java source code",
      "Abstract": "\"Code Smell\" or \"Bad Smell\", at the very least, is an indicator of badly for source code and is often analytical of deeper problems in software design. In layman terms, it signals flaws in the core foundation or architecture of the software that can cause any number of more serious problems - from usability and runtime performance to supportability and enhancement. These problems can mostly be prevented by the systematic refactoring of the code. Code smells are symptoms of deep-rooted problems in design, which, in most common cases, reduce the understandability of the system for present and future programmers, therefore depiction the program un-maintainable. Identification of these code smells has been thought of as an spontaneous art rather than an exact science, as there are very few empirical measures or methodologies for doing so. This paper implements JSmell, which will follow a scientific approach to detect five of these 22 code smells. JSmell will give suggestions to refactor the code for all five of these smells. Further, the tool will provide an interactive process to refactor two of these cases; while for the rest, it will suggest an ideal refactoring technique that would need to be applied manually.",
      "Keywords": "Code smell | detection techniques | JSmell detector | refactor | tools support",
      "Publication venue": "Proceedings of the International Conference on Inventive Systems and Control, ICISC 2017",
      "Publication date": "2017-10-13",
      "Publication type": "Conference Paper",
      "Authors": "Sangeetha, M.;Sengottuvelan, P.",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85034611197",
      "Primary study DOI": "10.1109/I-SMAC.2017.8058275",
      "Title": "A perspective approach for refactoring framework using monitor based approach",
      "Abstract": "Code smells are prototype of devise shortcomings in the code that preserve potentially basis problems during software maintenance. Detecting and resolving code smells is protracted progression. Many numerals of code smells have been accredited and the succession through which the detection and resolution of code smells are maneuvered infrequently because developers do not discriminate how to rectify the sequences of code smells. Refactoring tools are used to aid software refactoring and assists the developers to restructure the code. Obtainable refactoring tools are submissive and human driven. Few refactoring tools valor be outcome in poor software quality and delayed refactoring may lead to higher refactoring cost. A refactoring framework is proposed which right away detects the code smells and changes in the source code are analyzed by running a monitor at the background. The proposed framework is assessed on different non-Trivial open source applications and the evaluation results proposed that the refactoring framework would assist to avoid the more code smells and average life extent of resolved smells can be reduced.",
      "Keywords": "Code smell detection | feedback controller | Instant Refactoring | Monitor | Software Refactoring",
      "Publication venue": "Proceedings of the International Conference on IoT in Social, Mobile, Analytics and Cloud, I-SMAC 2017",
      "Publication date": "2017-10-04",
      "Publication type": "Conference Paper",
      "Authors": "Sangeetha, M.;Sengottuvelan, P.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85037814637",
      "Primary study DOI": "10.1145/3132498.3134268",
      "Title": "Applying sofware metric thresholds for detection of bad smells",
      "Abstract": "Software metrics can be an efective measurement tool to assess the quality of software. In the literature, there are a lot of software metrics applicable to systems implemented in diferent paradigms like Objects Oriented Programming (OOP). To guide the use of these metrics in the evaluation of the quality of software systems, it is important to defne their thresholds. The aim of this study is to investigate the efectiveness of the thresholds in the evaluation of the quality of object oriented software. To do that, we used a threshold catalog of 18 software metrics derived from 100 software systems to defne detection strategies for fve bad smells. They are: Large Class, Long Method, Data Class, Feature Envy and Refused Bequest. We investigate the efectiveness of the thresholds in detection analysis of 12 software systems using these strategies. The results obtained by the proposed strategies were compared with the results obtained by the tools JDeodorant and JSPiRIT, used to identify bad smells. This study shows that the metric thresholds were significantly efective in supporting the detection of bad smells.",
      "Keywords": "Bad smells detection | Software metrics | Software quality | Thresholds",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2017-09-18",
      "Publication type": "Conference Paper",
      "Authors": "Souza, Priscila P.;Ferreira, Kecia A.M.;Sousa, Bruno L.;Bigonha, Mariza A.S.",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85037728975",
      "Primary study DOI": "10.1145/3129790.3129808",
      "Title": "Are code smell detection tools suitable for detecting architecture degradation?",
      "Abstract": "Context: Several studies suggest that there is a relation between code smells and architecture degradation. They claim that classes, which have degraded architecture-wise, can be detected on the basis of code smells, at least if these are manually identified in the source code. Objective: To evaluate the suitability of contemporary code smell detection tools by combining different smell categories for finding classes that show symptoms of architecture degradation. Method: A case study is performed in which architectural inconsistencies in an open source system are detected via reflexion modeling and code smell metrics are collected through several tools. Using data mining techniques, we investigate if it is possible to automatically and accurately classify classes connected to architectural inconsistencies based on the gathered code smell data. Results: Results suggest that existing code smell detection techniques, as implemented in contemporary tools, are not sufficiently accurate for classifying whether a class contains architectural inconsistencies, even when combining categories of code smells. Conclusion: It seems that current automated code smell detection techniques require €ne-Tuning for a specific system if they are to be used for finding classes with architectural inconsistencies. More research on architecture violation causes is needed to build more accurate detection techniques that work out-of-The-box.",
      "Keywords": "Architecture erosion | Case study | Code smells | Data mining",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2017-09-11",
      "Publication type": "Conference Paper",
      "Authors": "Lenhard, Jörg;Hassan, Mohammad Mahdi;Blom, Martin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030831258",
      "Primary study DOI": "10.1109/SNPD.2017.8022723",
      "Title": "SQL antipatterns detection and database refactoring process",
      "Abstract": "SQL antipatterns are frequently-made missteps that are commonly found in the design of relational databases, the use of SQL, and the development of database applications. They are intended to solve certain problems but will eventually lead to other problems. The motivation of this paper is how to assist database administrators in diagnosing SQL antipatterns and suggest refactoring techniques to solve the antipatterns. Specifically, we attempt to automate the detection of logical database design antipatterns by developing a tool that uses Transact-SQL language to query and analyze the database schema. The tool reports on potential antipatterns and gives an instruction on how to refactor the database schema. In an evaluation based on three databases from the industry, the performance of the tool is satisfactory in terms of recall of the antipatterns but the tool detects a number of false positives which affect its precision. It is found that SQL antipatterns detection still largely depends on the semantics of the data and the detection tool should rather be used in a semi-automated manner, i.e it can point out potential problematic locations in the database schema which require further diagnosis by the database administrators. This approach would be useful especially in the context of large databases where manual antipatterns inspection is very difficult.",
      "Keywords": "Database refactoring | Relational database | SQL antipattern | Transact-SQL",
      "Publication venue": "Proceedings - 18th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD 2017",
      "Publication date": "2017-08-29",
      "Publication type": "Conference Paper",
      "Authors": "Khumnin, Poonyanuch;Senivongse, Twittie",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85076559059",
      "Primary study DOI": "10.1016/j.jss.2019.110486",
      "Title": "A machine-learning based ensemble method for anti-patterns detection",
      "Abstract": "Anti-patterns are poor solutions to recurring design problems. Several empirical studies have highlighted their negative impact on program comprehension, maintainability, as well as fault-proneness. A variety of detection approaches have been proposed to identify their occurrences in source code. However, these approaches can identify only a subset of the occurrences and report large numbers of false positives and misses. Furthermore, a low agreement is generally observed among different approaches. Recent studies have shown the potential of machine-learning models to improve this situation. However, such algorithms require large sets of manually-produced training-data, which often limits their application in practice. In this paper, we present SMAD (SMart Aggregation of Anti-patterns Detectors), a machine-learning based ensemble method to aggregate various anti-patterns detection approaches on the basis of their internal detection rules. Thus, our method uses several detection tools to produce an improved prediction from a reasonable number of training examples. We implemented SMAD for the detection of two well known anti-patterns: God Class and Feature Envy. With the results of our experiments conducted on eight java projects, we show that: (1) Our method clearly improves the so aggregated tools; (2) SMAD significantly outperforms other ensemble methods.",
      "Keywords": "Anti-patterns | Ensemble methods | Machine learning | Software quality",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2020-03-01",
      "Publication type": "Article",
      "Authors": "Barbez, Antoine;Khomh, Foutse;Guéhéneuc, Yann Gaël",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027080493",
      "Primary study DOI": "10.23919/CISTI.2017.7975961",
      "Title": "Code smells detection 2.0: Crowdsmelling and visualization",
      "Abstract": "Background: Code smells have long been catalogued with corresponding mitigating solutions called refactoring operations. However, while the latter are supported in several IDEs, code smells detection scaffolding still has many limitations. Another aspect deserving attention is code smells visualization, to increase software quality awareness, namely in large projects, where maintainability is often the dominating issue. Research problems: Researchers have pointed out that code smells detection is inherently a subjective process and that is probably the main hindrance on providing automatic support. Regarding visualization, customized views are required, because each code smell type may have a different scope. Choosing the right visualization for each code smell type is an open research topic. Expected contributions: This research work focuses on the code smells detection and awareness process, by proposing two symbiotic contributions: crowdsmelling and smelly maps. We envisage that such features will be available in a future generation of interactive development environments (aka IDE 2.0). Crowdsmelling uses the concept of collective intelligence through which programmers around the world will collaboratively contribute to the calibration of code smells detection algorithms (one per each code smell), hopefully improving the detection accuracy and mitigating the subjectivity problem. Smelly maps build upon the aforementioned code smells detection capability and on the previous experience at UNIFACS of setting up a software visualization infrastructure. We expect to represent detected code smells at different abstraction levels with the goal of increasing software quality awareness and facilitating refactoring decisions upon large software systems.",
      "Keywords": "Code Smell | Code Smells Detection | Crowdsmelling | Crowdsourcing | IDE 2.0 | Refactoring | Smelly Maps | Software Construction | Software Maintenance | Software Quality",
      "Publication venue": "Iberian Conference on Information Systems and Technologies, CISTI",
      "Publication date": "2017-07-11",
      "Publication type": "Conference Paper",
      "Authors": "Dos Reis, Jose Pereira;Abreu, E. Fernando Brito;Carneiro, De F.Glauco",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85077197058",
      "Primary study DOI": "10.1109/ICSME.2019.00021",
      "Title": "Deep Learning Anti-Patterns from Code Metrics History",
      "Abstract": "Anti-patterns are poor solutions to recurring design problems. Number of empirical studies have highlighted the negative impact of anti-patterns on software maintenance which motivated the development of various detection techniques. Most of these approaches rely on structural metrics of software systems to identify affected components while others exploit historical information by analyzing co-changes occurring between code components. By relying solely on one aspect of software systems (i.e., structural or historical), existing approaches miss some precious information which limits their performances. In this paper, we propose CAME (Convolutional Analysis of code Metrics Evolution), a deep-learning based approach that relies on both structural and historical information to detect anti-patterns. Our approach exploits historical values of structural code metrics mined from version control systems and uses a Convolutional Neural Network classifier to infer the presence of anti-patterns from this information. We experiment our approach for the widely know God Class anti-pattern and evaluate its performances on three software systems. With the results of our study, we show that: (1) using historical values of source code metrics allows to increase the precision; (2) CAME outperforms existing static machine-learning classifiers; and (3) CAME outperforms existing detection tools.",
      "Keywords": "Anti-patterns | Deep learning | Mining Software Repositories",
      "Publication venue": "Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019",
      "Publication date": "2019-09-01",
      "Publication type": "Conference Paper",
      "Authors": "Barbez, Antoine;Khomh, Foutse;Gueheneuc, Yann Gael",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029361856",
      "Primary study DOI": "10.1109/TSC.2015.2502595",
      "Title": "Search-Based Web Service Antipatterns Detection",
      "Abstract": "Service Oriented Architecture (SOA) is widely used in industry and is regarded as one of the preferred architectural design technologies. As with any other software system, service-based systems (SBSs) may suffer from poor design, i.e., antipatterns, for many reasons such as poorly planned changes, time pressure or bad design choices. Consequently, this may lead to an SBS product that is difficult to evolve and that exhibits poor quality of service (QoS). Detecting web service antipatterns is a manual, time-consuming and error-prone process for software developers. In this paper, we propose an automated approach for detection of web service antipatterns using a cooperative parallel evolutionary algorithm (P-EA). The idea is that several detection methods are combined and executed in parallel during an optimization process to find a consensus regarding the identification of web service antipatterns. We report the results of an empirical study using eight types of common web service antipatterns. We compare the implementation of our cooperative P-EA approach with random search, two single population-based approaches and one state-of-the-art detection technique not based on heuristic search. Statistical analysis of the obtained results demonstrates that our approach is efficient in antipattern detection, with a precision score of 89 percent and a recall score of 93 percent.",
      "Keywords": "antipattern | search-based software engineering | service-oriented computing | web service design | Web Services",
      "Publication venue": "IEEE Transactions on Services Computing",
      "Publication date": "2017-07-01",
      "Publication type": "Article",
      "Authors": "Ouni, Ali;Kessentini, Marouane;Inoue, Katsuro;Cinnéide, Mel",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026641335",
      "Primary study DOI": "10.1109/SERA.2017.7965737",
      "Title": "An automated code smell and anti-pattern detection approach",
      "Abstract": "Today, software maintenance is more expensive than development costs. As class complexity increases, it is increasingly difficult for new programmers to adapt to software projects, causing the cost of the software to go up. Therefore, it's important to produce faultless and understandable code. Moreover, software projects are not developed by one person alone; even a small-scale project needs 3 or more participants working on the code at the same time. Producing well designed code during the development stage has a significant value because this process makes software projects more understandable and leads to higher code quality. Consequently, the cost of software project maintenance will decrease. Code smells and anti-patterns are symptoms of poorly designed code. The aforementioned tendencies of software projects increase the possibility of poor implementations and code imperfections. Therefore it is necessary to detect and refactor poorly designed code. This paper describes an attempt to achieve their detection.",
      "Keywords": "Anti-patterns | Automatic detection | Brain method | Code smells | Data class",
      "Publication venue": "Proceedings - 2017 15th IEEE/ACIS International Conference on Software Engineering Research, Management and Applications, SERA 2017",
      "Publication date": "2017-06-30",
      "Publication type": "Conference Paper",
      "Authors": "Velioglu, Sevilay;Selcuk, Yunus Emre",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026780346",
      "Primary study DOI": "10.1109/ICSE-SEET.2017.13",
      "Title": "Teaching software engineering principles to K-12 students: A MOOC on scratch",
      "Abstract": "In the last few years, many books, online puzzles, apps and games have been created to teach young children programming. However, most of these do not introduce children to broader concepts from software engineering, such as debugging and code quality issues like smells, duplication, refactoring and naming. To address this, we designed and ran an online introductory Scratch programming course in which we teach elementary programming concepts and software engineering concepts simultaneously. In total 2,220 children actively participated in our course in June and July of 2016, most of which (73%) between the ages of 7 and 11. In this paper we describe our course design and analyze the resulting data. More specifically, we investigate whether 1) students find programming concepts more difficult than software engineering concepts, 2) there are age-related differences in their performance, and 3) we can predict successful course completion. Our results show that there is no difference in students' scores between the programming concepts and the software engineering concepts, suggesting that it is indeed possible to teach these concepts to this age group. We also find that students over 12 years of age perform significantly better in questions related to operators and procedures. Finally, we identify the factors from the students' profile and their behaviour in the first week of the course that can be used to predict its successful completion.",
      "Keywords": "code smells | dropout prediction | MOOC | Programming education | Scratch",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering and Education Track, ICSE-SEET 2017",
      "Publication date": "2017-06-29",
      "Publication type": "Conference Paper",
      "Authors": "Hermans, Felienne;Aivaloglou, Efthimia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85025175244",
      "Primary study DOI": "10.1109/ICPC.2017.8",
      "Title": "FindSmells: Flexible Composition of Bad Smell Detection Strategies",
      "Abstract": "Bad smells are symptoms of problems in the source code of software systems. They may harm the maintenance and evolution of systems on different levels. Thus, detecting smells is essential in order to support the software quality improvement. Since even small systems may contain several bad smell instances, and considering that developers have to prioritize their elimination, its automated detection is a necessary support for developers. Regarding that, detection strategies have been proposed to formalize rules to detect specific bad smells, such as Large Class and Feature Envy. Several tools like JDeodorant and JSpIRIT implement these strategies but, in general, they do not provide full customization of the formal rules that define a detection strategy. In this paper, we propose FindSmells, a tool for detecting bad smells in software systems through software metrics and their thresholds. With FindSmells, the user can compose and manage different strategies, which run without source code analysis. We also provide a running example of the tool. Video: https://youtu.be/LtomN93y6gg.",
      "Keywords": "Bad Smells | Detection Strategies | Software Maintenance | Support Tool",
      "Publication venue": "IEEE International Conference on Program Comprehension",
      "Publication date": "2017-06-28",
      "Publication type": "Conference Paper",
      "Authors": "Sousa, Bruno L.;Souza, Priscila P.;Fernandes, Eduardo M.;Ferreira, Kecia A.M.;Bigonha, Mariza A.S.",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85025822741",
      "Primary study DOI": "10.1109/CHASE.2017.11",
      "Title": "Towards effective teams for the identification of code smells",
      "Abstract": "Code smells are symptoms of poor design and implementation choices. Several techniques for the automated detection of code smells have been proposed, but their effectiveness is limited due to the inherent subjectivity of the task. Accepting false warnings generated by a tool may lead to unnecessary maintenance effort. Moreover, bypassing undetected smells may contribute to the software degradation. Thus, developers need to perform a subsequent manual identification of code smells to confirm their occurrences as well as address both false and missing warnings. However, performing ad hoc manual identification of smells does not assure more effective results. Indeed, different context factors may influence on the conclusion about the incidence of a code smell. Based on evidence collected from previous work, this paper presents and discusses a set of context factors that may influence the effectiveness of smell identification tasks. These factors are addressed to human aspects, such as the interaction among individuals and their professional roles. Based on such factors, we present an initial set of practical suggestions for composing more effective teams to the identification of code smells.",
      "Keywords": "awareness | code smells | collaboration | context factors | human factors | professional role",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 10th International Workshop on Cooperative and Human Aspects of Software Engineering, CHASE 2017",
      "Publication date": "2017-06-28",
      "Publication type": "Conference Paper",
      "Authors": "De Mello, Rafael;Oliveira, Roberto;Sousa, Leonardo;Garcia, Alessandro",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85025703376",
      "Primary study DOI": "10.1109/ICSAW.2017.16",
      "Title": "Arcan: A tool for architectural smells detection",
      "Abstract": "Code smells are sub-optimal coding circumstances such as blob classes or spaghetti code - they have received much attention and tooling in recent software engineering research. Higher-up in the abstraction level, architectural smells are problems or sub-optimal architectural patterns or other design-level characteristics. These have received significantly less attention even though they are usually considered more critical than code smells, and harder to detect, remove, and refactor. This paper describes an open-source tool called Arcan developed for the detection of architectural smells through an evaluation of several different architecture dependency issues. The detection techniques inside Arcan exploit graph database technology, allowing for high scalability in smells detection and better management of large amounts of dependencies of multiple kinds. In the scope of this paper, we focus on the evaluation of Arcan results carried out with real-life software developers to check if the architectural smells detected by Arcan are really perceived as problems and to get an overall usefulness evaluation of the tool.",
      "Keywords": "Architectural Smells | Dependency graph | Graph database | Software architecture",
      "Publication venue": "Proceedings - 2017 IEEE International Conference on Software Architecture Workshops, ICSAW 2017: Side Track Proceedings",
      "Publication date": "2017-06-23",
      "Publication type": "Conference Paper",
      "Authors": "Fontana, Francesca Arcelli;Pigazzini, Ilaria;Roveda, Riccardo;Tamburri, Damian;Zanoni, Marco;Nitto, Elisabetta Di",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85022041237",
      "Primary study DOI": "10.1109/AICCSA.2016.7945776",
      "Title": "AntiPattren-based cloud ontology evaluation",
      "Abstract": "Nowadays, cloud computing is an emerging technology thanks to its ability to provide on-demand computing services (hardware and software) with less description standardization effort. Multiple issues and challenges in discovering cloud services appear due to the lack of the cloud service description standardization. In fact, the existing cloud providers describe, their similar offered services in different ways. Thus, various existing works aim at standardizing the representation of cloud computing services while proposing ontologies. However, since the existing proposals were not evaluated, they might be less adopted and considered. Indeed, the ontology evaluation has a direct impact on its understandability and reusability. In this paper, we propose an evaluation approach to validate our proposed Cloud Service Ontology (CSO), to guarantee an adequate cloud service discovery. This paper contribution is threefold. First, it specifies a set of patterns and anti-patterns in order to evaluate CSO. Second, it defines an anti-pattern detection method based on SPARQL queries which provides a set of correction recommendations to help ontologists revise the ontology. Finally, some experiment tests were conducted in relation to: (i) the method efficiency and (ii) anti-pattern detection of design anomalies as well as taxonomic and domain errors within CSO.",
      "Keywords": "",
      "Publication venue": "Proceedings of IEEE/ACS International Conference on Computer Systems and Applications, AICCSA",
      "Publication date": "2016-07-02",
      "Publication type": "Conference Paper",
      "Authors": "Loukil, Faiza;Rekik, Molka;Boukadi, Khouloud",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84884938013",
      "Primary study DOI": "10.1007/978-3-642-39742-4_6",
      "Title": "Competitive coevolutionary code-smells detection",
      "Abstract": "Software bad-smells, also called design anomalies, refer to design situations that may adversely affect the maintenance of software. Bad-smells are unlikely to cause failures directly, but may do it indirectly. In general, they make a system difficult to change, which may in turn introduce bugs. Although these bad practices are sometimes unavoidable, they should be in general fixed by the development teams and removed from their code base as early as possible. In this paper, we propose, for the first time, the use of competitive coevolutionary search to the code-smells detection problem. We believe that such approach to code-smells detection is attractive because it allows combining the generation of code-smell examples with the production of detection rules based on quality metrics. The main idea is to evolve two populations simutaneously where the first one generates a set of detection rules (combination of quality metrics) that maximizes the coverage of a base of code-smell examples and the second one maximizes the number of generated \"artificial\" code-smells that are not covered by solutions (detection rules) of the first population. The statistical analysis of the obtained results shows that our proposed approach is promising when compared to two single population-based metaheuristics on a variety of benchmarks. © 2013 Springer-Verlag.",
      "Keywords": "",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2013-10-08",
      "Publication type": "Conference Paper",
      "Authors": "Boussaa, Mohamed;Kessentini, Wael;Kessentini, Marouane;Bechikh, Slim;Ben Chikha, Soukeina",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019261120",
      "Primary study DOI": "10.1142/S0218843017420011",
      "Title": "Semantic Analysis of RESTful APIs for the Detection of Linguistic Patterns and Antipatterns",
      "Abstract": "Identifier lexicon may have a direct impact on software understandability and reusability and, thus, on the quality of the final software product. Understandability and reusability are two important characteristics of software quality. REpresentational State Transfer (REST) style is becoming a de facto standard adopted by software organizations to build their Web applications. Understandable and reusable Uniform Resource Identifers (URIs) are important to attract client developers of RESTful APIs because good URIs support the client developers to understand and reuse the APIs. Consequently, the use of proper lexicon in RESTful APIs has also a direct impact on the quality of Web applications that integrate these APIs. Linguistic antipatterns represent poor practices in the naming, documentation, and choice of identifiers in the APIs as opposed to linguistic patterns that represent the corresponding best practices. In this paper, we present the Semantic Analysis of RESTful APIs (SARA) approach that employs both syntactic and semantic analyses for the detection of linguistic patterns and antipatterns in RESTful APIs. We provide detailed definitions of 12 linguistic patterns and antipatterns and define and apply their detection algorithms on 18 widely-used RESTful APIs, including Facebook, Twitter, and Dropbox. Our detection results show that linguistic patterns and antipatterns do occur in major RESTful APIs in particular in the form of poor documentation practices. Those results also show that SARA can detect linguistic patterns and antipatterns with higher accuracy compared to its state-of-the-art approach - DOLAR.",
      "Keywords": "detection | documentation | Latent Dirichlet Allocation (LDA) | linguistic antipatterns | patterns | RESTful APIs | second-order similarity | semantic analysis",
      "Publication venue": "International Journal of Cooperative Information Systems",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Palma, Francis;Gonzalez-Huerta, Javier;Founi, Mohamed;Moha, Naouel;Tremblay, Guy;Guéhéneuc, Yann Gaël",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029862351",
      "Primary study DOI": "10.1145/3120459.3120468",
      "Title": "Towards triaging code-smell candidates via runtime scenarios and method-call dependencies",
      "Abstract": "Managing technical debt includes the detection and assessment of debt at the code and design levels (such as bad smells). Existing approaches and tools for smell detection primarily use static program data for decision support. While a static analysis allows for identifying smell candidates without executing and instrumenting the system, such approaches also come with the risk of missing candidates or of producing false positives. Moreover, smell candidates might result from a deliberate design decision (e.g., of applying a particular design pattern). Such risks and the general ambivalence of smell detection require a manual design and/or code inspection for reviewing all alleged smells. In this paper, we propose an approach to obtain tailorable design documentation for object-oriented systems based on runtime tests. In particular, the approach supports a tool-supported triaging of code-smell candidates. We use runtime scenario tests to extract execution traces. Based on these execution traces, different (automatically derived) model perspectives on method-call dependencies (e.g., dependency structure matrices, DSMs; UML2 sequence diagrams) are then used as decision support for assessing smell candidates. Our approach is implemented as part of the KaleidoScope tool which is publicly available for download.",
      "Keywords": "Code smell | Decision support | Dependency structure matrix | Design documentation | Execution trace | Scenario-based testing | Software behavior | Technical debt | Unified modeling language (UML2)",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2017-05-22",
      "Publication type": "Conference Paper",
      "Authors": "Haendler, Thorsten;Sobernig, Stefan;Strembeck, Mark",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018943558",
      "Primary study DOI": "10.5815/ijisa.2017.05.04",
      "Title": "Source code author attribution using author's programming style and code smells",
      "Abstract": "Source code is an intellectual property and using it without author's permission is a violation of property right. Source code authorship attribution is vital for dealing with software theft, copyright issues and piracies. Characterizing author's signature for identifying their footprints is the core task of authorship attribution. Different aspects of source code have been considered for characterizing signatures including author's coding style and programming structure, etc. The objective of this research is to explore another trait of authors' coding behavior for personifying their footprints. The main question that we want to address is that \"can code smells are useful for characterizing authors' signatures? A machine learning based methodology is described not only to address the question but also for designing a system. Two different aspects of source code are considered for its representation into features: Author's style and code smells. The author's style related feature representation is used as baseline. Results have shown that code smell can improves the authorship attribution.",
      "Keywords": "Author style | Authorship | Code smell | Source Code | Stylistic feature",
      "Publication venue": "International Journal of Intelligent Systems and Applications",
      "Publication date": "2017-05-01",
      "Publication type": "Article",
      "Authors": "Gull, Muqaddas;Zia, Tehseen;Ilyas, Muhammad",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018327881",
      "Primary study DOI": "",
      "Title": "MaLTeSQuE 2017 - IEEE International Workshop on Machine Learning Techniques for Software Quality Evaluation, co-located with SANER 2017",
      "Abstract": "The proceedings contain 6 papers. The topics discussed include: using source code metrics to predict change-prone web services: a case-study on eBay services; investigating code smell co-occurrences using association rule learning: a replicated study; using machine learning to design a flexible LOC counter; machine learning for finding bugs: an initial report; automatic feature selection by regularization to improve bug prediction accuracy; and hyperparameter optimization to improve bug prediction accuracy.",
      "Keywords": "",
      "Publication venue": "MaLTeSQuE 2017 - IEEE International Workshop on Machine Learning Techniques for Software Quality Evaluation, co-located with SANER 2017",
      "Publication date": "2017-03-17",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85017135717",
      "Primary study DOI": "10.1109/MS.2017.51",
      "Title": "GitHub, Technical Debt, Code Formatting, and More",
      "Abstract": "This issue's column reports on papers from the 19th International Conference on Model Driven Engineering Languages and Systems, the 2016 ACM SIGPLAN International Conference on Software Language Engineering, the 12th International ACM SIGSOFT Conference on the Quality of Software Architectures, and the 13th Working IEEE/IFIP Conference on Software Architecture. Topics discussed include GitHub and open source, technical debt in model-driven engineering, a universal code formatter, assuring architectural quality, and continuous architecting.",
      "Keywords": "AADL | AQAF | architectural quality | Architecture Analysis and Design Language | Architecture Quality Assurance Framework | CAFFEA | code formatting | code smells | continuous architecting | Continuous Architecture Framework for Embedded and Agile Software Development | GitHub | machine learning | model-driven engineering | open source | software development | software engineering | technical debt | UML",
      "Publication venue": "IEEE Software",
      "Publication date": "2017-03-01",
      "Publication type": "Article",
      "Authors": "Carver, Jeffrey C.;Cabot, Jordi;Capilla, Rafael;Muccini, Henry",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85012226953",
      "Primary study DOI": "10.1007/s13369-016-2238-8",
      "Title": "A Lightweight Approach for Detection of Code Smells",
      "Abstract": "The accurate removal of code smells from source code supports activities such as refactoring, maintenance, examining code quality etc. A large number of techniques and tools are presented for the specification and detection of code smells from source code in the last decade, but they still lack accuracy and flexibility due to different interpretations of code smell definitions. Most techniques target just detection of few code smells and render different results on the same examined systems due to different informal definitions and threshold values of metrics used for detecting code smells. We present a flexible and lightweight approach based on multiple searching techniques for the detection and visualization of all 22 code smells from source code of multiple languages. Our approach is lightweight and flexible due to application of SQL queries on intermediate repository and use of regular expressions on selected source code constructs. The concept of approach is validated by performing experiments on eight publicly available open source software projects developed using Java and C# programming languages, and results are compared with existing approaches. The accuracy of presented approach varies from 86–97 % on the eight selected software projects.",
      "Keywords": "Code flaws | Code smells | Refactoring | Regular expressions | SQL",
      "Publication venue": "Arabian Journal for Science and Engineering",
      "Publication date": "2017-02-01",
      "Publication type": "Article",
      "Authors": "Rasool, Ghulam;Arshad, Zeeshan",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85017586793",
      "Primary study DOI": "10.1109/SCCC.2016.7836059",
      "Title": "AOLink-Automatic gerenation of refactoring suggestions for aspect-oriented anomalies",
      "Abstract": "Aspect-Orientation (AO) is a strategy to improve the internal structure of the software. However, using AO abstractions in an inappropriate way may also introduce bad smells in the software. To handle it, the developer needs to read catalogs of bad smells and refactorings. The problem is these catalogs are written in a non-standardized way, what make hard the understanding on them. Furthermore, the existing tools for AO bad smells detection do not present to the user the necessary refactoring to deal with them. This paper aims to present: (i) a standardized catalog of AO bad smells; and (ii) an extension, called AOLink, for a tool that detect AO bad smells. It is able to suggest the refactorings to deal with AO bad smells. The preliminary assessment performed on the proposed catalog showed that the recall and precision with respect to the bad smells detection may be improved.",
      "Keywords": "Aspect-Orientation | Bad Smells | Refactoring",
      "Publication venue": "Proceedings - International Conference of the Chilean Computer Science Society, SCCC",
      "Publication date": "2016-07-02",
      "Publication type": "Conference Paper",
      "Authors": "Costa, Renato Silva;Costa, Heitor;Parreira, Paulo A.",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85011035343",
      "Primary study DOI": "10.1109/INVENTIVE.2016.7824903",
      "Title": "Automatic detection of bad smells from excel sheets and refactor for performance improvement",
      "Abstract": "This paper presents the prevailing analysis, as well as some of experimental findings analogous to the factual study of so called Bad smells. An anatomy is presented which categorize bad smells given by Fowler in context of object-oriented programming. Many research papers have been published quantifying the effect of code smells on maintenance efforts. Notion of bad smells can be applied to multiple areas other than Software. This paper intends to give a review to look for bad smells in the context of spreadsheets. Many authors in past years have worked on spreadsheet bad smells. Here in this paper, a systematic review is proposed demonstrating list of bad smells for spreadsheets and related work issued in past in field of spreadsheet bad smells.",
      "Keywords": "Bad smells | Complexity | Inter-worksheet smells | Refactoring | Restructuring",
      "Publication venue": "Proceedings of the International Conference on Inventive Computation Technologies, ICICT 2016",
      "Publication date": "2016-07-02",
      "Publication type": "Conference Paper",
      "Authors": "Rajdev, Urja;Kaur, Anantdeep",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85011076178",
      "Primary study DOI": "10.1109/INVENTIVE.2016.7823284",
      "Title": "Bad smell refactor removable detection of audit spreadsheet",
      "Abstract": "Spreadsheets is one of the most generic software product to have its presence felt across industries ranging with individuals with simple needs to large companies with complex systems. In fact, mutiple judgements are disseminated owing to its high usage in U.S Firms; around 95% of the firms in U.S utilize it for financial reporting. Because of its major applicability around ample areas there is a demand to regulate and authorize its content. Many years of research study have communicated the need of identifying bad smells from spreadsheets which are very recurrent and failure of which can make a company loss billions. Bad smell is notable to be one of the leading genre of hindrance incorporated in the field of Software Engineering which can be made analogous to spreadsheet bad smells. In this paper we developed an automatic tool, a Windows Form Application embedded in.NET framework that efficiently AIDS in diagnosing catalog of spreadsheet bad smells. Main focus of our tool will be largely on diagnosing bad smells taking spreadsheet data generated during conducting audit process of a Company. Patterns categorizimg each smell is put to match against each spreadsheet cell to validate the reliability of the proposed tool. Main bad smells detected, concerning our tool is Long calculation Chain, Middle Man, Feature Envy, Multiple operations, Conditional Complexity and Multiple References. At the end, result of automatic detection tool on audit data is examined and future scope is considered to refine the process of automatic detection and refactor on Audit data.",
      "Keywords": "",
      "Publication venue": "Proceedings of the International Conference on Inventive Computation Technologies, ICICT 2016",
      "Publication date": "2016-07-02",
      "Publication type": "Conference Paper",
      "Authors": "Rajdev, Urja;Kaur, Anantdeep",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013798216",
      "Primary study DOI": "10.1109/QUATIC.2016.060",
      "Title": "Web systems quality evolution",
      "Abstract": "Software evolution is a well-established research area, but not in the area of web systems/applications. Web projects are normally more complex than other software development projects because they have both server and client code, encompass a variety of programming languages, and are multidisciplinary. We aim to produce a catalog of web smells to help avoiding the problems in web development code before they happen, thus saving time and reducing cost. By means of longitudinal studies we plan to analyze the impact of these web smells in web systems maintainability and reliability. This will require developing a tool to detect the proposed web smells. For validation sake, we will also use surveys among web systems developers and peer reviewing in academic fora.",
      "Keywords": "irregular time series | longitudinal studies | software evolution | software quality | web code smells | web engineering",
      "Publication venue": "Proceedings - 2016 10th International Conference on the Quality of Information and Communications Technology, QUATIC 2016",
      "Publication date": "2017-01-11",
      "Publication type": "Conference Paper",
      "Authors": "Rio, Americo;Brito E Abreu, Fernando",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-78751565069",
      "Primary study DOI": "10.1109/QUATIC.2010.60",
      "Title": "Reducing subjectivity in code smells detection: Experimenting with the Long Method",
      "Abstract": "Guidelines for refactoring are meant to improve software systems internal quality and are widely acknowledged as among software's best practices. However, such guidelines remain mostly qualitative in nature. As a result, judgments on how to conduct refactoring processes remain mostly subjective and therefore non-automatable, prone to errors and unrepeatable. The detection of the Long Method code smell is an example. To address this problem, this paper proposes a technique to detect Long Method objectively and automatically, using a Binary Logistic Regression model calibrated by expert's knowledge. The results of an experiment illustrating the use of this technique are reported. © 2010 IEEE.",
      "Keywords": "Binary Logistic Regression | Code smells | Long Method | Refactoring process",
      "Publication venue": "Proceedings - 7th International Conference on the Quality of Information and Communications Technology, QUATIC 2010",
      "Publication date": "2010-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Bryton, Sérgio;Brito E Abreu, Fernando;Monteiro, Miguel",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026726240",
      "Primary study DOI": "10.1007/978-3-319-62404-4_49",
      "Title": "A systematic literature review: Code bad smells in java source code",
      "Abstract": "Code smell is an indication of a software designing problem. The presence of code smells can have a severe impact on the software quality. Smells basically refers to the structure of the code which violates few of the design principals and so has negative effect on the quality of the software. Larger the source code, more is its presence. Software needs to be reliable, robust and easily maintainable so that it can minimize the cost of its development as well as maintenance. Smells may increase the chances of failure of the system during maintenance. A SLR has been performed based on the search of digital libraries that includes the publications since 1999 to 2016. 60 research papers are deeply analyzed that are most relevant. The objective of this paper is to provide an extensive overview of existing research in the field of bad smells, identify the detection techniques and correlation between the detection techniques, in addition to find the name of the code smells that need more attention in detection approaches. This SLR identified that code clone (code smell) receives most research attention. Our findings also show that very few papers report on the impact of code bad smells. Most of the papers focused on the detection techniques and tools. A significant correlation between detection techniques has been calculated. There are four code smells that are not yet detected are Primitive Obsession, Inappropriate Intimacy, Incomplete library class and Comments.",
      "Keywords": "",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2017-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Gupta, Aakanshi;Suri, Bharti;Misra, Sanjay",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85015905794",
      "Primary study DOI": "10.1007/978-981-10-3153-3_56",
      "Title": "Evaluation of machine learning approaches for change-proneness prediction using code smells",
      "Abstract": "In the field of technology, software is an essential driver of business and industry. Software undergoes changes due to maintenance activities initiated by bug fixing, improved documentation, and new requirements of users. In software, code smells are indicators of a system which may give maintenance problem in future. This paper evaluates six types of machine learning algorithms to predict change-proneness using code smells as predictors for various versions of four Java-coded applications. Two approaches are used: method 1-random undersampling is done before Feature selection; method 2-feature selection is done prior to random undersampling. This paper concludes that gene expression programming (GEP) gives maximum AUC value, whereas cascade correlation network (CCR), treeboost, and PNN\\GRNN algorithms are among top algorithms to predict F-measure, precision, recall, and accuracy. Also, GOD and L_M code smells are good predictors of software change-proneness. Results show that method 1 outperforms method 2.",
      "Keywords": "Code smells | Feature subset selection (FSS) | Machine learning algorithms | Software change-proneness | Undersampling",
      "Publication venue": "Advances in Intelligent Systems and Computing",
      "Publication date": "2017-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Kaur, Kamaldeep;Jain, Shilpi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85034647449",
      "Primary study DOI": "10.14232/actacyb.23.2.2017.3",
      "Title": "The Connection between antipatterns and maintainability in firefox",
      "Abstract": "The notion that antipatterns have a detrimental effect on source code maintainability is widely accepted, but there is relatively little objective evidence to support it. We seek to investigate this issue by analyzing the connection between antipatterns and maintainability in an empirical study of Firefox, an open source browser application developed in C++. After extracting antipattern instances and maintainability information from 45 revisions, we looked for correlations to uncover a connection between the two concepts. We found statistically significant negative values for both Pearson and Spearman correlations, most of which were under-0.65. These values suggest there are strong, inverse relationships, thereby supporting our initial assumption that the more antipatterns the source code contains, the harder it is to maintain. Lastly, we combined these data into a table applicable for machine learning experiments, which we conducted using Weka [10] and several of its classifier algorithms. All five regression types we tried had correlation coeficients over 0.77 and used mostly negative weights for the antipattern predictors in the models we constructed. In conclusion, we can say that this empirical study is another step towards objectively demonstrating that antipatterns have an adverse effect on software maintainability.",
      "Keywords": "Antipatterns | Correlation | Machine learning | Maintainability | Source code metrics | Static analysis",
      "Publication venue": "Acta Cybernetica",
      "Publication date": "2017-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Bán, Dénes",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85022334594",
      "Primary study DOI": "10.1007/978-3-319-60837-2_37",
      "Title": "Temporal relations of rough anti-patterns in software development",
      "Abstract": "Design anti-patterns are common wrong solutions in software, whose frequency has been proven to be correlated with poor system quality. This paper investigates temporal relations between different types of anti-patters, i.e. how the appearance of one type of anti-pattern in the source code increases the probability that different anti-pattern will appear shortly after in its neighbourhood. The notion of rough anti-pattern used to model the vagueness of anti-patterns allows us to reformulate the question and establish if certain rough patterns tend to be temporally correlated. The proposed framework was used to build a classifier, which can be employed to predict the appearance of some anti-patterns by looking at the temporal relations between other anti-patterns. The experiments conducted on two large open-source projects suggest that a few common anti-patterns tend to be temporally dependent on others, whereas a few others do not.",
      "Keywords": "Mining software repositories | Pattern recognition | Rough sets | Software design anti-patterns | Spatio-temporal patterns",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2017-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Puławski, Łukasz",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030776133",
      "Primary study DOI": "10.1016/j.infsof.2017.09.011",
      "Title": "Understanding metric-based detectable smells in Python software: A comparative study",
      "Abstract": "Context Code smells are supposed to cause potential comprehension and maintenance problems in software development. Although code smells are studied in many languages, e.g. Java and C#, there is a lack of technique or tool support addressing code smells in Python. Objective Due to the great differences between Python and static languages, the goal of this study is to define and detect code smells in Python programs and to explore the effects of Python smells on software maintainability. Method In this paper, we introduced ten code smells and established a metric-based detection method with three different filtering strategies to specify metric thresholds (Experience-Based Strategy, Statistics-Based Strategy, and Tuning Machine Strategy). Then, we performed a comparative study to investigate how three detection strategies perform in detecting Python smells and how these smells affect software maintainability with different detection strategies. This study utilized a corpus of 106 Python projects with most stars on GitHub. Results The results showed that: (1) the metric-based detection approach performs well in detecting Python smells and Tuning Machine Strategy achieves the best accuracy; (2) the three detection strategies discover some different smell occurrences, and Long Parameter List and Long Method are more prevalent than other smells; (3) several kinds of code smells are more significantly related to changes or faults in Python modules. Conclusion These findings reveal the key features of Python smells and also provide a guideline for the choice of detection strategy in detecting and analyzing Python smells.",
      "Keywords": "Code smell | Detection strategy | Python | Software maintainability",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2018-02-01",
      "Publication type": "Article",
      "Authors": "Chen, Zhifei;Chen, Lin;Ma, Wanwangying;Zhou, Xiaoyu;Zhou, Yuming;Xu, Baowen",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85023170959",
      "Primary study DOI": "",
      "Title": "An ontology-based approach to analyzing the occurrence of code smells in software",
      "Abstract": "Code Smells indicate potential flaws in software design that can lead to costly consequences. To mitigate the bad effects of Code Smells, it is necessary to detect and fix defective code. Programmatic processing of Code Smells is not new. Previous works have focused on detection and representation to support the analysis of faulty software. However, such works are based on a syntactic operation, without taking advantage on semantic properties of the software. On the other hand, there are several ways to provide semantic support in software development as a whole. Ontologies, for example, have recently been usedl. The application of ontologies for inferring semantic mechanisms to aid software engineers in dealing with smells may be of great value. As little attention has been given to this, we propose an ontology-based approach to analyze the occurrence of Code Smells in software projects. First, we present a comprehensive ontology that is capable of representing Code Smells and their association with software projects. We also introduce a tool that can manipulate our ontology in order to provide processing of Code Smells as it mines software source-code. Finally, we conducted an initial evaluation of our approach in a real usage scenario with two large open-source software repositories.",
      "Keywords": "Code smells | Ontocean | Ontology | Reasoner",
      "Publication venue": "ICEIS 2017 - Proceedings of the 19th International Conference on Enterprise Information Systems",
      "Publication date": "2017-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Da Silva Carvalho, Luis Paulo;Novais, Renato;Do Nascimento Salvador, Laís;De Mendonça Neto, Manoel Gomes",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85034654107",
      "Primary study DOI": "10.14232/actacyb.23.2.2017.16",
      "Title": "Automating the refactoring process",
      "Abstract": "To decrease software maintenance cost, software development companies use static source code analysis techniques. Static analysis tools are capable of finding potential bugs, anti-patterns, coding rule violations, and they can also enforce coding style standards. Although there are several available static analyzers to choose from, they only support issue detection. The elimination of the issues is still performed manually by developers. Here, we propose a process that supports the automatic elimination of coding issues in Java. We introduce a tool that uses a third-party static analyzer as input and enables developers to automatically fix the detected issues for them. Our tool uses a special technique, called reverse AST-search, to locate source code elements in a syntax tree, just based on location information. Our tool was evaluated and tested in a two-year project with six software development companies where thousands of code smells were identified and fixed in five systems that have altogether over five million lines of code.",
      "Keywords": "Code smells | Refactoring | Reverse ast-search | Spatial index",
      "Publication venue": "Acta Cybernetica",
      "Publication date": "2017-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Szoke, Gábor",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029523943",
      "Primary study DOI": "10.18293/SEKE2017-139",
      "Title": "A framework to build Bayesian networks to assess scrum-based software development methods",
      "Abstract": "Agile software development has been increasingly used to satisfy the need to respond to fast moving market demand and gain market share. Scrum, which is a project management framework, dominates as the most popular agile method. In the literature, there are a number of solutions to customize and assess Scrum-based agile methods, but they are limited to focus only on process factors, assume a predefined set of practices or rely only on subjective evaluation. This paper presents a framework to build a Bayesian Network to assist on the assessment of Scrum-based software development methods. The BN models the main entities of the software development process and can be complemented with software practices and metrics. To evaluate the completeness of our solution, we performed simulations to check if the proposed framework diagnoses 14 known Scrum anti-patterns extracted from the literature. 12 antipatterns were directly detected, 1 was indirectly detected by the BN and 1 was considered as invalid. We concluded that the proposed solution is complete to detect the major flaws of Scrum-based software development methods and can be used to assist on the configuration, adoption and continuous improvement of Scrum teams.",
      "Keywords": "Agile methods | Bayesian network | Method engineering | Scrum | Software metrics",
      "Publication venue": "Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",
      "Publication date": "2017-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Perkusich, Mirko;Gorgonioy, Kyller;Almeidaz, Hyggo;Perkusich, Angelo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85041449600",
      "Primary study DOI": "",
      "Title": "JAVACHECK: A domain specific language for the static analysis of Java code",
      "Abstract": "The increasing complexity of software systems has raised the need for code analysis tools to assess its quality. However, these tools offer predefined metrics or evaluation criteria, which are frequently hard to extend or modify. For this purpose, we have developed JAVACHECK, a Domain-Specific Language targeted to define expected properties of Java code bases. JAVACHECK can be used in a variety of scenarios related to quality assurance: to define expected code styles (e.g., naming conventions), specify programming conventions (e.g., private attributes), detect code smells possibly indicating errors (e.g., equals method with no hashCode), and detect patterns (e.g., uses of Singleton) or requirements demanded in a project (e.g., a class with name synonym to “Professor”).",
      "Keywords": "Domain-Specific Languages | Quality | Source code analysis",
      "Publication venue": "CEUR Workshop Proceedings",
      "Publication date": "2017-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Pérez-Soler, Sara;De Lara, Juan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85029306835",
      "Primary study DOI": "10.5220/0006465103940401",
      "Title": "Inference-based detection of architectural violations in MVC2",
      "Abstract": "Utilizing software architecture patterns is important for reducing maintenance costs. However, maintaining code according to the constraints defined by the architecture patterns is time-consuming work. As described herein, we propose a technique to detect code fragments that are incompliant to the architecture as finegrained architectural violations. For this technique, the dependence graph among code fragments extracted from the source code and the inference rules according to the architecture are the inputs. A set of candidate components to which a code fragment can be affiliated is attached to each node of the graph and is updated step-by-step. The inference rules express the components' responsibilities and dependency constraints. They remove candidate components of each node that do not satisfy the constraints from the current estimated state of the surrounding code fragment. If the current result does not include the current component, then it is detected as a violation. By defining inference rules for MVC2 architecture and applying the technique to web applications using Play Framework, we obtained accurate detection results.",
      "Keywords": "Architecture pattern | Code smell | Program dependence graph",
      "Publication venue": "ICSOFT 2017 - Proceedings of the 12th International Conference on Software Technologies",
      "Publication date": "2017-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Hayashi, Shinpei;Minami, Fumiki;Saeki, Motoshi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85009792764",
      "Primary study DOI": "10.1109/ISSREW.2016.36",
      "Title": "Antipattern-Based Problem Injection for Assessing Performance and Reliability Evaluation Techniques",
      "Abstract": "A challenging problem with today's increasingly large and distributed software systems is their performance behavior. To help developers avoid or detect mistakes that lead to performance problems, many researchers in software performance engineering have come up with classifications of such problems, called antipatterns. To test the approaches for antipattern detection, data from running systems is required. However, the usefulness of this data is doubtful as it may or may not include manifestations of performance problems. In this paper, we classify existing performance antipatterns w.r.t. their suitability for being injected and, based on this, introduce an extensible tool that allows to inject instances of these antipatterns into existing applications. The approach can be useful for researchers to test and validate their automated runtime problem evaluation and prevention techniques. Using two exemplary performance antipatterns, it is demonstrated that the injection is easily possible and produces feasible, though currently rather clinical results.",
      "Keywords": "Performance and reliability evaluation | Performance antipatterns | Problem injection",
      "Publication venue": "Proceedings - 2016 IEEE 27th International Symposium on Software Reliability Engineering Workshops, ISSREW 2016",
      "Publication date": "2016-12-16",
      "Publication type": "Conference Paper",
      "Authors": "Keck, Philipp;Hoorn, Andre Van;Okanovic, Dusan;Pitakrat, Teerat;Dullmann, Thomas F.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85011317373",
      "Primary study DOI": "10.1109/SBCARS.2016.18",
      "Title": "Identifying code smells with collaborative practices: A controlled experiment",
      "Abstract": "Code smells are often considered as key indicators of software quality degradation. If code smells are not systematically removed from a program, its continuous degradation may lead to either major maintenance effort or the complete redesign of the system. For several reasons, software developers introduce smells in their code as soon as they start to learn programming. If novice developers are ought to become either proficient programmers or skilled code reviewers, they should be early prepared to effectively identify code smells in existing programs. However, effective identification of code smells is often not a non-trivial task in particular to a novice developer working in isolation. Thus, the use of collaborative practices may have the potential to support developers in improving their effectiveness on this task at their early stages of their careers. These practices offer the opportunity for two or more developers analyzing the source code together and collaboratively reason about potential smells prevailing on it. Pair Programming (PP) and Coding Dojo Randori (CDR) are two increasingly adopted practices for improving the effectiveness of developers with limited or no knowledge in software engineering tasks, including code review tasks. However, there is no broad understanding about the impact of these collaborative practices on the effectiveness of code smell identification. This paper presents a controlled experiment involving 28 novice developers, aimed at assessing the effectiveness of collaborative practices in the identification of code smells. We compared PP and CDR with solo programming in order to better distinguish their impact on the effective identification of code smells. Our study is also the first in the literature to observe how novice developers work individually and together to identify smells. Our results suggest that collaborative practices contribute to the effectiveness on the identification of a wide range of code smells. Our findings can also be used in practice to guide educators, researchers or teams on improving detection and training on code smell identification.",
      "Keywords": "Code smells | Collaborative practices | Controlled experiment | Program comprehension | Software degradation",
      "Publication venue": "Proceedings - 2016 10th Brazilian Symposium on Components, Architectures and Reuse Software, SBCARS 2016",
      "Publication date": "2016-12-16",
      "Publication type": "Conference Paper",
      "Authors": "Oliveira, Roberto;Estácio, Bernardo;Garcia, Alessandro;Marczak, Sabrina;Prikladnicki, Rafael;Kalinowski, Marcos;Lucena, Carlos",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85010619005",
      "Primary study DOI": "10.1109/ICRITO.2016.7784998",
      "Title": "Predicting maintainability of open source software using Gene Expression Programming and bad smells",
      "Abstract": "Software maintenance phase of Software Development Lifecycle (SDLC) is the most expensive and complex phase that requires nearly 60-70% of the total project cost. Due to this, many software fails to get repair within real time constraint. Ascribe to technology advancements and changing requirements, software must be well developed and maintained to get adapted. Hence, it is necessary to predict software maintainability in the early phases of the lifecycle so that optimization of resources can be possible and cost can be reduced. Software Maintainability is the quality attribute of software product that explains the ease with which modifications can be performed. The main focus in this study is to propose the use of Gene Expression Programming (GEP) for the software maintainability prediction and measure its performance with various machine leaning techniques such as Decision Tree Forest, Support Vector Machine, Linear regression, Multilayer Perceptron and Radial basis function neural network. The empirical study is conducted with the help of four open source datasets. Eleven bad smells are identified and is considered as maintenance effort. Results of this study show that GEP algorithm performs better than machine learning classifiers; hence it can be used as sound alternative in the prediction of software maintainability. This study would be helpful in achieving better resource allocation hence it will be useful for developers and maintainers.",
      "Keywords": "Bad smell | Gene Expression Programming | Machine learning algorithm | Prediction accuracy measures | Software Maintainability | Software Metrics",
      "Publication venue": "2016 5th International Conference on Reliability, Infocom Technologies and Optimization, ICRITO 2016: Trends and Future Directions",
      "Publication date": "2016-12-15",
      "Publication type": "Conference Paper",
      "Authors": "Tarwani, Sandhya;Chug, Anuradha",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85010809669",
      "Primary study DOI": "10.1109/SCAM.2016.10",
      "Title": "Are my unit tests in the right package?",
      "Abstract": "The software development industry has adopted written and de facto standards for creating effective and maintainable unit tests. Unfortunately, like any other source code artifact, they are often written without conforming to these guidelines, or they may evolve into such a state. In this work, we address a specific type of issues related to unit tests. We seek to automatically uncover violations of two fundamental rules: 1) unit tests should exercise only the unit they were designed for, and 2) they should follow a clear packaging convention. Our approach is to use code coverage to investigate the dynamic behaviour of the tests with respect to the code elements of the program, and use this information to identify highly correlated groups of tests and code elements (using community detection algorithm). This grouping is then compared to the trivial grouping determined by package structure, and any discrepancies found are treated as 'bad smells.' We report on our related measurements on a set of large open source systems with notable unit test suites, and provide guidelines through examples for refactoring the problematic tests.",
      "Keywords": "Clusterization | Code coverage | Community detection | Package hierarchy | Test smells and refactoring | Unit testing",
      "Publication venue": "Proceedings - 2016 IEEE 16th International Working Conference on Source Code Analysis and Manipulation, SCAM 2016",
      "Publication date": "2016-12-12",
      "Publication type": "Conference Paper",
      "Authors": "Balogh, Gergo;Gergely, Tamas;Beszedes, Arpad;Gyimothy, Tibor",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85010628555",
      "Primary study DOI": "10.1109/SATE.2016.11",
      "Title": "Do we have a chance to fix bugs when refactoring code smells?",
      "Abstract": "Code smells are used to describe code structures that may cause detrimental effects on software and should be refactored. Previous studies show that some code smells have significant effect on faults. However, how to refactor code smells to reduce bugs still needs more concern. We investigate the possibility of prioritizing code smell refactoring with the help of fault prediction results. We also investigate the possibility of improving the performance of fault prediction by using code smell detection results. We use Cohen's Kappa statistic to report agreements between results of code smell detections and fault predictions. We use fault prediction result as an indicator to guide code smell refactoring. Our results show that refactoring Blob, Long Parameter List, and Refused Parent Be Request may have a good chance to detect and fix bugs, and some code smells are particularly useful for improving the recall of fault prediction.",
      "Keywords": "Code smell | Fault prediction | Refactoring | Refactoring prioritization",
      "Publication venue": "Proceedings - 2016 International Conference on Software Analysis, Testing and Evolution, SATE 2016",
      "Publication date": "2016-12-09",
      "Publication type": "Conference Paper",
      "Authors": "Ma, Wanwangying;Chen, Lin;Zhou, Yuming;Xu, Baowen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85010483446",
      "Primary study DOI": "10.1109/SATE.2016.10",
      "Title": "Detecting code smells in python programs",
      "Abstract": "As a traditional dynamic language, Python is increasingly used in various software engineering tasks. However, due to its flexibility and dynamism, Python is a particularly challenging language to write code in and maintain. Consequently, Python programs contain code smells which indicate potential comprehension and maintenance problems. With the aim of supporting refactoring strategies to enhance maintainability, this paper describes how to detect code smells in Python programs. We introduce 11 Python smells and describe the detection strategy. We also implement a smell detection tool named Pysmell and use it to identify code smells in five real world Python systems. The results show that Pysmell can detect 285 code smell instances in total with the average precision of 97.7%. It reveals that Large Class and Large Method are most prevalent. Our experiment also implies Python programs may be suffering code smells further.",
      "Keywords": "Code smells | Program maintenance | Python | Refactoring",
      "Publication venue": "Proceedings - 2016 International Conference on Software Analysis, Testing and Evolution, SATE 2016",
      "Publication date": "2016-12-09",
      "Publication type": "Conference Paper",
      "Authors": "Chen, Zhifei;Chen, Lin;Ma, Wanwangying;Xu, Baowen",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85010500631",
      "Primary study DOI": "",
      "Title": "Proceedings - 2016 International Conference on Software Analysis, Testing and Evolution, SATE 2016",
      "Abstract": "The proceedings contain 19 papers. The topics discussed include: an exploratory analysis on software developers' bug-introducing tendency over time; detecting code smells in python programs; how is code recommendation applied in Android development: a qualitative review; dynamically detecting DOM-related atomicity violations in JavaScript with asynchronous call; automatic reproducible crash detection; diagnosis of service failures by probabilistic inference with runtime activity dependences; test case prioritization approach to improving the effectiveness of fault localization; research on relations between software network structure and fault propagation; identify coincidental correct test cases based on fuzzy classification; debugging multithreaded programs as if they were sequential; which is more important for cross-project defect prediction: instance or feature?; lightweight fault localization combining with fault-context; cost-sensitive local collaborative representation for software defect prediction; and distance-based test-suite reduction for efficient testing-based fault localization.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 International Conference on Software Analysis, Testing and Evolution, SATE 2016",
      "Publication date": "2016-12-09",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85006929873",
      "Primary study DOI": "10.1109/JCSSE.2016.7748884",
      "Title": "Automated detection of code smells caused by null checking conditions in Java programs",
      "Abstract": "A null object in Java is occurred when an object is not initialized properly, but a property or a method in the object is called; resulting in the null pointer exception problem and low software quality. To solve this problem, developers usually create a number of null checking conditions in their codes. Duplicated null checking conditions affect the code quality. The duplicated code makes a computer program repeatedly and exhaustively checks for null value, therefore, it reduces the performance of the program. This research involves automatically detecting the code smells caused by null checking conditions in Java by using regular expression. The research specifically focuses on detecting (i) introduce null object, (ii) duplicated code, and (iii) null checking in a string comparison problem. Thirteen Java open-source projects were employed to verify and validate our approach. The detection results were significantly improved when using regular expression comparing to using Abstract Syntax Tree (AST) for detecting code smells caused by null checking conditions in Java programs.",
      "Keywords": "code smell | null detection | null object detection",
      "Publication venue": "2016 13th International Joint Conference on Computer Science and Software Engineering, JCSSE 2016",
      "Publication date": "2016-11-18",
      "Publication type": "Conference Paper",
      "Authors": "Sirikul, Kriangchai;Soomlek, Chitsutha",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85007329149",
      "Primary study DOI": "10.1109/ICACCI.2016.7732136",
      "Title": "Predicting software change-proneness with code smells and class imbalance learning",
      "Abstract": "The objective of this paper is to study the relationship between different types of object-oriented software metrics, code smells and actual changes in software code that occur during maintenance period. It is hypothesized that code smells are indicators of maintenance problems. To understand the relationship between code smells and maintenance problems, we extract code smells in a Java based mobile application called MOBAC. Four versions of MOBAC are studied. Machine learning techniques are applied to predict software change-proneness with code smells as predictor variables. The results of this paper indicate that codes smells are more accurate predictors of change-proneness than static code metrics for all machine learning methods. However, class imbalance techniques did not outperform class balance machine learning techniques in change-proneness prediction. The results of this paper are based on accuracy measures such as F-measure and area under ROC curve.",
      "Keywords": "Class imbalance learning | Code smells | Exception handling smells | Machine learning techniques | Software change-proneness",
      "Publication venue": "2016 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2016",
      "Publication date": "2016-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Kaur, Arvinder;Kaur, Kamaldeep;Jain, Shilpi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84997428839",
      "Primary study DOI": "10.1145/2950290.2950305",
      "Title": "Why we Refactor? Confessions of Github contributors",
      "Abstract": "Refactoring is a widespread practice that helps developers to improve the maintainability and readability of their code. However, there is a limited number of studies empirically investigating the actual motivations behind specific refac-toring operations applied by developers. To fill this gap, we monitored Java projects hosted on GitHub to detect re-cently applied refactorings, and asked the developers to ex-plain the reasons behind their decision to refactor the code. By applying thematic analysis on the collected responses, we compiled a catalogue of 44 distinct motivations for 12 well-known refactoring types. We found that refactoring ac-tivity is mainly driven by changes in the requirements and much less by code smells. Extract Method is the most versatile refactoring operation serving 11 different purposes. Finally, we found evidence that the IDE used by the devel-opers affects the adoption of automated refactoring tools.",
      "Keywords": "Code smells | GitHub | Refactoring | Software evolution",
      "Publication venue": "Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering",
      "Publication date": "2016-11-01",
      "Publication type": "Conference Paper",
      "Authors": "Silva, Danilo;Tsantalis, Nikolaos;Valente, Marco Tulio",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85020757592",
      "Primary study DOI": "10.1109/SEAA.2016.37",
      "Title": "ALM Tool Data Usage in Software Process Metamodeling",
      "Abstract": "Project Management and Software Process Improvement (SPI) are essential parts of software engineering. A multitude of project management techniques and tools, such as Application Lifecycle Management (ALM) ones, are available, and there is an abundance of software methodologies, process metamodels and best practice descriptions. Despite the role of tools in enacting the processes, the underlying domain models used in these two fields - ALM tools and software process metamodels - often significantly differ. This hinders the project execution analysis using the data available from the tools, such as the verification of the project's alignment with a given methodology or the detection of bad practices (anti-patterns). In this paper we describe our work towards enabling such analyses. First we survey the domain models of major process metamodels and tools, and discuss the commonalities and open issues. We then propose a software process metamodel inspired by some of the established metamodels and reflecting the structures of data which can be mined from project management tools. The proposed metamodel is being validated by a prototype process data repository with import interfaces for major ALM tools.",
      "Keywords": "ALM | Metamodel | Project management | Software process",
      "Publication venue": "Proceedings - 42nd Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2016",
      "Publication date": "2016-10-14",
      "Publication type": "Conference Paper",
      "Authors": "Picha, Petr;Brada, Premek",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84917706742",
      "Primary study DOI": "10.1007/s10515-014-0175-x",
      "Title": "An approach to prioritize code smells for refactoring",
      "Abstract": "Code smells are a popular mechanism to find structural design problems in software systems. Consequently, several tools have emerged to support the detection of code smells. However, the number of smells returned by current tools usually exceeds the amount of problems that the developer can deal with, particularly when the effort available for performing refactorings is limited. Moreover, not all the code smells are equally relevant to the goals of the system or its health. This article presents a semi-automated approach that helps developers focus on the most critical problems of the system. We have developed a tool that suggests a ranking of code smells, based on a combination of three criteria, namely: past component modifications, important modifiability scenarios for the system, and relevance of the kind of smell. These criteria are complementary and enable our approach to assess the smells from different perspectives. Our approach has been evaluated in two case-studies, and the results show that the suggested code smells are useful to developers.",
      "Keywords": "Code smells | Design problems | Refactoring | Software evolution",
      "Publication venue": "Automated Software Engineering",
      "Publication date": "2016-09-01",
      "Publication type": "Article",
      "Authors": "Vidal, Santiago A.;Marcos, Claudia;Díaz-Pace, J. Andrés",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84989186743",
      "Primary study DOI": "10.1145/2970276.2970338",
      "Title": "Continuous detection of design flaws in evolving object-oriented programs using incremental multi-pattern matching",
      "Abstract": "Design aws in object-oriented programs may seriously corrupt code quality thus increasing the risk for introducing subtle errors during software maintenance and evolution. Most recent approaches identify design aws in an ad-hoc manner, either focusing on software metrics, locally restricted code smells, or on coarse-grained architectural antipatterns. In this paper, we utilize an abstract program model capturing high-level object-oriented code entities, further augmented with qualitative and quantitative designrelated information such as coupling/cohesion. Based on this model, we propose a comprehensive methodology for specifying object-oriented design aws by means of compound rules integrating code metrics, code smells and antipatterns in a modular way. This approach allows for ef-ficient, automated design-aw detection through incremental multi-pattern matching, by facilitating systematic information reuse among multiple detection rules as well as between subsequent detection runs on continuously evolving programs. Our tool implementation comprises well-known anti-patterns for Java programs. The results of our experimental evaluation show high detection precision, scalability to real-size programs, as well as a remarkable gain in effi-ciency due to information reuse.",
      "Keywords": "Continuous software evolution | Design-aw detection | Objectoriented software architecture",
      "Publication venue": "ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering",
      "Publication date": "2016-08-25",
      "Publication type": "Conference Paper",
      "Authors": "Peldszus, Sven;Kulcsár, Géza;Lochau, Malte;Schulze, Sandro",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84989211424",
      "Primary study DOI": "10.1145/2970276.2970340",
      "Title": "An empirical investigation into the nature of test smells",
      "Abstract": "Test smells have been defined as poorly designed tests and, as reported by recent empirical studies, their presence may negatively affect comprehension and maintenance of test suites. Despite this, there are no available automated tools to support identification and repair of test smells. In this paper, we firstly investigate developers' perception of test smells in a study with 19 participants. The results show that developers generally do not recognize (potentially harmful) test smells, highlighting that automated tools for identifying such smells are much needed. However, to build effective tools, deeper insights into the test smells phenomenon are required. To this aim, we conducted a large-scale empirical investigation aimed at analyzing (i) when test smells occur in source code, (ii) what their survivability is, and (iii) whether their presence is associated with the presence of design problems in production code (code smells). The results indicate that test smells are usually introduced when the corresponding test code is committed in the repository for the first time, and they tend to remain in a system for a long time. Moreover, we found various unexpected relationships between test and code smells. Finally, we show how the results of this study can be used to build e.ective automated tools for test smell detection and refactoring.",
      "Keywords": "Mining Software Repositories | Software Evolution | Test Smells",
      "Publication venue": "ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering",
      "Publication date": "2016-08-25",
      "Publication type": "Conference Paper",
      "Authors": "Tufano, Michele;Palomba, Fabio;Bavota, Gabriele;Di Penta, Massimiliano;Oliveto, Rocco;De Lucia, Andrea;Poshyvanyk, Denys",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84987667374",
      "Primary study DOI": "10.1109/CSIT.2016.7549470",
      "Title": "Metric and rule based automated detection of antipatterns in object-oriented software systems",
      "Abstract": "Patterns are techniques to improve design and enhance reusability. Design patterns are general solutions which are used for common problems in object oriented systems. Code and design smells are symptoms of weak design and development, problems that reside deep in code and reduce the quality of software. The antipattern concept is also introduced as poor solutions to solve recurring problems, even though developers think that they practice a design pattern. It is proven that antipatterns have negative effects on maintainability, flexibility and readability of object oriented software systems. In this research, we propose a metric and a rule based automated antipattern detection system for object oriented software. This system consists of three main mechanisms to detect an antipattern. These mechanisms are 'Metric Analyzer', 'Static Code Analyzer' and 'Filtering Mechanism'. We specified three antipatterns to analyze; namely Blob, Swiss Army Knife and Lava Flow. Thresholds that are used to detect antipatterns are determined considering six reference projects' results and averages of the analyzed project itself. Detection algorithms have been applied on a set of hand-crafted Java classes and accuracy percentages are measured according to the produced results.",
      "Keywords": "antipattern | automated detection | code smell | metric based detection | rule based detection",
      "Publication venue": "Proceedings - CSIT 2016: 2016 7th International Conference on Computer Science and Information Technology",
      "Publication date": "2016-08-23",
      "Publication type": "Conference Paper",
      "Authors": "Aras, Mehmed Taha;Selcuk, Yunus Emre",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84989241809",
      "Primary study DOI": "10.1145/2938503.2938553",
      "Title": "Software analytics in practice: A defect prediction model using code smells",
      "Abstract": "In software engineering, maintainability is related to investigating the defects and their causes, correcting the defects and modifying the system to meet customer requirements. Maintenance is a time consuming activity within the software life cycle. Therefore, there is a need for efficiently organizing the software resources in terms of time, cost and personnel for maintenance activity. One way of efficiently managing maintenance resources is to predict defects that may occur after the deployment. Many researchers so far have built defect prediction models using different sets of metrics such as churn and static code metrics. However, hidden causes of defects such as code smells have not been investigated thoroughly. In this study we propose using data science and analytics techniques on software data to build defect prediction models. In order to build the prediction model we used code smells metrics, churn metrics and combination of churn and code smells metrics. The results of our experiments on two different software companies show that code smells is a good indicator of defect proneness of the software product. Therefore, we recommend that code smells metrics should be used to train a defect prediction model to guide the software maintenance team.",
      "Keywords": "Code smells | Defect predictionmodel | Mining software repositories",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2016-07-11",
      "Publication type": "Conference Paper",
      "Authors": "Soltanifar, Behjat;Akbarinasaji, Shirin;Caglayan, Bora;Bener, Ayse Basar;Filiz, Asli;Kramer, Bryan M.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85077682252",
      "Primary study DOI": "10.1109/TENCON.2019.8929628",
      "Title": "Detecting Code Smells using Deep Learning",
      "Abstract": "A smell in software refers to a symptom introduced in software artifacts such as architecture, design or code. A code smell can potentially cause deeper and serious problems, while dealing with mainly non-functional requirements such as testability, maintainability, extensibility and scalability. The detection of code smell is an essential step in the refactoring process, which facilitates non functional requirements in a software. The existing approaches for detecting code smells use detection rules or standards using a combination of different object-oriented metrics. Although a variety of code smell detection tools have been developed, they still have limitations and constraints in their capabilities. The most well-known object-oriented metrics are considered to identify the presence of smells in software. This paper proposes a deep learning based approach to detect two code smells (Brain Class and Brain Method). The proposed system uses thirty open source Java projects, which are shared by many users in GitHub repositories. The dataset of these Java projects is partitioned into mutually exclusive training and test sets. Our experiments have demonstrated high accuracy results for both the code smells.",
      "Keywords": "Code smell detection tools | Code smells | Deep Learning | Object-oriented metrics",
      "Publication venue": "IEEE Region 10 Annual International Conference, Proceedings/TENCON",
      "Publication date": "2019-10-01",
      "Publication type": "Conference Paper",
      "Authors": "Das, Ananta Kumar;Yadav, Shikhar;Dhal, Subhasish",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84978468668",
      "Primary study DOI": "10.1145/2915970.2915984",
      "Title": "A review-based comparative study of bad smell detection tools",
      "Abstract": "Bad smells are symptoms that something may be wrong in the system design or code. There are many bad smells defined in the literature and detecting them is far from trivial. Therefore, several tools have been proposed to automate bad smell detection aiming to improve software maintainability. However, we lack a detailed study for summarizing and comparing the wide range of available tools. In this paper, we first present the findings of a systematic literature review of bad smell detection tools. As results of this review, we found 84 tools; 29 of them available online for download. Altogether, these tools aim to detect 61 bad smells by relying on at least six different detection techniques. They also target different programming languages, such as Java, C, C++, and C#. Following up the systematic review, we present a comparative study of four detection tools with respect to two bad smells: Large Class and Long Method. This study relies on two software systems and three metrics for comparison: agreement, recall, and precision. Our findings support that tools provide redundant detection results for the same bad smell. Based on quantitative and qualitative data, we also discuss relevant usability issues and propose guidelines for developers of detection tools.",
      "Keywords": "Bad smells | Comparative study | Detection tools | Systematic literature review",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2016-06-01",
      "Publication type": "Conference Paper",
      "Authors": "Fernandes, Eduardo;Oliveira, Johnatan;Vale, Gustavo;Paiva, Thanis;Figueiredo, Eduardo",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85049733358",
      "Primary study DOI": "10.1109/SANER.2018.8330265",
      "Title": "Keep it simple: Is deep learning good for linguistic smell detection?",
      "Abstract": "Deep neural networks is a popular technique that has been applied successfully to domains such as image processing, sentiment analysis, speech recognition, and computational linguistic. Deep neural networks are machine learning algorithms that, in general, require a labeled set of positive and negative examples that are used to tune hyper-parameters and adjust model coefficients to learn a prediction function. Recently, deep neural networks have also been successfully applied to certain software engineering problem domains (e.g., bug prediction), however, results are shown to be outperformed by traditional machine learning approaches in other domains (e.g., recovering links between entries in a discussion forum). In this paper, we report our experience in building an automatic Linguistic Antipattern Detector (LAPD) using deep neural networks. We manually build and validate an oracle of around 1,700 instances and create binary classification models using traditional machine learning approaches and Convolutional Neural Networks. Our experience is that, considering the size of the oracle, the available hardware and software, as well as the theory to interpret results, deep neural networks are outperformed by traditional machine learning algorithms in terms of all evaluation metrics we used and resources (time and memory). Therefore, although deep learning is reported to produce results comparable and even superior to human experts for certain complex tasks, it does not seem to be a good fit for simple classification tasks like smell detection. Researchers and practitioners should be careful when selecting machine learning models for the problem at hand.",
      "Keywords": "Antipattern Detection | Con-volutional Neural Networks | Machine Learning",
      "Publication venue": "25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings",
      "Publication date": "2018-04-02",
      "Publication type": "Conference Paper",
      "Authors": "Fakhoury, Sarah;Arnaoudova, Venera;Noiseux, Cedric;Khomh, Foutse;Antoniol, Giuliano",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84991503926",
      "Primary study DOI": "10.1145/2962695.2962716",
      "Title": "On evaluating the impact of the refactoring of architectural problems on software quality",
      "Abstract": "We can improve software quality in different ways and by removing different kinds of problems. In this paper, we focus our attention on architectural problems, as architectural smells or antipatterns represent, we remove some of these problems through refactoring steps and we check the impact that the refactoring has on different quality metrics. In particular, we focus our attention on some Quality Indexes computed by four tools. These tools are used also for the detection of the architectural problems. We present the results and outline different issues related to the impact of the refactoring of these architectural problems on the Quality Indexes and the difficulties in the choice of the problems to be refactored.",
      "Keywords": "Antipatterns | Architectural Smells | Refactoring | Software Quality Evaluation",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2016-05-24",
      "Publication type": "Conference Paper",
      "Authors": "Fontana, Francesca Arcelli;Roveda, Riccardo;Vittori, Stefano;Metelli, Andrea;Saldarini, Stefano;Mazzei, Francesco",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84991000274",
      "Primary study DOI": "10.1109/SANER.2016.84",
      "Title": "Anti-pattern and code smell false positives: Preliminary conceptualisation and classification",
      "Abstract": "Anti-patterns and code smells are archetypes used for describing software design shortcomings that can negatively affect software quality, in particular maintainability. Tools, metrics and methodologies have been developed to identify these archetypes, based on the assumption that they can point at problematic code. However, recent empirical studies have shown that some of these archetypes are ubiquitous in real world programs, and many of them are found not to be as detrimental to quality as previously conjectured. We are therefore interested in revisiting common anti-patterns and code smells, and building a catalogue of cases that constitute candidates for \"false positives\". We propose a preliminary classification of such false positives with the aim of facilitating a better understanding of the effects of anti-patterns and code smells in practice. We hope that the development and further refinement of such a classification can support researchers and tool vendors in their endeavour to develop more pragmatic, context-relevant detection and analysis tools for anti-patterns and code smells.",
      "Keywords": "Anti-patterns | Code smells | Conceptual framework | Detection accuracy | False positives",
      "Publication venue": "2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2016",
      "Publication date": "2016-05-20",
      "Publication type": "Conference Paper",
      "Authors": "Fontana, Francesca Arcelli;Dietrich, Jens;Walter, Bartosz;Yamashita, Aiko;Zanoni, Marco",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026637464",
      "Primary study DOI": "10.1145/2889160.2889272",
      "Title": "When more heads are better than one?: Understanding and improving collaborative identification of code smells",
      "Abstract": "Code smells are program structures that often indicate software design problems. Their efficient identification is required in order to ensure software longevity. However, the identification of code smells often cannot be performed in isolation by a single developer. This task might require the knowledge of various program parts, which are better understood by different developers. However, there is little guidance to support software teams on efficient identification of code smells. In this research, we investigate how to improve efficiency on the collaborative identification of code smells. Our investigation is based on a set of controlled experiments conducted with more than 58 novice and professional developers. Our preliminary results suggest the use of collaborative practices significantly increases the efficiency of code smell identification. We also compiled a set of guidelines and heuristics to support an effective collaborative strategy for code smell identification.",
      "Keywords": "Code smells | Collaborative practices | Controlled experiment",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2016-05-14",
      "Publication type": "Conference Paper",
      "Authors": "Oliveira, Roberto",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84983535038",
      "Primary study DOI": "10.1145/2897073.2897100",
      "Title": "An empirical study of the performance impacts of android code smells",
      "Abstract": "Android code smells are bad implementation practices within Android applications (or apps) that may lead to poor software quality, in particular in terms of performance. Yet, performance is a main software quality concern in the development of mobile apps. Correcting Android code smells is thus an important activity to increase the performance of mobile apps and to provide the best experience to mobile end-users while considering the limited constraints of mobile devices (e.g., CPU, memory, battery). However, no empirical study has assessed the positive performance impacts of correcting mobile code smells. In this paper, we therefore conduct an empirical study focusing on the individual and combined performance impacts of three Android performance code smells (namely, Internal Getter/Setter, Member Ignoring Method, and HashMap Usage) on two open source Android apps. To perform this study, we use the Paprika toolkit to detect these three code smells in the analyzed apps, and we derive four versions of the apps by correcting each detected smell independently, and all of them. Then, we evaluate the performance of each version on a common user scenario test. In particular, we evaluate the UI and memory performance using the following metrics: frame time, number of delayed frames, memory usage, and number of garbage collection calls. Our results show that correcting these Android code smells effectively improve the UI and memory performance. In particular, we observe an improvement up to 12.4% on UI metrics when correcting Member Ignoring Method and up to 3.6% on memory-related metrics when correcting the three Android code smells. We believe that developers can benefit from these results to guide their refactoring, and thus improve the quality of their mobile apps.",
      "Keywords": "Android | Code smells | Metrics | Mobile computing | Performance",
      "Publication venue": "Proceedings - International Conference on Mobile Software Engineering and Systems, MOBILESoft 2016",
      "Publication date": "2016-05-14",
      "Publication type": "Conference Paper",
      "Authors": "Hecht, Geoffrey;Moha, Naouel;Rouvoy, Romain",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026680508",
      "Primary study DOI": "10.1145/2889160.2891036",
      "Title": "Safely evolving preprocessor-based configurable systems",
      "Abstract": "Since the 70s, the C preprocessor is still widely used in practice in a numbers of projects, including Apache, Linux, and Libssh, to tailor systems to different platforms. To better understand the C preprocessor challenges, we conducted 40 interviews and a survey among 202 developers. We found that developers deal with three common problems: configuration-related bugs, combinatorial testing, and code comprehension. To safely evolve preprocessor-based configurable systems, we proposed strategies to detect preprocessor-related bugs and bad smells, and a set of 16 refactorings to remove bad smells. To better deal with exponential configuration spaces, we compared 10 sampling algorithms with respect to effort (i.e., number of configurations to test) and bug-detection capabilities (i.e., number of bugs detected in the sampled configurations). Based on the results, we proposed a sampling algorithm with a useful balance between effort and bug-detection capability. By evaluating the proposed solution using 40 popular projects, we found 131 preprocessor-related bugs and more than 5K opportunities to apply the refactorings in practice.",
      "Keywords": "",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2016-05-14",
      "Publication type": "Conference Paper",
      "Authors": "Medeiros, Flávio",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84971383866",
      "Primary study DOI": "10.1145/2884781.2884879",
      "Title": "Finding and analyzing compiler warning defects",
      "Abstract": "Good compiler diagnostic warnings facilitate software development as they indicate likely programming mistakes or code smells. However, due to compiler bugs, the warnings may be erroneous, superfluous or missing, even for mature production compilers like GCC and Clang. In this paper, we (1) propose the first randomized differential testing technique to detect compiler warning defects and (2) describe our extensive evaluation in finding warning defects in widely-used C compilers. At the high level, our technique starts with generating random programs to trigger compilers to emit a variety of compiler warnings, aligns the warnings from different compilers, and identifies inconsistencies as potential bugs. We develop effective techniques to overcome three specific challenges: (1) How to generate random programs, (2) how to align textual warnings, and (3) how to reduce test programs for bug reporting? Our technique is very effective-we have found and reported 60 bugs for GCC (38 confirmed, assigned or fixed) and 39 for Clang (14 confirmed or fixed). This case study not only demonstrates our technique's effectiveness, but also highlights the need to continue improving compilers' warning support, an essential, but rather neglected aspect of compilers.",
      "Keywords": "",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2016-05-14",
      "Publication type": "Conference Paper",
      "Authors": "Sun, Chengnian;Le, Vu;Su, Zhendong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84983326764",
      "Primary study DOI": "10.1145/2889160.2889260",
      "Title": "Architectural-based speculative analysis to predict bugs in a software system",
      "Abstract": "Over time, a software system's code and its underlying design tend to decay steadily and, in turn, to complicate the system's maintenance. In order to address that phenomenon, many researchers tried to help engineers predict parts of a system that are most likely to create problems while or even before they are modifying the system. Problems that creep into a system may manifest themselves as bugs, in which case engineers have no choice but to fix them or develop workarounds. However, these problems may also be more subtle, such as code clones, circular dependencies among system elements, very large APIs, individual elements that implement multiple diffuse concerns, etc. Even though such architectural and code \"smells\" may not crash a system outright, they impose real costs in terms of engineers' time and effort, as well as system correctness and performance. Along the time, implicit problems may be revealed as explicit problems. However, most current techniques predict explicit problems of a system only based on explicit problems themselves. Our research takes a further step by using implicit problems, e.g., architectural- and code-smells, in combination with explicit problems to provide an accurate, systematic and in depth approach to predict potential system problems, particularly bugs.",
      "Keywords": "Architectural decay | Architectural-based analysis | Bug prediction | Speculative analysis",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2016-05-14",
      "Publication type": "Conference Paper",
      "Authors": "Le, Duc;Medvidovic, Nenad",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84974604506",
      "Primary study DOI": "10.1145/2901739.2901742",
      "Title": "A large-scale empirical study on self-admitted technical debt",
      "Abstract": "Technical debt is a metaphor introduced by Cunningham to indicate \"not quite right code which we postpone making it right\". Examples of technical debt are code smells and bug hazards. Several techniques have been proposed to detect different types of technical debt. Among those, Potdar and Shihab defined heuristics to detect instances of self-admitted technical debt in code comments, and used them to perform an empirical study on five software systems to investigate the phenomenon. Still, very little is known about the diffusion and evolution of technical debt in software projects. This paper presents a differentiated replication of the work by Potdar and Shihab. We run a study across 159 software projects to investigate the diffusion and evolution of self-admitted technical debt and its relationship with software quality. The study required the mining of over 600K commits and 2 Billion comments as well as a qualitative analysis performed via open coding. Our main findings show that self-admitted technical debt (i) is diffused, with an average of 51 instances per system, (ii) is mostly represented by code (30%), defect, and requirement debt (20% each), (iii) increases over time due to the introduction of new instances that are not fixed by developers, and (iv) even when fixed, it survives long time (over 1,000 commits on average) in the system.",
      "Keywords": "Empirical software engineering | Mining software repositories | Technical debt",
      "Publication venue": "Proceedings - 13th Working Conference on Mining Software Repositories, MSR 2016",
      "Publication date": "2016-05-14",
      "Publication type": "Conference Paper",
      "Authors": "Bavota, Gabriele;Russo, Barbara",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84973463344",
      "Primary study DOI": "10.1145/2896982.2896991",
      "Title": "Model level design pattern instance detection using answer set programming",
      "Abstract": "Software engineering is becoming increasingly model-centric. Engineers are using models more within projects and their models are growing in complexity. A challenge facing the modeling community is evaluation of these models. One technique for software evaluation is detecting instances of established \"good\" or \"bad\" solutions in a system, often termed design patterns or antipatterns, respectively. Most approaches require implemented code for detection. However, this precludes early-stage analysis, and the evaluation of purely or mostly model-centric systems. In this position paper, we introduce a detection technique that uses answer set programming to find occurrences of patterns within sets of structural and behavioral models. We represent the patterns as rules and the structural and behavioral system models as facts, requiring both model types since some patterns specify both. We provide an overview of our proposed approach, contrast existing work, and present discussion points on its impact on model evaluation and anticipated challenges.",
      "Keywords": "Answer set programming | Antipatterns | Design patterns | Model evaluation | Model patterns | Model quality",
      "Publication venue": "Proceedings - 8th International Workshop on Modeling in Software Engineering, MiSE 2016",
      "Publication date": "2016-05-14",
      "Publication type": "Conference Paper",
      "Authors": "Luitel, Gaurab;Stephan, Matthew;Inclezan, Daniela",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85018456236",
      "Primary study DOI": "10.1145/2889160.2889182",
      "Title": "How surveys, tutors, and software help to assess Scrum adoption in a classroom software engineering project",
      "Abstract": "Agile methods are best taught in a hands-on fashion in realistic projects. The main challenge in doing so is to assess whether students apply the methods correctly without requiring complete supervision throughout the entire project. This paper presents experiences from a classroom project where 38 students developed a single system using a scaled version of Scrum. Surveys helped us to identify which elements of Scrum correlated most with student satisfaction or posed the biggest challenges. These insights were augmented by a team of tutors, which accompanied main meetings throughout the project to provide feedback to the teams, and captured impressions of method application in practice. Finally, we performed a post-hoc, tool-supported analysis of collaboration artifacts to detect concrete indicators for anti-patterns in Scrum adoption. Through the combination of these techniques we were able to understand how students implemented Scrum in this course and which elements require further lecturing and tutoring in future iterations. Automated analysis of collaboration artifacts proved to be a promising addition to the development process that could potentially reduce manual efforts in future courses and allow for more concrete and targeted feedback, as well as more objective assessment.",
      "Keywords": "Classroom project | Collaboration analysis | Multi-level assessment | Scrum",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2016-05-14",
      "Publication type": "Conference Paper",
      "Authors": "Matthies, Christoph;Kowark, Thomas;Richly, Keven;Uflacker, Matthias;Plattner, Hasso",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84975841224",
      "Primary study DOI": "10.1145/2851613.2851968",
      "Title": "A graph-based approach to detect unreachable methods in Java software",
      "Abstract": "In this paper, we have defined a static approach named DUM (Detecting Unreachable Methods) that works on Java byte-code and detects unreachable methods by traversing a graph-based representation of the software to be analyzed. To assess the validity of our approach, we have implemented it in a prototype software system. Both our approach and prototype have been validated on four open-source software. Results have shown the correctness, the completeness, and the accuracy of the methods that our solution detected as unreachable. We have also compared our solution with: JTombstone and Google CodePro AnalytiX. This comparison suggested that DUM outperforms baselines.",
      "Keywords": "Bad smells | Software maintenance | Unreachable methods",
      "Publication venue": "Proceedings of the ACM Symposium on Applied Computing",
      "Publication date": "2016-04-04",
      "Publication type": "Conference Paper",
      "Authors": "Romano, Simone;Scanniello, Giuseppe;Sartiani, Carlo;Risi, Michele",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84968649853",
      "Primary study DOI": "10.1142/S0218194016500212",
      "Title": "Aspect Oriented Re-engineering of Legacy Software Using Cross-Cutting Concern Characterization and Significant Code Smells Detection",
      "Abstract": "Although object-oriented programming (OOP) methodologies immensely promote reusable and well-factored decomposition of complex source code, legacy software systems often show symptoms of deteriorating design over time due to lack of maintenance. Software systems may have different business and application contexts, but most of these systems require similar maintenance mechanism of understanding, analysis and transformation. As a consequence, intensive re-engineering efforts based on the model driven approach can be effective ensuring that best practices are followed during maintenance and eventually reducing the development cost. In this paper, we suggest detailed framework of re-engineering which includes: (i) rigorous and automated source code analysis technique for identification, characterization and prioritization of most prominent and threatening design flaws in legacy software, (ii) migration of existing the code to aspect-oriented programming (AOP) code by exploiting current state of art for aspect mining mechanism and incorporating behavioral knowledge of cross-cutting concerns. To exemplify how the approach works a case study has been conducted to experimentally validate the idea and analyze the effect of process on specific software quality spectrum. An explicit analysis of prevalent work on the subject and their critical reviews are also presented to further enhance the recognition of proposed re-engineering framework.",
      "Keywords": "Aspect-oriented programming | characterization of cross-cutting concerns | code smells | formal concept analysis | object-oriented programming | re-factoring",
      "Publication venue": "International Journal of Software Engineering and Knowledge Engineering",
      "Publication date": "2016-04-01",
      "Publication type": "Article",
      "Authors": "Shaikh, Mohsin;Lee, Chan Gun",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84966533888",
      "Primary study DOI": "",
      "Title": "Proceedings of 2015 International Conference on Data and Software Engineering, ICODSE 2015",
      "Abstract": "The proceedings contain 44 papers. The topics discussed include: a metamodel for disaster risk models; a proposal of software maintainability model using code smell measurement; aircraft anomaly detection using algoritmic model and data model trained on FQDA data; confidentiality and privacy information security risk assessment for Android-based mobile devices; decision tree modeling for predicting research productivity of university faculty members; defending one-time pad cryptosystems from denial-of-service attacks; deriving labeled training data for topic link detection by alternating words; fake smile detection using linear support vector machine; grid-based histogram of oriented optical flow for analyzing movements on video data; implementation and validation of business process deviation detection framework; multiple MapReduce and derivative projected database : new approach for supporting prefixspan scalability; and optimization of real-time multiple-face detection in the classroom using Adaboost algorithm.",
      "Keywords": "",
      "Publication venue": "Proceedings of 2015 International Conference on Data and Software Engineering, ICODSE 2015",
      "Publication date": "2016-03-18",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019032286",
      "Primary study DOI": "10.1145/2851553.2851563",
      "Title": "Automatically detecting \"excessive dynamic memory allocations\" software performance anti-pattern",
      "Abstract": "This paper presents a methodology for automatically detecting the excessive dynamic memory allocation software performance antipattern, which is implemented in a tool named Excessive Memory Allocation Detector (EMAD). To the best of author's knowledge, EMAD is the first attempt to detect excessive dynamic memory allocation anti-pattern without human intervention. EMAD uses dynamic binary instrumentation and exploratory data analysis to determine if an application (or middleware) exhibits excessive dynamic memory allocations. Unlike traditional approaches, EMAD's technique does not rely on source code analysis. Results of applying EMAD to several open-source projects show that EMAD can detect the excessive dynamic memory allocations antipattern correctly. The results also show that application performance improves when the detected excessive dynamic memory allocations are resolved.",
      "Keywords": "Detection | Dynamic binary instrumentation | Excessive dynamic memory allocation | Software performance antipattern",
      "Publication venue": "ICPE 2016 - Proceedings of the 7th ACM/SPEC International Conference on Performance Engineering",
      "Publication date": "2016-03-12",
      "Publication type": "Conference Paper",
      "Authors": "Peiris, Manjula;Hill, James H.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84962788202",
      "Primary study DOI": "10.17485/ijst/2016/v9i10/85110",
      "Title": "Neural network based refactoring area identification in Software System with object oriented metrics",
      "Abstract": "Objectives of the Study (a) To study previously designed models for identification of refactoring area in Object Oriented Software Systems. (b) To design a general framework or model that helps to easily identify the software code smells for a good quality of coding. (c) To identify the bad smells in the code with a design of neural network based model with the help of object-oriented metrics and further to predict the performance of the proposed model using various evaluation parameters of confusion matrix. Analysis/Methods: In this study, two different versions of Rhino (1.7r1 and 1.7r2) were taken as dataset. Object-Oriented metrics were taken as input data and the probability factor (occurrence or non-occurrence of a bad smell as output. Presence of a bad smell was considered as 1 and 0 means absence of bad smell. If there was at least one bad smell present in the code in a class, it was marked as smelly class. The tool used to extract the databases for collected object-oriented metrics and bad smells of these Rhino versions is PTIDEJ. Further, the data was tested on neural networks for different epochs to predict their performance. Findings: a) Bad Smell Analysis: Twelve design smells were considered to detect the presence of bad smell in code. If there was at least one bad smell present in the code in a class, it was marked as smelly class. b) Neural Network Model Table: Weight and bias factor for various predictors were calculated for different epochs (500, 1000, and 2000). It shows the weights assigned from input layer to hidden layer and from hidden layer to output neurons layer. After the training, the weights were tested on various datasets. C) Performance Tables and Graphs: In this, the Neural network proposed model was trained using different number of epochs to examine if the number of epochs used in training has any impact on the results or not. Further, the results for the accuracy of these models were shown. Novelty/Improvement: When the data was highly trained then the results were better. When the data was trained with 500 epochs, it was suitable for only with-in company projects but when the data was more trained than the model was also appropriate for cross projects. It was seen that when the data was trained with 1000 and 2000 epochs, the results of the proposed model were improved.",
      "Keywords": "Artificial Neural Networks (ANN) | Bad smells | Logistic regression | Object oriented metrics | Refactoring | Software maintainability",
      "Publication venue": "Indian Journal of Science and Technology",
      "Publication date": "2016-03-01",
      "Publication type": "Article",
      "Authors": "Kaur, Jaspreet;Singh, Satwinder",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84945242383",
      "Primary study DOI": "10.1016/j.eswa.2015.09.023",
      "Title": "Co-changing code volume prediction through association rule mining and linear regression model",
      "Abstract": "Code smells are symptoms in the source code that indicate possible deeper problems and may serve as drivers for code refactoring. Although effort has been made on identifying divergent changes and shotgun surgeries, little emphasis has been put on predicting the volume of co-changing code that appears in the code smells. More specifically, when a software developer intends to perform a particular modification task on a method, a predicted volume of code that will potentially be co-changed with the method could be considered as significant information for estimating the modification effort. In this paper, we propose an approach to predicting volume of co-changing code affected by a method to be modified. The approach has the following key features: co-changing methods can be identified for detecting divergent changes and shotgun surgeries based on association rules mined from change histories; and volume of co-changing code affected by a method to be modified can be predicted through a derived fitted regression line with t-test based on the co-changing methods identification results. The experimental results show that the success rate of co-changing methods identification is 82% with a suggested threshold, and the numbers of correct identifications would not be influenced by the increasing number of commits as a project continuously evolves. Additionally, the mean absolute error of co-changing code volume predictions is 133 lines of code which is 95.3% less than the one of a naive approach.",
      "Keywords": "Co-changing code volume prediction | Co-changing methods identification",
      "Publication venue": "Expert Systems with Applications",
      "Publication date": "2016-03-01",
      "Publication type": "Article",
      "Authors": "Lee, Shin Jie;Lo, Li Hsiang;Chen, Yu Cheng;Shen, Shi Min",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84963765824",
      "Primary study DOI": "10.1109/SCCC.2015.7416572",
      "Title": "JSpIRIT: A flexible tool for the analysis of code smells",
      "Abstract": "Code smells are a popular mechanism to identify structural design problems in software systems. Since it is generally not feasible to f x all the smells arising in the code, some of them are often postponed by developer s to be resolved in the future. One reason for this decision is that the improvement of the code structure, to achieve modifability goals, requires extra effort from developer s. Therefore, they might not always spend this additional effort, particularly when they are focused on delivering customer-visible features. This postponement of code smells are seen as a source of technical debt. Furthermore, not all the code smells may be urgent to f x in the context of the system's modifability and business goals. While there are a number of tools to detect smells, they do not allow developer s to discover the most urgent smells according to their goals. In this article, we present a fexible tool to prioritize technical debt in the form of code smells. The tool is fexible to allow developer s to add new smell detection strategies and to prioritize smells, and groups of smells, based on the confguration of their manifold criteria. To illustrate this fexibility, we present an application example of our tool. The results suggest that our tool can be easily extended to be aligned with the developer's goals.",
      "Keywords": "",
      "Publication venue": "Proceedings - International Conference of the Chilean Computer Science Society, SCCC",
      "Publication date": "2016-02-23",
      "Publication type": "Conference Paper",
      "Authors": "Vidal, Santiago;Vazquez, Hernan;Diaz-Pace, J. Andres;Marcos, Claudia;Garcia, Alessandro;Oizumi, Willian",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84959352795",
      "Primary study DOI": "10.1007/s10796-014-9528-z",
      "Title": "Using model transformation to refactor use case models based on antipatterns",
      "Abstract": "Use Case modeling is a popular technique for documenting functional requirements of software systems. Refactoring is the process of enhancing the structure of a software artifact without changing its intended behavior. Refactoring, which was first introduced for source code, has been extended for use case models. Antipatterns are low quality solutions to commonly occurring design problems. The presence of antipatterns in a use case model is likely to propagate defects to other software artifacts. Therefore, detection and refactoring of antipatterns in use case models is crucial for ensuring the overall quality of a software system. Model transformation can greatly ease several software development activities including model refactoring. In this paper, a model transformation approach is proposed for improving the quality of use case models. Model transformations which can detect antipattern instances in a given use case model, and refactor them appropriately are defined and implemented. The practicability of the approach is demonstrated by applying it on a case study that pertains to biodiversity database system. The results show that model transformations can efficiently improve quality of use case models by saving time and effort.",
      "Keywords": "Antipatterns | Model transformation | Refactoring | UML | Use case modeling quality attributes | Use cases",
      "Publication venue": "Information Systems Frontiers",
      "Publication date": "2016-02-01",
      "Publication type": "Article",
      "Authors": "Khan, Yasser A.;El-Attar, Mohamed",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84891688656",
      "Primary study DOI": "10.1109/ICSM.2013.56",
      "Title": "Code smell detection: Towards a machine learning-based approach",
      "Abstract": "Several code smells detection tools have been developed providing different results, because smells can be subjectively interpreted and hence detected in different ways. Usually the detection techniques are based on the computation of different kinds of metrics, and other aspects related to the domain of the system under analysis, its size and other design features are not taken into account. In this paper we propose an approach we are studying based on machine learning techniques. We outline some common problems faced for smells detection and we describe the different steps of our approach and the algorithms we use for the classification. © 2013 IEEE.",
      "Keywords": "Code smells detection | Machine learning techniques",
      "Publication venue": "IEEE International Conference on Software Maintenance, ICSM",
      "Publication date": "2013-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Fontana, Francesca Arcelli;Zanoni, Marco;Marino, Alessandro;Mäntylä, Mika V.",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84963827907",
      "Primary study DOI": "10.1109/ASE.2015.46",
      "Title": "Tracking the software quality of android applications along their evolution",
      "Abstract": "Mobile apps are becoming complex software systems that must be developed quickly and evolve continuously to fit new user requirements and execution contexts. However, addressing these requirements may result in poor design choices, also known as antipatterns, which may incidentally degrade software quality and performance. Thus, the automatic detection and tracking of antipatterns in this apps are important activities in order to ease both maintenance and evolution. Moreover, they guide developers to refactor their applications and thus, to improve their quality. While antipatterns are well-known in object-oriented applications, their study in mobile applications is still in its infancy. In this paper, we analyze the evolution of mobile apps quality on 3, 568 versions of 106 popular Android applications downloaded from the Google Play Store. For this purpose, we use a tooled approach, called PAPRIKA, to identify 3 object-oriented and 4 Android-specific antipatterns from binaries of mobile apps, and to analyze their quality along evolutions.",
      "Keywords": "Android | Antipattern | Mobile app | Software quality",
      "Publication venue": "Proceedings - 2015 30th IEEE/ACM International Conference on Automated Software Engineering, ASE 2015",
      "Publication date": "2016-01-04",
      "Publication type": "Conference Paper",
      "Authors": "Hecht, Geoffrey;Benomar, Omar;Rouvoy, Romain;Moha, Naouel;Duchien, Laurence",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85019274375",
      "Primary study DOI": "10.15866/irecos.v11i11.10590",
      "Title": "Automatic detection of bad smells from code changes",
      "Abstract": "Code bad smells are indicators of code bad design that affects its quality attributes like understandability and readability. This effect has a direct impact on future maintenance tasks and code changing activities. Badly written code is hard to understand, change and test. The goal of this paper is to present an approach, supported by a tool, to automatically detect bad smells from code changes on the fly during code changing activities. An Eclipse plug-in tool (JFly) is developed to realize the approach. The tool analyzes code changes, detects bad smells and notifies developers about the location and the type of the detected bad smell. Nine bad smells are detected by JFly. A set of bad smells rules is defined, based on software metrics, to determine if code changes have one or more bad smells. JFly has been tested by different scenarios to evaluate its performance, usability and correctness. Results showed that JFly is very fast, easy to use and achieved high recall and precision values. By providing the JFly tool, developers are kept aware about code bad smells as soon as they implemented. As a result, the code is kept clean without the need to go over it periodically to check bad smells which consumes time and effort.",
      "Keywords": "Automatic tool | Code bad smells | Software maintenance",
      "Publication venue": "International Review on Computers and Software",
      "Publication date": "2016-01-01",
      "Publication type": "Article",
      "Authors": "Hammad, Maen;Labadi, Asma",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84961616362",
      "Primary study DOI": "10.1109/ESEM.2015.7321194",
      "Title": "Code Bad Smell Detection through Evolutionary Data Mining",
      "Abstract": "The existence of code bad smell has a severe impact on the software quality. Numerous researches show that ignoring code bad smells can lead to failure of a software system. Thus, the detection of bad smells has drawn the attention of many researchers and practitioners. Quite a few approaches have been proposed to detect code bad smells. Most approaches are solely based on structural information extracted from source code. However, we have observed that some code bad smells have the evolutionary property, and thus propose a novel approach to detect three code bad smells by mining software evolutionary data: duplicated code, shotgun surgery, and divergent change. It exploits association rules mined from change history of software systems, upon which we define heuristic algorithms to detect the three bad smells. The experimental results on five open source projects demonstrate that the proposed approach achieves higher precision, recall and F-measure.",
      "Keywords": "bad smell detection | data mining | software evolutionary history",
      "Publication venue": "International Symposium on Empirical Software Engineering and Measurement",
      "Publication date": "2015-11-05",
      "Publication type": "Conference Paper",
      "Authors": "Fu, Shizhe;Shen, Beijun",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84886420447",
      "Primary study DOI": "10.1109/ICSE.2013.6606670",
      "Title": "Semantic smells and errors in access control models: A case study in PHP",
      "Abstract": "Access control models implement mechanisms to restrict access to sensitive data from unprivileged users. Access controls typically check privileges that capture the semantics of the operations they protect. Semantic smells and errors in access control models stem from privileges that are partially or totally unrelated to the action they protect. This paper presents a novel approach, partly based on static analysis and information retrieval techniques, for the automatic detection of semantic smells and errors in access control models. Investigation of the case study application revealed 31 smells and 2 errors. Errors were reported to developers who quickly confirmed their relevance and took actions to correct them. Based on the obtained results, we also propose three categories of semantic smells and errors to lay the foundations for further research on access control smells in other systems and domains. © 2013 IEEE.",
      "Keywords": "access control models | code smells | information retrieval | security | static analysis",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2013-10-30",
      "Publication type": "Conference Paper",
      "Authors": "Gauthier, Francois;Merlo, Ettore",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84979642838",
      "Primary study DOI": "10.5220/0005891602100215",
      "Title": "Automatic Refactoring of component-based software by detecting and eliminating bad smells a search-based approach",
      "Abstract": "Refactoring has been proposed as a de facto behavior-preserving mean to eliminate bad smells. However manually determining and performing useful refactorings is a though challenge because seemingly useful refactorings can improve some aspect of a software while making another aspect worse. Therefore it has been proposed to view object-oriented automated refactoring as a search-based technique. Nevertheless the review of the literature shows that automated refactoring of component-based software has not been investigated yet. Recently a catalogue of component-relevant bad smells has been proposed in the literature but there is a lack of component-relevant refactorings. In this paper we propose detection rules for component-relevant bad smells as well as a catalogue of component-relevant refactorings. Then we rely on these two elements to propose a search-based approach for automated refactoring of component-based software systems by detecting and eliminating bad smells. Finally, we experiment our approach on a medium-sized component-based software and we assess the efficieny and accuracy of our approach.",
      "Keywords": "Automatic refactoring | Bad smells | Component-based software engineering | Genetic algorithm | Search-based software engineering",
      "Publication venue": "ENASE 2016 - Proceedings of the 11th International Conference on Evaluation of Novel Software Approaches to Software Engineering",
      "Publication date": "2016-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Kebir, Salim;Borne, Isabelle;Meslati, Djamel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84988981993",
      "Primary study DOI": "",
      "Title": "Tool to measure and refactor complex UML models",
      "Abstract": "Modifying and maintaining the source code of existing software products take the majority of time in the software development lifecycle. The same problem appears when the software is designed in a modeling environment with UML. Therefore, providing the same toolchain that already exists in the area of source code based development is required for UML modeling as well. This toolchain includes not just editors, but debugging tools, version controlling systems, static analysers and refactoring tools as well. In this paper we introduce a refactoring tool for UML models built within the Papyrus framework. Beside the transformations, the tool is able to measure the complexity of UML models and propose transformations to reduce the complexity.",
      "Keywords": "Bad smell detection | EMF | Metrics | Model quality | Papyrus | Refactoring | UML model",
      "Publication venue": "CEUR Workshop Proceedings",
      "Publication date": "2016-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Ambrus, Tamás;Tóth, Melinda;Asztalos, Domonkos;Borbély, Zsófia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013173229",
      "Primary study DOI": "",
      "Title": "Review on software cloning and clone detection",
      "Abstract": "Today in this modern era of science, technology has developed its roots deep into the world, where most of the things are done with the help of automated tools and techniques to do more work in less time and with great efficiency. This is the case with software industry also. In software industries, a technique called software cloning has come into existence. Software cloning has various broad aspects, out of them; the shadow of light is thrown on one of the aspect called code cloning. In code cloning, some significant quantity of code as desired by the user is taken from some pre-existing code and copied into some another code. In short, it is a kind of copying or pasting of code where some desired code is copied from one source and pasted into another source. The code in which pasting is done is called the replica of original code. In other words, the code which contains the replicated code is called the clone. It leads to the fast development of the software systems. But despite of having so many boons like time saving technique, fast development of software systems, reuse, etc; it has some drawbacks as well like bug propagation.",
      "Keywords": "Clone | Code smells | Hybrid | Match detection | Plagiarism",
      "Publication venue": "International Journal of Control Theory and Applications",
      "Publication date": "2016-01-01",
      "Publication type": "Article",
      "Authors": "Kaur, Manjit;Kaur, Sandeep;Sohal, Bhavneesh",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84989808367",
      "Primary study DOI": "",
      "Title": "8th International Symposium on Search Based Software Engineering, SSBSE 2016",
      "Abstract": "The proceedings contain 28 papers. The special focus in this conference is on Search Based Software Engineering. The topics include: Java enterprise edition support in search-based JUnit test generation; searching higher order mutants for software improvement; a co-driven method; validation of constraints among configuration parameters using search-based combinatorial interaction testing; search-based testing of procedural programs; a search based approach for stress-testing integrated circuits; improved crossover operators for genetic programming for program repair; scaling up the fitness function for reverse engineering feature models; search based clustering for protecting software with diversified updates; amortised deep parameter optimisation of GPGPU work group size for OpenCV; API-constrained genetic improvement; deep parameter optimisation for face detection using the viola-jones algorithm in OpenCV; multi-objective regression test suite minimisation for mockito; interactive code smells detection; applying monte carlo tree search for program synthesis and dynamic bugs prioritization in open source repositories with evolutionary techniques.",
      "Keywords": "",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2016-01-01",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84960453473",
      "Primary study DOI": "",
      "Title": "Hidden truths in dead software paths",
      "Abstract": "Approaches and techniques for statically finding a multitude of issues in source code have been developed in the past. A core property of these approaches is that they are usually targeted towards finding only a very specfic kind of issue and that the effort to develop such an analysis is significant. This strictly limits the number of kinds of issues that can be detected. In this paper, we discuss a generic approach based on the detection of infeasible paths in code that can discover a wide range of code smells ranging from useless code that hinders comprehension to real bugs. Code issues are identified by calculating the difference between the control-flow graph that contains all technically possible edges and the corresponding graph recorded while performing a more precise analysis using abstract interpretation. We have evaluated the approach using the Java Development Kit as well as the Qualitas Corpus (a curated collection of over 100 Java Applications) and were able to find thousands of issues across a wide range of categories.",
      "Keywords": "",
      "Publication venue": "",
      "Publication date": "",
      "Publication type": "",
      "Authors": "Eichberg, M.;Hermann, B.;Mezini, M.;Glanz, L.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84964828802",
      "Primary study DOI": "10.1109/BLOCKS.2015.7369015",
      "Title": "Programming environments for blocks need first-class software refactoring support: A position paper",
      "Abstract": "Block-based programming languages and their development environments have become a widely used educational platform for novices to learn how to program. In addition, these languages and environments have been increasingly embraced by domain experts to develop end-user software. Though popular for having a «low floor» (easy to get started), programs written in block-based languages often become unwieldy as projects grow progressively more complex. Software refactoring -improving the design quality of a codebase while preserving its external functionality -has been shown highly effective as a means of improving the quality of software written in text-based languages. Unfortunately, programming environments for blocks lack systematic software refactoring support. In this position paper, we argue that first-class software refactoring support must become an essential feature in programming environments for blocks; we present our research vision and concrete research directions, including program analysis to detect «code smells,» automated transformations for block-based programs to support common refactoring techniques, and integration of refactoring into introductory computing curricula.",
      "Keywords": "block-based programming languages | code smells | computer science curriculum | end-user software engineering | introductory programming | metrics | refactoring",
      "Publication venue": "Proceedings - 2015 IEEE Blocks and Beyond Workshop, Blocks and Beyond 2015",
      "Publication date": "2015-12-30",
      "Publication type": "Conference Paper",
      "Authors": "Techapalokul, Peeratham;Tilevich, Eli",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84961620203",
      "Primary study DOI": "10.1109/MODELS.2015.7338258",
      "Title": "Identification of Simulink model antipattern instances using model clone detection",
      "Abstract": "One challenge facing the Model-Driven Engineering community is the need for model quality assurance. Specifically, there should be better facilities for analyzing models automatically. One measure of quality is the presence or absence of good and bad properties, such as patterns and antipatterns, respectively. We elaborate on and validate our earlier idea of detecting patterns in model-based systems using model clone detection by devising a Simulink antipattern instance detector. We chose Simulink because it is prevalent in industry, has mature model clone detection techniques, and interests our industrial partners. We demonstrate our technique using near-miss cross-clone detection to find instances of Simulink antipatterns derived from the literature in four sets of public Simulink projects. We present our detection results, highlight interesting examples, and discuss potential improvements to our approach. We hope this work provides a first step in helping practitioners improve Simulink model quality and further research in the area.",
      "Keywords": "Analytical models | Cloning | Computational modeling | Mathematical model | Software packages | Unified modeling language",
      "Publication venue": "2015 ACM/IEEE 18th International Conference on Model Driven Engineering Languages and Systems, MODELS 2015 - Proceedings",
      "Publication date": "2015-11-25",
      "Publication type": "Conference Paper",
      "Authors": "Stephan, Matthew;Cordy, James R.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84963610946",
      "Primary study DOI": "10.1109/SCAM.2015.7335413",
      "Title": "When code smells twice as much: Metric-based detection of variability-aware code smells",
      "Abstract": "Code smells are established, widely used characterizations of shortcomings in design and implementation of software systems. As such, they have been subject to intensive research regarding their detection and impact on understandability and changeability of source code. However, current methods do not support highly configurable software systems, that is, systems that can be customized to fit a wide range of requirements or platforms. Such systems commonly owe their configurability to conditional compilation based on C preprocessor annotations (a. k. a. #ifdefs). Since annotations directly interact with the host language (e. g., C), they may have adverse effects on understandability and changeability of source code, referred to as variability-aware code smells. In this paper, we propose a metric-based method that integrates source code and C preprocessor annotations to detect such smells. We evaluate our method for one specific smell on five open-source systems of medium size, thus, demonstrating its general applicability. Moreover, we manually reviewed 100 instances of the smell and provide a qualitative analysis of its potential impact as well as common causes for the occurrence.",
      "Keywords": "",
      "Publication venue": "2015 IEEE 15th International Working Conference on Source Code Analysis and Manipulation, SCAM 2015 - Proceedings",
      "Publication date": "2015-11-20",
      "Publication type": "Conference Paper",
      "Authors": "Fenske, Wolfram;Schulze, Sandro;Meyer, Daniel;Saake, Gunter",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84961644289",
      "Primary study DOI": "10.1109/ICSM.2015.7332523",
      "Title": "Code smells in highly configurable software",
      "Abstract": "Modern software systems are increasingly configurable. Conditional compilation based on C preprocessor directives (i. e., #ifdefs) is a popular variability mechanism to implement this configurability in source code. Although C preprocessor usage has been subject to repeated criticism, with regard to variability implementation, there is no thorough understanding of which patterns are particularly harmful. Specifically, we lack empirical evidence of how frequently reputedly bad patterns occur in practice and which negative effect they have. For object-oriented software, in contrast, code smells are commonly used to describe source code that exhibits known design flaws, which negatively affect understandability or changeability. Established code smells, however, have no notion of variability. Consequently, they cannot characterize flawed patterns of variability implementation. The goal of my research is therefore to create a catalog of variability-aware code smells. I will collect empirical proof of how frequently these smells occur and what their negative impact is on understandability, changeability, and fault-proneness of affected code. Moreover, I will develop techniques to detect variability-aware code smells automatically and reliably.",
      "Keywords": "Computer architecture | Feature extraction | Interviews | Measurement | Programming | Software systems",
      "Publication venue": "2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings",
      "Publication date": "2015-11-19",
      "Publication type": "Conference Paper",
      "Authors": "Fenske, Wolfram",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84961613759",
      "Primary study DOI": "10.1109/ICSM.2015.7332488",
      "Title": "Code smells in spreadsheet formulas revisited on an industrial dataset",
      "Abstract": "In previous work, code smells have been adapted to be applicable on spreadsheet formulas. The smell detection algorithm used in this earlier study was validated on a small dataset of industrial spreadsheets by interviewing the users of these spreadsheets and asking them about their opinion about the found smells. In this paper a more in depth validation of the algorithm is done by analyzing a set of spreadsheets of which users indicated whether or not they are smelly. This new dataset gives us the unique possibility to get more insight in how we can distinguish 'bad' spreadsheets from 'good' spreadsheets. We do that in two ways: For both the smelly and non smelly spreadsheets we 1) have calculated the metrics that detect the smells and 2) have calculated metrics with respect to size, level of coupling, and the use of functions. The results show that indeed the metrics for the smells decrease in spreadsheets that are not smelly. With respect to size we found to our surprise that the improved spreadsheets were not smaller, but bigger. With regard to coupling and the use of functions both datasets are similar. It indicates that it is difficult to use metrics with respect to size, degree of coupling or use of functions to draw conclusions on the complexity of a spreadsheet.",
      "Keywords": "Companies | Complexity theory | Couplings | Length measurement | Programming | Standards",
      "Publication venue": "2015 IEEE 31st International Conference on Software Maintenance and Evolution, ICSME 2015 - Proceedings",
      "Publication date": "2015-11-19",
      "Publication type": "Conference Paper",
      "Authors": "Jansen, Bas;Hermans, Felienne",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84962289963",
      "Primary study DOI": "10.1109/SBES.2015.9",
      "Title": "A Method to Derive Metric Thresholds for Software Product Lines",
      "Abstract": "A software product line (SPL) is a set of software systems that share a common and variable set of components (features). Software metrics provide basic means to quantify several quality aspects of SPL components. However, the effectiveness of the SPL measurement process is directly dependent on the definition of reliable thresholds. If thresholds are not properly defined, it is difficult to actually know whether a given metric value indicates a potential problem in the component implementation. There are several methods to derive thresholds for software metrics. However, there is little understanding about their appropriateness for the context of SPLs. This paper aims to propose a method to derive thresholds in the SPL context. Our method is evaluated in terms of recall and precision using two code smells (God Class and Lazy Class) detection strategies. The evaluation of our method is performed based on a benchmark of 33 SPLs and the results were compared with a method (baseline) with the same purpose used in the context of SPLs (not proposed). The results show that our method has better recall when compared with baseline.",
      "Keywords": "Metrics | Software Product Lines | Thresholds",
      "Publication venue": "Proceedings - 29th Brazilian Symposium on Software Engineering, SBES 2015",
      "Publication date": "2015-11-11",
      "Publication type": "Conference Paper",
      "Authors": "Vale, Gustavo;Figueiredo, Eduardo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85083207122",
      "Primary study DOI": "10.1007/s11219-020-09498-y",
      "Title": "Code smell detection using multi-label classification approach",
      "Abstract": "Code smells are characteristics of the software that indicates a code or design problem which can make software hard to understand, evolve, and maintain. There are several code smell detection tools proposed in the literature, but they produce different results. This is because smells are informally defined or subjective in nature. Machine learning techniques help in addressing the issues of subjectivity, which can learn and distinguish the characteristics of smelly and non-smelly source code elements (classes or methods). However, the existing machine learning techniques can only detect a single type of smell in the code element that does not correspond to a real-world scenario as a single element can have multiple design problems (smells). Further, the mechanisms proposed in the literature could not detect code smells by considering the correlation (co-occurrence) among them. To address these shortcomings, we propose and investigate the use of multi-label classification (MLC) methods to detect whether the given code element is affected by multiple smells or not. In this proposal, two code smell datasets available in the literature are converted into a multi-label dataset (MLD). In the MLD, we found that there is a positive correlation between the two smells (long method and feature envy). In the classification phase, the two methods of MLC considered the correlation among the smells and enhanced the performance (on average more than 95% accuracy) for the 10-fold cross-validation with the ten iterations. The findings reported help the researchers and developers in prioritizing the critical code elements for refactoring based on the number of code smells detected.",
      "Keywords": "Code smell correlation | Code smells | Code smells detection | Machine learning techniques | Multi-label classification | Refactoring | Software quality",
      "Publication venue": "Software Quality Journal",
      "Publication date": "2020-09-01",
      "Publication type": "Article",
      "Authors": "Guggulothu, Thirupathi;Moiz, Salman Abdul",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "",
      "Primary study DOI": "",
      "Title": "International Symposium on Empirical Software Engineering and Measurement",
      "Abstract": "'",
      "Keywords": "",
      "Publication venue": "",
      "Publication date": "",
      "Publication type": "",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84962708036",
      "Primary study DOI": "10.1145/2857218.2857224",
      "Title": "On the use of time series and search based software engineering for refactoring recommendation",
      "Abstract": "To improve the quality of software systems, one of the widely used techniques is refactoring, defined as the process of improving the design of an existing system by changing its internal structure without altering the external behavior. The majority of existing refactoring works do not consider the impact of recommended refactorings on the quality of future releases of a system. In this paper, we propose to combine the use of search-based software engineering with time series to recommend good refactoring strategies in order to manage technical debt. We used a multi-objective algorithm to generate refactoring solutions that maximize the correction of important quality issues and minimize the effort. For these two fitness functions, we adapted time series forecasting to estimate the impact of the generated refactorings solution on future next releases of the system by predicting the evolution of the remaining code smells in the system, after refactoring, using different quality metrics. We evaluated our approach on one industrial project and a benchmark of 4 open source systems. The results confirm the efficiency of our technique to provide better refactoring management comparing to several existing refactoring techniques.",
      "Keywords": "Data-mining | Heuristic search | Refactoring",
      "Publication venue": "7th International ACM Conference on Management of Computational and CollEctive Intelligence in Digital EcoSystems, MEDES 2015",
      "Publication date": "2015-10-25",
      "Publication type": "Conference Paper",
      "Authors": "Wang, Hanzhang;Kessentini, Marouane;Grosky, William;Meddeb, Haythem",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84941995253",
      "Primary study DOI": "10.1007/s10270-013-0390-0",
      "Title": "A pattern-based approach for improving model quality",
      "Abstract": "UML class diagrams play a central role in modeling activities, and it is essential that class diagrams keep their high quality all along a product life cycle. Correctness problems in class diagrams are mainly caused by complex interactions among class-diagram constraints. Detection, identification, and repair of such problems require background training. In order to improve modelers’ capabilities in these directions, we have constructed a catalog of anti-patterns of correctness and quality problems in class diagrams, where an anti-pattern analyzes a typical constraint interaction that causes a correctness or a quality problem and suggests possible repairs. This paper argues that exposure to correctness anti-patterns improves modeling capabilities. The paper introduces the catalog and its pattern language, and describes experiments that test the impact of awareness to modeling problems in class diagrams (via concrete examples and anti-patterns) on the analysis capabilities of modelers. The experiments show that increased awareness implies increased identification. The improvement is remarkably noticed when the awareness is stimulated by anti-patterns, rather than by concrete examples.",
      "Keywords": "Analysis capabilities | Anti-patterns | Correctness | Experiments | Modeling problems | Pattern awareness | Pattern languages | Quality | Software engineering education",
      "Publication venue": "Software and Systems Modeling",
      "Publication date": "2015-10-22",
      "Publication type": "Article",
      "Authors": "Balaban, Mira;Maraee, Azzam;Sturm, Arnon;Jelnov, Pavel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84961175149",
      "Primary study DOI": "10.1109/MobileSoft.2015.38",
      "Title": "Detecting Antipatterns in Android Apps",
      "Abstract": "Mobile apps are becoming complex software systems that must be developed quickly and evolve continuously to fit new user requirements and execution contexts. However, addressing these constraints may result in poor design choices, known as antipatterns, which may incidentally degrade software quality and performance. Thus, the automatic detection of antipatterns is an important activity that eases both maintenance and evolution tasks. Moreover, it guides developers to refactor their applications and thus, to improve their quality. While antipatterns are well-known in object-oriented applications, their study in mobile applications is still in their infancy. In this paper, we propose a tooled approach, called Paprika, to analyze Android applications and to detect object-oriented and Android-specific antipatterns from binaries of mobile apps. We validate the effectiveness of our approach on a set of popular mobile apps downloaded from the Google Play Store.",
      "Keywords": "Androids | Humanoid robots | Java | Measurement | Mobile applications | Mobile communication | Software",
      "Publication venue": "Proceedings - 2nd ACM International Conference on Mobile Software Engineering and Systems, MOBILESoft 2015",
      "Publication date": "2015-09-28",
      "Publication type": "Conference Paper",
      "Authors": "Hecht, Geoffrey;Rouvoy, Romain;Moha, Naouel;Duchien, Laurence",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84962787122",
      "Primary study DOI": "10.1145/2818187.2818286",
      "Title": "Enabling open software project management data with antipatterns",
      "Abstract": "Antipatterns describe commonly occurring solutions to problems that generate negative consequences. By defining a vocabulary of terms for commonly occurring problematic processes and implementations within organisations, antipatterns help in the identification of poor design decisions and offer suggestions on how software can be refactored or improved. Seventeen years have passed since the first publication on Software Project Management (SPM) Antipatterns. Over this period of time a considerable amount of literature has been published on SPM Antipatterns and a significant amount of antipatterns has been listed and documented on Web pages. Despite the fact that a significant body of antipattern research focuses on the identification and documentation of new antipatterns, difficulties associated with SPM antipattern searches on research databases (i.e. the ACM Portal, IEEE Xplore, the Web of Knowledge and Google Scholar) are still being reported in the relevant literature. Furthermore, leveraging from the antipatterns that are listed on Web pages and consolidating them in a single knowledge base with open access remains an open issue. This paper presents a set of tools that transform SPM antipatterns to open SPM data in order to overcome the difficulties associated with detecting and using SPM antipatterns. The common characteristic of these tools is the open data architecture that is achieved with a combination of Semantic Web, Web Interface and Open Source technologies that allows open access to SPM antipattern data, collaborative development of antipatterns, as well as intelligent detection of antipatterns that exist in software projects. These tools have lead to the creation of good quality SPM antipattern data that can be easily accessed via the Web.",
      "Keywords": "Antipatterns | Collaborative learning | Ontology | Open data",
      "Publication venue": "ACADEMICMINDTREK 2015 - Proceedings of the 19th International Academic Mindtrek Conference",
      "Publication date": "2015-09-22",
      "Publication type": "Conference Paper",
      "Authors": "Fitsilis, Panos;Tilentzidis, Kyriakos;Moustakas, Iiias;Stamelos, Ioannis;Settas, Dimitrios",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84961575663",
      "Primary study DOI": "10.17485/ijst/2015/v8i24/81850",
      "Title": "Detecting resemblances in anti-pattern ideologies using social networks",
      "Abstract": "A key argument for modelling knowledge in ideologies is the simple reuse of the facts. However, nearby reliability checking, current ideology engineering tools give only essential functionalities for analyzing ideologies. Since ideologies can be considered as graphs, graph analysis techniques are an apt answer for this necessity. The anti-pattern ideology has been recently proposed as a knowledge base for SPARSE, an intelligent system that can detect the anti-patterns that exist in a software project. However, apart from the excess of anti-patterns that are intrinsically informal and vague, the data used in the anti-pattern ideology itself is many times inexactly defined. We exemplify in this paper the benefits of applying social networks to ontologies and the Semantic Web and discuss which research themes happen on the edge between the two particular fields. Particularly, we confer how different ideas of centrality portray the core content and structure of ontology.",
      "Keywords": "Anti-patterns | Bad code smell | Sparse",
      "Publication venue": "Indian Journal of Science and Technology",
      "Publication date": "2015-09-01",
      "Publication type": "Article",
      "Authors": "Soman, Saini Jacob",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84944564016",
      "Primary study DOI": "",
      "Title": "A systematic literature review: Recent trends and open issues in software refactoring",
      "Abstract": "Software refactoring is a process of improving the internal structure of software artifacts through various steps of transformations without affecting the externally observed behavior. Refactoring aims to improve the quality of the software in several aspects like code understandability, maintainability and modularity. Extensive researches are taking place in this area for the last decade and several papers are available for review in various angles of software like code smell detection, refactoring algorithms, patterns and refactoring, program evolution and refactoring and code clone detection. The aim of this review paper is to structure and organize the major findings published since 2004 with more emphasis given to papers published for the last five years to understand the current trends in refactoring and also to formulate better research problems for further research.",
      "Keywords": "AntiPatterns | Code clones | Code smells | Metaprogramming | Software Refactoring | Web mashups",
      "Publication venue": "International Journal of Applied Engineering Research",
      "Publication date": "2015-09-01",
      "Publication type": "Article",
      "Authors": "Sreeji, K. S.;Lakshmi, C.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84960453473",
      "Primary study DOI": "10.1145/2786805.2786865",
      "Title": "Hidden truths in dead software paths",
      "Abstract": "Approaches and techniques for statically finding a multitude of issues in source code have been developed in the past. A core property of these approaches is that they are usually targeted towards finding only a very specfic kind of issue and that the effort to develop such an analysis is significant. This strictly limits the number of kinds of issues that can be detected. In this paper, we discuss a generic approach based on the detection of infeasible paths in code that can discover a wide range of code smells ranging from useless code that hinders comprehension to real bugs. Code issues are identified by calculating the difference between the control-flow graph that contains all technically possible edges and the corresponding graph recorded while performing a more precise analysis using abstract interpretation. We have evaluated the approach using the Java Development Kit as well as the Qualitas Corpus (a curated collection of over 100 Java Applications) and were able to find thousands of issues across a wide range of categories.",
      "Keywords": "Abstract interpretation | Finding bugs | Infeasible paths | Java | Scalable analysis",
      "Publication venue": "2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings",
      "Publication date": "2015-08-30",
      "Publication type": "Conference Paper",
      "Authors": "Eichberg, Michael;Hermann, Ben;Mezini, Mira;Glanz, Leonid",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84951817322",
      "Primary study DOI": "10.1109/ICSE.2015.256",
      "Title": "Poster: Filtering Code Smells Detection Results",
      "Abstract": "Many tools for code smell detection have been devel- oped, providing often different results. This is due to the informal definition of code smells and to the subjective interpretation of them. Usually, aspects related to the domain, size, and design of the system are not taken into account when detecting and analyzing smells. These aspects can be used to filter out the noise and achieve more relevant results. In this paper, we propose different filters that we have identified for five code smells. We provide two kind of filters, Strong and Weak Filters, that can be integrated as part of a detection approach.",
      "Keywords": "",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2015-08-12",
      "Publication type": "Conference Paper",
      "Authors": "Arcelli Fontana, Francesca;Ferme, Vincenzo;Zanoni, Marco",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84951733226",
      "Primary study DOI": "10.1109/ICSE.2015.243",
      "Title": "An Approach to Detect Android Antipatterns",
      "Abstract": "Mobile applications are becoming complex software systems that must be developed quickly and evolve regularly to fit new user requirements and execution contexts. However, addressing these constraints may result in poor design choices, known as antipatterns, which may degrade software quality and performance. Thus, the automatic detection of antipatterns is an important activity that eases the future maintenance and evolution tasks. Moreover, it helps developers to refactor their applications and thus, to improve their quality. While antipatterns are well-known in object-oriented applications, their study in mobile applications is still in their infancy. In this paper, we presents a tooled approach, called Paprika, to analyze Android applications and to detect object-oriented and Android-specific antipatterns from binaries of applications.",
      "Keywords": "",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2015-08-12",
      "Publication type": "Conference Paper",
      "Authors": "Hecht, Geoffrey",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85074849764",
      "Primary study DOI": "10.1145/3361242.3361257",
      "Title": "Deep semantic-based feature envy identification",
      "Abstract": "Code smells regularly cause potential software quality problems in software development. Thus, code smell detection has attracted the attention of many researchers. A number of approaches have been suggested in order to improve the accuracy of code smell detection. Most of these approaches rely solely on structural information (code metrics) extracted from source code and heuristic rules designed by people. In this paper, We propose a method-representation based model to represent the methods in textual code, which can effectively reflect the semantic relationships embedded in textual code. We also propose a deep learning based approach that combines method-representation and a CNN model to detect feature envy. The proposed approach can automatically extract semantic and features from textual code and code metrics, and can also automatically build complex mapping between these features and predictions. Evaluation results on open-source projects demonstrate that our proposed approach achieves better performance than the state-of-the-art in detecting feature envy.",
      "Keywords": "Code Smell | Deep Learning | Deep Semantic | Feature Envy | Software Refactoring",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2019-10-28",
      "Publication type": "Conference Paper",
      "Authors": "Guo, Xueliang;Shi, Chongyang;Jiang, He",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84951802980",
      "Primary study DOI": "10.1109/ICSE.2015.135",
      "Title": "Automatic and Continuous Software Architecture Validation",
      "Abstract": "Software systems tend to suffer from architectural problems as they are being developed. While modern software development methodologies such as Agile and Dev-Ops suggest different ways of assuring code quality, very little attention is paid to maintaining high quality of the architecture of the evolving systems. By detecting and alerting about violations of the intended software architecture, one can often avoid code-level bad smells such as spaghetti code. Typically, if one wants to reason about the software architecture, the burden of first defining the intended architecture falls on the developer's shoulders. This includes definition of valid and invalid dependencies between software components. However, the developers are seldom familiar with the entire software system, which makes this task difficult, time consuming and error-prone. We propose and implement a solution for automatic detection of architectural violations in software artifacts. The solution, which utilizes a number of predefined and user-defined patterns, does not require prior knowledge of the system or its intended architecture. We propose to leverage this solution as part of the nightly build process used by development teams, thus achieving continuous automatic validation of the system's software architecture. As we show in multiple open-source and proprietary cases, a small set of predefined patterns can detect architectural violations as they are introduced over the course of development, and also capture deterioration in existing architectural problems. By evaluating the tool on relatively large open-source projects, we also validate its scalability and practical applicability to large software systems.",
      "Keywords": "",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2015-08-12",
      "Publication type": "Conference Paper",
      "Authors": "Goldstein, Maayan;Segall, Itai",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84960441661",
      "Primary study DOI": "10.1109/COUFLESS.2015.11",
      "Title": "Cross-technology, cross-layer defect detection in IT systems - Challenges and achievements",
      "Abstract": "Although critical for delivering resilient, secure, efficient, and easily changed IT systems, cross-technology, cross layer quality defect detection in IT systems still faces hurdles. Two hurdles involve the absence of an absolute target architecture and the difficulty of apprehending multi-component anti-patterns. However, Static analysis and measurement technologies are now able to both consume contextual input and detect system-level antipatterns. This paper will provide several examples of the information required to detect system-level anti-patterns using examples from the Common Weakness Enumeration repository maintained by MITRE Corp.",
      "Keywords": "CWE | IT systems | Software anti-patterns | Software architecture | Software pattern detection | Software quality measures | Structural quality",
      "Publication venue": "Proceedings - 1st International Workshop on Complex Faults and Failures in Large Software Systems, COUFLESS 2015",
      "Publication date": "2015-08-05",
      "Publication type": "Conference Paper",
      "Authors": "Douziech, Philippe Emmanuel;Curtis, Bill",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84946903258",
      "Primary study DOI": "10.1109/SAM.2015.8",
      "Title": "Towards Assessing Software Architecture Quality by Exploiting Code Smell Relations",
      "Abstract": "We can evaluate software architecture quality using a plethora of metrics proposed in the literature, but interpreting and exploiting in the right way these metrics is not always a simple task. This is true for both fixing the right metric threshold values and determining the actions to be taken to improve the quality of the system. Instead of metrics, we can detect code or architectural anomalies that give us useful hints on the possible architecture degradation. In this paper, we focus our attention on the detection of code smells and in particular on their relations and co-occurrences, with the aim to evaluate technical debt in an architectural context. We start from the assumption that certain patterns of code anomalies tend to be better indicators of architectural degradation than simple metrics evaluation.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2nd International Workshop on Software Architecture and Metrics, SAM 2015",
      "Publication date": "2015-07-31",
      "Publication type": "Conference Paper",
      "Authors": "Fontana, Francesca Arcelli;Ferme, Vincenzo;Zanoni, Marco",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84963704437",
      "Primary study DOI": "10.1145/2739480.2754724",
      "Title": "Web service antipatterns detection using genetic programming",
      "Abstract": "Service-Oriented Architecture (SOA) is an emerging paradigm that has radically changed the way software applications are architected, designed and implemented. SOA allows developers to structure their systems as a set of ready-made, reusable and compostable services. The leading technology used today for implementing SOA is Web Services. Indeed, like all software, Web services are prone to change constantly to add new user requirements or to adapt to environment changes. Poorly planned changes may risk introducing antipatterns into the system. Consequently, this may ultimately leads to a degradation of software quality, evident by poor quality of service (QoS). In this paper, we introduce an automated approach to detect Web service antipatterns using genetic programming. Our approach consists of using knowledge from real-world examples of Web service antipatterns to generate detection rules based on combinations of metrics and threshold values. We evaluate our approach on a benchmark of 310 Web services and a variety of five types of Web service antipatterns. The statistical analysis of the obtained results provides evidence that our approach is efficient to detect most of the existing antipatterns with a score of 85% of precision and 87% of recall.",
      "Keywords": "Antipatterns | Search-based software engineering | Web Services",
      "Publication venue": "GECCO 2015 - Proceedings of the 2015 Genetic and Evolutionary Computation Conference",
      "Publication date": "2015-07-11",
      "Publication type": "Conference Paper",
      "Authors": "Ouni, Ali;Kula, Raula Gaikovina;Kessentini, Marouane;Inoue, Katsuro",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84925763509",
      "Primary study DOI": "10.1007/s11219-014-9233-7",
      "Title": "Prioritizing code-smells correction tasks using chemical reaction optimization",
      "Abstract": "The presence of code-smells increases significantly the cost of maintenance of systems and makes them difficult to change and evolve. To remove code-smells, refactoring operations are used to improve the design of a system by changing its internal structure without altering the external behavior. In large-scale systems, the number of code-smells to fix can be very large and not all of them can be fixed automatically. Thus, the prioritization of the list of code-smells is required based on different criteria such as the risk and importance of classes. However, most of the existing refactoring approaches treat the code-smells to fix with the same importance. In this paper, we propose an approach based on a chemical reaction optimization metaheuristic search to find the suitable refactoring solutions (i.e., sequence of refactoring operations) that maximize the number of fixed riskiest code-smells according to the maintainer’s preferences/criteria. We evaluate our approach on five medium- and large-sized open-source systems and seven types of code-smells. Our experimental results show the effectiveness of our approach compared to other existing approaches and three different others metaheuristic searches.",
      "Keywords": "Chemical reaction optimization | Code-smells | Refactoring, software quality | Search-based software engineering",
      "Publication venue": "Software Quality Journal",
      "Publication date": "2015-06-01",
      "Publication type": "Article",
      "Authors": "Ouni, Ali;Kessentini, Marouane;Bechikh, Slim;Sahraoui, Houari",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84961248339",
      "Primary study DOI": "10.1145/2764979.2764990",
      "Title": "Including structural factors into the metrics-based code smells detection",
      "Abstract": "Code smells help to discover and describe deeper problems in software design. Several automated methods of smell detection are based the analysis of a combination of code-related metrics relevant for a given flaw. However, some smells reflect more complex issues and require a holistic perspective that woudl cover a number of different sources of data. In this paper we experimentally verify the usefulness of including structural factors into a metrics-based detection of God Class and Brain Class code smells.",
      "Keywords": "Code smell detection | Detection strategies",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2015-05-25",
      "Publication type": "Conference Paper",
      "Authors": "Walter, Bartosz;Matuszyk, Błazej;Fontana, Francesca Arcelli",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84929317902",
      "Primary study DOI": "10.1109/TSE.2014.2372760",
      "Title": "Mining version histories for detecting code smells",
      "Abstract": "Code smells are symptoms of poor design and implementation choices that may hinder code comprehension, and possibly increase change- and fault-proneness. While most of the detection techniques just rely on structural information, many code smells are intrinsically characterized by how code elements change over time. In this paper, we propose H istorical Information for Smell deTection (HIST), an approach exploiting change history information to detect instances of five different code smells, namely Divergent Change, Shotgun Surgery, Parallel Inheritance, Blob, and Feature Envy. We evaluate HIST in two empirical studies. The first, conducted on 20 open source projects, aimed at assessing the accuracy of HIST in detecting instances of the code smells mentioned above. The results indicate that the precision of HIST ranges between 72 and 86 percent, and its recall ranges between 58 and 100 percent. Also, results of the first study indicate that HIST is able to identify code smells that cannot be identified by competitive approaches solely based on code analysis of a single system's snapshot. Then, we conducted a second study aimed at investigating to what extent the code smells detected by HIST (and by competitive code analysis techniques) reflect developers' perception of poor design and implementation choices. We involved 12 developers of four open source projects that recognized more than 75 percent of the code smell instances identified by HIST as actual design/implementation problems.",
      "Keywords": "Code smells | empirical studies | mining software repositories",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2015-05-01",
      "Publication type": "Article",
      "Authors": "Palomba, Fabio;Bavota, Gabriele;Di Penta, Massimiliano;Oliveto, Rocco;Poshyvanyk, Denys;De Lucia, Andrea",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84955495269",
      "Primary study DOI": "10.1145/2695664.2696059",
      "Title": "Using developers' feedback to improve code smell detection",
      "Abstract": "Several studies are focused on the study of code smells and many detection techniques have been proposed. In this scenario, the use of rules involving software-metrics has been widely used in refactoring tools as a mechanism to detect code smells automatically. However, actual approaches present two unsatisfactory aspects: they present a low agreement in its results and, they do not consider the developers' feedback. In this way, these approaches detect smells that are not relevant to the developers. In order to solve the above mentioned unsatisfactory aspects in the state-of the-art of code smells detection, we propose the Smell Platform able to recognize code smells more relevant to developers by using its feedback. In this paper we present how such platform is able to detect four well known code smells. Finally, we evaluate the Smell Platform comparing its results with traditional detection techniques.",
      "Keywords": "Code smell detection | Developer's feedback | Refactoring",
      "Publication venue": "Proceedings of the ACM Symposium on Applied Computing",
      "Publication date": "2015-04-13",
      "Publication type": "Conference Paper",
      "Authors": "Hozano, Mario;Ferreira, Henrique;Silva, Italo;Fonseca, Baldoino;Costa, Evandro",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84955479553",
      "Primary study DOI": "10.1145/2695664.2695682",
      "Title": "Exploring decision drivers on god class detection in three controlled experiments",
      "Abstract": "Context: Code smells define potential problems in design of software. However, some empirical studies on the topic have shown findings in opposite direction. The misunderstanding is mainly caused by lack of works focusing on human role on code smell detection. Objective: Our aim is to build empirical support to exploration of the human role on code smell detection. Specifically, we investigated what issues in code make a human identify a class as a code smell. We called these issues decision drivers. Method: We performed a controlled experiment and replicated it twice. We asked participants to detect god class (one of the most known smell) on different software, indicating what decision drivers they adopted. Results: The stronger drivers were \"class is high complex\" and \"method is misplaced\". We also found the agreement on drivers' choice is low. Another finding is: some important drivers are dependent of alternative support. In our case, \"dependency\" was an important driver only when visual resources were permitted. Conclusion: This study contributes with the comprehension of the human role on smell detection through the exploration of decision drivers. This perception contributes to characterize what we called the \"code smell conceptualization problem\".",
      "Keywords": "Code smell | Controlled experiment | God class",
      "Publication venue": "Proceedings of the ACM Symposium on Applied Computing",
      "Publication date": "2015-04-13",
      "Publication type": "Conference Paper",
      "Authors": "Santos, José Amancio M.;De Mendonça, Manoel G.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84955505554",
      "Primary study DOI": "10.1145/2695664.2695670",
      "Title": "Towards a catalog of usability smells",
      "Abstract": "This paper presents a catalog of smells in the context of interactive applications. These so-called usability smells are indicators of poor design on an application's user interface, with the potential to hinder not only its usability but also its maintenance and evolution. To eliminate such usability smells we discuss a set of program/usability refactorings. In order to validate the presented usability smells catalog, and the associated refactorings, we present a preliminary empirical study with software developers in the context of a real open source hospital management application. Moreover, a tool that computes graphical user interface behavior models, giving the applications' source code, is used to automatically detect usability smells at the model level.",
      "Keywords": "Code smells | Empirical studies | Graphical user interfaces",
      "Publication venue": "Proceedings of the ACM Symposium on Applied Computing",
      "Publication date": "2015-04-13",
      "Publication type": "Conference Paper",
      "Authors": "Almeida, Diogo;Campos, José Creissac;Saraiva, João;Silva, João Carlos",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84928707326",
      "Primary study DOI": "10.1007/s10664-013-9296-2",
      "Title": "Detecting and refactoring code smells in spreadsheet formulas",
      "Abstract": "Spreadsheets are used extensively in business processes around the world and just like software, spreadsheets are changed throughout their lifetime causing understandability and maintainability issues. This paper adapts known code smells to spreadsheet formulas. To that end we present a list of metrics by which we can detect smelly formulas; a visualization technique to highlight these formulas in spreadsheets and a method to automatically suggest refactorings to resolve smells. We implemented the metrics, visualization and refactoring suggestions techniques in a prototype tool and evaluated our approach in three studies. Firstly, we analyze the EUSES spreadsheet corpus, to study the occurrence of the formula smells. Secondly, we analyze ten real life spreadsheets, and interview the spreadsheet owners about the identified smells. Finally, we generate refactoring suggestions for those ten spreadsheets and study the implications. The results of these evaluations indicate that formula smells are common, that they can reveal real errors and weaknesses in spreadsheet formulas and that in simple cases they can be refactored.",
      "Keywords": "Code smells | Refactoring | Spreadsheets",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2015-04-01",
      "Publication type": "Article",
      "Authors": "Hermans, Felienne;Pinzger, Martin;van Deursen, Arie",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84908541659",
      "Primary study DOI": "10.1016/j.eswa.2014.09.022",
      "Title": "AutoRefactoring: A platform to build refactoring agents",
      "Abstract": "Software maintenance may degrade the software quality. One of the primary ways to reduce undesired effects of maintenance is refactoring, which is a technique to improve software code quality without changing its observable behavior. To safely apply a refactoring, several issues must be considered: (i) identify the code parts that should be improved; (ii) determine the changes that must be applied to the code in order to improve its; (iii) evaluate the corrections impacts on code quality; and (iv) check that the observable behavior of the software will be preserved after applying the corrections. Given the amount of issues to consider, refactoring by hand has been assumed to be an expensive and error-prone task. Therefore, in this paper, we propose an agent-based platform that enables to implement an agent able to autonomously deal with the above mentioned refactoring issues. To evaluate our approach, we performed an empirical study on code smells detection and correction, code quality improvement and preservation of the software observable behavior. To answer our research questions, we analyze 5 releases of Java open source projects, ranging from 166 to 711 classes.",
      "Keywords": "Autonomous agents | Code quality | Smells correction | Smells detection | Software refactoring",
      "Publication venue": "Expert Systems with Applications",
      "Publication date": "2015-02-15",
      "Publication type": "Article",
      "Authors": "Santos Neto, Baldoino Fonseca Dos;Ribeiro, Márcio;Da Silva, Viviane Torres;Braga, Christiano;De Lucena, Carlos José Pereira;De Barros Costa, Evandro",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84906761449",
      "Primary study DOI": "10.1016/B978-0-12-800160-8.00004-8",
      "Title": "Anti-pattern detection: Methods, challenges, and open issues",
      "Abstract": "Anti-patterns are poor solutions to recurring design problems. They occur in object-oriented systems when developers unwillingly introduce them while designing and implementing the classes of their systems. Several empirical studies have highlighted that anti-patterns have a negative impact on the comprehension and maintainability of a software systems. Consequently, their identification has received recently more attention from both researchers and practitioners who have proposed various approaches to detect them. This chapter discusses on the approaches proposed in the literature. In addition, from the analysis of the state-of-the-art, we will (i) derive a set of guidelines for building and evaluating recommendation systems supporting the detection of anti-patterns; and (ii) discuss some problems that are still open, to trace future research directions in the field. For this reason, the chapter provides a support to both researchers, who are interested in comprehending the results achieved so far in the identification of anti-patterns, and practitioner, who are interested in adopting a tool to identify anti-patterns in their software systems. © 2014 Elsevier Inc.",
      "Keywords": "Anti-pattern | Code bad smells | Linguistic anti-pattern | Software metrics",
      "Publication venue": "Advances in Computers",
      "Publication date": "2014-01-01",
      "Publication type": "Book Chapter",
      "Authors": "Palomba, Fabio;De Lucia, Andrea;Bavota, Gabriele;Oliveto, Rocco",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84952321173",
      "Primary study DOI": "10.1007/978-3-662-48616-0_11",
      "Title": "Are RESTful APIs well-designed? Detection of their linguistic (Anti)patterns",
      "Abstract": "Identifier lexicon has a direct impact on software understandability and reusability and, thus, on the quality of the final software product. Understandability and reusability are two important characteristics of software quality. REST (REpresentational State Transfer) style is becoming a de facto standard adopted by many software organisations. The use of proper lexicon in RESTful APIs might make them easier to understand and reuse by client developers, and thus, would ease their adoption. Linguistic antipatterns represent poor practices in the naming, documentation, and choice of identifiers in the APIs as opposed to linguistic patterns that represent best practices. We present the DOLAR approach (Detection Of Linguistic Antipatterns in REST), which applies syntactic and semantic analyses for the detection of linguistic (anti)patterns in RESTful APIs. We provide detailed definitions of ten (anti)patterns and define and apply their detection algorithms on 15 widely-used RESTful APIs, including Facebook, Twitter, and YouTube. The results show that DOLAR can indeed detect linguistic (anti)patterns with high accuracy and that they do occur in major RESTful APIs.",
      "Keywords": "Antipatterns | Detection | Patterns | REST | Semantic analysis",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2015-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Palma, Francis;Gonzalez-Huerta, Javier;Moha, Naouel;Guéhéneuc, Yann Gaël;Tremblay, Guy",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84926431830",
      "Primary study DOI": "10.3233/978-1-61499-484-8-1763",
      "Title": "Code smells detection and visualization of software systems",
      "Abstract": "Bad smells are symptoms in the source code that indicate possible deeper problems and may serve as drivers for code refactoring. Although efforts have been made on measuring code complexity in object-oriented systems, such as CK metrics, little emphasis has been put on analyzing code smells through a visualization manner. In this paper, we present a system for detecting and visualizing three kinds of code smells of software systems: Long Method, Large Class, and Long Parameter List. Thresholds for identifying the code smells are calculated based on statistics analysis on the source code of 50 open source projects. Code smells are visualized as graphs with colored nodes according to their different severity degrees.",
      "Keywords": "code analysis | Code smell | code visualization",
      "Publication venue": "Frontiers in Artificial Intelligence and Applications",
      "Publication date": "2015-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Lee, Shin Jie;Lin, Xavier;Lo, Li Hsiang;Chen, Yu Cheng;Lee, Jonathan",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84946201767",
      "Primary study DOI": "10.1002/smr.1737",
      "Title": "A review of code smell mining techniques",
      "Abstract": "Over the past 15 years, researchers presented numerous techniques and tools for mining code smells. It is imperative to classify, compare, and evaluate existing techniques and tools used for the detection of code smells because of their varying features and outcomes. This paper presents an up-to-date review on the state-of-the-art techniques and tools used for mining code smells from the source code of different software applications. We classify selected code smell detection techniques and tools based on their detection methods and analyze the results of the selected techniques. We present our observations and recommendations after our critical analysis of existing code smell techniques and tools. Our recommendations may be used by existing and new tool developers working in the field of code smell detection. The scope of this review is limited to research publications in the area of code smells that focus on detection of code smells as compared with previous reviews that cover all aspects of code smells.",
      "Keywords": "code quality | code smells | design flaws | detection techniques | literature review",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2015-11-01",
      "Publication type": "Review",
      "Authors": "Rasool, Ghulam;Arshad, Zeeshan",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84962551602",
      "Primary study DOI": "",
      "Title": "Automatic recommendation of software design patterns using anti-patterns in the design phase: A case study on abstract factory",
      "Abstract": "Anti-patterns, one of the reasons for software design problems, can be solved by applying proper design patterns. If anti-patterns are discovered in the design phase, this should lead an early pattern recommendation by using relationships between anti- and design patterns. This paper presents an idea called Antipattern based Design Pattern Recommender (ADPR), that uses design diagrams i.e. class and sequence diagrams to detect antipatterns and recommend corresponding design patterns. First of all, anti-patterns relating to specific design patterns are analyzed. Those anti-patterns are detected in the faulty software design to identify the required design patterns. For assessment, a case study is shown along with the experimental result analysis. Initially, ADPR is prepared for recommendation of the Abstract Factory design pattern only, and compared to an existing code-based recommender. The comparative results are promising, as ADPR was successful for all cases of Abstract Factory.",
      "Keywords": "Abstract factory | Anti-pattern | Design pattern | Design pattern recommendation | Software design",
      "Publication venue": "CEUR Workshop Proceedings",
      "Publication date": "2015-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Nahar, Nadia;Sakib, Kazi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84944250248",
      "Primary study DOI": "10.1007/978-3-662-46675-9_10",
      "Title": "Performance-based software model refactoring in fuzzy contexts",
      "Abstract": "The detection of causes of performance problems in software systems and the identification of refactoring actions that can remove the problems are complex activities (even in small/medium scale systems). It has been demonstrated that software models can nicely support these activities, especially because they enable the introduction of automation in the detection and refactoring steps. In our recent work we have focused on performance antipattern-based detection and refactoring of software models. However performance antipatterns suffer from the numerous thresholds that occur in their representations and whose binding has to be performed before the detection starts (as for many pattern/antipattern categories). In this paper we introduce an approach that aims at overcoming this limitation. We work in a fuzzy context where threshold values cannot be determined, but only their lower and upper bounds do. On this basis, the detection task produces a list of performance antipatterns along with their probabilities to occur in the model. Several refactoring alternatives can be available to remove each performance antipattern. Our approach associates an estimate of how effective each alternative can be in terms of performance benefits. We demonstrate that the joint analysis of antipattern probability and refactoring benefits drives the designers to identify the alternatives that heavily improve the software performance.",
      "Keywords": "Model refactoring | Performance antipatterns | Software performance",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2015-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Arcelli, Davide;Cortellessa, Vittorio;Trubiani, Catia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84936818110",
      "Primary study DOI": "10.1109/ITNG.2015.76",
      "Title": "Detecting Code Smells in Software Product Lines-An Exploratory Study",
      "Abstract": "Code smells are symptoms that something is wrong in the source code. They have been catalogued and investigated in several programming techniques. These techniques can be used to develop Software Product Lines (SPL). However, feature-oriented programming (FOP) is a specific technique to deal with the modularization of features in SPL. One of the most popular FOP languages is AHEAD and, as far as we are concerned, we still lack systematic studies on the categorization and detection of code smells in AHEAD-based SPL. To fill this gap, this paper extends the definitions of three traditional code smells, namely God Method, God Class, and Shotgun Surgery, to take into account FOP abstractions. We then proposed 8 new FOP measures to quantify specific characteristics of compositional approaches like AHEAD. Finally, we combine the proposed and existing measures to define 3 detection strategies for identifying the investigated code smells. To evaluate the detection strategies, we performed an exploratory study involving 26 participants. The study participants rely on metrics to identify code smells in 8 AHEAD systems. Our results show that the proposed detection strategies can be used as code smell predictor since statistical tests indicate agreement among them and the study participants.",
      "Keywords": "Code Smells | Detection Strategies | Software Product Lines",
      "Publication venue": "Proceedings - 12th International Conference on Information Technology: New Generations, ITNG 2015",
      "Publication date": "2015-05-26",
      "Publication type": "Conference Paper",
      "Authors": "Abilio, Ramon;Padilha, Juliana;Figueiredo, Eduardo;Costa, Heitor",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84962572236",
      "Primary study DOI": "",
      "Title": "Correctness of semantic code smell detection tools",
      "Abstract": "Refactoring is a set of techniques used to enhance the quality of code by restructuring existing code/design without changing its behavior. Refactoring tools can be used to detect specific code smells, propose relevant refactorings, and in some cases automate the refactoring process. However, usage of refactoring tools in industry is still relatively low. One of the major reasons being the veracity of the detected code smells, especially smells that aren't purely syntactic in nature. We conduct an empirical study on some refactoring tools and evaluate the correctness of the code smells they identify. We analyze the level of confidence users have on the code smells detected by the tools and discuss some issues with such tools.",
      "Keywords": "Correctness | Detection | Maintenance | Refactoring | Semantic code smells",
      "Publication venue": "CEUR Workshop Proceedings",
      "Publication date": "2015-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Mathur, Neeraj;Reddy, Y. Raghu",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84942568065",
      "Primary study DOI": "10.1007/978-3-319-17957-5_18",
      "Title": "Towards quality-driven SOA systems refactoring through planning",
      "Abstract": "Service Based Systems (SBSs), like other software systems, evolve due to changes in both user requirements and execution contexts. Continuous evolution could easily deteriorate the design and reduce the Quality of Service (QoS) of SBSs and may result in poor design solutions, commonly known as SOA (Service Oriented Architecture) antipatterns. SOA antipatterns lead to a reduced maintainability and re-usability of SBSs. It is therefore critical to be able to detect and remove them to ensure the architectural quality of the software during its lifetime. In this paper, we present a novel approach named SOMAD-R (Service Oriented Mining for Antipattern Detection-Refactoring) which allows the refactoring of SOA antipatterns by building on a previously published tool named SOMAD (Service Oriented Mining for Antipattern Detection). SOMAD-R combines planning solving techniques and SOMAD detection algorithms to enable antipatterns driven refactoring of SBSs. As a first step towards refactoring antipatterns for SBSs, we successfully applied SOMAD-R to HomeAutomation, a SCA (Service Component Architecture) application and we removed three antipatterns (out of five) while improving application performance by 32%.",
      "Keywords": "Quality-driven refactoring | Services orchestration | SOA antipatterns | SOA planning | SOA refactoring",
      "Publication venue": "Lecture Notes in Business Information Processing",
      "Publication date": "2015-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Nayrolles, Mathieu;Beaudry, Eric;Moha, Naouel;Valtchev, Petko;Hamou-Lhadj, Abdelwahab",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84942590036",
      "Primary study DOI": "10.1007/978-3-319-17957-5_3",
      "Title": "Specification and detection of business process antipatterns",
      "Abstract": "Structured business processes (SBPs) are now in enterprises the prominent solution to software development problems through orchestrating Web services. By their very nature, SBPs evolve through adding new or modifying existing functionalities. Those changes may deteriorate the process design and introduce process antipatterns—poor but recurring solutions that may degrade processes design quality and hinder their maintenance and evolution. However, to date, few solutions exist to detect such antipatterns to facilitate the maintenance and evolution and improve the quality of process design. We propose SODA-BP (Service Oriented Detection for Antipatterns in Business Processes), supported by a framework for specifying and detecting process antipatterns. To validate SODA-BP, we specify eight antipatterns and perform their detection on a set of randomly selected 35 SBPs form a corpus of more than 150 collected processes from an open-source search engine. Some of the SBPs were modified by adding, removing, or modifying process elements to introduce noise in them. Results shows that SODA-BP has an average detection precision of more than 75% and recall of 100%.",
      "Keywords": "Antipatterns | Business process | Detection | Specification",
      "Publication venue": "Lecture Notes in Business Information Processing",
      "Publication date": "2015-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Palma, Francis;Moha, Naouel;Guéhéneuc, Yann Gaël",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84935034558",
      "Primary study DOI": "",
      "Title": "Manual detection of feature envy bad smell in software code",
      "Abstract": "Good software is considered to be well design in terms of low coupling and high cohesion. But still we are lack in design quality of the software. And also it is the important one for development of the software code because lots of difficulties with this. Because some people may create it some other involved to develop the project. In such situation maintenance or development cost is more than production cost. For avoiding this code must contain good software design but it is not possible at the time of coding it will be done after software coding. Refactoring is a methodology to change the software code without any change in the output of the system. Many refactoring methodologies are available; here we propose Move Method Refactoring methodology to remove Feature Envy Bad Smell. Feature Envy Bad smell is a major design fault when programmers create software. By removing this smell we can enhance the design quality. So the refactored code will be made easy to develop. Already lots of plugins and tools are available to perform Move Method Refactoring, in this paper we proposed manual step by step manner to identify and understand what is Feature Envy Bad smell , How it will affect Design quality and how it will be solved based on the famous video Rental store application.",
      "Keywords": "Design quality | Eclipse plug-in | Feature envy bad smell | Move method refactoring | Refactoring",
      "Publication venue": "International Journal of Applied Engineering Research",
      "Publication date": "2015-01-01",
      "Publication type": "Article",
      "Authors": "Durai, S.;Kishore Kumar, K.;Manikandan, N. K.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84965107309",
      "Primary study DOI": "",
      "Title": "Hidden technical debt in machine learning systems",
      "Abstract": "Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.",
      "Keywords": "",
      "Publication venue": "Advances in Neural Information Processing Systems",
      "Publication date": "2015-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Sculley, D.;Holt, Gary;Golovin, Daniel;Davydov, Eugene;Phillips, Todd;Ebner, Dietmar;Chaudhary, Vinay;Young, Michael;Crespo, Jean François;Dennison, Dan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84928678295",
      "Primary study DOI": "10.1109/SANER.2015.7081891",
      "Title": "Towards a framework for automatic correction of anti-patterns",
      "Abstract": "One of the biggest concerns in software maintenance is design quality; poor design hinders software maintenance and evolution. One way to improve design quality is to detect and correct anti-patterns (i.e., poor solutions to design and implementation problems), for example through refactorings. There are several approaches to detect anti-patterns, that rely on metrics and structural properties. However, finding a specific solution to remove anti-patterns is a challenging task as candidate refactorings can be conflicting and their number very large, making it costly. Hence, development teams often have to prioritize the refactorings to be applied on a system. In addition to this, refactoring is risky, since non-experienced developers can change the behaviour of a system, without a comprehensive test suite. Therefore, there is a need for tools that can automatically remove anti-patterns. We will apply meta-heuristics to propose a technique for automated refactoring that improves design quality.",
      "Keywords": "",
      "Publication venue": "2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2015 - Proceedings",
      "Publication date": "2015-04-08",
      "Publication type": "Conference Paper",
      "Authors": "Morales, Rodrigo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84925876400",
      "Primary study DOI": "10.1109/KST.2015.7051460",
      "Title": "Feature envy factor: A metric for automatic feature envy detection",
      "Abstract": "As a software system evolves, its design get deteriorated and the system becomes difficult to maintain. In order to improve such an internal quality, the system must be restructured without affecting its external behavior. The process involves detecting the design flaws (or code smells) and applying appropriate refactorings that could help remove such flaws. One of the design flaws in many object-oriented systems is placing members in the wrong class. This code smell is called Feature Envy and it is a sign of inappropriate coupling and cohesion. This work proposes a metric to detect Feature Envy code smell that can be removed by relocating the method. Our evaluation shows promising results as the overall system's complexity is reduced after suggested Move Method refactorings are applied.",
      "Keywords": "code smells | design flaws | feature envy | refactoring | software metric | software quality",
      "Publication venue": "Proceedings of the 2015-7th International Conference on Knowledge and Smart Technology, KST 2015",
      "Publication date": "2015-02-27",
      "Publication type": "Conference Paper",
      "Authors": "Nongpong, Kwankamol",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84955311758",
      "Primary study DOI": "",
      "Title": "Detection and handling of model smells for MATLAB/simulink models",
      "Abstract": "Code smells in traditional software artifacts are common symptoms of the violation of fundamental design principles which negatively impact the quality of the resulting software product. Symptoms of code smells commonly occur in traditional software artifacts and cannot be directly mapped to model-based software artifacts. In this paper, we present a catalog for the detection and handling of model smells for MATLAB/Simulink, a widely used tool for model-based software development in the automotive domain. These model smells describe antipattern against universal quality requirements and have been collected in cooperation with an OEM from the automotive domain. To show the feasibility of detecting the proposed model smells, we realized a model smell detector and evaluated it within an industrial case study.",
      "Keywords": "MATLAB/Simulink | Model quality | Model refactoring | Model smells | Model-based software engineering",
      "Publication venue": "CEUR Workshop Proceedings",
      "Publication date": "2015-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Gerlitz, Thomas;Tran, Quang Minh;Dziobek, Christian",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84947562197",
      "Primary study DOI": "10.1145/2810146.2810155",
      "Title": "Size and cohesion metrics as indicators of the long method bad smell: An empirical study",
      "Abstract": "Source code bad smells are usually resolved through the application of well-defined solutions, i.e., refactorings. In the literature, software metrics are used as indicators of the existence and prioritization of resolving bad smells. In this paper, we focus on the long method smell (i.e. one of the most frequent and persistent bad smells) that can be resolved by the extract method refactoring. Until now, the identification of long methods or extract method opportunities has been performed based on cohesion, size or complexity metrics. However, the empirical validation of these metrics has exhibited relatively low accuracy with regard to their capacity to indicate the existence of long methods or extract method opportunities. Thus, we empirically explore the ability of size and cohesion metrics to predict the existence and the refactoring urgency of long method occurrences, through a case study on Java open-source methods. The results of the study suggest that one size and four cohesion metrics are capable of characterizing the need and urgency for resolving the long method bad smell, with a higher accuracy compared to the previous studies. The obtained results are discussed by providing possible interpretations and implications to practitioners and researchers.",
      "Keywords": "Case study | Cohesion | Long method | Metrics | Size",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2015-10-21",
      "Publication type": "Conference Paper",
      "Authors": "Charalampidou, Sofia;Ampatzoglou, Apostolos;Avgeriou, Paris",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84964239528",
      "Primary study DOI": "10.1109/MiSE.2015.16",
      "Title": "Identifying instances of model design patterns and antipatterns using model clone detection",
      "Abstract": "A hurdle in the growth of model driven software engineering is our ability to evaluate the quality of models automatically. One perspective is that software quality is a function of the existence, or lack thereof, of good and bad properties, also known as patterns and antipatterns, respectively. In this paper, we introduce the notion of using model clone detection to detect model pattern and antipattern instances by looking for models that are cross clones of pattern models. By detecting patterns at the model level, analysis is accomplished earlier in the engineering process, can be applied to primarily modelbased projects, and remains at the same level of abstraction that engineers are used to. We outline the process of using model clone detection for this purpose, including representing the patterns and detection of instances. We present some Simulink examples of pattern representations and discuss future work and research in the area.",
      "Keywords": "",
      "Publication venue": "Proceedings - 7th International Workshop on Modeling in Software Engineering, MiSE 2015",
      "Publication date": "2015-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Stephan, Matthew;Cordy, James R.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84930381813",
      "Primary study DOI": "10.1002/spe.2268",
      "Title": "A tool to improve code-first Web services discoverability through text mining techniques",
      "Abstract": "SummaryService-oriented development is challenging mainly because Web service developers tend to disregard the importance of the exposed service APIs, which are specified using Web Service Description Language (WSDL) documents. Methodologically, WSDL documents can be either manually generated or inferred from service implementations using WSDL generation tools. The latter option, called code first, is the most used approach in the industry. However, it is known that there are some bad practices in service implementations or defects in WSDL generation tools that may cause WSDL documents to present WSDL anti-patterns, which in turn compromise the chances of documents of being discovered and understood. In this paper, we present a software tool that assists developers in obtaining WSDL documents with as few WSDL anti-patterns as possible. The tool combines text mining and meta-programming techniques to process service implementations and is developed as an Eclipse plug-in. An evaluation of the tool by using a data-set of real service implementations in terms of anti-pattern avoidance accuracy and discovery performance by using classical Information Retrieval metrics - Precision-at-n, Recall and Normalized Discounted Cumulative Gain - is also reported.",
      "Keywords": "automatic detection | code first | service discovery | Web services | WSDL anti-patterns",
      "Publication venue": "Software - Practice and Experience",
      "Publication date": "2015-07-01",
      "Publication type": "Conference Paper",
      "Authors": "Mateos, Cristian;Rodriguez, Juan Manuel;Zunino, Alejandro",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84962583142",
      "Primary study DOI": "",
      "Title": "Towards a unifying approach for performance-driven software model refactoring",
      "Abstract": "Performance is a pervasive quality attribute of software systems. Since it plays a key role in the success of many projects, it is important to introduce approaches aimed at satisfying performance requirements from the early phases of software life-cycle. However, this is a complex problem, because a large gap exists between performance analysis results and the feedback expected by software designers. Some approaches proposed in the last few years aim at reducing such gap, based on automated Model-Driven Engineering techniques, but they are fragmented across different paradigms, languages and metamodels. The goal of this paper is to work towards an approach that enables performance problems detection and solution within an unique supporting environment. We rely on the Epsilon platform, which provides an ecosystem of task-specific languages, interpreters, and tools for MDE. We describe the approach that we are implementing, and we show how some of these languages nicely fit the needs of a unifying paradigm for performance-driven software model refactoring.",
      "Keywords": "Model-Driven Engineering | Performance Antipatterns | Software Performance Engineering | Software Refactoring",
      "Publication venue": "CEUR Workshop Proceedings",
      "Publication date": "2015-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Arcelli, Davide;Cortellessa, Vittorio;Di Pompeo, Daniele",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84929512012",
      "Primary study DOI": "10.1016/j.infsof.2015.01.003",
      "Title": "Performance comparison of query-based techniques for anti-pattern detection",
      "Abstract": "Context Program queries play an important role in several software evolution tasks like program comprehension, impact analysis, or the automated identification of anti-patterns for complex refactoring operations. A central artifact of these tasks is the reverse engineered program model built up from the source code (usually an Abstract Semantic Graph, ASG), which is traditionally post-processed by dedicated, hand-coded queries. Objective Our paper investigates the costs and benefits of using the popular industrial Eclipse Modeling Framework (EMF) as an underlying representation of program models processed by four different general-purpose model query techniques based on native Java code, OCL evaluation and (incremental) graph pattern matching. Method We provide in-depth comparison of these techniques on the source code of 28 Java projects using anti-pattern queries taken from refactoring operations in different usage profiles. Results Our results show that general purpose model queries can outperform hand-coded queries by 2-3 orders of magnitude, with the trade-off of an increased in memory consumption and model load time of up to an order of magnitude. Conclusion The measurement results of usage profiles can be used as guidelines for selecting the appropriate query technologies in concrete scenarios.",
      "Keywords": "Anti-patterns | Columbus | EMF-IncQuery | OCL | Performance measurements | Refactoring",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2015-09-01",
      "Publication type": "Conference Paper",
      "Authors": "Ujhelyi, Zoltán;Szoke, Gábor;Horváth, Ákos;Csiszár, Norbert István;Vidács, László;Varró, Dániel;Ferenc, Rudolf",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84929224193",
      "Primary study DOI": "",
      "Title": "WOSP-C 2015 - Proceedings of the 2015 ACM/SPEC Workshop on Challenges in Performance Methods for Software Development, in Conjunction with ICPE 2015",
      "Abstract": "The proceedings contain 8 papers. The topics discussed include: software performance engineering then and now: a position paper; towards a DevOps approach for software quality engineering; autoperf: workflow support for performance experiments; runtime performance challenges in big data systems; performance anti-pattern detection through fUML model library; beyond simulation: composing scalability, elasticity, and efficiency analyses from preexisting analysis results; integrating formal timing analysis in the real-time software development process; and challenges in integrating the analysis of multiple non-functional properties in model-driven software engineering.",
      "Keywords": "",
      "Publication venue": "WOSP-C 2015 - Proceedings of the 2015 ACM/SPEC Workshop on Challenges in Performance Methods for Software Development, in Conjunction with ICPE 2015",
      "Publication date": "2015-01-01",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84929076446",
      "Primary study DOI": "10.1145/2693561.2693565",
      "Title": "Performance antipattern detection through fUML model library",
      "Abstract": "Identifying performance problems is critical in the software design, mostly because the results of performance analysis (i.e., mean values, variances, and probability distributions) are difficult to be interpreted for providing feedback to software designers. Performance antipatterns support the interpretation of performance analysis results and help to fill the gap between numbers and design alternatives. In this paper, we present a model-driven framework that enables an early detection of performance antipatterns, i.e., without generating performance models. Specific design features (e.g., the number of sent messages) are monitored while simulating the specified software model, in order to point out the model elements that most likely contribute for performance flaws. To this end, we propose to use fUML models instrumented with a reusable library that provides data structures (as Classes) and algorithms (as Activities) to detect performance antipatterns while simulating the fUML model itself. A case study is provided to show our framework at work, its current capabilities and future challenges.",
      "Keywords": "Design feedback | Foundational UML | Performance antipatterns",
      "Publication venue": "WOSP-C 2015 - Proceedings of the 2015 ACM/SPEC Workshop on Challenges in Performance Methods for Software Development, in Conjunction with ICPE 2015",
      "Publication date": "2015-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Arcelli, Davide;Berardinelli, Luca;Trubiani, Catia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84964039922",
      "Primary study DOI": "",
      "Title": "Preliminary study on assessing software defects using nano-pattern detection",
      "Abstract": "Defects in software systems directly impact a product's quality and overall customer satisfaction. Assessing defective code for the purpose of locating vulnerable areas and improving software quality is important for sustained software development efforts. Various techniques have been used to determine the likelihood that code fragments contain defects (e.g., identifying code smells), but these techniques have drawbacks. Datadriven approaches are needed. This paper presents a study using nano-patterns (i.e., method-level traceable constructs) to evaluate software defects. We demonstrate that certain categories of nano-patterns are more defect-prone than others. We conducted a casestudy on Apache Hive and found that ObjectCreator, FieldReader, TypeManipulator, Looping, Exceptions, LocalReader, and LocalWriter nano-patterns are more defect-prone than others. Apart from assessing software defects, we expect this new finding will contribute to further research on other data-driven applications of nano-patterns and improve coding practices.",
      "Keywords": "Defect detection | Nano-patterns | Software patterns | Traceable patterns",
      "Publication venue": "24th International Conference on Software Engineering and Data Engineering, SEDE 2015",
      "Publication date": "2015-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Deo, Ajay K.;Williams, Byron J.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84985902355",
      "Primary study DOI": "10.1145/2677855.2677909",
      "Title": "Optimized unit testing for antipattern detection",
      "Abstract": "Antipatterns are poor design choices that are conjectured to make object oriented systems harder to maintain. We investigate the impact of antipatterns on classes in object-oriented systems by studying the relation between the presence of antipatterns and the change- and fault-proneness of the classes. Due to increased complexities in the software development, there is huge need of testing process to be carried on in better way. Also as the computer systems are significant to our society in everyday life and are performing an increasing number of critical tasks, so more work in software testing and analysis has become of great importance. Anti pattern testing is type of testing which is used to cut off the directly price associated with testing of different modules. This paper discusses various anti pattern detection techniques and proposes a new testing technique based on GUI for detection of anti patterns during software development.",
      "Keywords": "Antipattern | Fault loop | Loose connectors | Maintenance development | Software development techniques",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2014-11-14",
      "Publication type": "Conference Paper",
      "Authors": "Kaur, Harvinder;Kaur, Puneet Jai",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85074957138",
      "Primary study DOI": "10.1109/IEMECONX.2019.8877082",
      "Title": "An empirical framework for code smell prediction using extreme learning machineâ ",
      "Abstract": "The software containing code smells indicates the violation of standard design and coding practices by developer during the development of the software system. Recent empirical studies observed that classes having code smells have higher probability of change proneness or fault proneness with respect to classes having no code smells [1]. The effort of removing bugs due to code smells increases exponentially if the smells are not identified during the earlier phases of software development. The code smell prediction using source code metrics can be used in starting phases of software development life cycle to reduce the maintenance and testing effort of software and also help in improving the quality of the software. The work in this paper empirically investigates and evaluates different classification techniques, feature selection techniques, and data sampling techniques to handle imbalance data in predicting 7 different types of code smell. The conclusion of this research is assessed over 629 application packages. The experimental finding confirms the estimating capability of different classifiers, feature selection, and data imbalance techniques for developing code smell prediction models. Our analysis also reveals that the models developed using one technique are superior than the models developed using other techniques.",
      "Keywords": "Code Smell | Feature selection | Machine Learning | Software Engineering | Source Code Metrics",
      "Publication venue": "IEMECON 2019 - 9th Annual Information Technology, Electromechanical Engineering and Microelectronics Conference",
      "Publication date": "2019-03-01",
      "Publication type": "Conference Paper",
      "Authors": "Gupta, Himanshu;Kumar, Lov;Neti, Lalita Bhanu Murthy",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84928169106",
      "Primary study DOI": "10.1007/s11219-013-9200-8",
      "Title": "Comparing four approaches for technical debt identification",
      "Abstract": "Software systems accumulate technical debt (TD) when short-term goals in software development are traded for long-term goals (e.g., quick-and-dirty implementation to reach a release date versus a well-refactored implementation that supports the long-term health of the project). Some forms of TD accumulate over time in the form of source code that is difficult to work with and exhibits a variety of anomalies. A number of source code analysis techniques and tools have been proposed to potentially identify the code-level debt accumulated in a system. What has not yet been studied is if using multiple tools to detect TD can lead to benefits, that is, if different tools will flag the same or different source code components. Further, these techniques also lack investigation into the symptoms of TD “interest” that they lead to. To address this latter question, we also investigated whether TD, as identified by the source code analysis techniques, correlates with interest payments in the form of increased defect- and change-proneness. Comparing the results of different TD identification approaches to understand their commonalities and differences and to evaluate their relationship to indicators of future TD “interest.” We selected four different TD identification techniques (code smells, automatic static analysis issues, grime buildup, and Modularity violations) and applied them to 13 versions of the Apache Hadoop open source software project. We collected and aggregated statistical measures to investigate whether the different techniques identified TD indicators in the same or different classes and whether those classes in turn exhibited high interest (in the form of a large number of defects and higher change-proneness). The outputs of the four approaches have very little overlap and are therefore pointing to different problems in the source code. Dispersed Coupling and Modularity violations were co-located in classes with higher defect-proneness. We also observed a strong relationship between Modularity violations and change-proneness. Our main contribution is an initial overview of the TD landscape, showing that different TD techniques are loosely coupled and therefore indicate problems in different locations of the source code. Moreover, our proxy interest indicators (change- and defect-proneness) correlate with only a small subset of TD indicators.",
      "Keywords": "ASA | Code smells | Grime | Modularity violations | Software maintenance | Software quality | Source code analysis | Technical debt",
      "Publication venue": "Software Quality Journal",
      "Publication date": "2014-09-01",
      "Publication type": "Article",
      "Authors": "Zazworka, Nico;Vetro’, Antonio;Izurieta, Clemente;Wong, Sunny;Cai, Yuanfang;Seaman, Carolyn;Shull, Forrest",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84994157759",
      "Primary study DOI": "10.1145/2568225.2568259",
      "Title": "Detecting performance anti-patterns for applications developed using object-relational mapping",
      "Abstract": "Object-Relational Mapping (ORM) provides developers a conceptual abstraction for mapping the application code to the underlying databases. ORM is widely used in industry due to its convenience; permitting developers to focus on developing the business logic without worrying too much about the database access details. However, developers often write ORM code without considering the impact of such code on database performance, leading to cause transactions with timeouts or hangs in large-scale systems. Unfortunately, there is little support to help developers automatically detect suboptimal database accesses. In this paper, we propose an automated framework to detect ORM performance anti-patterns. Our framework automatically flags performance anti-patterns in the source code. Furthermore, as there could be hundreds or even thousands of instances of anti-patterns, our framework provides sup- port to prioritize performance bug fixes based on a statistically rigorous performance assessment. We have successfully evaluated our framework on two open source and one large-scale industrial systems. Our case studies show that our framework can detect new and known real-world performance bugs and that fixing the detected performance anti- patterns can improve the system response time by up to 98%.",
      "Keywords": "Dynamic Analysis | Performance | Performance Anti-pattern | Performance Evaluation | Static Analysis",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2014-05-31",
      "Publication type": "Conference Paper",
      "Authors": "Chen, Tse Hsun;Shang, Weiyi;Jiang, Zhen Ming;Hassan, Ahmed E.;Nasser, Mohamed;Flora, Parminder",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84893945079",
      "Primary study DOI": "10.1007/s11334-013-0205-z",
      "Title": "Investigating the evolution of code smells in object-oriented systems",
      "Abstract": "Software design problems are known and perceived under many different terms, such as code smells, flaws, non-compliance to design principles, violation of heuristics, excessive metric values and anti-patterns, signifying the importance of handling them in the construction and maintenance of software. Once a design problem is identified, it can be removed by applying an appropriate refactoring, improving in most cases several aspects of quality such as maintainability, comprehensibility and reusability. This paper, taking advantage of recent advances and tools in the identification of non-trivial code smells, explores the presence and evolution of such problems by analyzing past versions of code. Several interesting questions can be investigated such as whether the number of problems increases with the passage of software generations, whether problems vanish by time or only by targeted human intervention, whether code smells occur in the course of evolution of a module or exist right from the beginning and whether refactorings targeting at smell removal are frequent. In contrast to previous studies that investigate the application of refactorings in the history of a software project, we attempt to analyze the evolution from the point of view of the problems themselves. To this end, we classify smell evolution patterns distinguishing deliberate maintenance activities from the removal of design problems as a side effect of software evolution. Results are discussed for two open-source systems and four code smells. © 2013 Springer-Verlag London.",
      "Keywords": "Code smell | Evolution | Refactoring | Software history | Software repositories",
      "Publication venue": "Innovations in Systems and Software Engineering",
      "Publication date": "2014-03-01",
      "Publication type": "Article",
      "Authors": "Chatzigeorgiou, Alexander;Manakos, Anastasios",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84918555406",
      "Primary study DOI": "10.1109/IIAI-AAI.2014.139",
      "Title": "A method for detecting bad smells and ITS application to software engineering education",
      "Abstract": "In order to extend and maintenance software systems, it is necessary to remove factors behind bad smells from source code through refactoring. However, it is time-consuming process to detect and remove factors behind bad smells manually from large source code. And, learning how to refactor bad smells can be difficult for students because they are not yet software development experts. Therefore, we propose a method for detecting bad smells using declarative meta programming that can be applied to software development training. In this manner, software development training is facilitated.",
      "Keywords": "Declarative meta programming | Detectiong bad smells | Program refactorings | Software engineering education",
      "Publication venue": "Proceedings - 2014 IIAI 3rd International Conference on Advanced Applied Informatics, IIAI-AAI 2014",
      "Publication date": "2014-09-29",
      "Publication type": "Conference Paper",
      "Authors": "Ito, Yuki;Hazeyama, Atsuo;Morimoto, Yasuhiko;Kaminaga, Hiroaki;Nakamura, Shoichi;Miyadera, Youzou",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84900501872",
      "Primary study DOI": "10.12733/jcis9479",
      "Title": "Distribution rule based bad smell detection and refactoring scheme",
      "Abstract": "Bad smells are signs of potential problems in code, which may reduce the design quality of software. Detecting bad smells, however, remains time consuming for software engineers despite proposals on bad smell detection and refactoring tools. Large Class is a kind of bad smells whose specific characteristics are hard to determine, and the detection are hard to achieve automatically. In this paper, a Large Class bad smell detection approach based on class length distribution model and cohesion metrics is proposed. In programs, the lengths of classes are confirmed according to the certain distributions. The class length distribution model is generalized to detect programs after grouping. Meanwhile, cohesion metrics are analyzed for bad smell detection. The bad smell detection experiments of open source programs show that Large Class bad smell can be detected effectively and accurately with this approach, and refactoring scheme can be proposed for design quality improvements of programs. © 2014 Binary Information Press.",
      "Keywords": "Bad smell detection | Class length distribution model | Cohesion metrics | Distribution rule | Refactoring scheme",
      "Publication venue": "Journal of Computational Information Systems",
      "Publication date": "2014-01-01",
      "Publication type": "Article",
      "Authors": "Jiang, Dexun;Ma, Peijun;Su, Xiaohong;Wang, Tiantian",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84904904008",
      "Primary study DOI": "10.1007/978-3-319-09156-3_25",
      "Title": "Recognizing antipatterns and analyzing their effects on software maintainability",
      "Abstract": "Similarly to design patterns and their inherent extra information about the structure and design of a system, antipatterns - or bad code smells - can also greatly influence the quality of software. Although the belief that they negatively impact maintainability is widely accepted, there are still relatively few objective results that would support this theory. In this paper we show our approach of detecting antipatterns in source code by structural analysis and use the results to reveal connections among antipatterns, number of bugs, and maintainability. We studied 228 open-source Java based systems and extracted bug-related information for 34 of them from the PROMISE database. For estimating the maintainability, we used the ColumbusQM probabilistic quality model. We found that there is a statistically significant, 0.55 Spearman correlation between the number of bugs and the number of antipatterns. Moreover, there is an even stronger, -0.62 reverse Spearman correlation between the number of antipatterns and code maintainability. We also found that even these few implemented antipatterns could nearly match the machine learning based bug-predicting power of 50 class level source code metrics. Although the presented analysis is not conclusive by far, these first results suggest that antipatterns really do decrease code quality and can highlight spots that require closer attention. © 2014 Springer International Publishing.",
      "Keywords": "Antipatterns | Empirical validation | ISO/IEC 25010 | OO design | Software maintainability | SQuaRE",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2014-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Bán, Dénes;Ferenc, Rudolf",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84899859703",
      "Primary study DOI": "",
      "Title": "Distance metric based divergent change bad smell detection and refactoring scheme analysis",
      "Abstract": "Bad smells are signs of potential problems in codes. Bad smells decrease the design quality of software, so the codes are hard to analyze, understand, test or reuse. Divergent Change is a common and classical bad smell in object oriented programs. The detection of this bad smell is difficult, because the features of Divergent Change are not obvious, and the detecting and refactoring of this bad smell are on the later steps of software life cycle. In this paper, the detection method of Divergent Change bad smell based on distance metric and K-nearest neighbor clustering technology is proposed. The features of Divergent Change are analyzed and transformed to distances between enti- ties. Divergent Change bad smells are detected with the results of K-nearest neighbor clustering, and targeted refactoring schemes are provided. After comparisons with sim- ilar researches, the experiments results on open source programs show that the proposed method behaves well on refactoring evaluation with low time complexity. © 2014 ICIC International.",
      "Keywords": "Bad smell detection | Entity distance | K-nearest neighbor clustering | Refactoring scheme",
      "Publication venue": "International Journal of Innovative Computing, Information and Control",
      "Publication date": "2014-01-01",
      "Publication type": "Article",
      "Authors": "Jiang, Dexun;Ma, Peijun;Su, Xiaohong;Wang, Tiantian",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84910605559",
      "Primary study DOI": "10.1007/978-3-662-45391-9_16",
      "Title": "Detection of REST patterns and antipatterns: A heuristics-based approach",
      "Abstract": "REST (REpresentational State Transfer), relying on resources as its architectural unit, is currently a popular architectural choice for building Web-based applications. It is shown that design patterns—good solutions to recurring design problems—improve the design quality and facilitate maintenance and evolution of software systems. Antipatterns, on the other hand, are poor and counter-productive solutions. Therefore, the detection of REST (anti)patterns is essential for improving the maintenance and evolution of RESTful systems. Until now, however, no approach has been proposed. In this paper, we propose SODA-R (Service Oriented Detection for Antipatterns in REST), a heuristics-based approach to detect (anti)patterns in RESTful systems. We define detection heuristics for eight REST antipatterns and five patterns, and perform their detection on a set of 12 widely-used REST APIs including BestBuy, Facebook, and DropBox. The results show that SODA-R can perform the detection of REST (anti)patterns with high accuracy. We also found that Twitter and DropBox are not well-designed, i.e., contain more antipatterns. In contrast, Facebook and BestBuy are well-designed, i.e., contain more patterns and less antipatterns.",
      "Keywords": "Antipatterns | Design | Detection | Heuristics | Patterns | REST",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2014-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Palma, Francis;Dubois, Johann;Moha, Naouel;Guéhéneuc, Yann Gaël",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84928309550",
      "Primary study DOI": "10.11591/ijece.v4i1.4097",
      "Title": "A systematic method for identification of anti-patterns in service oriented system development",
      "Abstract": "Service-Oriented Architecture is one of the popular software architecture's patterns used for developing lots of modern systems. However, it has been involved in many failures. Anti-patterns are solutions which have good view, but in fact they are wrong solutions that cause failure of systems. There are a lot of anti-patterns for SOA and new anti-patterns are revealed every day. Anti-patterns have their own reasons for being formed and also they are appeared in special area of the problem. As human's mind is restricted and it can process a limited number of states (piece of information) therefore identification of anti-patterns will be difficult for architects. In this paper, we propose a systematic method based on repository of anti-patterns along with a check list to identify anti-patterns of SOA. This method will assist architects to easily detect and avoid anti-patterns in development process and so escape from risks which related to anti-patterns. Furthermore, in this paper, we present a repository of forty five general anti-patterns in SOA. Reviewing these anti-patterns will help developers to work with clear understanding of patterns in phases of software development and so avoid from many potential problems. Also, our method is evaluated in action.",
      "Keywords": "Anti-patterns | Check list | Repository | Service-Oriented architecture",
      "Publication venue": "International Journal of Electrical and Computer Engineering",
      "Publication date": "2014-02-01",
      "Publication type": "Article",
      "Authors": "Torkamani, Mohammad Ali;Bagheri, Hamid",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84903188813",
      "Primary study DOI": "10.1007/978-3-319-07881-6_44",
      "Title": "On the effectiveness of concern metrics to detect code smells: An empirical study",
      "Abstract": "Traditional software metrics have been used to evaluate the maintainability of software programs by supporting the identification of code smells. Recently, concern metrics have also been proposed with this purpose. While traditional metrics quantify properties of software modules, concern metrics quantify concern properties, such as scattering and tangling. Despite being increasingly used in empirical studies, there is a lack of empirical knowledge about the effectiveness of concern metrics to detect code smells. This paper reports the results of an empirical study to investigate whether concern metrics can be useful indicators of three code smells, namely Divergent Change, Shotgun Surgery, and God Class. In this study, 54 subjects from two different institutions have analyzed traditional and concern metrics aiming to detect instances of these code smells in two information systems. The study results indicate that, in general, concern metrics support developers detecting code smells. In particular, we observed that (i) the time spent in code smell detection is more relevant than the developers' expertise; (ii) concern metrics are clearly useful to detect Divergent Change and God Class; and (iii) the concern metric Number of Concerns per Component is a reliable indicator of Divergent Change. © 2014 Springer International Publishing.",
      "Keywords": "Code Smells | Concerns | Empirical evaluation | Metrics",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2014-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Padilha, Juliana;Pereira, Juliana;Figueiredo, Eduardo;Almeida, Jussara;Garcia, Alessandro;Sant'Anna, Cláudio",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84919684873",
      "Primary study DOI": "10.1109/SCC.2014.103",
      "Title": "Performance antipatterns: Detection and evaluation of their effects in the cloud",
      "Abstract": "The way an application is designed and certain patterns thereof, play a significant role and might have a positive or a negative effect on the performance of the application. Some design patterns that have a negative effect on performance, also called performance antipatterns, may become important when evaluating migrating the application to the Cloud. Although there has been work done in the past related to defining performance antipatterns, there has been none that highlights the importance and effects of these performance antipatterns when an application is migrated to Cloud. In this work we present an approach to automatically detect important performance antipatterns in an application, by leveraging static code analysis and information about prospective deployment of the application components on the Cloud. We also experimentally show that these antipatterns may become prominent and pull down the application's performance if the application is migrated to the Cloud. Our results show that the performance of the parts of the application with such antipatterns suffer significantly and hence, the detection of these antipatterns has an overarching significance in the domain of software development for the Cloud. The approach we present here has also been implemented in a prototype cloud migration assessment tool.",
      "Keywords": "Cloud computing | Cloud migration | CloudFoundry | PaaS | Performance antipatterns",
      "Publication venue": "Proceedings - 2014 IEEE International Conference on Services Computing, SCC 2014",
      "Publication date": "2014-10-17",
      "Publication type": "Conference Paper",
      "Authors": "Sharma, Vibhu Saujanya;Anwer, Samit",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84914166230",
      "Primary study DOI": "10.1109/CONFLUENCE.2014.6949316",
      "Title": "A GUI based unit testing technique for antipattern identification",
      "Abstract": "Anti-patterns are considered as deficient programming practices that are unacceptable as a solution. They can be thought of as certain patterns in software development that are undesirable in comparison to design patterns which are acceptable solutions formalized for a common problem. From the study on anti-pattern classes involved in object-oriented systems, we examined the impact of presence of anti-patterns on the system. There are certain issues with the testing strategies used for finding bugs in the software systems such as objectives of testing, procedure used to test new functions, maximum resources and time needed to execute the project and the environment in which testing needs to be carried out. There is an enormous requirement to carry out testing process in a more suitable way due to the enlarged complications in the development of software systems. Anti-pattern testing is concerned with the reduction of testing cost for different modules. In this paper, we propose a new GUI based testing technique for the identification of anti patterns in object-oriented systems through automation testing.",
      "Keywords": "Anti-pattern | Detection Techniques | Object-oriented | Software systems | Testing",
      "Publication venue": "Proceedings of the 5th International Conference on Confluence 2014: The Next Generation Information Technology Summit",
      "Publication date": "2014-11-06",
      "Publication type": "Conference Paper",
      "Authors": "Kaur, Harvinder;Kaur, Puneet Jai",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85073195905",
      "Primary study DOI": "10.1109/IJCNN.2019.8851854",
      "Title": "Deep Representation Learning for Code Smells Detection using Variational Auto-Encoder",
      "Abstract": "Detecting code smells is an important research problem in the software maintenance. It assists the subsequent steps of the refactoring process so as to improve the quality of the software system. However, most of existing approaches have been limited to the use of structural information. There have been few researches to detect code smells using semantic information although its proven effectiveness in many software engineering problems. In addition, they do not capture entirely the semantic embedded in the source code. This paper attempts to fill this gap by proposing a semantic-based approach that detects bad smells which are scattered at different levels of granularity in the source code. To this end, we use an Abstract Syntax Tree with a Variational Auto-Encoder in the detection of three code smells. The code smells are Blob, Feature Envy and Long Method. We have performed our experimental evaluation on nine open-source projects and the results have achieved a considerable overall accuracy. To further evaluate the performance of our approach, we compare our results with a state-of-the-art method on the same publicly available dataset.",
      "Keywords": "",
      "Publication venue": "Proceedings of the International Joint Conference on Neural Networks",
      "Publication date": "2019-07-01",
      "Publication type": "Conference Paper",
      "Authors": "Hadj-Kacem, Mouna;Bouassida, Nadia",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84958538370",
      "Primary study DOI": "10.1007/978-3-319-09970-5_6",
      "Title": "Specification and detection of SOA antipatterns in web services",
      "Abstract": "Service Based Systems, composed of Web Services (WSs), offer promising solutions to software development problems for companies. Like other software artefacts, WSs evolve due to the changed user requirements and execution contexts, which may introduce poor solutions-Antipatterns-may cause (1) degradation of design and quality of service (QoS) and (2) difficult maintenance and evolution. Thus, the automatic detection of antipatterns in WSs, which aims at evaluating their design and QoS requires attention. We propose SODA-W (Service Oriented Detection for Antipatterns in Web services), an approach supported by a framework for specifying and detecting antipatterns in WSs. Using SODA-W, we specify ten antipatterns, including God Object Web Service and Fine Grained Web Service, and perform their detection in two different corpora: (1) 13 weather-related and (2) 109 financial-related WSs. SODA-W can specify and detect antipatterns in WSs with an average precision of more than 75% and a recall of 100%. © 2014 Springer International Publishing Switzerland.",
      "Keywords": "Antipatterns | Detection | Specification | Web Services",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2014-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Palma, Francis;Moha, Naouel;Tremblay, Guy;Guéhéneuc, Yann Gaël",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84921047198",
      "Primary study DOI": "10.1109/QUATIC.2014.49",
      "Title": "Streamlining code smells: Using collective intelligence and visualization",
      "Abstract": "Code smells have long been catalogued with corresponding mitigating solutions called refactoring operations. However, despite the successful initiatives of integrating many of the latter in current IDEs (e.g., Eclipse), code smells detection has not gained the same status. Researchers have pointed out that the code smells detection process is inherently subjective and this fact is probably the main hindrance on providing automatic support. This research work focuses on the code smells detection process. To this end, it aims at proposing two contributions that, when used in a combined fashion, are expected to mitigate the aforementioned problem: crowdsmellling and smelly maps. We envisage that such features will be available in a future generation of interactive development environments (aka IDE 2.0) to help software developers to identify a set of code smells in Java source code. Crowdsmelling uses the concept of collective intelligence through which users around the world are encouraged to collaborate to a knowledge base that runs on a cloud server. This process will improve the assessment of the detection accuracy using a calibration process that occurs on the server after receiving data and suggestions from each user. Through the process of code smells detection based and referenced to a knowledge base (one for each code smell), we expect to some extent mitigate the subjectivity problem. Smelly maps builds upon the previous experience of setting up a software visualization infrastructure. It involves a strategy to represent detected code smells at different abstraction levels with the goal to increase software quality awareness. We expect to integrate both proposals with the aforementioned visualization infrastructure developed as an Eclipse plugin to facilitate refactoring decisions upon large software systems. We have laid out several evidence-based validation experiments, which will hopefully demonstrate that a step forward in the code smelling process will be achieved.",
      "Keywords": "Code Smell Detection (Code Smell Detection Tools) | Software Construction (Software Construction Tools) | Software Maintenance (Reengineering) | Software Quality (Software Quality Improvement)",
      "Publication venue": "Proceedings - 2014 9th International Conference on the Quality of Information and Communications Technology, QUATIC 2014",
      "Publication date": "2014-12-12",
      "Publication type": "Conference Paper",
      "Authors": "Conceição, Carlos Fábio Ramos;Carneiro, Glauco De Figueiredo;Brito E Abreu, Fernando",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84893724525",
      "Primary study DOI": "10.1007/s10270-012-0246-z",
      "Title": "An approach for modeling and detecting software performance antipatterns based on first-order logics",
      "Abstract": "The problem of interpreting the results of performance analysis is quite critical in the software performance domain. Mean values, variances and probability distributions are hard to interpret for providing feedback to software architects. Instead, what architects expect are solutions to performance problems, possibly in the form of architectural alternatives (e.g. split a software component in two components and re-deploy one of them). In a software performance engineering process, the path from analysis results to software design or implementation alternatives is still based on the skills and experience of analysts. In this paper, we propose an approach for the generation of feedback based on performance antipatterns. In particular, we focus on the representation and detection of antipatterns. To this goal, we model performance antipatterns as logical predicates and we build an engine, based on such predicates, aimed at detecting performance antipatterns in an XML representation of the software system. Finally, we show the approach at work on a case study. © 2012 Springer-Verlag.",
      "Keywords": "Performance analysis | Software architectures | Software performance antipatterns",
      "Publication venue": "Software and Systems Modeling",
      "Publication date": "2014-02-01",
      "Publication type": "Article",
      "Authors": "Cortellessa, Vittorio;Di Marco, Antinisca;Trubiani, Catia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84938367824",
      "Primary study DOI": "",
      "Title": "Identifying strategies on god class detection in two controlled experiments",
      "Abstract": "Context: \"Code smell\" is commonly presented as indicative of problems in design of object-oriented systems. However, some empirical studies have presented findings refuting this idea. One of the reasons of the misunderstanding is the low number of studies focused on the role of human on code smell detection. Objective: Our aim is to build empirical support to exploration of the human role on code smell detection. Specifically, we investigated strategies adopted by developers on god class detection. God class is one of the most known code smell. Method: We performed a controlled experiment and replicated it. We explored the strategies from the participant's actions logged during the detection of god classes. Result: One of our findings was that the observation of coupling is more relevant than the observation of attributes like LOC or complexity and the hierarchical relation among these. We also noted that reading source code is important, even with visual resources enhancing the general comprehension of the software. Conclusion: This study contributes to expand the comprehension of the human role on code smell detection through the use of automatic logging. We suggest that this approach brings a complementary perspective of analysis in discussions about the topic.",
      "Keywords": "Code smell | Controlled experiment | God class",
      "Publication venue": "Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",
      "Publication date": "2014-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Santos, José Amancio M.;De Mendonça, Manoel G.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84922111509",
      "Primary study DOI": "10.1109/ICCCT.2014.7001467",
      "Title": "Two level dynamic approach for Feature Envy detection",
      "Abstract": "Refactoring leads to more maintainable software. To refactor the code, it must be known which part of code needs to be refactored. For this purpose code smells are used. Detecting code smells in itself is a challenging task. In this paper we propose a technique based on dynamic analysis for the detection of Feature Envy code smell. Feature envy is a method level smell and occurs when a method is more interested in another class than its own class. Previous approaches detects feature envy by analyzing each method in the system, which incurs an overhead as those methods which are closely bounded to their classes are included in the detection mechanism. To tackle this problem, we have devised a two level mechanism. First level filters out the methods which are suspects of being feature envious and second level analyzes those suspects to identify the actual culprits. The proposed technique has been evaluated and compared with existing techniques.",
      "Keywords": "code smell | dynamic analysis | object-oriented programming | software maintenance",
      "Publication venue": "Proceedings - 5th IEEE International Conference on Computer and Communication Technology, ICCCT 2014",
      "Publication date": "2014-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Kumar, Swati;Chhabra, Jitender Kumar",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84904488882",
      "Primary study DOI": "10.1145/2602576.2602579",
      "Title": "Automatic detection of performance anti-patterns in inter-component communications",
      "Abstract": "Performance problems such as high response times in software applications have a significant effect on the customer's satisfaction. In enterprise applications, performance problems are frequently manifested in inefficient or unnecessary communication patterns between software components originating from poor architectural design or implementation. Due to high manual effort, thorough performance analysis is often neglected, in practice. In order to overcome this problem, automated engineering approaches are required for the detection of performance problems. In this paper, we introduce several heuristics for measurement-based detection of well-known performance anti-patterns in inter-component communications. The detection heuristics comprise load and instrumentation descriptions for performance tests as well as corresponding detection rules. We integrate these heuristics with Dynamic Spotter, a framework for automatic detection of performance problems. We evaluate our heuristics on four evaluation scenarios based on an e-commerce benchmark (TPC-W) where the heuristics detect the expected communication performance anti-patterns and pinpoint their root causes. Copyright © 2014 ACM 978-1-4503-2577-6/14/06 ...$15.00.",
      "Keywords": "",
      "Publication venue": "QoSA 2014 - Proceedings of the 10th International ACM SIGSOFT Conference on Quality of Software Architectures (Part of CompArch 2014)",
      "Publication date": "2014-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Wert, Alexander;Oehler, Marius;Heger, Christoph;Farahbod, Roozbeh",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84927537382",
      "Primary study DOI": "10.1007/978-81-322-1695-7_17",
      "Title": "Identifying accurate refactoring opportunities using metrics",
      "Abstract": "Cloned code, also known as duplicated code, is among the bad “code smells.” Refactoring can be used to remove clones and makes a software system more maintainable. However, there is a problem that causes the output results of the clone code detection tool cannot be directly refactored. The problem is not all the clone groups are suitable for refactoring. To address it, we propose a metric method to identify clone groups that are suitable for refactoring. The results of several large-scale software system studies indicate that our method can significantly increase the accuracy of identifying clone groups that are suitable for refactoring. It is not only beneficial to the following study of refactoring, but also it connects the entire process from clone detection to clone refactoring.",
      "Keywords": "Cloned code | Metric | Refactoring",
      "Publication venue": "Advances in Intelligent Systems and Computing",
      "Publication date": "2014-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Bian, Yixin;Su, Xiaohong;Ma, Peijun",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84907027182",
      "Primary study DOI": "10.1145/2629648",
      "Title": "Some code smells have a significant but small effect on faults",
      "Abstract": "We investigate the relationship between faults and five of Fowler et al.'s least-studied smells in code: Data Clumps, Switch Statements, Speculative Generality, Message Chains, and Middle Man. We developed a tool to detect these five smells in three open-source systems: Eclipse, ArgoUML, and Apache Commons. We collected fault data from the change and fault repositories of each system. We built Negative Binomial regression models to analyse the relationships between smells and faults and report the McFadden effect size of those relationships. Our results suggest that Switch Statements had no effect on faults in any of the three systems; Message Chains increased faults in two systems; Message Chains which occurred in larger files reduced faults; Data Clumps reduced faults in Apache and Eclipse but increased faults in ArgoUML; Middle Man reduced faults only in ArgoUML, and Speculative Generality reduced faults only in Eclipse. File size alone affects faults in some systems but not in all systems. Where smells did significantly affect faults, the size of that effect was small (always under 10 percent). Our findings suggest that some smells do indicate fault-prone code in some circumstances but that the effect that these smells have on faults is small. Our findings also show that smells have different effects on different systems. We conclude that arbitrary refactoring is unlikely to significantly reduce fault-proneness and in some cases may increase fault-proneness.",
      "Keywords": "Defects | Software code smells",
      "Publication venue": "ACM Transactions on Software Engineering and Methodology",
      "Publication date": "2014-09-05",
      "Publication type": "Conference Paper",
      "Authors": "Hall, Tracy;Zhang, Min;Bowes, David;Sun, Yi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84910008933",
      "Primary study DOI": "10.1007/s10664-013-9297-1",
      "Title": "Performance assessment of an architecture with adaptative interfaces for people with special needs",
      "Abstract": "People in industrial societies carry more and more portable electronic devices (e.g., smartphone or console) with some kind of wireless connectivity support. Interaction with auto-discovered target devices present in the environment (e.g., the air conditioning of a hotel) is not so easy since devices may provide inaccessible user interfaces (e.g., in a foreign language that the user cannot understand). Scalability for multiple concurrent users and response times are still problems in this domain. In this paper, we assess an interoperable architecture, which enables interaction between people with some kind of special need and their environment. The assessment, based on performance patterns and antipatterns, tries to detect performance issues and also tries to enhance the architecture design for improving system performance. As a result of the assessment, the initial design changed substantially. We refactorized the design according to the Fast Path pattern and The Ramp antipattern. Moreover, resources were correctly allocated. Finally, the required response time was fulfilled in all system scenarios. For a specific scenario, response time was reduced from 60 seconds to less than 6 seconds.",
      "Keywords": "ICT for people with special needs | Industrial report | Performance assessment | Performance patterns and antipatterns | Software architecture",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2014-10-12",
      "Publication type": "Article",
      "Authors": "Gómez-Martínez, Elena;Gonzalez-Cabero, Rafael;Merseguer, Jose",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84898475693",
      "Primary study DOI": "10.1109/CSMR-WCRE.2014.6747181",
      "Title": "Anti-pattern detection with model queries: A comparison of approaches",
      "Abstract": "Program queries play an important role in several software evolution tasks like program comprehension, impact analysis, or the automated identification of anti-patterns for complex refactoring operations. A central artifact of these tasks is the reverse engineered program model built up from the source code (usually an Semantic Graph, ASG), which is traditionally post-processed by dedicated, hand-coded queries. Our paper investigates the use of the popular industrial Eclipse Modeling Framework (EMF) as an underlying representation of program models processed by three general-purpose model query techniques based on native Java code, local-search and incremental evaluation. We provide in-depth comparison of these techniques on the source code of 17 Java projects using queries taken from refactoring operations in different usage profiles. Our results show that general purpose model queries outperform hand-coded queries by 2-3 orders of magnitude, while there is a 5-10 times increase in memory consumption and model load time. In addition, measurement results of usage profiles can be used as guidelines for selecting the appropriate query technologies in concrete scenarios. © 2014 IEEE.",
      "Keywords": "",
      "Publication venue": "2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering, CSMR-WCRE 2014 - Proceedings",
      "Publication date": "2014-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Ujhelyi, Zoltán;Horváth, Ákos;Varró, Dániel;Csiszár, Norbert István;Szóke, Gábor;Vidács, László;Ferenc, Rudolf",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84904498206",
      "Primary study DOI": "",
      "Title": "QoSA 2014 - Proceedings of the 10th International ACM SIGSOFT Conference on Quality of Software Architectures (Part of CompArch 2014)",
      "Abstract": "The proceedings contain 17 papers. The topics discussed include: automatic detection of performance anti-patterns in inter-component communications; architectural tactics support in cloud computing providers: the jelastic case; performance-based selection of software and hardware features under parameter uncertainty; dealing with uncertainties in the performance modelling of software systems; experiences with modeling memory contention for multi-core industrial real-time systems; using architecture-level performance models as resource profiles for enterprise applications; empirical resilience evaluation of an architecture-based self-adaptive software system; architecture management and evaluation in mature products: experiences from a lightweight approach; efficient re-resolution of SMT specifications for evolving software architectures; and an empirical investigation of modularity metrics for indicating architectural technical debt.",
      "Keywords": "",
      "Publication venue": "QoSA 2014 - Proceedings of the 10th International ACM SIGSOFT Conference on Quality of Software Architectures (Part of CompArch 2014)",
      "Publication date": "2014-01-01",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84903466057",
      "Primary study DOI": "",
      "Title": "Identifying refactoring opportunities from code clones based on SOM clustering",
      "Abstract": "Code clones are regarded as typical bad smell because they usually make software maintenance more difficult and may even propagate bugs. Code Clones almost exist in all kinds of source programs of any nontrivial software. In order to reduce the harmful impacts on software quality caused by code clones, some clones satisfying certain conditions should be removed by refactoring. However, for subsequent refactoring, those refactor-able code clones should be identified first. In this paper, a method based on SOM (Self Organizing Map) clustering is proposed to identify code clones which can satisfy certain conditions for refactoring. The method has 5 basic steps: code clone detection, refactoring sample selection, data preprocessing, SOM modeling and refactoring opportunity identification. The results of related experiments on 6 open source software show that the method has better precision than the method using single metric, and has better validity than the method using multiple metrics. © 2014 ICIC International.",
      "Keywords": "Code clone | Refactoring opportunity | Software quality | SOM clustering",
      "Publication venue": "ICIC Express Letters, Part B: Applications",
      "Publication date": "2014-01-01",
      "Publication type": "Article",
      "Authors": "Meng, Fanqi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84938576151",
      "Primary study DOI": "10.4204/EPTCS.147.6",
      "Title": "A model-driven approach to broaden the detection of software performance antipatterns at runtime",
      "Abstract": "Performance antipatterns document bad design patterns that have negative influence on system performance. In our previous work we formalized such antipatterns as logical predicates that build on four different views: (i) the static view that captures the software elements (e.g. classes, components) and the static relationships among them; (ii) the dynamic view that represents the interaction (e.g. messages) that occurs between the software elements to provide system functionalities; (iii) the deployment view that describes the hardware elements (e.g. processing nodes) and the mapping of software resources onto hardware platforms; (iv) the performance view that collects a set of specific performance indices. In this paper we present a lightweight infrastructure that enables the detection of software performance antipatterns at runtime through the monitoring of specific performance indices. The proposed approach precalculates the logical predicates of antipatterns and identifies the ones whose static, dynamic and deployment sub-predicates occur in the current system configuration and brings at runtime the verification of performance sub-predicates. The proposed infrastructure leverages model-driven techniques to generate probes for monitoring the performance sub-predicates thus to support the detection of antipatterns at runtime.",
      "Keywords": "",
      "Publication venue": "Electronic Proceedings in Theoretical Computer Science, EPTCS",
      "Publication date": "2014-04-02",
      "Publication type": "Conference Paper",
      "Authors": "Di Marco, Antinisca;Trubiani, Catia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84942521255",
      "Primary study DOI": "10.1145/2593812.2593817",
      "Title": "Rapid requirements checks with requirements smells: Two case studies",
      "Abstract": "Bad requirements quality can have expensive consequences during the software development lifecycle. Especially, if it- erations are long and feedback comes late - The faster a problem is found, the cheaper it is to fix. We propose to detect issues in requirements based on re- quirements (bad) smells by applying a light-weight static requirements analysis. This light-weight technique allows for instant checks as soon as a requirement is written down. In this paper, we derive a set of smells, including automatic smell detection, from the natural language criteria of the ISO/IEC/IEEE 29148 standard. We evaluated the approach with 336 requirements and 53 use cases from 9 specifications that were written by the car manufacturer Daimler AG and the chemical business companyWacker Chemie AG, and discussed the results with their requirements and domain experts. While not all problems can be detected, the case study shows that lightweight smell analysis can uncover many practically relevant requirements defects. Based on these results and the discussion with our industry partners, we conclude that requirements smells can serve as an effcient supplement to traditional reviews or team discussions, in order to create fast feedback on requirements quality.",
      "Keywords": "Analytical quality assurance | Requirements engineering | Requirements smells",
      "Publication venue": "1st International Workshop on Rapid Continuous Software Engineering, RCoSE 2014 - Proceedings",
      "Publication date": "2014-06-03",
      "Publication type": "Conference Paper",
      "Authors": "Femmer, Henning;Fernández, Daniel Méndez;Juergens, Elmar;Klose, Michael;Zimmer, Ilona;Zimmer, Jörg",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84898402943",
      "Primary study DOI": "10.1109/CSMR-WCRE.2014.6747168",
      "Title": "The vision of software clone management: Past, present, and future (Keynote paper)",
      "Abstract": "Duplicated code or code clones are a kind of code smell that have both positive and negative impacts on the development and maintenance of software systems. Software clone research in the past mostly focused on the detection and analysis of code clones, while research in recent years extends to the whole spectrum of clone management. In the last decade, three surveys appeared in the literature, which cover the detection, analysis, and evolutionary characteristics of code clones. This paper presents a comprehensive survey on the state of the art in clone management, with in-depth investigation of clone management activities (e.g., tracing, refactoring, cost-benefit analysis) beyond the detection and analysis. This is the first survey on clone management, where we point to the achievements so far, and reveal avenues for further research necessary towards an integrated clone management system. We believe that we have done a good job in surveying the area of clone management and that this work may serve as a roadmap for future research in the area © 2014 IEEE.",
      "Keywords": "Clone Analysis | Clone Management | Code Clones | Future Research Directions",
      "Publication venue": "2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering, CSMR-WCRE 2014 - Proceedings",
      "Publication date": "2014-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Roy, Chanchal K.;Zibran, Minhaz F.;Koschke, Rainer",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84938262783",
      "Primary study DOI": "",
      "Title": "Comparing heuristic search methods for selecting sequence of refactoring techniques usage for code changing",
      "Abstract": "Refactoring is the process of changing the internal structures, that preserves external behaviors of software. To improve software maintainability, we can apply several refactoring techniques to source code; applying different sequence of refactoring techniques to different parts of the source code results in different code changes and different level of software maintainability. This research uses Heuristic Search methods to find a sequence of refactoring techniques usage for code changing from a search space. Each Heuristic Search method has different characteristics and algorithm to reach an optimal result in solving the problem. Heuristic Search methods including Greedy Algorithm, Breadth First Search, Hill Climbing and A∗ (A Star) are used to search for sequence of refactoring techniques usage from search space and compare the effort and result of the search methods. The purpose of the research is to find the most appropriate Heuristic Search method for searching sequence of refactoring techniques usage with maximum software maintainability and least searching time. The researcher evaluates each Heuristic Search method with source code containing Long Method, Large Class and Feature Envy bad smell. The result shows that Greedy Algorithm shows the best results with maximum software maintainability and the least searching time.",
      "Keywords": "Heuristic search | Refactoring | Refactoring sequencing | Software maintainability",
      "Publication venue": "Lecture Notes in Engineering and Computer Science",
      "Publication date": "2014-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Wongpiang, Ratapong;Muenchaisri, Pornsiri",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84908235542",
      "Primary study DOI": "10.1016/j.scico.2014.02.014",
      "Title": "A logic foundation for a general-purpose history querying tool",
      "Abstract": "Version control systems (VCS) have become indispensable software development tools. The version snapshots they store to provide support for change coordination and release management, effectively track the evolution of the versioned software and its development process. Despite this wealth of historical information, it has only been leveraged by tools that are dedicated to a specific task such as empirical validation of software engineering practices or fault prediction. General-purpose tool support for reasoning about the historical information stored in a version control system is limited. This paper provides a comprehensive description of a logic-based, general-purpose history query tool called Absinthe supports querying versioned Smalltalk system using logic queries in which quantified regular path expressions are embedded. These expressions lend themselves to specifying the properties that each individual version in a sequence of successive software versions ought to exhibit. To demonstrate the general-purpose nature of our history query tool, we use it to verify development process constraints, to identify temporal bad smells and to answer questions that developers commonly ask. Finally, we compare a query written in Absinthe to an equivalent one written in Smalltalk.",
      "Keywords": "History queries | Logic programming | Meta-programming | Program analysis | Smalltalk",
      "Publication venue": "Science of Computer Programming",
      "Publication date": "2014-12-15",
      "Publication type": "Conference Paper",
      "Authors": "Stevens, Reinout;De Roover, Coen;Noguera, Carlos;Kellens, Andy;Jonckers, Viviane",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84938815036",
      "Primary study DOI": "10.1145/2597073.2597081",
      "Title": "Process mining multiple repositories for software defect resolution from control and organizational perspective",
      "Abstract": "Issue reporting and resolution is a software engineering process supported by tools such as Issue Tracking System (ITS), Peer Code Review (PCR) system and Version Control System (VCS). Several open source software projects such as Google Chromium and Android follow process in which a defect or feature enhancement request is reported to an issue tracker followed by source-code change or patch review and patch commit using a version control system. We present an application of process mining three software repositories (ITS, PCR and VCS) from control flow and organizational perspective for effective process management. ITS, PCR and VCS are not explicitly linked so we implement regular expression based heuristics to integrate data from three repositories for Google Chromium project. We define activities such as bug reporting, bug fixing, bug verification, patch submission, patch review, and source code commit and create an event log of the bug resolution process. The extracted event log contains audit trail data such as caseID, timestamp, activity name and performer. We discover runtime process model for bug resolution process spanning three repositories using process mining tool, Disco, and conduct process performance and efficiency analysis. We identify bottlenecks, define and detect basic and composite anti-patterns. In addition to control flow analysis, we mine event log to perform organizational analysis and discover metrics such as handover of work, subcontracting, joint cases and joint activities.",
      "Keywords": "Empirical software engineering and measurements | Issue Tracking System | Peer code review system | Process mining | Social network analysis | Software maintenance",
      "Publication venue": "11th Working Conference on Mining Software Repositories, MSR 2014 - Proceedings",
      "Publication date": "2014-05-31",
      "Publication type": "Conference Paper",
      "Authors": "Gupta, Monika;Sureka, Ashish;Padmanabhuni, Srinivas",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84931058154",
      "Primary study DOI": "10.1109/ICSME.2014.73",
      "Title": "On the impact of refactoring operations on code quality metrics",
      "Abstract": "Refactorings are behavior-preserving source code transformations. While tool support exists for (semi) automatically identifying refactoring solutions, applying or not a recommended refactoring is usually up to the software developers, who have to assess the impact that the transformation will have on their system. Evaluating the pros (e.g., the bad smell removal) and cons (e.g., side effects of the change) of a refactoring is far from trivial. We present RIPE (Refactoring Impact Prediction), a technique that estimates the impact of refactoring operations on source code quality metrics. RIPE supports 12 refactoring operations and 11 metrics and it can be used together with any refactoring recommendation tool. RIPE was used to estimate the impact on 8,103 metric values, for 504 refactorings from 15 open source systems. 38% of the estimates are correct, whereas the median deviation of the estimates from the actual values is 5% (with a 31% average).",
      "Keywords": "Code Quality | Refactoring Impact",
      "Publication venue": "Proceedings - 30th International Conference on Software Maintenance and Evolution, ICSME 2014",
      "Publication date": "2014-12-04",
      "Publication type": "Conference Paper",
      "Authors": "Chaparro, Oscar;Bavota, Gabriele;Marcus, Andrian;Di Penta, Massimiliano",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84942917227",
      "Primary study DOI": "10.1016/j.scico.2013.09.015",
      "Title": "A graph mining approach for detecting identical design structures in object-oriented design models",
      "Abstract": "The object-oriented approach has been the most popular software design methodology for the past twenty-five years. Several design patterns and principles are defined to improve the design quality of object-oriented software systems. In addition, designers can use unique design motifs that are designed for the specific application domains. Another commonly used technique is cloning and modifying some parts of the software while creating new modules. Therefore, object-oriented programs can include many identical design structures. This work proposes a sub-graph mining-based approach for detecting identical design structures in object-oriented systems. By identifying and analyzing these structures, we can obtain useful information about the design, such as commonly-used design patterns, most frequent design defects, domain-specific patterns, and reused design clones, which could help developers to improve their knowledge about the software architecture. Furthermore, problematic parts of frequent identical design structures are appropriate refactoring opportunities because they affect multiple areas of the architecture. Experiments with several open-source and industrial projects show that we can successfully find many identical design structures within a project (intra-project) and between different projects (inter-project). We observe that usually most of the detected identical structures are an implementation of common design patterns; however, we also detect various anti-patterns, domain-specific patterns, reused design parts and design-level clones.",
      "Keywords": "Graph mining | Identical design structures | Pattern extraction | Software design models | Software motifs",
      "Publication venue": "Science of Computer Programming",
      "Publication date": "2014-01-01",
      "Publication type": "Article",
      "Authors": "Tekin, Umut;Buzluca, Feza",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84931095956",
      "Primary study DOI": "10.1109/ICSME.2014.125",
      "Title": "Specification and detection of SOA antipatterns",
      "Abstract": "Service-oriented architecture (SOA) provides a collection of principles and methodologies for designing and developing service-based systems (SBSs). SBSs are composed of loosely-coupled, platform independent, and reusable functional units, i.e., services. Alternative technologies to implement SBSs are REST-style (Representational State Transfer), Service Component Architecture (SCA), SOAP-based Web service, and so on. However, SBSs cannot overcome some common software engineering challenges, e.g., evolution, to fit new user requirements or changes in execution contexts. All these changes may degrade the quality of design and quality of service of SBSs and may cause the presence of common bad practiced solutions - antipatterns.",
      "Keywords": "",
      "Publication venue": "Proceedings - 30th International Conference on Software Maintenance and Evolution, ICSME 2014",
      "Publication date": "2014-12-04",
      "Publication type": "Conference Paper",
      "Authors": "Palma, Francis",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84917737866",
      "Primary study DOI": "",
      "Title": "Capturing software evolution and change through code repository smells",
      "Abstract": "In the last years we have seen the rise and the fall of many version control systems. These systems collect a large amount of data spanning from the path of the files involved in changes to the exact text changed in every file. This data can be exploited to produce an overview about how the system changed over time and evolved. We have developed a tool, called VCS-Analyzer, to use this information, both for data retrieval and analysis tasks. Currently, VCS-Analyzer implements six different analyses: two based on source code for the computation of metrics and the detection of code smells, and four original analysis based on repositories metadata, which are based on the concepts of Repository Metrics and Code Repository Smells. In this paper, we describe one smell and two metrics we have defined for source code repositories analysis.",
      "Keywords": "Code changes | Code Repository smells | Repository analysis | Repository Metrics",
      "Publication venue": "Lecture Notes in Business Information Processing",
      "Publication date": "2014-01-01",
      "Publication type": "Article",
      "Authors": "Fontana, Francesca Arcelli;Rolla, Matteo;Zanoni, Marco",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84908680092",
      "Primary study DOI": "10.1145/2566486.2568002",
      "Title": "Test-driven evaluation of Linked Data quality",
      "Abstract": "Linked Open Data (LOD) comprises an unprecedented volume of structured data on the Web. However, these datasets are of varying quality ranging from extensively curated datasets to crowd-sourced or extracted data of often relatively low quality. We present a methodology for test-driven quality assessment of Linked Data, which is inspired by testdriven software development. We argue that vocabularies, ontologies and knowledge bases should be accompanied by a number of test cases, which help to ensure a basic level of quality. We present a methodology for assessing the quality of linked data resources, based on a formalization of bad smells and data quality problems. Our formalization employs SPARQL query templates, which are instantiated into concrete quality test case queries. Based on an extensive survey, we compile a comprehensive library of data quality test case patterns. We perform automatic test case instantiation based on schema constraints or semi-automatically enriched schemata and allow the user to generate specific test case instantiations that are applicable to a schema or dataset. We provide an extensive evaluation of five LOD datasets, manual test case instantiation for five schemas and automatic test case instantiations for all available schemata registered with Linked Open Vocabularies (LOV). One of the main advantages of our approach is that domain specific semantics can be encoded in the data quality test cases, thus being able to discover data quality problems beyond conventional quality heuristics. Copyright is held by the International World Wide Web Conference Committee (IW3C2).",
      "Keywords": "Data quality | DBpedia | Linked data",
      "Publication venue": "WWW 2014 - Proceedings of the 23rd International Conference on World Wide Web",
      "Publication date": "2014-04-07",
      "Publication type": "Conference Paper",
      "Authors": "Kontokostas, Dimitris;Westphal, Patrick;Auer, Sören;Hellmann, Sebastian;Lehmann, Jens;Cornelissen, Roland;Zaveri, Amrapali",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84891073356",
      "Primary study DOI": "10.1109/INTECH.2013.6653701",
      "Title": "A novel approach to effective detection and analysis of code clones",
      "Abstract": "Code clones are found in most of the software systems. They play a major role in the field of software engineering. The presence of clones in a particular module will either improve or degrade the quality of the overall software system. Poor quality software indirectly leads to strenuous software maintenance. Detecting the code clones will also pave way for analyzing them. The existing approaches detect the clones efficiently. Though some of the tools analyze the clones, accuracy is still missing. In this paper, a novel method is proposed, which exhibits the use of an efficient data mining technique in the phase of analysis. Based on the outcome of the analysis, the clones are either removed or retained in the software system. © 2013 IEEE.",
      "Keywords": "Bad smells | Code clones | Data Mining Technique | Software Engineering",
      "Publication venue": "2013 3rd International Conference on Innovative Computing Technology, INTECH 2013",
      "Publication date": "2013-12-31",
      "Publication type": "Conference Paper",
      "Authors": "Rajakumari, Kavitha Esther;Jebarajan, T.",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84894444585",
      "Primary study DOI": "10.1109/ICCCNT.2013.6726851",
      "Title": "Agent based tool for topologically sorting badsmells and refactoring by analyzing complexities in source code",
      "Abstract": "Code smells are smells found in source code. As the source code becomes larger and larger we find bad smells in the source code. These bad smells are removed using Refactoring. Hence experts say the method of removing bad smells without changing the quality of code is called as Refactoring[1]. But this Refactoring if not done properly is risky, and can take time (i.e.) might be days or weeks. Hence here we provide a technique to arrange these Bad smells analyze the complexities found in the source code and then Refactor them. These Bad smell detection and scheduling has been done manually or semi automatically. This paper provides a method of automatically detecting theses Bad smells. This Automatic detection of Bad smells are done with the help of Java Agent DEvelopment.(JADE) © 2013 IEEE.",
      "Keywords": "Refactoring | Scheduling Badsmells | Software Engineering",
      "Publication venue": "2013 4th International Conference on Computing, Communications and Networking Technologies, ICCCNT 2013",
      "Publication date": "2013-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Ayshwaryalakshmi, S.;Mary, S. A.Sahaaya Arul;Vadivu, S. Shanmuga",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84891720206",
      "Primary study DOI": "10.1109/ICSM.2013.97",
      "Title": "How good are code smells for evaluating software maintainability? Results from a comparative case study",
      "Abstract": "An advantage of code smells over traditional software measures is that the former are associated with an explicit set of refactorings to improve the existing design. Past research on code smells has emphasized the formalization and automated detection of code smells, but much less has been done to empirically investigate how good are code smells for evaluating software maintainability. This paper presents a summary of the findings in the thesis by Yamashita, which aimed at investigating the strengths and limitations of code smells for evaluating software maintainability. The study conducted comprised an outsourced maintenance project involving four Java web systems with equivalent functionality but dissimilar implementation, six software professionals, and two software companies. A main result from the study is that the usefulness of code smells differs according to the granularity level (e.g., whether the assessment is done at file or system level) and the particular operationalization of maintainability (e.g., maintainability can be measured via maintenance effort, or problems encountered during maintenance, etc). This paper summarises the most relevant findings from the thesis, discusses a series of lessons learned from conducting this study, and discusses avenues for new research in the area of code smells. © 2013 IEEE.",
      "Keywords": "Bad smells | Code smells | Comparative Case Study | Empirical study | Software maintenance | Software quality",
      "Publication venue": "IEEE International Conference on Software Maintenance, ICSM",
      "Publication date": "2013-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Yamashita, Aiko",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84891705224",
      "Primary study DOI": "10.1109/ICSM.2013.37",
      "Title": "Investigating the impact of code smells on system's quality: An empirical study on systems of different application domains",
      "Abstract": "There are various activities that support software maintenance. Program comprehension and detection of design anomalies and their symptoms, like code smells and anti patterns, are particularly relevant for improving the quality and facilitating evolution of a system. In this paper we describe an empirical study on the detection of code smells, aiming at identifying the most frequent smells in systems of different domains and hence the domains characterized by more smells. Moreover, we study possible correlations existing among smells and the values of a set of software quality metrics using Spearman's rank correlation and Principal Component Analysis. © 2013 IEEE.",
      "Keywords": "Code smell detection | Domain-dependent analysis | Metric and smell correlations | Software evolution | Software maintenance",
      "Publication venue": "IEEE International Conference on Software Maintenance, ICSM",
      "Publication date": "2013-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Fontana, Francesca Arcelli;Ferme, Vincenzo;Marino, Alessandro;Walter, Bartosz;Martenka, Paweł",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85051992969",
      "Primary study DOI": "10.5220/0006709801370146",
      "Title": "A hybrid approach to detect code smells using deep learning",
      "Abstract": "The detection of code smells is a fundamental prerequisite for guiding the subsequent steps in the refactoring process. The more the detection results are accurate, the more the performance of the refactoring on the software is improved. Given its influential role in the software maintenance, this challenging research topic has so far attracted an increasing interest. However, the lack of consensus about the definition of code smells in the literature has led to a considerable diversity of the existing results. To reduce the confusion associated with this lack of consensus, there is a real need to achieve a deep and consistent representation of the code smells. Recently, the advance of deep learning has demonstrated an undeniable contribution in many research fields including the pattern recognition issues. In this paper, we propose a hybrid detection approach based on deep Auto-encoder and Artificial Neural Network algorithms. Four code smells (God Class, Data Class, Feature Envy and Long Method) are the focus of our experiment on four adopted datasets that are extracted from 74 open source systems. The values of recall and precision measurements have demonstrated high accuracy results.",
      "Keywords": "Artificial Neural Networks | Auto-encoder | Code Smell Detection | Deep Learning | Hybrid Approach",
      "Publication venue": "ENASE 2018 - Proceedings of the 13th International Conference on Evaluation of Novel Approaches to Software Engineering",
      "Publication date": "2018-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Hadj-Kacem, Mouna;Bouassida, Nadia",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85083421464",
      "Primary study DOI": "10.1007/978-3-030-34706-2_8",
      "Title": "Code smell prediction employing machine learning meets emerging Java language constructs",
      "Abstract": "Background: Defining code smell is not a trivial task. Their recognition tends to be highly subjective. Nevertheless some code smells detection tools have been proposed. Other recent approaches incline towards machine learning (ML) techniques to overcome disadvantages of using automatic detection tools. Objectives: We aim to develop a research infrastructure and reproduce the process of code smell prediction proposed by Arcelli Fontana et al. We investigate ML algorithms performance for samples including major modern Java language features. Those such as lambdas can shorten the code causing code smell presence not as obvious to detect and thus pose a challenge to both existing code smell detection tools and ML algorithms. Method: We extend the study with dataset consisting of 281 Java projects. For driving samples selection we define metrics considering lambdas and method reference, derived using custom JavaParser-based solution. Tagged samples with new constructions are used as an input for the utilized detection techniques. Results: Detection rules derived from the best performing algorithms like J48 and JRip incorporate newly introduced metrics. Conclusions: Presence of certain new Java language constructs may hide Long Method code smell or indicate a God Class. On the other hand, their absence or low number can suggest a Data Class.",
      "Keywords": "Code smells detection | Machine learning | Replication study",
      "Publication venue": "Lecture Notes on Data Engineering and Communications Technologies",
      "Publication date": "2020-01-01",
      "Publication type": "Book Chapter",
      "Authors": "Grodzicka, Hanna;Ziobrowski, Arkadiusz;Łakomiak, Zofia;Kawa, Michał;Madeyski, Lech",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84893376511",
      "Primary study DOI": "10.1109/WCRE.2013.6671299",
      "Title": "Do developers care about code smells? An exploratory survey",
      "Abstract": "Code smells are a well-known metaphor to describe symptoms of code decay or other issues with code quality which can lead to a variety of maintenance problems. Even though code smell detection and removal has been well-researched over the last decade, it remains open to debate whether or not code smells should be considered meaningful conceptualizations of code quality issues from the developer's perspective. To some extent, this question applies as well to the results provided by current code smell detection tools. Are code smells really important for developers? If they are not, is this due to the lack of relevance of the underlying concepts, due to the lack of awareness about code smells on the developers' side, or due to the lack of appropriate tools for code smell analysis or removal? In order to align and direct research efforts to address actual needs and problems of professional developers, we need to better understand the knowledge about, and interest in code smells, together with their perceived criticality. This paper reports on the results obtained from an exploratory survey involving 85 professional software developers. © 2013 IEEE.",
      "Keywords": "code analysis tools | code smell detection | code smells | maintainability | refactoring | survey | usability",
      "Publication venue": "Proceedings - Working Conference on Reverse Engineering, WCRE",
      "Publication date": "2013-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Yamashita, Aiko;Moonen, Leon",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84893415440",
      "Primary study DOI": "10.1109/WCRE.2013.6671310",
      "Title": "Mining the relationship between anti-patterns dependencies and fault-proneness",
      "Abstract": "Anti-patterns describe poor solutions to design and implementation problems which are claimed to make object oriented systems hard to maintain. Anti-patterns indicate weaknesses in design that may slow down development or increase the risk of faults or failures in the future. Classes in anti-patterns have some dependencies, such as static relationships, that may propagate potential problems to other classes. To the best of our knowledge, the relationship between anti-patterns dependencies (with non anti-patterns classes) and faults has yet to be studied in details. This paper presents the results of an empirical study aimed at analysing anti-patterns dependencies in three open source software systems, namely ArgoUML, JFreeChart, and XerecesJ. We show that, in almost all releases of the three systems, classes having dependencies with anti-patterns are more fault-prone than others. We also report other observations about these dependencies such as their impact on fault prediction. Software organizations could make use of these knowledge about anti-patterns dependencies to better focus their testing and reviews activities toward the most risky classes, e.g., classes with fault-prone dependencies with anti-patterns. © 2013 IEEE.",
      "Keywords": "Anti-patterns | co-change | empirical software engineering | fault-proneness | mining software repositories | static relationships",
      "Publication venue": "Proceedings - Working Conference on Reverse Engineering, WCRE",
      "Publication date": "2013-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Jaafar, Fehmi;Gueheneuc, Yann Gael;Hamel, Sylvie;Khomh, Foutse",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84892905220",
      "Primary study DOI": "10.1007/978-3-642-37804-1_43",
      "Title": "Detection of SOA antipatterns",
      "Abstract": "Like any other large and complex systems, user requirements may change for Service Based Systems (SBSs), as well as their execution contexts, in the form of evolution and maintenance. Consequently, these changes may cause degradation of design, and Quality of Service (QoS), resulting to the bad practiced solutions, commonly known as Antipatterns. Therefore, detecting SOA (Service Oriented Architecture) antipatterns deserves an extra importance for assessing the design and QoS of SBSs. Also, this detection may facilitate the future evolution and maintenance. Despite of its importance, there are no methods and techniques for detecting SOA antipatterns within SBSs. The subject of my PhD thesis is to propose a novel and innovative approach, supported by a framework for specifying and detecting SOA antipatterns. My contributions are: (1) an approach for SOA antipatterns detection, (2) a framework supporting analysis and detection for SOA antipatterns in SBSs, and finally (3) a concrete empirical evidence to show the effectiveness of the proposed approach and framework. © Springer-Verlag 2013.",
      "Keywords": "Design | Detection | Quality of service | Service based systems | SOA antipatterns | Software evolution and maintenance",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2013-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Palma, Francis",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84893390884",
      "Primary study DOI": "10.1109/WCRE.2013.6671307",
      "Title": "Improving SOA antipatterns detection in Service Based Systems by mining execution traces",
      "Abstract": "Service Based Systems (SBSs), like other software systems, evolve due to changes in both user requirements and execution contexts. Continuous evolution could easily deteriorate the design and reduce the Quality of Service (QoS) of SBSs and may result in poor design solutions, commonly known as SOA antipatterns. SOA antipatterns lead to a reduced maintainability and reusability of SBSs. It is therefore important to first detect and then remove them. However, techniques for SOA antipattern detection are still in their infancy, and there are hardly any tools for their automatic detection. In this paper, we propose a new and innovative approach for SOA antipattern detection called SOMAD (Service Oriented Mining for Antipattern Detection) which is an evolution of the previously published SODA (Service Oriented Detection For Antpatterns) tool. SOMAD improves SOA antipattern detection by mining execution traces: It detects strong associations between sequences of service/method calls and further filters them using a suite of dedicated metrics. We first present the underlying association mining model and introduce the SBS-oriented rule metrics. We then describe a validating application of SOMAD to two independently developed SBSs. A comparison of our new tool with SODA reveals superiority of the former: Its precision is better by a margin ranging from 2.6% to 16.67% while the recall remains optimal at 100% and the speed is significantly reduces (2.5+ times on the same test subjects). © 2013 IEEE.",
      "Keywords": "Mining Execution Traces | Sequential Association Rules | Service Oriented Architecture | SOA Antipatterns",
      "Publication venue": "Proceedings - Working Conference on Reverse Engineering, WCRE",
      "Publication date": "2013-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Nayrolles, Mathieu;Moha, Naouel;Valtchev, Petko",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84891683788",
      "Primary study DOI": "10.1109/ICSM.2013.38",
      "Title": "Predicting bugs using antipatterns",
      "Abstract": "Bug prediction models are often used to help allocate software quality assurance efforts. Software metrics (e.g., process metrics and product metrics) are at the heart of bug prediction models. However, some of these metrics like churn are not actionable, on the contrary, antipatterns which refer to specific design and implementation styles can tell the developers whether a design choice is \"poor\" or not. Poor designs can be fixed by refactoring. Therefore in this paper, we explore the use of antipatterns for bug prediction, and strive to improve the accuracy of bug prediction models by proposing various metrics based on antipatterns. An additional feature to our proposed metrics is that they take into account the history of antipatterns in files from their inception into the system. Through a case study on multiple versions of Eclipse and ArgoUML, we observe that (i) files participating in antipatterns have higher bug density than other files, (ii) our proposed antipattern based metrics can provide additional explanatory power over traditional metrics, and (iii) improve the F-measure of cross-system bug prediction models by 12.5% in average. Managers and quality assurance personnel can use our proposed metrics to better improve their bug prediction models and better focus testing activities and the allocation of support resources. © 2013 IEEE.",
      "Keywords": "Antipattern | Bug prediction | Software quality",
      "Publication venue": "IEEE International Conference on Software Maintenance, ICSM",
      "Publication date": "2013-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Taba, Seyyed Ehsan Salamati;Khomh, Foutse;Zou, Ying;Hassan, Ahmed E.;Nagappan, Meiyappan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84893746495",
      "Primary study DOI": "10.1109/CINTI.2013.6705180",
      "Title": "Semi-automatic refactoring to aspect-oriented platform",
      "Abstract": "Refactoring is necessary in large software projects and an aspect-oriented approach can help to maintain concerns in the source code. While tools for object-oriented refactoring are the usual part of development environments, support for aspect-oriented refactoring is minimal. We analyzed object-oriented bad smells applicable for an aspect-oriented approach and propose a method for the detection of crosscutting concerns and consecutive refactoring. © 2013 IEEE.",
      "Keywords": "",
      "Publication venue": "CINTI 2013 - 14th IEEE International Symposium on Computational Intelligence and Informatics, Proceedings",
      "Publication date": "2013-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Roman, Pipik;Polasek, Ivan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84891772484",
      "Primary study DOI": "10.1142/S0218843013410049",
      "Title": "Soa antipatterns: An approach for their specification and detection",
      "Abstract": "Like any other large and complex software systems, Service-Based Systems (SBSs) must evolve to fit new user requirements and execution contexts. The changes resulting from the evolution of SBSs may degrade their design and quality of service (QoS) and may often cause the appearance of common poor solutions in their architecture, called antipatterns, in opposition to design patterns, which are good solutions to recurring problems. Antipatterns resulting from these changes may hinder the future maintenance and evolution of SBSs. The detection of antipatterns is thus crucial to assess the design and QoS of SBSs and facilitate their maintenance and evolution. However, methods and techniques for the detection of antipatterns in SBSs are still in their infancy despite their importance. In this paper, we introduce a novel and innovative approach supported by a framework for specifying and detecting antipatterns in SBSs. Using our approach, we specify 10 well-known and common antipatterns, including Multi Service and Tiny Service, and automatically generate their detection algorithms. We apply and validate the detection algorithms in terms of precision and recall two systems developed independently, (1) Home-Automation, an SBS with 13 services, and (2) FraSCAti, an open-source implementation of the Service Component Architecture (SCA) standard with more than 100 services. This validation demonstrates that our approach enables the specification and detection of Service Oriented Architecture (SOA) antipatterns with an average precision of 90% and recall of 97.5%. © 2013 World Scientific Publishing Company.",
      "Keywords": "Antipatterns | design | detection | quality of service | service component architecture | service-based systems | software evolution and maintenance | specification",
      "Publication venue": "International Journal of Cooperative Information Systems",
      "Publication date": "2013-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Palma, Francis;Nayrolles, Mathieu;Moha, Naouel;Guéhéneuc, Yann Gaël;Baudry, Benoit;Jézéquel, Jean Marc",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84894127835",
      "Primary study DOI": "10.1007/978-3-642-35992-7_3",
      "Title": "Leveraging static analysis in an IDE",
      "Abstract": "In recent years, Integrated Development Environments (IDEs) have risen from nicety to an essential part of most programming language tool-chains. Indeed, they are widely seen as critical to the widespread adoption of a new programming language. This is due in part to the emergence of a higher-level dialogue between a developer and his code, made possible by advanced tooling that aids in navigating, understanding, and manipulating code. In turn, much of this advanced tooling relies heavily on various forms of static analysis. Unfortunately, many practitioners of static analysismethods are notwell skilled in incorporating their analyses into an IDEcontext. The result is often high-powered tools that lack key usability characteristics, and thus fall short of their potential to assist developers. This tutorial attempts to help bridge the skill gap by describing several applications of static analysis within an IDE setting. We describe the computation and presentation of type hierarchy information, data flow information in the form of def/use chains, the use of a type inferencing engine to detect type-related code \"smells\", and the use of memory effects analysis to determine the safety of certain parallelization-related refactorings. © Springer-Verlag Berlin Heidelberg 2013.",
      "Keywords": "IDEs | Software development tooling | Static analysis",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2013-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Fuhrer, Robert M.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84883681660",
      "Primary study DOI": "10.1109/ICSE.2013.6606575",
      "Title": "Data clone detection and visualization in spreadsheets",
      "Abstract": "Spreadsheets are widely used in industry: it is estimated that end-user programmers outnumber programmers by a factor 5. However, spreadsheets are error-prone, numerous companies have lost money because of spreadsheet errors. One of the causes for spreadsheet problems is the prevalence of copy-pasting. In this paper, we study this cloning in spreadsheets. Based on existing text-based clone detection algorithms, we have developed an algorithm to detect data clones in spreadsheets: formulas whose values are copied as plain text in a different location. To evaluate the usefulness of the proposed approach, we conducted two evaluations. A quantitative evaluation in which we analyzed the EUSES corpus and a qualitative evaluation consisting of two case studies. The results of the evaluation clearly indicate that 1) data clones are common, 2) data clones pose threats to spreadsheet quality and 3) our approach supports users in finding and resolving data clones. © 2013 IEEE.",
      "Keywords": "clone detection | code smells | spreadsheet smells | spreadsheets",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2013-10-30",
      "Publication type": "Conference Paper",
      "Authors": "Hermans, Felienne;Sedee, Ben;Pinzger, Martin;Van Deursen, Arie",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-78751529388",
      "Primary study DOI": "10.1109/QUATIC.2010.61",
      "Title": "IDS: An immune-inspired approach for the detection of software design smells",
      "Abstract": "We propose a parallel between object-oriented system designs and living creatures. We suggest that, like any living creature, system designs are subject to diseases, which are design smells (code smells and anti patterns). Design smells are conjectured in the literature to impact the quality and life of systems and, therefore, their detection has drawn the attention of both researchers and practitioners with various approaches. With our parallel, we propose a novel approach built on models of the immune system responses to pathogenic material. We show that our approach can detect more than one smell at a time. We build and test our approach on Gantt Project v1.10.2 and Xerces v2.7.0, for which manually-validated and publicly-available smells exist. The results show a significant improvement in detection time, precision, and recall, in comparison to the state-of-the-art approaches. © 2010 IEEE.",
      "Keywords": "Antipatterns | Artificial immune systems | Code smells | Reverse engineering | System design",
      "Publication venue": "Proceedings - 7th International Conference on the Quality of Information and Communications Technology, QUATIC 2010",
      "Publication date": "2010-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Hassaine, Salima;Khomh, Foutse;Guéhéneucy, Yann Gaël;Hamel, Sylvie",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84886418892",
      "Primary study DOI": "10.1109/ICSE.2013.6606601",
      "Title": "Supporting swift reaction: Automatically uncovering performance problems by systematic experiments",
      "Abstract": "Performance problems pose a significant risk to software vendors. If left undetected, they can lead to lost customers, increased operational costs, and damaged reputation. Despite all efforts, software engineers cannot fully prevent performance problems being introduced into an application. Detecting and resolving such problems as early as possible with minimal effort is still an open challenge in software performance engineering. In this paper, we present a novel approach for Performance Problem Diagnostics (PPD) that systematically searches for well-known performance problems (also called performance antipatterns) within an application. PPD automatically isolates the problem's root cause, hence facilitating problem solving. We applied PPD to a well established transactional web e-Commerce benchmark (TPC-W) in two deployment scenarios. PPD automatically identified four performance problems in the benchmark implementation and its deployment environment. By fixing the problems, we increased the maximum throughput of the benchmark from 1800 requests per second to more than 3500. © 2013 IEEE.",
      "Keywords": "measurement | performance | problem detection",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2013-10-30",
      "Publication type": "Conference Paper",
      "Authors": "Wert, Alexander;Happe, Jens;Happe, Lucia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84885208758",
      "Primary study DOI": "10.1109/USER.2013.6603077",
      "Title": "Surveying developer knowledge and interest in code smells through online freelance marketplaces",
      "Abstract": "This paper discusses the use of freelance marketplaces to conduct a survey amongst professional developer's about specific software engineering phenomena, in our case their knowledge and interest in code smells and their detection/removal. We present the context and motivation of our research, and the idea of using freelance marketplaces for conducting studies involving software professionals. Next, we describe the design of the survey and the specifics on the selected freelance marketplace (i.e., Freelancer.com). Finally, we discuss why freelance markets constitute a feasible and advantageous approach for conducting user evaluations that involve large numbers of software professionals, and what challenges such an approach may entail. © 2013 IEEE.",
      "Keywords": "developer knowledge | survey | user evaluation",
      "Publication venue": "2013 2nd International Workshop on User Evaluations for Software Engineering Researchers, USER 2013 - Proceedings",
      "Publication date": "2013-10-15",
      "Publication type": "Conference Paper",
      "Authors": "Yamashita, Aiko;Moonen, Leon",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85023165696",
      "Primary study DOI": "10.5220/0006338804740482",
      "Title": "Evaluating the accuracy of machine learning algorithms on detecting code smells for different developers",
      "Abstract": "Code smells indicate poor implementation choices that may hinder the system maintenance. Their detection is important for the software quality improvement, but studies suggest that it should be tailored to the perception of each developer. Therefore, detection techniques must adapt their strategies to the developer's perception. Machine Learning (ML) algorithms is a promising way to customize the smell detection, but there is a lack of studies on their accuracy in detecting smells for different developers. This paper evaluates the use of MLalgorithms on detecting code smells for different developers, considering their individual perception about code smells. We experimentally compared the accuracy of 6 algorithms in detecting 4 code smell types for 40 different developers. For this, we used a detailed dataset containing instances of 4 code smell types manually validated by 40 developers. The results show that ML-Algorithms achieved low accuracies for the developers that participated of our study, showing that are very sensitive to the smell type and the developer. These algorithms are not able to learn with limited training set, an important limitation when dealing with diverse perceptions about code smells.",
      "Keywords": "Code smell | Experiment | Machine learning",
      "Publication venue": "ICEIS 2017 - Proceedings of the 19th International Conference on Enterprise Information Systems",
      "Publication date": "2017-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Hozano, Mário;Antunes, Nuno;Fonseca, Baldoino;Costa, Evandro",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84884959625",
      "Primary study DOI": "",
      "Title": "Search Based Software Engineering - 5th International Symposium, SSBSE 2013, Proceedings",
      "Abstract": "The proceedings contain 29 papers. The topics discussed include: advances in automated program repair and a call to arms; some recent work on multi-objective approaches to search-based software engineering; on the application of the multi-evolutionary and coupling-based approach with different aspect-class integration testing strategies; an experimental study on incremental search-based software engineering; competitive coevolutionary code-smells detection; a multi-objective genetic algorithm to rank state-based test cases; automated model-in-the-loop testing of continuous controllers using search; predicting regression test failures using genetic algorithm-selected dynamic performance analysis metrics; a recoverable robust approach for the next release problem; and regression testing for model transformations: a multi-objective approach.",
      "Keywords": "",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2013-10-08",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84881048345",
      "Primary study DOI": "10.1109/TSE.2013.4",
      "Title": "Monitor-based instant software refactoring",
      "Abstract": "Software refactoring is an effective method for improvement of software quality while software external behavior remains unchanged. To facilitate software refactoring, a number of tools have been proposed for code smell detection and/or for automatic or semi-automatic refactoring. However, these tools are passive and human driven, thus making software refactoring dependent on developers' spontaneity. As a result, software engineers with little experience in software refactoring might miss a number of potential refactorings or may conduct refactorings later than expected. Few refactorings might result in poor software quality, and delayed refactorings may incur higher refactoring cost. To this end, we propose a monitor-based instant refactoring framework to drive inexperienced software engineers to conduct more refactorings promptly. Changes in the source code are instantly analyzed by a monitor running in the background. If these changes have the potential to introduce code smells, i.e., signs of potential problems in the code that might require refactorings, the monitor invokes corresponding smell detection tools and warns developers to resolve detected smells promptly. Feedback from developers, i.e., whether detected smells have been acknowledged and resolved, is consequently used to optimize smell detection algorithms. The proposed framework has been implemented, evaluated, and compared with the traditional human-driven refactoring tools. Evaluation results suggest that the proposed framework could drive inexperienced engineers to resolve more code smells (by an increase of 140 percent) promptly. The average lifespan of resolved smells was reduced by 92 percent. Results also suggest that the proposed framework could help developers to avoid similar code smells through timely warnings at the early stages of software development, thus reducing the total number of code smells by 51 percent. © 1976-2012 IEEE.",
      "Keywords": "code smell detection | instant refactoring | monitor | Software refactoring",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2013-08-08",
      "Publication type": "Article",
      "Authors": "Liu, Hui;Guo, Xue;Shao, Weizhong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84877282906",
      "Primary study DOI": "10.1145/2460999.2461007",
      "Title": "An exploratory study to investigate the impact of conceptualization in god class detection",
      "Abstract": "Context: The concept of code smells is widespread in Software Engineering. However, in spite of the many discussions and claims about them, there are few empirical studies to support or contest these ideas. In particular, the study of the human perception of what is a code smell and how to deal with it has been mostly neglected. Objective: To build empirical support to understand the effect of god classes, one of the most known code smells. In particular, this paper focuses on how conceptualization affects identification of god classes, i.e., how different people perceive the god class concept. Method: A controlled experiment that extends and builds upon another empirical study about how humans detect god classes [19]. Our study: i) deepens and details some of the research questions of the previous study, ii) introduces a new research question and, iii) when possible, compares the results of both studies. Result: Our findings show that participants have different personal criteria and preferences in choosing drivers to identify god classes. The agreement between participants is not high, which is in accordance with previous studies. Conclusion: This study contributes to expand the empirical data about the human perception of code smells. It also presents a new way to evaluate effort and distraction in experiments through the use of automatic logging of participant actions. Copyright 2013 ACM.",
      "Keywords": "Code smell | Controlled experiment | God class",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2013-05-13",
      "Publication type": "Conference Paper",
      "Authors": "Santos, José A.M.;De Mendonça, Manoel G.;Silva, Carlos V.A.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84874898771",
      "Primary study DOI": "10.1145/2430536.2430541",
      "Title": "Detecting missing method calls as violations of the majority rule",
      "Abstract": "When using object-oriented frameworks it is easy to overlook certain important method calls that are required at particular places in code. In this article, we provide a comprehensive set of empirical facts on this problem, starting from traces of missing method calls in a bug repository. We propose a new system that searches for missing method calls in software based on the other method calls that are observable. Our key insight is that the voting theory concept of majority rule holds for method calls: a call is likely to be missing if there is a majority of similar pieces of code where this call is present. The evaluation shows that the system predictions go further missing method calls and often reveal different kinds of code smells (e.g., violations of API best practices). © 2013 ACM.",
      "Keywords": "Verification",
      "Publication venue": "ACM Transactions on Software Engineering and Methodology",
      "Publication date": "2013-02-01",
      "Publication type": "Article",
      "Authors": "Monperrus, Martin;Mezini, Mira",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84885178498",
      "Primary study DOI": "10.1016/j.infsof.2013.08.002",
      "Title": "To what extent can maintenance problems be predicted by code smell detection? -An empirical study",
      "Abstract": "Context: Code smells are indicators of poor coding and design choices that can cause problems during software maintenance and evolution. Objective: This study is aimed at a detailed investigation to which extent problems in maintenance projects can be predicted by the detection of currently known code smells. Method: A multiple case study was conducted, in which the problems faced by six developers working on four different Java systems were registered on a daily basis, for a period up to four weeks. Where applicable, the files associated to the problems were registered. Code smells were detected in the pre-maintenance version of the systems, using the tools Borland Together and InCode. In-depth examination of quantitative and qualitative data was conducted to determine if the observed problems could be explained by the detected smells. Results: From the total set of problems, roughly 30% percent were related to files containing code smells. In addition, interaction effects were observed amongst code smells, and between code smells and other code characteristics, and these effects led to severe problems during maintenance. Code smell interactions were observed between collocated smells (i.e., in the same file), and between coupled smells (i.e., spread over multiple files that were coupled). Conclusions: The role of code smells on the overall system maintainability is relatively minor, thus complementary approaches are needed to achieve more comprehensive assessments of maintainability. Moreover, to improve the explanatory power of code smells, interaction effects amongst collocated smells and coupled smells should be taken into account during analysis. © 2013 Elsevier B.V. All rights reserved.",
      "Keywords": "Code smells | Empirical study | Maintainability",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2013-01-01",
      "Publication type": "Article",
      "Authors": "Yamashita, Aiko;Moonen, Leon",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84893501345",
      "Primary study DOI": "10.1109/EDOCW.2013.26",
      "Title": "Detection of process antipatterns: A BPEL perspective",
      "Abstract": "moha.naouel@uqam.ca With the increasing significance of the serviceoriented paradigm for implementing business solutions, assessing and analyzing such solutions also becomes an essential task to ensure and improve their quality of design. One way to develop such solutions, a.k.a., Service-Based systems (SBSs) is to generate BPEL (Business Process Execution Language) processes via orchestrating Web services. Development of large business processes (BPs) involves design decisions. Improper and wrong design decisions in software engineering are commonly known as antipatterns, i.e., poor solutions that might affect the quality of design. The detection of antipatterns is thus important to ensure and improve the quality of BPs. However, although BP antipatterns have been defined in the literature, no effort was given to detect such antipatterns within BPEL processes. With the aim of improving the design and quality of BPEL processes, we propose the first rule-based approach to specify and detect BP antipatterns. We specify 7 BP antipatterns from the literature and perform the detection for 4 of them in an initial experiment with 3 BPEL processes. © 2013 IEEE.",
      "Keywords": "Antipatterns | Business processes | Design | Detection | Service-based systems | Specification",
      "Publication venue": "Proceedings - IEEE International Enterprise Distributed Object Computing Workshop, EDOC",
      "Publication date": "2013-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Palma, Francis;Moha, Naouel;Guéhéneuc, Yann Gaël",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85008254621",
      "Primary study DOI": "10.5626/JCSE.2013.7.4.251",
      "Title": "A catalog of bad smells in design-by-contract methodologies with Java modeling language",
      "Abstract": "Bad smells are usually related to program source code, arising from bad design and programming practices. Refactoring activities are often motivated by the detection of bad smells. With the increasing adoption of Design-by-Contract (DBC) methodologies in formal software development, evidence of bad design practices can similarly be found in programs that combine actual production code with interface contracts. These contracts can be written in languages, such as the Java Modeling Language (JML), an extension to the Java syntax. This paper presents a catalog of bad smells that appear during DBC practice, considering JML as the language for specifying contracts. These smells are described over JML constructs, although several can appear in other DBC languages. The catalog contains 6 DBC smells. We evaluate the recurrence of DBC smells in two ways: first by describing a small study with graduate student projects, and second by counting occurrences of smells in contracts from the JML models application programming interface (API). This API contains classes with more than 1,600 lines in contracts. Along with the documented smells, suggestions are provided for minimizing the impact or even removing a bad smell. It is believed that initiatives towards the cataloging of bad smells are useful for establishing good design practices in DBC. © 2013. The Korean Institute of Information Scientists and Engineers.",
      "Keywords": "Bad smells | Design-by-contract | Java modeling language | Refactoring",
      "Publication venue": "Journal of Computing Science and Engineering",
      "Publication date": "2013-01-01",
      "Publication type": "Article",
      "Authors": "Viana, Thiago",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84921052834",
      "Primary study DOI": "",
      "Title": "Experimenting the influence of numerical thresholds on model-based detection and refactoring of performance antipatterns",
      "Abstract": "Performance antipatterns are well-known bad design practices that lead to software products suffering from poor performance. A number of performance antipatterns has been defined and classified and refactoring actions have also been suggested to remove them. In the last few years, we have dedicated some effort to the detection and refactoring of performance antipatterns in software models. A specific characteristic of performance antipatterns is that they contain numerical parameters that may represent thresholds referring to either performance indices (e.g., a device utilization) or design features (e.g., number of interface operations of a software component). In this paper, we analyze the influence of such thresholds on the capability of detecting and refactoring performance antipatterns. In particular, (i) we analyze how a set of detected antipatterns may change while varying the threshold values and (ii) we discuss the influence of thresholds on the complexity of refactoring actions. With the help of a leading example, we quantify the influence using precision and recall metrics.",
      "Keywords": "Detection | Numerical thresholds | Refactoring | Software Performance Antipatterns",
      "Publication venue": "Electronic Communications of the EASST",
      "Publication date": "2013-01-01",
      "Publication type": "Article",
      "Authors": "Arcelli, Davide;Cortellessa, Vittorio;Trubiani, Catia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84924075893",
      "Primary study DOI": "",
      "Title": "Combinations of antipattern heuristics in software architecture optimization for embedded systems",
      "Abstract": "A large number of quality properties need to be addressed in nowadays complex embedded systems by architects. Evolutionary algorithms can help architects to find optimal solutions which meet these conicting quality attributes. Also, architectural patterns and antipatterns give the architect knowledge of solving design bottlenecks. Hence, antipatterns heuristics have been used as domain-specific search operators within the evolutionary optimization. However, these heuristics usually improve only one quality attribute and using them in multiobjective problem is challenging. This paper studies the extent to which heuristic-based search operators can improve multiobjective optimization of software architecture for embedded systems. It compares various combinations of heuristic-based operators in a real world automotive system case study.",
      "Keywords": "Architectural antipatterns | Domain-specific search operators | Embedded system architecture design optimization | Evolutionary multiobjective optimization (EMO)",
      "Publication venue": "CEUR Workshop Proceedings",
      "Publication date": "2013-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Etemaadi, Ramin;Chaudron, Michel R.V.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84901676583",
      "Primary study DOI": "10.1109/UCC.2013.70",
      "Title": "Quality-aware refactoring for early detection and resolution of energy deficiencies",
      "Abstract": "Software development processes usually target requirements regarding particular qualities in late iteration phases. The developed system is optimised in terms of quality issues, such as, e.g., energy efficiency, without altering the software's behaviour. Bad structures in terms of specific qualities can be considered as bad smells and refactorings can be used to resolve them to preserve its semantics. The problem is that no explicit relationship between smells, qualities and refactorings exists. Without such a relation it is not possible to give evidence about which quality requirements are not satisfied by detected smells. It cannot be specified which smells are resolved by particular refactorings. Thus, developers are not supported in focusing specific qualities and cannot detect and resolve badly structured code in combination. In this paper we present an approach for correlating smells, qualities and refactorings explicitly which supports to focus on specific qualities in early development phases already. We introduce the new term quality smell and come up with a metamodel and architecture enabling developers to establish such relations. A small evaluation regarding energy efficiency in Java code and discussion completes this paper. © 2013 IEEE.",
      "Keywords": "energy efficiency | quality smell | quality-aware refactoring",
      "Publication venue": "Proceedings - 2013 IEEE/ACM 6th International Conference on Utility and Cloud Computing, UCC 2013",
      "Publication date": "2013-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Reimann, Jan;Aßsmann, Uwe",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84936864017",
      "Primary study DOI": "10.1109/APSEC.2013.25",
      "Title": "Fault-prone module prediction using a prediction model and manual inspection",
      "Abstract": "This paper proposes a fault-prone prediction approach that combines a fault-prone prediction model and manual inspection. Manual inspection is conducted by a predefined checklist that consists of questions and scoring procedures. The questions capture the fault signs or indications that are difficult to be captured by source code metrics used as input by prediction models. Our approach consists of two steps. In the first, the modules are prioritized by a fault-prone prediction model. In the second step, an inspector inspects and scores - percent of the prioritized modules. We conducted a case study of source code modules in commercial software that had been maintained and evolved over ten years and compared AUC (Area Under the Curve) values of Alberg Diagram among three prediction models: (A) support vector machines, (B) lines of code, and (C) random predictor with four prioritization orders. Our results indicated that the maximum AUC values under appropriate - and the coefficient of the inspection score were larger than the AUC values of the prediction models without manual inspection in each of the four combinations and the three models in our context. In two combinations, our approach increased the AUC values to 0.860 from 0.774 and 0.724. Our results also indicated that one of the combinations monotonically increased the AUC values with the numbers of manually inspected modules. This might lead to flexible inspection; the number of manually inspected modules has not been preliminary determined, and the inspectors can inspect as many modules as possible, depending on the available effort.",
      "Keywords": "Code smell | Hybrid faultprone prediction | Source code inspection",
      "Publication venue": "Proceedings - Asia-Pacific Software Engineering Conference, APSEC",
      "Publication date": "2013-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Kasai, Norimitsu;Morisaki, Shuji;Matsumoto, Kenichi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84871602467",
      "Primary study DOI": "10.1109/IWESEP.2012.12",
      "Title": "An empirical analysis on fault-proneness of well-commented modules",
      "Abstract": "Comment statements are useful to enhance the readability and/or understandability of software modules. However, some comments may adjust the readability/understandability of code fragments that are too complicated and hard to understand - a kind of code smell. Consequently, some well-written comments may be signs of poor-quality modules. This paper focuses on the lines of comments written in modules, and performs an empirical analysis with three major open source software and their fault data. The empirical results show that the risk of being faulty in well-commented modules is about 2 to 8 times greater than non-commented modules. © 2012 IEEE.",
      "Keywords": "code smell | comment | fault-prone module | prediction",
      "Publication venue": "Proceedings - 2012 4th International Workshop on Empirical Software Engineering in Practice, IWESEP 2012",
      "Publication date": "2012-12-31",
      "Publication type": "Conference Paper",
      "Authors": "Aman, Hirohisa",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84870689505",
      "Primary study DOI": "10.1109/SCET.2012.6342082",
      "Title": "Code smell detecting tool and code smell-structure bug relationship",
      "Abstract": "This paper proposes an approach for detecting the socalled bad smells in software known as Code Smell. In considering software bad smells, object-oriented software metrics were used to detect the source code whereby Eclipse Plugins were developed for detecting in which location of Java source code the bad smell appeared so that software refactoring could then take place. The detected source code was classified into 7 types: Large Class, Long Method, Parallel Inheritance Hierarchy, Long Parameter List, Lazy Class, Switch Statement, and Data Class. This work conducted analysis by using 323 java classes to ascertain the relationship between the code smell and structural defects of software by using the data mining techniques of Naive Bayes and Association Rules. The result of the Naive Bayes test showed that the Lazy Class caused structural defects in DLS, DE, and Se. Also, Data Class caused structural defects in UwF, DE, and Se, while Long Method, Large Class, Data Class, and Switch Statement caused structural defects in UwF and Se. Finally, Parallel Inheritance Hierarchy caused structural defects in Se. However, Long Parameter List caused no structural defects whatsoever. The results of the Association Rules test found that the Lazy Class code smell caused structural defects in DLS and DE, which corresponded to the results of the Naive Bayes test. ©2012 IEEE.",
      "Keywords": "Code smell | Refactoring | Software metric | Structural bugs",
      "Publication venue": "2012 Spring World Congress on Engineering and Technology, SCET 2012 - Proceedings",
      "Publication date": "2012-12-12",
      "Publication type": "Conference Paper",
      "Authors": "Danphitsanuphan, Phongphan;Suwantada, Thanitta",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84880272372",
      "Primary study DOI": "10.5381/jot.2012.11.2.a5",
      "Title": "Automatic detection of bad smells in code: An experimental assessment",
      "Abstract": "Code smells are structural characteristics of software that may indicate a code or design problem that makes software hard to evolve and maintain, and may trigger refactoring of code. Recent research is active in defining automatic detection tools to help humans in finding smells when code size becomes unmanageable for manual review. Since the definitions of code smells are informal and subjective, assessing how effective code smell detection tools are is both important and hard to achieve. This paper reviews the current panorama of the tools for automatic code smell detection. It defines research questions about the consistency of their responses, their ability to expose the regions of code most affected by structural decay, and the relevance of their responses with respect to future software evolution. It gives answers to them by analyzing the output of four representative code smell detectors applied to six different versions of GanttProject, an open source system written in Java. The results of these experiments cast light on what current code smell detection tools are able to do and what the relevant areas for further improvement are. © JOT 2011.",
      "Keywords": "Code smell detection tools | Code smells | Refactoring | Software quality evaluation",
      "Publication venue": "Journal of Object Technology",
      "Publication date": "2012-12-01",
      "Publication type": "Article",
      "Authors": "Fontana, Francesca Arcelli;Braione, Pietro;Zanoni, Marco",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85075312465",
      "Primary study DOI": "10.35940/ijitee.A5328.119119",
      "Title": "Design and analysis of improvised genetic algorithm with particle swarm optimization for code smell detection",
      "Abstract": "Software development phase is very important in the Software Development Life Cycle. Software maintenance is a difficult process if code smells exist in the code. The poor design of code development is called code smells. The code smells are identified by various tools using various approaches. Many code smell approaches are rule based. The rule based approaches are based on trial and error method. Genetic Algorithm is a heuristic Algorithm by Darwin’s Theory. This paper presents a metric based code smell detection approach by Genetic Algorithm with particle swarm optimization based on Euclidean data distance. The Euclidean data distance gives best proximity value between two points. Our approach is evaluated on the three open source projects like JFreeChart v1.0.9, Log4J v1.2.1 and Xerces-J for identifying the eight types of code smells namely Functional Decomposition, Feature Envy, Blob, Long Parameter List, Spaghetti Code, Data Class, Lazy Class, Shotgun Surgery.",
      "Keywords": "Code smell | Genetic Algorithm | Improvised Genetic Algorithm",
      "Publication venue": "International Journal of Innovative Technology and Exploring Engineering",
      "Publication date": "2019-11-01",
      "Publication type": "Article",
      "Authors": "James Benedict Felix, S.;Vinod, Viji",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84873465028",
      "Primary study DOI": "10.1145/2430475.2430496",
      "Title": "Detecting anti-patterns in java EE runtime system model",
      "Abstract": "With the increasing complexity of enterprise applications, it becomes very challenging to create software systems which can exhibit a satisfactory performance behavior. In current system development practice, it often inevitably exists some \\anti-patterns\", which usually impede the performance or maintainability of software systems. Manually investigat- ing anti-patterns in systems is a time-consuming and labor intensive task. To deal with this problem, we propose a general anti-pattern detection approach for Java EE appli- cation. Firstly, we propose a Java EE meta-model, based on which, we use QVT language to specify the detection pro- cess of anti-patterns. Secondly, we implement our approach on a runtime architecture-based reective framework. When a Java EE application runs on one of the supported applica- tion servers, we can execute QVT script to detect whether or not there exists a specific anti-pattern in current system and get the report of potential problem components. At last, we perform a case study based on 35 well-known anti- patterns to evaluate the effectiveness and applicability of our approach. Copyright 2012 ACM.",
      "Keywords": "Anti pattern | Java ee | Runtime model",
      "Publication venue": "4th Asia-Pacific Symposium on Internetware, Internetware 2012",
      "Publication date": "2012-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Zhang, Lei;Sun, Yanchun;Song, Hui;Wang, Weihu;Huang, Gang",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84891805953",
      "Primary study DOI": "",
      "Title": "Automatic forecasting of design anti-patterns in software source code",
      "Abstract": "The paper presents a framework for automatic inferring knowledge about reasons for the appearance of anti-patterns in the program source code during its development. Experiments carried out on histories of development of few open-source java projects shown that we can effciently detect temporal patterns, which are indicators of likely appearance of future anti-pattern. The approach presented in this paper uses expert knowledge (formal description of anti-patterns) to automatically produce extra knowledge (with machine learning algorithm) about the evolution of bad structures in the program source code. The research can be used to build scalable and adaptive tools, which warns development teams about the fact that system architecture is drifting in the wrong direction, before this is reported by typical static source code analysis tools.",
      "Keywords": "Machine learning | Software design anti- pattern | Software evolution | Temporal patterns",
      "Publication venue": "CEUR Workshop Proceedings",
      "Publication date": "2012-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Puławski, Lukasz",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84873152381",
      "Primary study DOI": "10.1109/ICSM.2012.6405300",
      "Title": "Detecting code smells in spreadsheet formulas",
      "Abstract": "Spreadsheets are used extensively in business processes around the world and just like software, spreadsheets are changed throughout their lifetime causing maintainability issues. This paper adapts known code smells to spreadsheet formulas. To that end we present a list of metrics by which we can detect smelly formulas and a visualization technique to highlight these formulas in spreadsheets. We implemented the metrics and visualization technique in a prototype tool to evaluate our approach in two ways. Firstly, we analyze the EUSES spreadsheet corpus, to study the occurrence of the formula smells. Secondly, we analyze ten real life spreadsheets, and interview the spreadsheet owners about the identified smells. The results of these evaluations indicate that formula smells are common and that they can reveal real errors and weaknesses in spreadsheet formulas. © 2012 IEEE.",
      "Keywords": "code smells | refactoring | spreadsheets",
      "Publication venue": "IEEE International Conference on Software Maintenance, ICSM",
      "Publication date": "2012-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Hermans, Felienne;Pinzger, Martin;Van Deursen, Arie",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84881245137",
      "Primary study DOI": "",
      "Title": "Antipatterns detection approaches in Object-Oriented Design: A literature review",
      "Abstract": "Pattern is one of the most simple and powerful techniques to improve the design, and consequently enhance the maintainability, reusability, and reverse engineering. Design pattern detection is a useful technique for gaining knowledge on the design issues of existing systems and improves the system's comprehension, which consequently enhance the software maintainability and evolution. Numerous studies have been conducted and many tools have been developed to detect design patterns, whereas only few studies considered the antipattern, which has not been investigated with the same extent of patterns detection. This study presents the antipatterns of object oriented design, their definitions, detecting approaches and issues related to their detection. That provides a clear state of the antipatterns detection and its issues that need to be addressed in the future. The result indicates that the antipatterns detection requires further investigation for several limitations and issues. © 2012 AICIT.",
      "Keywords": "Antipatterns | Antipatterns Detection | OOD | OOD Issues",
      "Publication venue": "Proceedings - 2012 7th International Conference on Computing and Convergence Technology (ICCIT, ICEI and ICACT), ICCCT 2012",
      "Publication date": "2012-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Din, Jamilah;Al-Badareen, Anas Bassam;Jusoh, Yusmadi Yah",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84871999968",
      "Primary study DOI": "",
      "Title": "A model transformation approach towards refactoring use case models based on antipatterns",
      "Abstract": "The quality of use case models is crucial towards the success of a software development project, especially projects that utilize a use case driven development approach. Hence, the literature has presented many approaches to improve the quality of use case models. A category of such approaches is based on revising the use case model to better reflect the intended functional requirements. These approaches are based on the detection of unsound structures in existing diagrams and then refactoring the diagrams to improve their quality. One such promising approach is the utilization of antipatterns. An antipattern will describe a debatable structure in use case diagrams and its harmful consequences. The antipattern will also describe the proper form the diagram should be in. While the detection of the unsound structures described in antipatterns has been automated, the refactoring effort to enhance the use case diagram remains a manual process, which can be cumbersome and error-prone. In this paper, we present an approach to automate the refactoring of use case diagrams using model transformation. The transformation algorithms were tested using a biodiversity database system case study. The results show the refactorings have been applied correctly and accurately.",
      "Keywords": "Antipatterns | ATL | Refactoring | Use Case",
      "Publication venue": "Proceedings of the 21st International Conference on Software Engineering and Data Engineering, SEDE 2012",
      "Publication date": "2012-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Khan, Yasser;El-Attar, Mohamed",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84872333083",
      "Primary study DOI": "10.1109/WCRE.2012.33",
      "Title": "Can Lexicon Bad Smells improve fault prediction?",
      "Abstract": "In software development, early identification of fault-prone classes can save a considerable amount of resources. In the literature, source code structural metrics have been widely investigated as one of the factors that can be used to identify faulty classes. Structural metrics measure code complexity, one aspect of the source code quality. Complexity might affect program understanding and hence increase the likelihood of inserting errors in a class. Besides the structural metrics, we believe that the quality of the identifiers used in the code may also affect program understanding and thus increase the likelihood of error insertion. In this study, we measure the quality of identifiers using the number of Lexicon Bad Smells (LBS) they contain. We investigate whether using LBS in addition to structural metrics improves fault prediction. To conduct the investigation, we assess the prediction capability of a model while using i) only structural metrics, and ii) structural metrics and LBS. The results on three open source systems, ArgoUML, Rhino, and Eclipse, indicate that there is an improvement in the majority of the cases. © 2012 IEEE.",
      "Keywords": "Fault prediction | lexicon bad smells | program understanding | structural metrics",
      "Publication venue": "Proceedings - Working Conference on Reverse Engineering, WCRE",
      "Publication date": "2012-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Abebe, Surafel Lemma;Arnaoudova, Venera;Tonella, Paolo;Antoniol, Giuliano;Guéhéneuc, Yann Gaël",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84872332293",
      "Primary study DOI": "",
      "Title": "Proceedings - 19th Working Conference on Reverse Engineering, WCRE 2012",
      "Abstract": "The proceedings contain 55 papers. The topics discussed include: assuring software quality by code smell detection; structured binary editing with a CFG transformation algebra; Astra: bottom-up construction of structured artifact repositories; detection and recovery of functions and their arguments in a retargetable decompiler; towards static analysis of virtualization-obfuscated binaries; understanding android fragmentation with topic analysis of vendor-specific bugs; using network analysis for recommendation of central software classes; TRIS: a fast and accurate identifiers splitting and expansion algorithm; using bug report similarity to enhance bug localisation; SCAN: an approach to label and relate execution trace segments; feature location in a collection of product variants; reverse engineering iOS mobile applications; and precise detection of uninitialized variables using dynamic analysis - extending to aggregate and vector types.",
      "Keywords": "",
      "Publication venue": "Proceedings - Working Conference on Reverse Engineering, WCRE",
      "Publication date": "2012-12-01",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84869773862",
      "Primary study DOI": "10.1145/2384716.2384727",
      "Title": "Upload your program, share your model",
      "Abstract": "We demonstrate the Massey Architecture Explorer (MAE), a browser-based application to visualise and analyse the architecture of JVM applications. The MAE extracts a graphbased model from Java byte code, and visualises it using a force directed layout on an HTML5 canvas. A novel scalable algorithm is used to detect architectural antipatterns. The antipatterns detected focus on problems software architects face when trying to refactor applications into OSGi or similar dynamic component models. A unique feature of this system is that the state of the application is encoded in the URL (\"URL memento\"). These URLs can be shared and bookmarked, facilitating the sharing of architectural knowledge amongst software engineers.",
      "Keywords": "Software architecture recovery | Software as a service",
      "Publication venue": "SPLASH'12 - Proceedings of the 2012 ACM Conference on Systems, Programming, and Applications: Software for Humanity",
      "Publication date": "2012-11-27",
      "Publication type": "Conference Paper",
      "Authors": "Dietrich, Jens",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84868702556",
      "Primary study DOI": "",
      "Title": "On the existence of high-impact refactoring opportunities in programs",
      "Abstract": "The refactoring of large systems is difficult, with the possibility of many refactorings having to be done before any useful benefit is attained. We present a novel approach to detect starting points for the architectural refactoring of large and complex systems based on the analysis and manipulation of the type dependency graph extracted from programs. The proposed algorithm is based on the simultaneous analysis of multiple architectural antipatterns, and outputs dependencies between artefacts that participate in large numbers of instances of these antipatterns. If these dependencies can be removed, they represent high-impact refactoring opportunities: a small number of changes that have a major impact on the overall quality of the system, measured by counting architectural antipattern instances. The proposed algorithm is validated using an experiment where we analyse a set of 95 open-source Java programs for instances of four architectural patterns representing modularisation problems. We discuss some examples demonstrating how the computed dependencies can be removed from programs. This research is motivated by the emergence of technologies such as dependency injection frameworks and dynamic component models. These technologies try to improve the maintainability of systems by removing dependencies between system parts from program source code and managing them explicitly in configuration files. Copyright © 2012, Australian Computer Society, Inc.",
      "Keywords": "",
      "Publication venue": "Conferences in Research and Practice in Information Technology Series",
      "Publication date": "2012-11-16",
      "Publication type": "Conference Paper",
      "Authors": "Dietrich, Jens;McCartin, Catherine;Tempero, Ewan;Shah, Syed M.Ali",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84867507242",
      "Primary study DOI": "10.1145/2372251.2372269",
      "Title": "Questioning software maintenance metrics: A comparative case study",
      "Abstract": "Context: Many metrics are used in software engineering research as surrogates for maintainability of software systems. Aim: Our aim was to investigate whether such metrics are consistent among themselves and the extent to which they predict maintenance effort at the entire system level. Method: The Maintainability Index, a set of structural measures, two code smells (Feature Envy and God Class) and size were applied to a set of four functionally equivalent systems. The metrics were compared with each other and with the outcome of a study in which six developers were hired to perform three maintenance tasks on the same systems. Results: The metrics were not mutually consistent. Only system size and low cohesion were strongly associated with increased maintenance effort. Conclusion: Apart from size, surrogate maintainability measures may not reflect future maintenance effort. Surrogates need to be evaluated in the contexts for which they will be used. While traditional metrics are used to identify problematic areas in the code, the improvements of the worst areas may, inadvertently, lead to more problems for the entire system. Our results suggest that local improvements should be accompanied by an evaluation at the system level. Copyright 2012 ACM.",
      "Keywords": "Software maintenance | Software metrics",
      "Publication venue": "International Symposium on Empirical Software Engineering and Measurement",
      "Publication date": "2012-10-22",
      "Publication type": "Conference Paper",
      "Authors": "Sjøberg, Dag I.K.;Anda, Bente;Mockus, Audris",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84866893561",
      "Primary study DOI": "10.1145/2351676.2351723",
      "Title": "Support vector machines for anti-pattern detection",
      "Abstract": "Developers may introduce anti-patterns in their software systems because of time pressure, lack of understanding, communication, and-or skills. Anti-patterns impede development and maintenance activities by making the source code more difficult to understand. Detecting anti-patterns in a is important to ease the maintenance of software. Detecting anti-patterns could reduce costs, effort, and resources. Researchers have proposed approaches to detect occurrences of anti-patterns but these approaches have currently some limitations: they require extensive knowledge of anti-patterns, they have limited precision and recall, and they cannot be applied on subsets of systems. To overcome these limitations, we introduce SVMDetect, a novel approach to detect anti-patterns, based on a machine learning technique- support vector machines. Indeed, through an empirical study involving three subject systems and four anti-patterns, we showed that the accuracy of SVMDetect is greater than of DETEX when detecting anti-patterns occurrences on a set of classes. Concerning, the whole system, SVMDetect is able to find more anti-patterns occurrences than DETEX. Copyright 2012 ACM.",
      "Keywords": "Anti-pattern | Empirical software engineering | Program comprehension | Program maintenance",
      "Publication venue": "2012 27th IEEE/ACM International Conference on Automated Software Engineering, ASE 2012 - Proceedings",
      "Publication date": "2012-10-05",
      "Publication type": "Conference Paper",
      "Authors": "Maiga, Abdou;Ali, Nasir;Bhattacharya, Neelesh;Sabané, Aminata;Guéhéneuc, Yann Gaël;Antoniol, Giuliano;Aïmeur, Esma",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84866893850",
      "Primary study DOI": "10.1145/2351676.2351724",
      "Title": "Detection of embedded code smells in dynamic web applications",
      "Abstract": "In dynamic Web applications, there often exists a type of code smells, called embedded code smells, that violate important principles in software development such as software modularity and separation of concerns, resulting in much maintenance effort. Detecting and fixing those code smells is crucial yet challenging since the code with smells is embedded and generated from the server-side code. We introduce WebScent, a tool to detect such embedded code smells. WebScent first detects the smells in the generated code, and then locates them in the server-side code using the mapping between client-side code fragments and their embedding locations in the server program, which is captured during the generation of those fragments. Our empirical evaluation on real-worldWeb applications shows that 34%-81% of the tested server files contain embedded code smells. We also found that the source files with more embedded code smells are likely to have more defects and scattered changes, thus potentially require more maintenance effort. Copyright 2012 ACM.",
      "Keywords": "Code smells | Dynamic web applications | Embedded code",
      "Publication venue": "2012 27th IEEE/ACM International Conference on Automated Software Engineering, ASE 2012 - Proceedings",
      "Publication date": "2012-10-05",
      "Publication type": "Conference Paper",
      "Authors": "Nguyen, Hung Viet;Nguyen, Hoan Anh;Nguyen, Tung Thanh;Nguyen, Anh Tuan;Nguyen, Tien N.",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84866692279",
      "Primary study DOI": "10.1109/INES.2012.6249814",
      "Title": "On extended similarity scoring and bit-vector algorithms for design smell detection",
      "Abstract": "The occurrence of design smells or anti-patterns in software models complicate development process and reduce the software quality. The contribution proposes an extension to Similarity Scoring Algorithm and Bit-vector Algorithm, originally used for design patterns detection. This paper summarizes both original approaches, important differences between design patterns and anti-patterns structures, modifications and extensions of algorithms and their application to detect selected design smells. © 2012 IEEE.",
      "Keywords": "anti-pattern | bit-vector algorithm | design smell | refactoring | similarity scoring",
      "Publication venue": "INES 2012 - IEEE 16th International Conference on Intelligent Engineering Systems, Proceedings",
      "Publication date": "2012-10-01",
      "Publication type": "Conference Paper",
      "Authors": "Polášek, I.;Líška, P.;Kelemen, J.;Lang, J.",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84865756638",
      "Primary study DOI": "10.3923/itj.2012.1677.1686",
      "Title": "Analytical learning based on a meta-programming approach for the detection of object-oriented design defects",
      "Abstract": "This study proposed a new defect-detection approach using a declarative meta-programming technique to analytical learning for object-oriented software. The extrapolating patterns are generated using analytical learning in which certain design defect characteristics can be understood through deductive learning. This study uses declarative meta-programming to represent the specific object-oriented components as logic rules with which design defects can finally be described. Using the two complementary techniques, the object-oriented software is transformed into the narrow related problem domain, in which design defect problems can be managed and simplified. The approach is validated by detecting design defects in certain open-source systems. The results obtained exhibit a superior precision to the conventional method. In application, the proposed strategy can be recognised as a flexible and automated system for detecting software design defects which many object-oriented software systems are able to use. © 2012 Asian Network for Scientific Information.",
      "Keywords": "Analytical learning | Antipatterns | Code smell | Defect detection | Meta-programming",
      "Publication venue": "Information Technology Journal",
      "Publication date": "2012-09-10",
      "Publication type": "Article",
      "Authors": "Mekruksavanich, Sakorn;Yupapin, Preecha P.;Muenchaisri, Pornsiri",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85084085773",
      "Primary study DOI": "10.1109/ICCS45141.2019.9065317",
      "Title": "Identification of code smell using machine learning",
      "Abstract": "Code smells are used to improve the quality of the software. Code smell detection detects the code problems like long method, large class, lazy class, long parameter list, feature envy, primitive obsession detector and too many literal detectors present in the code. In this work, two algorithms are used namely Support Vector Machine (SVM) and Random Forest algorithm. Support Vector Machine acts as classifier and the Random Forest algorithm are used for predicting the range of data. Decision making technique is used to identify the various problems present in the code. Code smell detection is a testing tool and it is mainly used by the developers, when the size of the code becomes unmanageable for manual detection. It also identifies deeper problems like syntax error, runtime error. It gives the output by analyzing the code in six different modules in this work.",
      "Keywords": "Bad Smells | Bloated Code Detector | Code Smell | Feature Envy | Large Class | Lazy Class Detector | Long method | Primitive Obsession Detector",
      "Publication venue": "2019 International Conference on Intelligent Computing and Control Systems, ICCS 2019",
      "Publication date": "2019-05-01",
      "Publication type": "Conference Paper",
      "Authors": "Jesudoss, A.;Maneesha, S.;Lakshmi Naga Durga, T.",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84865145742",
      "Primary study DOI": "10.1007/978-3-642-32211-2_7",
      "Title": "Clones in logic programs and how to detect them",
      "Abstract": "In this paper, we propose a theoretical framework that allows us to capture, by program analysis, the notion of code clone in the context of logic programming. Informally, two code fragments are considered as cloned if they implement the same functionality. Clone detection can be advantageous from a software engineering viewpoint, as the presence of code clones inside a program reveals redundancy, broadly considered a \"bad smell\". In the paper, we present a detailed definition of a clone in a logic program and provide an efficient detection algorithm able to identify an important subclass of code clones that could be used for various applications such as program refactoring and plagiarism recognition. Our clone detection algorithm is not tied to a particular logic programming language, and can easily be instantiated for different such languages. © 2012 Springer-Verlag.",
      "Keywords": "clone detection algorithm | code clone | code duplication | logic programming languages",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2012-08-23",
      "Publication type": "Conference Paper",
      "Authors": "Dandois, Céline;Vanhoof, Wim",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84859213691",
      "Primary study DOI": "10.1016/j.eswa.2012.02.049",
      "Title": "Enhancing ontology-based antipattern detection using Bayesian networks",
      "Abstract": "Antipatterns provide information on commonly occurring solutions to problems that generate negative consequences. The antipattern ontology has been recently proposed as a knowledge base for SPARSE, an intelligent system that can detect the antipatterns that exist in a software project. However, apart from the plethora of antipatterns that are inherently informal and imprecise, the information used in the antipattern ontology itself is many times imprecise or vaguely defined. For example, the certainty in which a cause, symptom or consequence of an antipattern exists in a software project. Taking into account probabilistic information would yield more realistic, intelligent and effective ontology-based applications to support the technology of antipatterns. However, ontologies are not capable of representing uncertainty and the effective detection of antipatterns taking into account the uncertainty that exists in software project antipatterns still remains an open issue. Bayesian Networks (BNs) have been previously used in order to measure, illustrate and handle antipattern uncertainty in mathematical terms. In this paper, we explore the ways in which the antipattern ontology can be enhanced using Bayesian networks in order to reinforce the existing ontology-based detection process. This approach allows software developers to quantify the existence of an antipattern using Bayesian networks, based on probabilistic knowledge contained in the antipattern ontology regarding relationships of antipatterns through their causes, symptoms and consequences. The framework is exemplified using a Bayesian network model of 13 antipattern attributes, which is constructed using BNTab, a plug-in developed for the Protege ontology editor that generates BNs based on ontological information. © 2012 Elsevier Ltd. All rights reserved.",
      "Keywords": "Antipattern detection | Antipatterns | Bayesian networks | Ontology",
      "Publication venue": "Expert Systems with Applications",
      "Publication date": "2012-01-01",
      "Publication type": "Article",
      "Authors": "Settas, Dimitrios;Cerone, Antonio;Fenz, Stefan",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84864266284",
      "Primary study DOI": "10.1109/ICSE.2012.6227171",
      "Title": "Detecting and visualizing inter-worksheet smells in spreadsheets",
      "Abstract": "Spreadsheets are often used in business, for simple tasks, as well as for mission critical tasks such as finance or forecasting. Similar to software, some spreadsheets are of better quality than others, for instance with respect to usability, maintainability or reliability. In contrast with software however, spreadsheets are rarely checked, tested or certified. In this paper, we aim at developing an approach for detecting smells that indicate weak points in a spreadsheet's design. To that end we first study code smells and transform these code smells to their spreadsheet counterparts. We then present an approach to detect the smells, and to communicate located smells to spreadsheet users with data flow diagrams. To evaluate our apporach, we analyzed occurrences of these smells in the Euses corpus. Furthermore we conducted ten case studies in an industrial setting. The results of the evaluation indicate that smells can indeed reveal weaknesses in a spreadsheet's design, and that data flow diagrams are an appropriate way to show those weaknesses. © 2012 IEEE.",
      "Keywords": "code smells | data flow diagrams | refactoring | spreadsheets",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2012-07-30",
      "Publication type": "Conference Paper",
      "Authors": "Hermans, Felienne;Pinzger, Martin;Van Deursen, Arie",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84864222234",
      "Primary study DOI": "10.1109/ICSE.2012.6227152",
      "Title": "Build code analysis with symbolic evaluation",
      "Abstract": "Build process is crucial in software development. However, the analysis support for build code is still limited. In this paper, we present SYMake, an infrastructure and tool for the analysis of build code in make. Due to the dynamic nature of make language, it is challenging to understand and maintain complex Makefiles. SYMake provides a symbolic evaluation algorithm that processes Makefiles and produces a symbolic dependency graph (SDG), which represents the build dependencies (i.e. rules) among files via commands. During the symbolic evaluation, for each resulting string value in an SDG that represents a part of a file name or a command in a rule, SYMake provides also an acyclic graph (called T-model) to represent its symbolic evaluation trace. We have used SYMake to develop algorithms and a tool 1) to detect several types of code smells and errors in Makefiles, and 2) to support build code refactoring, e.g. renaming a variable/target even if its name is fragmented and built from multiple substrings. Our empirical evaluation for SYMake's renaming on several real-world systems showed its high accuracy in entity renaming. Our controlled experiment showed that with SYMake, developers were able to understand Makefiles better and to detect more code smells as well as to perform refactoring more accurately. © 2012 IEEE.",
      "Keywords": "build code analysis | build code maintenance",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2012-07-30",
      "Publication type": "Conference Paper",
      "Authors": "Tamrawi, Ahmed;Nguyen, Hoan Anh;Nguyen, Hung Viet;Nguyen, Tien N.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84864225735",
      "Primary study DOI": "10.1109/IWSC.2012.6227865",
      "Title": "Mining object-oriented design models for detecting identical design structures",
      "Abstract": "The object-oriented design is the most popular design methodology of the last twenty-five years. Several design patterns and principles are defined to improve the design quality of object-oriented software systems. In addition, designers can use unique design motifs which are particular for the specific application domain. Another common habit is cloning and modifying some parts of the software while creating new modules. Therefore, object-oriented programs can include many identical design structures. This work proposes a sub-graph mining based approach to detect identical design structures in object-oriented systems. By identifying and analyzing these structures, we can obtain useful information about the design, such as commonly-used design patterns, most frequent design defects, domain-specific patterns, and design clones, which may help developers to improve their knowledge about the software architecture. Furthermore, problematic parts of frequent identical design structures are the appropriate refactoring opportunities because they affect multiple areas of the architecture. Experiments with several open-source projects show that we can successfully find many identical design structures in each project. We observe that usually most of the identical structures are an implementation of common design patterns; however we also detect various anti-patterns, domain-specific patterns, and design-level clones. © 2012 IEEE.",
      "Keywords": "clones | graph mining | identical design structures | pattern extraction | software design models | software motifs",
      "Publication venue": "2012 6th International Workshop on Software Clones, IWSC 2012 - Proceedings",
      "Publication date": "2012-07-30",
      "Publication type": "Conference Paper",
      "Authors": "Tekin, Umut;Erdemir, Ural;Buzluca, Feza",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84864270532",
      "Primary study DOI": "10.1109/ICSE.2012.6227036",
      "Title": "On the analysis of evolution of software artefacts and programs",
      "Abstract": "The literature describes several approaches to identify the artefacts of programs that evolve together to reveal the (hidden) dependencies among these artefacts and to infer and describe their evolution trends. We propose the use of biological methods to group artefacts, to detect co-evolution among them, and to construct their phylogenic trees to express their evolution trends. First, we introduced the novel concepts of macro co-changes (MCCs), i.e., of artefacts that co-change within a large time interval and of dephase macro co-changes (DMCCs), i.e., macro co-changes that always happen with the same shifts in time. We developped an approach, Macocha, to identify these new patterns of artefacts co-evolution in large programs. Now, we are analysing the evolution of classes playing roles in design patterns and - or antipatterns. In parallel to previous work, we are detecting what classes are in macro co-change or in dephase macro co-change with the design motifs. Results try to show that classes playing roles in design motifs have specifics evolution trends. Finally, we are implementing an approach, Profilo, to achieve the analysis of the evolution of artefacts and versions of large object-oriented programs. Profilo creates a phylogenic tree of different versions of program that describes versions evolution and the relation among versions and programs. We will, also, evaluate the usefulness of our tools using lab and field studies. © 2012 IEEE.",
      "Keywords": "change impact | co-change | phylogenic tree | Software evolution | stability",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2012-07-30",
      "Publication type": "Conference Paper",
      "Authors": "Jaafar, Fehmi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84864151618",
      "Primary study DOI": "10.1109/MTD.2012.6225993",
      "Title": "Investigating the impact of code smells debt on quality code evaluation",
      "Abstract": "Different forms of technical debt exist that have to be carefully managed. In this paper we focus our attention on design debt, represented by code smells. We consider three smells that we detect in open source systems of different domains. Our principal aim is to give advice on which design debt has to be paid first, according to the three smells we have analyzed. Moreover, we discuss if the detection of these smells could be tailored to the specific application domain of a system. © 2012 IEEE.",
      "Keywords": "code smell refactoring | design debt | software quality metrics",
      "Publication venue": "2012 3rd International Workshop on Managing Technical Debt, MTD 2012 - Proceedings",
      "Publication date": "2012-07-27",
      "Publication type": "Conference Paper",
      "Authors": "Fontana, Francesca Arcelli;Ferme, Vincenzo;Spinelli, Stefano",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84864060856",
      "Primary study DOI": "",
      "Title": "QoSA'12 - Proceedings of the 8th International ACM SIGSOFT Conference on the Quality of Software Architectures",
      "Abstract": "The proceedings contain 19 papers. The topics discussed include: NASA's advanced multimission operations system: a case study in software architecture evolution; architectural flexibility in a software-system's life-cycle: systematic construction and exploitation of flexibility; antipattern-based model refactoring for software performance improvement; supporting quality-driven design decisions by modeling variability; approach for architectural design and modelling with documented design decisions (ADMD3); performance-driven architectural refactoring through bidirectional model transformations; DSL-based support for semi-automated architectural component model abstraction throughout the software lifecycle; automotive ADLS: a study on enforcing consistency through multiple architectural levels; modeling dynamic virtualized resource landscapes; and improving performance predictions by accounting for the accuracy of composed performance models.",
      "Keywords": "",
      "Publication venue": "QoSA'12 - Proceedings of the 8th International ACM SIGSOFT Conference on the Quality of Software Architectures",
      "Publication date": "2012-07-25",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84863558744",
      "Primary study DOI": "10.1145/2245276.2231970",
      "Title": "IDE-based real-time focused search for near-miss clones",
      "Abstract": "Code clone is a well-known code smell that needs to be detected and managed during the software development process. However, the existing clone detectors have one or more of the three shortcomings: (a) limitation in detecting Type-3 clones, (b) they come as stand-alone tools separate from IDE and thus cannot support clone-aware development, (c) they overwhelm the developer with all clones from the entire code-base, instead of a focused search for clones of a selected code segment of the developer's interest. This paper presents our IDE-integrated clone search tool, that addresses all the above issues. For clone detection, we adapt a suffix-tree-based hybrid algorithm. Through an asymptotic analysis, we show that our approach for clone detection is both time and memory efficient. Moreover, using three separate empirical studies, we demonstrate that our tool is flexibly usable for searching exact (Type-1) and near-miss (Type-2 and Type-3) clones with high precision and recall. © 2012 ACM.",
      "Keywords": "clone detection | clone search | maintenance | reengineering",
      "Publication venue": "Proceedings of the ACM Symposium on Applied Computing",
      "Publication date": "2012-07-12",
      "Publication type": "Conference Paper",
      "Authors": "Zibran, Minhaz F.;Roy, Chanchal K.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84857356474",
      "Primary study DOI": "10.1007/s10664-011-9171-y",
      "Title": "An exploratory study of the impact of antipatterns on class change- and fault-proneness",
      "Abstract": "Antipatterns are poor design choices that are conjectured to make object-oriented systems harder to maintain. We investigate the impact of antipatterns on classes in object-oriented systems by studying the relation between the presence of antipatterns and the change- and fault-proneness of the classes. We detect 13 antipatterns in 54 releases of ArgoUML, Eclipse, Mylyn, and Rhino, and analyse (1) to what extent classes participating in antipatterns have higher odds to change or to be subject to fault-fixing than other classes, (2) to what extent these odds (if higher) are due to the sizes of the classes or to the presence of antipatterns, and (3) what kinds of changes affect classes participating in antipatterns. We show that, in almost all releases of the four systems, classes participating in antipatterns are more change-and fault-prone than others. We also show that size alone cannot explain the higher odds of classes with antipatterns to underwent a (fault-fixing) change than other classes. Finally, we show that structural changes affect more classes with antipatterns than others. We provide qualitative explanations of the increase of change- and fault-proneness in classes participating in antipatterns using release notes and bug reports. The obtained results justify a posteriori previous work on the specification and detection of antipatterns and could help to better focus quality assurance and testing activities. © 2011 Springer Science+Business Media, LLC.",
      "Keywords": "Antipatterns | Empirical software engineering | Mining software repositories",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2012-06-01",
      "Publication type": "Article",
      "Authors": "Khomh, Foutse;Penta, Massimiliano Di;Guéhéneuc, Yann Gaël;Antoniol, Giuliano",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84860449376",
      "Primary study DOI": "10.1145/2162049.2162069",
      "Title": "Are automatically-detected code anomalies relevant to architectural modularity? An exploratory analysis of evolving systems",
      "Abstract": "As software systems are maintained, their architecture modularity often degrades through architectural erosion and drift. More directly, however, the modularity of software implementations degrades through the introduction of code anomalies, informally known as code smells. A number of strategies have been developed for supporting the automatic identification of implementation anomalies when only the source code is available. However, it is still unknown how reliable these strategies are when revealing code anomalies related to erosion and drift processes. In this paper, we present an exploratory analysis that investigates to what extent the automatically-detected code anomalies are related to problems that occur with an evolving system's architecture. We analyzed code anomaly occurrences in 38 versions of 5 applications using existing detection strategies. The outcome of our evaluation suggests that many of the code anomalies detected by the employed strategies were not related to architectural problems. Even worse, over 50% of the anomalies not observed by the employed techniques (false negatives) were found to be correlated with architectural problems. © 2012 ACM.",
      "Keywords": "Architectural anomalies | Architectural degradation symptoms | Architectural violations | Code anomalies",
      "Publication venue": "AOSD'12 - Proceedings of the 11th Annual International Conference on Aspect Oriented Software Development",
      "Publication date": "2012-05-07",
      "Publication type": "Conference Paper",
      "Authors": "Macia, Isela;Garcia, Joshua;Popescu, Daniel;Garcia, Alessandro;Medvidovic, Nenad;Von Staa, Arndt",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84860000289",
      "Primary study DOI": "10.1109/SEW.2011.13",
      "Title": "Metrics and antipatterns for software quality evaluation",
      "Abstract": "In the context of software evolution, many activities are involved and are very useful, like being able to evaluate the design quality of an evolving system, both to locate the parts that need particular refactoring or reengineering efforts, and to evaluate parts that are well designed. This paper aims to give support hints for the evaluation of the code and design quality of a system and in particular we suggest to use metrics computation and antipatterns detection together. We propose metrics computation based on particular kinds of micro-structures and the detection of structural and object-oriented antipatterns with the aim of identifying areas of design improvements. We can evaluate the quality of a system according to different issues, for example by understanding its global complexity, analyzing the cohesion and coupling of system modules and locating the most critical and complex components that need particular refactoring or maintenance. © 2011 IEEE.",
      "Keywords": "Antipatterns detection | Maintainability | Metrics computation | Software quality assurance",
      "Publication venue": "Proceedings - 2011 34th IEEE Software Engineering Workshop, SEW 2011",
      "Publication date": "2011-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Fontana, Francesca Arcelli;Maggioni, Stefano",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84856518770",
      "Primary study DOI": "10.1109/TSE.2011.9",
      "Title": "Schedule of bad smell detection and resolution: A new way to save effort",
      "Abstract": "Bad smells are signs of potential problems in code. Detecting and resolving bad smells, however, remain time-consuming for software engineers despite proposals on bad smell detection and refactoring tools. Numerous bad smells have been recognized, yet the sequences in which the detection and resolution of different kinds of bad smells are performed are rarely discussed because software engineers do not know how to optimize sequences or determine the benefits of an optimal sequence. To this end, we propose a detection and resolution sequence for different kinds of bad smells to simplify their detection and resolution. We highlight the necessity of managing bad smell resolution sequences with a motivating example, and recommend a suitable sequence for commonly occurring bad smells. We evaluate this recommendation on two nontrivial open source applications, and the evaluation results suggest that a significant reduction in effort ranging from 17.64 to 20 percent can be achieved when bad smells are detected and resolved using the proposed sequence. © 2006 IEEE.",
      "Keywords": "bad smell | detection | effort | schedule | Scheme | software refactoring",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2012-01-09",
      "Publication type": "Article",
      "Authors": "Liu, Hui;Ma, Zhiyi;Shao, Weizhong;Niu, Zhendong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84879070534",
      "Primary study DOI": "10.3233/978-1-61499-094-9-161",
      "Title": "An automatic approach for detecting early indicators of design anti-patterns",
      "Abstract": "The paper presents a framework for automatic inferring knowledge about reasons for the appearance of anti-patterns in the program source code during its development. Experiments carried out on histories of development of few open-source java projects shown that we can efficiently detect temporal patterns, which are indicators of likely appearance of future anti-pattern. The approach presented in this paper uses expert knowledge (formal description of anti-patterns) to automatically produce extra knowledge (with machine learning algorithm) about the evolution of bad structures in the program source code. The research can be used to build scalable and adaptive tools, which warns development teams about the fact that system architecture is drifting in the wrong direction, before this is reported by typical static source code analysis tools. © 2012 The authors and IOS Press. All rights reserved.",
      "Keywords": "machine learning | software design antipattern | Software evolution | temporal patterns",
      "Publication venue": "Frontiers in Artificial Intelligence and Applications",
      "Publication date": "2012-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Puławski, Łukasz",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-83455218309",
      "Primary study DOI": "10.1109/ICSSEM.2011.6081283",
      "Title": "EXTRACTOR: An extensible framework for identifying aspect-oriented refactoring opportunities",
      "Abstract": "Automatic refactoring techniques guarantee the correctness and effectiveness for the transformation of legacy software systems. Existing techniques are not effective to identify refactoring opportunities because of the complexity of composite refactoring and the behavior preservation for Aspect-Oriented refactoring. To address these challenges, we design EXTRACTOR, which is an extensible framework to identify Aspect-Oriented refactoring opportunities. In the framework, the bad smell detector provides significant query ability to detect bad smells, while the template manager enables the customization of bad smell and composite refactoring. Then refactoring opportunities are identified using logic transformation managed by EXTRACTOR Constructor. All these functionalities are based on the logic query engine, which manages the logic representation of programs. Finally we illustrate the effectiveness of the framework using case study. © 2011 IEEE.",
      "Keywords": "aspect-oriented program | automatic software refactoring | logic transformation | software engineering",
      "Publication venue": "2011 International Conference on System Science, Engineering Design and Manufacturing Informatization, ICSEM 2011",
      "Publication date": "2011-12-19",
      "Publication type": "Conference Paper",
      "Authors": "Huang, Jin;Carminati, Federico;Betev, Latchezar;Zhu, Jianlin;Luzzi, Cinzia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84857333653",
      "Primary study DOI": "10.1007/978-3-642-25813-8_26",
      "Title": "An ontology based e-learning system using antipatterns",
      "Abstract": "Antipatterns are mechanisms that describe how to arrive at a good (refactored) solution from a fallacious solution that has negative consequences. These mechanisms are used in a variety of computer science topics and although their integration in teaching and computer science curriculum has been proposed, the development of an e-learning system using antipatterns, still remains an open issue. Previous work has proposed the use of WebProtege, a Web-based environment that allows collaborative editing as well as annotation and voting of both components and changes of the antipattern ontology. This ontology has been implemented as the knowledge base of SPARSE, an intelligent system that uses semantic web tools and techniques in order to detect the antipatterns that exist in a software project. In this paper, we leverage this semantic web technology and the formalism of ontology in order to propose a peer-production based e-learning system for the electronically supported learning of antipatterns. We illustrate how this Web-based system can transfer antipattern knowledge using an e-learning scenario as an example. © 2011 Springer-Verlag.",
      "Keywords": "",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2011-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Settas, Dimitrios;Cerone, Antonio",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84871960818",
      "Primary study DOI": "",
      "Title": "TrueRefactor: An automated refactoring tool to improve legacy system and application comprehensibility",
      "Abstract": "Manual refactoring is a complicated process requiring intimate knowledge of the software design and underlying intended behavior of a system. This knowledge is not always available. Fully automated refactoring, using a meta-heuristic based search that is dependent on software quality metrics and code smells as a guide, eliminates the need for the developer to be intimately connected to the software under modification. Computer applications in industry and engineering benefit significantly from new approaches to self-correcting refactoring software. TrueRefactor is an automated refactoring tool that significantly improves the comprehensibility of legacy systems. The goal of TrueRefactor is to modify legacy object-oriented systems in order to increase the understandability, maintainability and reusability aspects of legacy software, and to simultaneously generate new UML documentation in order to help developers understand the changes being made. This paper presents the research behind the design, as well as a technical overview of the implementation of TrueRefactor. We summarize the research goals that TrueRefactor addresses, and identify opportunities where it can be actively utilized.",
      "Keywords": "",
      "Publication venue": "Proceedings of the ISCA 24th International Conference on Computer Applications in Industry and Engineering, CAINE 2011",
      "Publication date": "2011-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Griffith, Isaac;Wahl, Scott;Izurieta, Clemente",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-80052962441",
      "Primary study DOI": "10.1007/s10515-011-0084-1",
      "Title": "Mining temporal specifications from object usage",
      "Abstract": "A caller must satisfy the callee's precondition-that is, reach a state in which the callee may be called. Preconditions describe the state that needs to be reached, but not how to reach it. We combine static analysis with model checking to mine Fair Computation Tree Logic (CTLF) formulas that describe the operations a parameter goes through: \"In parseProperties(String xml), the parameter xml normally stems from getProperties().\" Such operational preconditions can be learned from program code, and the code can be checked for their violations. Applied to AspectJ, our Tikanga prototype found 169 violations of operational preconditions, uncovering 7 unique defects and 27 unique code smells-with 52% true positives in the 25% top-ranked violations. © Springer Science+Business Media, LLC 2011.",
      "Keywords": "Automatic defect detection | Computation Tree Logic | Mining specifications | Program analysis | Temporal logic",
      "Publication venue": "Automated Software Engineering",
      "Publication date": "2011-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Wasylkowski, Andrzej;Zeller, Andreas",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-82155175199",
      "Primary study DOI": "10.1109/SERA.2011.15",
      "Title": "Towards automatic generation of ontology-based antipattern Bayesian network models",
      "Abstract": "Previous work has proposed the ontology-based semi-automatic generation of antipattern Bayesian Network(BN) models. The generated BN model can be used to illustrate the effects of uncertainty on antipatterns using Bayesian propagation. This can guide users in detecting particular antipattern attributes of importance based on uncertain ontological information. However, the proposed approach has been implemented in the Protege ontology editor environment and requires human intervention to specify how the BN model will be generated. The fully automated generation of ontology-based antipattern BN models still remains an open issue. SPARSE is an OWL ontology based intelligent system that assists software project managers in the antipattern detection process. In this paper, we propose the use of the resulting detected antipatterns of SPARSE, their attributes (i.e. causes, symptoms, consequences) and the ontological relationships between these attributes, in order to automatically generate BN models of the detected antipatterns. We illustrate how this approach can be implemented using an example of 8 antipattern attributes of 6 inter-related antipatterns detected using SPARSE. © 2011 IEEE.",
      "Keywords": "antipatterns | Bayesian networks | intelligent systems | ontology",
      "Publication venue": "Proceedings - 2011 9th International Conference on Software Engineering Research, Management and Applications, SERA 2011",
      "Publication date": "2011-11-30",
      "Publication type": "Conference Paper",
      "Authors": "Settas, Dimitrios;Cerone, Antonio;Fenz, Stefan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-82155165741",
      "Primary study DOI": "",
      "Title": "Proceedings - 2011 9th International Conference on Software Engineering Research, Management and Applications, SERA 2011",
      "Abstract": "The proceedings contain 36 papers. The topics discussed include: introducing support for embedded and real-time devices into existing hierarchical component system: lessons learned; requirements engineering current practice and capability in small and medium software development enterprises in New Zealand; towards automatic generation of ontology-based anti-pattern Bayesian network models; the general variation models of additive and multiplicative noise removal of color images and their split Bregman algorithms; architectural design and development of a hybrid robotics control system; an improved data encryption standard to secure data using smart cards; tool to support constructing, analyzing, comparing and composing models; business architecture elicitation for enterprise architecture: VMOST versus conventional strategy capture; and on testing of implementation correctness of protocol based intrusion detection systems.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2011 9th International Conference on Software Engineering Research, Management and Applications, SERA 2011",
      "Publication date": "2011-11-30",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-80055032162",
      "Primary study DOI": "10.1109/ECBS-EERC.2011.39",
      "Title": "Design smell detection with similarity scoring and fingerprinting: Preliminary study",
      "Abstract": "Design smells in software models reduce the software quality. Smells identification supports the refactoring, which is a way to improve the quality of models and subsequently increasing software readability, maintainability and extensibility. We propose a preliminary study of using Similarity Scoring Algorithm and Fingerprinting Algorithm for design smells detection. In the future, we plan to do extensive verification on several large projects, integrate these methods to the smells detection framework and compare effectiveness with other approaches. © 2011 IEEE.",
      "Keywords": "algorithms | anti-patterns | design smells | fingerprinting | refactoring | similarity scoring",
      "Publication venue": "Proceedings - 2011 2nd Eastern European Regional Conference on the Engineering of Computer Based Systems, ECBS-EERC 2011",
      "Publication date": "2011-11-02",
      "Publication type": "Conference Paper",
      "Authors": "Liška, Peter;Polášek, Ivan",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-80053195129",
      "Primary study DOI": "10.1007/978-3-642-23798-0_38",
      "Title": "Software is a directed multigraph",
      "Abstract": "The architecture of a software system is typically defined as the organization of the system, the relationships among its components and the principles governing their design. By including artifacts coresponding to software engineering processes, the definition gets naturally extended into the architecture of a software system and process. In this paper we propose a holistic model to organize knowledge of such architectures. This model is graph-based. It collects architectural artifacts as vertices and their relationships as edges. It allows operations like metric calculation, refactoring, bad smell detection and pattern discovery as algorithmic transformations on graphs. It is independent of development languages. It can be applied for both formal and adaptive projects. We have implemented prototype tools supporting this model. The artifacts are stored in a graph database. The operations are defined in a graph query language. They have short formulation and are efficiently executed by the graph database engine. © 2011 Springer-Verlag.",
      "Keywords": "architecture | graph | metric | model | software",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2011-09-30",
      "Publication type": "Conference Paper",
      "Authors": "Da̧browski, Robert;Stencel, Krzysztof;Timoszuk, Grzegorz",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-80052323769",
      "Primary study DOI": "10.1007/978-3-642-23397-5_13",
      "Title": "OpenMPspy: Leveraging quality assurance for parallel software",
      "Abstract": "OpenMP is widely used in practice to create parallel software, however, software quality assurance tool support is still immature. OpenMPspy introduces a new approach, with a short-term and a long-term perspective, to aid software engineers write better parallel programs in OpenMP. On the one hand, OpenMPspy acts like an online-debugger that statically detects problems with incorrect construct usage and which reports problems while programmers are typing code in Eclipse. We detect simple slips as well as more complex anti-patterns that can lead to correctness problems and performance problems. In addition, OpenMPspy can aggregate statistics about OpenMP language usage and bug patterns from many projects. Insights generated from such data help OpenMP language designers improve the usability of constructs and reduce error potential, thus enhancing parallel software quality in the long run. Using OpenMPspy, this paper presents one of the first detailed empirical studies of over 40 programs with more than 4 million lines of code, which shows how OpenMP constructs are actually used in practice. Our results reveal that constructs believed to be frequently used are actually rarely used. Our insights give OpenMP language and compiler designers a clearer picture on where to focus the efforts for future improvements. © 2011 Springer-Verlag.",
      "Keywords": "",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2011-09-07",
      "Publication type": "Conference Paper",
      "Authors": "Pankratius, Victor;Knittel, Fabian;Masing, Leonard;Walser, Martin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-80051650792",
      "Primary study DOI": "10.1109/ICSTW.2011.12",
      "Title": "An experience report on using code smells detection tools",
      "Abstract": "Detecting code smells in the code and consequently applying the right refactoring steps when necessary is very important to improve the quality of the code. Different tools have been proposed for code smell detection, each one characterized by particular features. The aim of this paper is to describe our experience on using different tools for code smell detection. We outline the main differences among them and the different results we obtained. © 2011 IEEE.",
      "Keywords": "Code smell detection tools | Code smells | Quality code evaluation | Refactoring",
      "Publication venue": "Proceedings - 4th IEEE International Conference on Software Testing, Verification, and Validation Workshops, ICSTW 2011",
      "Publication date": "2011-08-18",
      "Publication type": "Conference Paper",
      "Authors": "Fontana, Francesca Arcelli;Mariani, Elia;Morniroli, Andrea;Sormani, Raul;Tonello, Alberto",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-80051629554",
      "Primary study DOI": "10.1109/ICSTW.2011.14",
      "Title": "On investigating code smells correlations",
      "Abstract": "Code smells are characteristics of the software that may indicate a code or design problem that can make software hard to evolve and maintain. Detecting and removing code smells, when necessary, improves the quality and maintainability of a system. Code smells have been defined in [5], and different detection tools have been developed, each one characterized by particular features and providing often different results. Usually detection techniques are based on the computation of a particular set of combined metrics, or standard object-oriented metrics [8] or metrics defined ad hoc for the smell detection. As outlined in [3] there is the need for a clearer research strategy on smells identification and measurement. Other knowledge has to be exploited for their detection. In this work we are interested to investigate the direct and indirect correlations existing between smells. Moreover we propose to analyze if other relations exist between code smell and another kind of micro structure, called micro pattern[6]. We started this research since we think that the knowledge on these relations between smells could be exploited by code smell detection tools to improve their results. If one code smell exists, this can imply the existence of another code smell, or if one smell exists, another one cannot be there, or perhaps we could observe that some code smells tend to go together. © 2011 IEEE.",
      "Keywords": "",
      "Publication venue": "Proceedings - 4th IEEE International Conference on Software Testing, Verification, and Validation Workshops, ICSTW 2011",
      "Publication date": "2011-08-18",
      "Publication type": "Conference Paper",
      "Authors": "Fontana, Francesca Arcelli;Zanoni, Marco",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79960940136",
      "Primary study DOI": "10.1007/978-3-642-21378-6_2",
      "Title": "A metric-based approach for anti-pattern detection in UML designs",
      "Abstract": "Anti-patterns are poor solutions of recurring design problems, which decrease software quality. Numerous anti-patterns have been outlined in the literature as violations of various quality rules. Most of these anti-patterns have been defined in terms of code quality metrics. However, identifying anti-patterns at the design level would improve considerably the code quality and substantially reduce the cost of correcting their effects during the coding and maintenance phases. Within this context, we propose an approach that identifies anti-patterns in UML designs through the use of existing and newly defined quality metrics. Operating at the design level, our approach examines structural and behavioral information through the class and sequence diagrams. It is illustrated through five, well-known anti-patterns: Blob, Lava Flow, Functional Decomposition, Poltergeists, and Swiss Army Knife. © 2011 Springer-Verlag Berlin Heidelberg.",
      "Keywords": "",
      "Publication venue": "Studies in Computational Intelligence",
      "Publication date": "2011-08-04",
      "Publication type": "Conference Paper",
      "Authors": "Fourati, Rahma;Bouassida, Nadia;Abdallah, Hanêne Ben",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79960640352",
      "Primary study DOI": "10.1109/EUROCON.2011.5929241",
      "Title": "Applying source code analysis techniques: A case study for a large mission-critical software system",
      "Abstract": "Source code analysis has been and still is extensively researched topic with various applications to the modern software industry. In this paper we share our experience in applying various source code analysis techniques for assessing the quality of and detecting potential defects in a large mission-critical software system. The case study is about the maintenance of a software system of a Bulgarian government agency. The system has been developed by a third-Party software vendor over a period of four years. The development produced over 4 million LOC using more than 20 technologies. MuSala Soft won a tender for maintaining this system in 2008. Although the system was operational, there were various issues that were known to its users. So, a decision was made to assess the system's quality with various source code analysis tools. The expectation was that the findings will reveal some of the problems' cause, allowing us to correct the issues and thus improve the quality and focus on functional enhancements. MuSala Soft had already established a special unit Applied Research and Development Center dealing with research and advancements in the area of software system analysis. Thus, a natural next step was for this unit to use the know-how and in-house developed tools to do the assessment. The team used various techniques that had been subject to intense research, more precisely: software metrics, code clone detection, defect and code smells detection through flow-sensitive and points-to analysis, software visualization and graph drawing. In addition to the open-source and free commercial tools, the team used internally developed ones that complement or improve what was available. The internally developed Smart Source Analyzer platform that was used is focused on several analysis areas: source code modeling, allowing easy navigation through the code elements and relations for different programming languages; quality audit through software metrics by aggregating various metrics into a more meaningful quality characteristic (e.g. \"maintainability\" ); source code pattern recognition to detect various security issues and \"code smells\". The produced results presented information about both the structure of the system and its quality. As the analysis was executed in the beginning of the maintenance tenure, it was vital for the team members to quickly grasp the architecture and the business logic. On the other hand, it was important to review the detected quality problems as this guided the team to quick solutions for the existing issues and also highlighted areas that would impede future improvements. The tool IPlasma and its System Complexity View (Fig. 1) revealed where the business logic is concentrated, which are the most important and which are the most complex elements of the system. The analysis with our internal metrics framework (Fig. 2) pointed out places that need refactoring because the code is hard to modify on request or testing is practically impossible. The code clone detection tools showed places where copy and paste programming has been applied. PMD, Find Bugs and Klockwork Solo tools were used to detect various \"code smells\" (Fig. 3). There were a number of occurrences that were indeed bugs in the system. Although these results were productive for the successful execution of the project, there were some challenges that should be addressed in the future through more extensive research. The two aspects we consider the most important are uSability and integration. As most of the tools require very deep understanding of the underlying analysis, the whole process requires tight cooperation between the analysis team and the maintenance team. For example, most of the metrics tools available provide specific values for a given metric without any indication what the value means and what is the threshold. Our internal metrics framework aggregates the metrics into meaningful quality characteristics, which solves the issue Partially. However, the user still often wonders about the justification behind the meaning of the given quality characteristic. There is a need for an explanation system one, which could point out the source code elements and explain why they are considered good or bad. The integration aspect is considered important because such analysis should be performed continuously. In our experience, the analysis is usually performed subsequent to an important event in this case: beginning of maintenance tenure. Some quality assurance practices should be developed and then adopted by the development teams so that the implementation quality is checked continuously. This should cover various activities and instruments, such as the integrated development environment, the code review process, automated builds, etc. In conclusion, we think that implementation quality audit and management is a vital activity that should be integrated into the software development process and the tools that support it should be uSable by the development team members without much knowledge of the underlying analysis. In this paper we presented a case study that showed the benefits of such a process. © 2011 IEEE.",
      "Keywords": "case study | implementation quality | software maintenance | software metrics | source code analysis",
      "Publication venue": "EUROCON 2011 - International Conference on Computer as a Tool - Joint with Conftele 2011",
      "Publication date": "2011-07-27",
      "Publication type": "Conference Paper",
      "Authors": "Haralambiev, Haralambi;Boychev, Stanimir;Lilov, Delyan;Kraichev, Kraicho",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79960598858",
      "Primary study DOI": "10.1145/1985362.1985372",
      "Title": "Prioritizing design debt investment opportunities",
      "Abstract": "Technical debt is the technical work developers owe a system, typically caused by speeding up development, e.g. before a software release. Approaches, such as code smell detection, have been developed to identify particular kinds of debt, e.g. design debt. Up until now, code smell detection has been used to help point to components that need to be freed from debt by refactoring. To date, a number of methods have been described for finding code smells in a system. However, typical debt properties, such as the value of the debt and interest rate to be paid, have not been well established. This position paper proposes an approach to using cost/benefit analysis to prioritize technical debt reduction work by ranking the value and interest of design debt caused by god classes. The method is based on metric analysis and software repository mining and is demonstrated on a commercial software application at a mid-size development company. The results are promising: the method helps to identify which refactoring activities should be performed first because they are likely to be cheap to make yet have significant effect, and which refactorings should be postponed due to high cost and low payoff. © 2011 ACM.",
      "Keywords": "code smells | design debt | god class | maintainability | refactoring | technical debt",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2011-07-26",
      "Publication type": "Conference Paper",
      "Authors": "Zazworka, Nico;Seaman, Carolyn;Shull, Forrest",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79960575289",
      "Primary study DOI": "10.1145/1985362.1985366",
      "Title": "Investigating the impact of design debt on software quality",
      "Abstract": "Technical debt is a metaphor describing situations where developers accept sacrifices in one dimension of development (e.g. software quality) in order to optimize another dimension (e.g. implementing necessary features before a deadline). Approaches, such as code smell detection, have been developed to identify particular kinds of debt, e.g. design debt. What has not yet been understood is the impact design debt has on the quality of a software product. Answering this question is important for understanding how growing debt affects a software product and how it slows down development, e.g. though introducing rework such as fixing bugs. In this case study we investigate how design debt, in the form of god classes, affects the maintainability and correctness of software products by studying two sample applications of a small-size software development company. The results show that god classes are changed more often and contain more defects than non-god classes. This result complements findings of earlier research and suggests that technical debt has a negative impact on software quality, and should therefore be identified and managed closely in the development process. © 2011 ACM.",
      "Keywords": "code smells | design debt | god class | maintainability | refactoring | technical debt",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2011-07-26",
      "Publication type": "Conference Paper",
      "Authors": "Zazworka, Nico;Shaw, Michele A.;Shull, Forrest;Seaman, Carolyn",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79960533364",
      "Primary study DOI": "10.1145/2000259.2000265",
      "Title": "Combining clustering and pattern detection for the reengineering of component-based software systems",
      "Abstract": "During the software lifecycle, software systems have to be continuously maintained to counteract architectural deterioration and retain their software quality. In order to maintain a software it has to be understood first which can be supported by (semi-)automatic reverse engineering approaches. Reverse engineering is the analysis of software for the purpose of recovering its design documentation, e.g., in form of the conceptual architecture. Today, the most prevalent reverse engineering approaches are (1) the clustering based approach which groups the elements of a given software system based on metric values in order to provide an overview of the system and (2) the pattern-based approach which tries to detect pre-defined patterns in the software which can give insight about the original developers' intentions. In this paper, we present an approach towards combining these techniques: we show how the detection and removal of certain bad smells in a software system can improve the results of a clustering-based analysis. We propose to integrate this combination of reverse engineering approaches into a reengineering process for component-based software systems. © 2011 ACM.",
      "Keywords": "bad smell detection | clustering | metrics | reengineering | software architecture",
      "Publication venue": "CompArch'11 - Proceedings of the 2011 Federated Events on Component-Based Software Engineering and Software Architecture - QoSA+ISARCS'11",
      "Publication date": "2011-07-25",
      "Publication type": "Conference Paper",
      "Authors": "Von Detten, Markus;Becker, Steffen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79960385525",
      "Primary study DOI": "10.1109/JCSSE.2011.5930143",
      "Title": "Bad-smell prediction from software design model using machine learning techniques",
      "Abstract": "Bad-smell prediction significantly impacts on software quality. It is beneficial if bad-smell prediction can be performed as early as possible in the development life cycle. We present methodology for predicting bad-smells from software design model. We collect 7 data sets from the previous literatures which offer 27 design model metrics and 7 bad-smells. They are learnt and tested to predict bad-smells using seven machine learning algorithms. We use cross-validation for assessing the performance and for preventing over-fitting. Statistical significance tests are used to evaluate and compare the prediction performance. We conclude that our methodology have proximity to actual values. © 2011 IEEE.",
      "Keywords": "Bad-smell | Design Diagram Metrics | Machine Learners | Prediction models | Random Forest | Software Design Model",
      "Publication venue": "Proceedings of the 2011 8th International Joint Conference on Computer Science and Software Engineering, JCSSE 2011",
      "Publication date": "2011-07-21",
      "Publication type": "Conference Paper",
      "Authors": "Maneerat, Nakarin;Muenchaisri, Pomsiri",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79959890854",
      "Primary study DOI": "10.1145/1985793.1986003",
      "Title": "Detecting architecturally-relevant code smells in evolving software systems",
      "Abstract": "Refactoring tends to avoid the early deviation of a program from its intended architecture design. However, there is little knowledge about whether the manifestation of code smells in evolving software is indicator of architectural deviations. A fundamental difficulty in this process is that developers are only equipped with static analysis techniques for the source code, which do not exploit traceable architectural information. This work addresses this problem by: (1) identifying a family of architecturally-relevant code smells; (2) providing empirical evidence about the correlation of code smell patterns and architectural degeneration; (3) proposing a set of metrics and detection strategies and that exploit traceable architectural information in smell detection; and (4) conceiving a technique to support the early identification of architecture degeneration symptoms by reasoning about code smell patterns. © 2011 Author.",
      "Keywords": "architectural degeneration | code smells | design rule",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2011-07-07",
      "Publication type": "Conference Paper",
      "Authors": "Bertran, Isela Mac Ia",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79959864367",
      "Primary study DOI": "10.1145/1985793.1985850",
      "Title": "Detecting software modularity violations",
      "Abstract": "This paper presents Clio, an approach that detects modularity violations, which can cause software defects, modularity decay, or expensive refactorings. Clio computes the discrepancies between how components should change together based on the modular structure, and how components actually change together as revealed in version history. We evaluated Clio using 15 releases of Hadoop Common and 10 releases of Eclipse JDT. The results show that hundreds of violations identified using Clio were indeed recognized as design problems or refactored by the developers in later versions. The identified violations exhibit multiple symptoms of poor design, some of which are not easily detectable using existing approaches. © 2011 ACM.",
      "Keywords": "bad code smells | design structure matrix | modularity violation detection | refactoring",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2011-07-07",
      "Publication type": "Conference Paper",
      "Authors": "Wong, Sunny;Cai, Yuanfang;Kim, Miryung;Dalton, Michael",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79951576634",
      "Primary study DOI": "10.1016/j.eswa.2010.12.097",
      "Title": "SPARSE: A symptom-based antipattern retrieval knowledge-based system using Semantic Web technologies",
      "Abstract": "Antipatterns provide information on commonly occurring solutions to problems that generate negative consequences. The number of software project management antipatterns that appears in the literature and the Web increases to the extent that makes using antipatterns problematic. Furthermore, antipatterns are usually inter-related and rarely appear in isolation. As a result, detecting which antipatterns exist in a software project is a challenging task which requires expert knowledge. This paper proposes SPARSE, an OWL ontology based knowledge-based system that aims to assist software project managers in the antipattern detection process. The antipattern ontology documents antipatterns and how they are related with other antipatterns through their causes, symptoms and consequences. The semantic relationships that derive from the antipattern definitions are determined using the Pellet DL reasoner and they are transformed into the COOL language of the CLIPS production rule engine. The purpose of this transformation is to create a compact representation of the antipattern knowledge, enabling a set of object-oriented CLIPS production rules to run and retrieve antipatterns relevant to some initial symptoms. SPARSE is exemplified through 31 OWL ontology antipattern instances of software development antipatterns that appear on the Web. © 2010 Elsevier Ltd. All rights reserved.",
      "Keywords": "Antipatterns | Objects | OWL ontology | Production rules | Symptom-based retrieval",
      "Publication venue": "Expert Systems with Applications",
      "Publication date": "2011-06-01",
      "Publication type": "Article",
      "Authors": "Settas, Dimitrios L.;Meditskos, Georgios;Stamelos, Ioannis G.;Bassiliades, Nick",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79955165301",
      "Primary study DOI": "10.1109/CSMR.2011.24",
      "Title": "An empirical study of the impact of two antipatterns, Blob and Spaghetti Code, on program comprehension",
      "Abstract": "Antipatterns are \"poor\" solutions to recurring design problems which are conjectured in the literature to make object-oriented systems harder to maintain. However, little quantitative evidence exists to support this conjecture. We performed an empirical study to investigate whether the occurrence of antipatterns does indeed affect the under-standability of systems by developers during comprehension and maintenance tasks. We designed and conducted three experiments, with 24 subjects each, to collect data on the performance of developers on basic tasks related to program comprehension and assessed the impact of two antipatterns and of their combinations: Blob and Spaghetti Code. We measured the developers' performance with: (1) the NASA task load index for their effort; (2) the time that they spent performing their tasks; and, (3) their percentages of correct answers. Collected data show that the occurrence of one antipattern does not significantly decrease developers' performance while the combination of two antipatterns impedes significantly developers. We conclude that developers can cope with one antipattern but that combinations of antipatterns should be avoided possibly through detection and refactorings. © 2011 IEEE.",
      "Keywords": "Antipatterns | Blob | Empirical software engineering | Program comprehension | Program maintenance | Spaghetti Code",
      "Publication venue": "Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR",
      "Publication date": "2011-04-29",
      "Publication type": "Conference Paper",
      "Authors": "Abbes, Marwen;Khomh, Foutse;Guéhéneuc, Yann Gaël;Antoniol, Giuliano",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79955151863",
      "Primary study DOI": "10.1145/1960314.1960335",
      "Title": "Hist-inspect: A tool for history-sensitive detection of code smells",
      "Abstract": "Hist-Inspect is a tool that allows the specification and evaluation of different configurations for detection strategies by means of a domain-specific language. The tool enables to easily adjust thresholds and combination of software metrics as well as compare the performance of conventional and history-sensitive detection strategies. The tool also provides a diverse set of views, including graphical representation of module evolution measures. These views enable the code reviewer to reason about the stability of individual modules, the growth or decline of a particular structural property (e.g. coupling or cohesion), without the burden of recovering all the values for each version under analysis.",
      "Keywords": "Detection strategy | Metrics",
      "Publication venue": "Proceedings of the 10th International Conference on Aspect-Oriented Software Development Companion, AOSD.11",
      "Publication date": "2011-04-29",
      "Publication type": "Conference Paper",
      "Authors": "Mara, Leandra;Honorato, Gustavo;Dantas, Francisco;Garcia, Alessandro;Lucena, Carlos",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79955138448",
      "Primary study DOI": "",
      "Title": "Proceedings of the 10th International Conference on Aspect-Oriented Software Development Companion, AOSD.11",
      "Abstract": "The proceedings contain 25 papers. The topics discussed include: experiences documenting and preserving software constraints using aspects; ASystemC: an AOP extension for hardware description language; using aspect-orientation to simplify concurrent programming; flexible, dynamic injection of structured advice using Byteman; the theory and practice of modern modeling language design for model-based software engineering; rulemakers and toolmakers: adaptive object-models as an agile division of labor: ultimate agility: let your users do your work!; the aspect-oriented user requirements notation: aspects, goals, and scenarios; analyzing architectural conformance of layered aspect-oriented systems with ArchE Meter; Hist-Inspect: a tool for history-sensitive detection of code smells; GenArch+: an extensible infrastructure for building framework-based software product lines; and revealing architecturally-relevant flaws in aspectual decompositions.",
      "Keywords": "",
      "Publication venue": "Proceedings of the 10th International Conference on Aspect-Oriented Software Development Companion, AOSD.11",
      "Publication date": "2011-04-29",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79955161150",
      "Primary study DOI": "10.1109/CSMR.2011.7",
      "Title": "Ranking refactoring suggestions based on historical volatility",
      "Abstract": "The widespread acceptance of refactorings as a simple yet effective approach to improve the design of object-oriented systems, has stimulated an effort to develop semi-automatic tools for detecting design flaws, with simultaneous suggestions for their removal. However, even in medium-sized projects the number of detected occurrences can be so large that the refactoring process becomes intractable for the designer. It is reasonable to expect that some of the suggested refactorings will have a significant effect on the improvement of maintainability while others might be less important. This implies that the suggested solutions can be ranked according to one or more criteria. In this paper we propose the exploitation of past source code versions in order to rank refactoring suggestions according to the number, proximity and extent of changes related with the corresponding code smells. The underlying philosophy is that code fragments which have been subject to maintenance tasks in the past, are more likely to undergo changes in a future version and thus refactorings involving the corresponding code should have a higher priority. To this end, historical volatility models drawn from the field of forecasting risk in financial markets, are investigated as measures expressing the urgency to resolve a given design problem. The approach has been integrated into an existing smell detection Eclipse plug-in, while the evaluation results focus on the forecast accuracy of the examined models. © 2011 IEEE.",
      "Keywords": "Code smell | Forecasting models | Historical volatility | Refactoring | Software history | Software repositories",
      "Publication venue": "Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR",
      "Publication date": "2011-04-29",
      "Publication type": "Conference Paper",
      "Authors": "Tsantalis, Nikolaos;Chatzigeorgiou, Alexander",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79953880789",
      "Primary study DOI": "10.1145/1958746.1958755",
      "Title": "Detection and solution of software performance antipatterns in Palladio architectural models",
      "Abstract": "Antipatterns are conceptually similar to patterns in that they document recurring solutions to common design problems. Performance Antipatterns document, from a performance perspective, common mistakes made during software development as well as their solutions. The definition of performance antipatterns concerns software properties that can include static, dynamic, and deployment aspects. Currently, such knowledge is only used by domain experts; the problem of automatically detecting and solving antipatterns within an architectural model had not yet been empirically addressed. In this paper we present an approach to automatically detect and solve software performance antipatterns within the Palladio architectural models: the detection of an antipattern provides a software performance feedback to designers, since it suggests the architectural alternatives to overcome specific performance problems. We implemented the approach and a case study is presented to demonstrate its validity. The system performance under study has been improved by 50% with the use of antipatterns' solutions.",
      "Keywords": "Palladio component model | Performance antipatterns | Software performance feedback",
      "Publication venue": "ICPE'11 - Proceedings of the 2nd Joint WOSP/SIPEW International Conference on Performance Engineering",
      "Publication date": "2011-04-18",
      "Publication type": "Conference Paper",
      "Authors": "Trubiani, Catia;Koziolek, Anne",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79953199444",
      "Primary study DOI": "10.1007/978-3-642-19811-3_25",
      "Title": "Incremental clone detection and elimination for Erlang programs",
      "Abstract": "A well-known bad code smell in refactoring and software maintenance is the existence of code clones, which are code fragments that are identical or similar to one another. This paper describes an approach to incrementally detecting 'similar' code based on the notion of least-general common abstraction, or anti-unification, as well as a framework for user-controlled incremental elimination of code clones within the context of Erlang programs. The clone detection algorithm proposed in this paper achieves 100% precision, high recall rate, and is user-customisable regarding the granularity of the clone classes reported. By detecting and eliminating clones in an incremental way, we make it possible for the tool to be used in an interactive way even with large codebases. Both the clone detection and elimination functionalities are integrated with Wrangler, a tool for interactive refactoring of Erlang programs. We evaluate the approach with various case studies. © 2011 Springer-Verlag.",
      "Keywords": "Code clone detection | Erlang | Program analysis | Program transformation | Refactoring | Software maintenance | Wrangler",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2011-04-04",
      "Publication type": "Conference Paper",
      "Authors": "Li, Huiqing;Thompson, Simon",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79952932091",
      "Primary study DOI": "10.1002/smr.521",
      "Title": "Code Bad Smells: A review of current knowledge",
      "Abstract": "Fowler et al. identified 22 Code Bad Smells to direct the effective refactoring of code. These are increasingly being taken up by software engineers. However, the empirical basis of using Code Bad Smells to direct refactoring and to address 'trouble' in code is not clear, i.e., we do not know whether using Code Bad Smells to target code improvement is effective. This paper aims to identify what is currently known about Code Bad Smells. We have performed a systematic literature review of 319 papers published since Fowler et al. identified Code Bad Smells (2000 to June 2009). We analysed in detail 39 of the most relevant papers. Our findings indicate that Duplicated Code receives most research attention, whereas some Code Bad Smells, e.g., Message Chains, receive little. This suggests that our knowledge of some Code Bad Smells remains insufficient. Our findings also show that very few studies report on the impact of using Code Bad Smells, with most studies instead focused on developing tools and methods to automatically detect Code Bad Smells. This indicates an important gap in the current knowledge of Code Bad Smells. Overall this review suggests that there is little evidence currently available to justify using Code Bad Smells. Copyright © 2010 John Wiley & Sons, Ltd.",
      "Keywords": "Code Bad Smells | empirical software engi-neering | refactoring | Systematic Literature Review",
      "Publication venue": "Journal of Software Maintenance and Evolution",
      "Publication date": "2011-04-01",
      "Publication type": "Article",
      "Authors": "Zhang, Min;Hall, Tracy;Baddoo, Nathan",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79960294851",
      "Primary study DOI": "10.1007/978-3-642-22056-2_50",
      "Title": "Detecting antipatterns using a web-based collaborative antipattern ontology knowledge base",
      "Abstract": "The enrichment of the antipattern ontology that acts as the lexicon of terms to communicate antipatterns between people and software tools, is a labor intensive task. Existing work has implemented SPARSE, an ontology based intelligent system that uses a symptom based approach in order to semantically detect and retrieve inter-related antipatterns that exist in a software project. In this paper, we propose a Web-based environment that uses the Protege platform, in order to allow collaborative ontology editing as well as annotation and voting of both ontology components and ontology changes. This technology allows multiple users to edit and enrich the antipattern ontology simultaneously. Preliminary results on SPARSE show the effectiveness of the antipattern detection process during the research and development of a software project. © 2011 Springer-Verlag.",
      "Keywords": "Antipatterns | Collaborative Ontology Development | Collaborative software engineering",
      "Publication venue": "Lecture Notes in Business Information Processing",
      "Publication date": "2011-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Settas, Dimitrios;Meditskos, Georgios;Bassiliades, Nick;Stamelos, Ioannis G.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-78650166517",
      "Primary study DOI": "10.1109/ICSM.2010.5609564",
      "Title": "Are all code smells harmful? A study of God Classes and Brain Classes in the evolution of three open source systems",
      "Abstract": "Code smells are particular patterns in object-oriented systems that are perceived to lead to difficulties in the maintenance of such systems. It is held that to improve maintainability, code smells should be eliminated by refactoring. It is claimed that classes that are involved in certain code smells are liable to be changed more frequently and have more defects than other classes in the code. We investigated the extent to which this claim is true for God Classes and Brain Classes, withand without normalizing the effects with respect to the class size. We analyzed historical data from 7 to 10 years of the development of three open-source software systems. The results show that God and Brain Classes were changed more frequently and contained more defects than other kinds of class. However, when we normalized the measured effects with respect to size, then God and Brain Classes were less subject to change and had fewer defects than other classes. Hence, under the assumption that God and Brain Classes contain on average as much functionality per line of code as other classes, the presence of God and Brain Classes is not necessarily harmful; in fact, such classes may be an efficient way of organizing code. © 2010 IEEE.",
      "Keywords": "Change frequency | Code smells | Defects | Detection strategies | Open source | Software evolution",
      "Publication venue": "IEEE International Conference on Software Maintenance, ICSM",
      "Publication date": "2010-12-20",
      "Publication type": "Conference Paper",
      "Authors": "Olbrich, Steffen M.;Cruzes, Daniela S.;Sjoøberg, Dag I.K.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-78650078104",
      "Primary study DOI": "10.1145/1869459.1869475",
      "Title": "Scalable and systematic detection of buggy inconsistencies in source code",
      "Abstract": "Software developers often duplicate source code to replicate functionality. This practice can hinder the maintenance of a software project: bugs may arise when two identical code segments are edited inconsistently. This paper presents DejaVu, a highly scalable system for detecting these general syntactic inconsistency bugs. DejaVu operates in two phases. Given a target code base, a parallel inconsistent clone analysis first enumerates all groups of source code fragments that are similar but not identical. Next, an extensible buggy change analysis framework refines these results, separating each group of inconsistent fragments into a fine-grained set of inconsistent changes and classifying each as benign or buggy. On a 75+ million line pre-production commercial code base, DejaVu executed in under five hours and produced a report of over 8,000 potential bugs. Our analysis of a sizable random sample suggests with high likelihood that at this report contains at least 2,000 true bugs and 1,000 code smells. These bugs draw from a diverse class of software defects and are often simple to correct: syntactic inconsistencies both indicate problems and suggest solutions. © 2010 ACM.",
      "Keywords": "Algorithms | Experimentation | Languages | Reliability",
      "Publication venue": "Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",
      "Publication date": "2010-12-20",
      "Publication type": "Conference Paper",
      "Authors": "Gabel, Mark;Yang, Junfeng;Yu, Yuan;Goldszmidt, Moises;Su, Zhendong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85060586083",
      "Primary study DOI": "10.1109/UBMK.2018.8566561",
      "Title": "Comparison of Machine Learning Methods for Code Smell Detection Using Reduced Features",
      "Abstract": "We examine a machine learning approach for detecting common Class and Method level code smells (Data Class and God Class, Feature Envy and Long Method). The focus of the work is selection of reduced set of features that will achieve high classification accuracy. The proposed features may be used by the developers to develop better quality software since the selected features focus on the most critical parts of the code that is responsible for creation of common code smells. We obtained a high accuracy results for all four code smells using the selected features: 98.57% for Data Class, 97.86% for God Class, 99.67% for Feature Envy, and 99.76% for Long Method.",
      "Keywords": "code smells | feature selection | machine learning",
      "Publication venue": "UBMK 2018 - 3rd International Conference on Computer Science and Engineering",
      "Publication date": "2018-12-06",
      "Publication type": "Conference Paper",
      "Authors": "Karaduzovic-Hadziabdic, Kanita;Spahic, Rialda",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-78751548749",
      "Primary study DOI": "10.1109/QUATIC.2010.16",
      "Title": "Investigating the evolution of bad smells in object-oriented code",
      "Abstract": "Software design problems are known and perceived under many different terms such as bad smells, flaws, non-compliance to design principles, violation of heuristics, excessive metric values and antipatterns, signifying the importance of handling them in the construction and maintenance of software. Once a design problem is identified, it can be removed by applying an appropriate refactoring, improving in most cases several aspects of quality such as maintainability, comprehensibility and reusability. This paper, taking advantage of recent advances and tools in the identification of non-trivial bad smells, explores the presence and evolution of such problems by analyzing past versions of code. Several interesting questions can be investigated such as whether the number of problems increases with the passage of software generations, whether problems vanish by time or only by targeted human intervention, whether bad smells occur in the course of evolution of a module or exist right from the beginning and whether refactorings targeting at smell removal are frequent. In contrast to previous studies that investigate the application of refactorings in the history of a software project, we attempt to study the subject from the point of view of the problems themselves distinguishing deliberate maintenance activities from the removal of design problems as a side effect of software evolution. Results are discussed for two open-source systems and three bad smells. © 2010 IEEE.",
      "Keywords": "Bad smell | Evolution | Refactoring | Software history | Software repositories",
      "Publication venue": "Proceedings - 7th International Conference on the Quality of Information and Communications Technology, QUATIC 2010",
      "Publication date": "2010-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Chatzigeorgiou, Alexander;Manakos, Anastasios",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85064464805",
      "Primary study DOI": "10.1007/s00521-019-04175-z",
      "Title": "SP-J48: a novel optimization and machine-learning-based approach for solving complex problems: special application in software engineering for detecting code smells",
      "Abstract": "This paper presents a novel hybrid algorithm based on optimization and machine-learning approaches for solving real-life complex problems. The optimization algorithm is inspired from the searching and attacking behaviors of sandpipers, called as Sandpiper Optimization Algorithm (SPOA). These two behaviors are modeled and implemented computationally to emphasize intensification and diversification in the search space. A comparison of the proposed SPOA algorithm is performed with nine competing optimization algorithms over 23 benchmark test functions. The proposed SPOA is further hybridized with B-J48 pruned machine-learning approach for efficiently detecting the code smells from the data set. The results reveal that the proposed technique is able to solve challenging problems and outperforms the other well-known approaches.",
      "Keywords": "Bio-inspired metaheuristic techniques | Code smells | Machine-learning | Optimization",
      "Publication venue": "Neural Computing and Applications",
      "Publication date": "2020-06-01",
      "Publication type": "Article",
      "Authors": "Kaur, Amandeep;Jain, Sushma;Goel, Shivani",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-78650855103",
      "Primary study DOI": "10.1145/1879211.1879214",
      "Title": "An interactive ambient visualization for code smells",
      "Abstract": "Code smells are characteristics of software that indicate that code may have a design problem. Code smells have been proposed as a way for programmers to recognize the need for restructuring their software. Because code smells can go unnoticed while programmers are working, tools called smell detectors have been developed to alert programmers to the presence of smells in their code, and to help them understand the cause of those smells. In this paper, we propose a novel smell detector called Stench Blossom that provides an interactive ambient visualization designed to first give programmers a quick, high-level overview of the smells in their code, and then, if they wish, to help in understanding the sources of those code smells. We also describe a laboratory experiment with 12 programmers that tests several hypotheses about our tool. Our findings suggest that programmers can use our tool effectively to identify smells and to make refactoring judgements. This is partly because the tool serves as a memory aid, and partly because it is more reliable and easier to use than heuristics for analyzing smells. Copyright 2010 ACM.",
      "Keywords": "Code smells | Refactoring | Software | Usability",
      "Publication venue": "Proceedings of the ACM Conference on Computer and Communications Security",
      "Publication date": "2010-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Murphy-Hill, Emerson;Black, Andrew P.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79952062628",
      "Primary study DOI": "10.1109/SBES.2010.21",
      "Title": "Identifying code smells with multiple concern views",
      "Abstract": "Code smells are anomalies often caused by the way concerns are realized in the source code. Their identification might depend on properties governing the structure of individual concerns and their inter-dependencies in the system implementation. Although code visualization tools are increasingly applied to support anomaly detection, they are mostly limited to represent modular structures, such as methods, classes and packages. This paper presents a multiple views approach that enriches four categories of code views with concern properties, namely: (i) concern's package-classmethod structure, (ii) concern's inheritance-wise structure, (iii) concern dependency, and (iv) concern dependency weight. An exploratory study was conducted to assess the extent to which visual views support code smell detection. Developers identified a set of well-known code smells on five versions of an opensource system. Two important results came out of this study. First, the concern-driven views provided useful support to identify God Class and Divergent Change smells. Second, strategies for smell detection supported by the multiple concern views were uncovered. © 2010 IEEE.",
      "Keywords": "Code smells | Concerns | Program comprehension | Software visualization",
      "Publication venue": "Proceedings - 24th Brazilian Symposium on Software Engineering, SBES 2010",
      "Publication date": "2010-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Carneiro, Glauco De F.;Silva, Marcos;Mara, Leandra;Figueiredo, Eduardo;Sant'Anna, Claudio;Garcia, Alessandro;Mendonça, Manoel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79952032698",
      "Primary study DOI": "10.1109/ICECCS.2010.24",
      "Title": "Performance antipatterns as logical predicates",
      "Abstract": "The problem of interpreting the results of performance analysis is quite critical in the software performance domain. Mean values, variances, probability distributions are hard to interpret for providing feedback to software architects. Instead, what architects expect are solutions to performance problems, possibly in the form of architectural alternatives (e.g. split a software component in two components and re-deploy one of them). In a software performance engineering approach this path from analysis results to software alternatives still lacks of automation and is based on the skills and experience of analysts. In this paper we propose an automated approach for the performance feedback generation process based on performance antipatterns. To this aim, we model performance antipatterns as logical predicates and we provide a java engine, based on such predicates, that is able to detect performance antipatterns in an XML representation of the software system. Finally, we show the approach at work on a simple case study. © 2010 IEEE.",
      "Keywords": "Antipatterns | Performance analysis | Software performance engineering",
      "Publication venue": "Proceedings of the IEEE International Conference on Engineering of Complex Computer Systems, ICECCS",
      "Publication date": "2010-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Cortellessa, Vittorio;Di Marco, Antinisca;Trubiani, Catia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-78651089155",
      "Primary study DOI": "10.1007/978-3-642-17578-7_28",
      "Title": "Bug forecast: A method for automatic bug prediction",
      "Abstract": "In this paper we present an approach and a toolset for automatic bug prediction during software development and maintenance. The toolset extends the Columbus source code quality framework, which is able to integrate into the regular builds, analyze the source code, calculate different quality attributes like product metrics and bad code smells; and monitor the changes of these attributes. The new bug forecast toolset connects to the bug tracking and version control systems and assigns the reported and fixed bugs to the source code classes from the past. It then applies machine learning methods to learn which values of which quality attributes typically characterized buggy classes. Based on this information it is able to predict bugs in current and future versions of the classes. The toolset was evaluated on an industrial software system developed by a large software company called evosoft. We studied the behavior of the toolset through a 1,5 year development period during which 128 snapshots of the software were analyzed. The toolset reached an average bug prediction precision of 72%, reaching many times 100%. We concentrated on high precision, as the primary purpose of the toolset is to aid software developers and testers in pointing out the classes which contain bugs with a high probability and keep the number of false positives relatively low. © 2010 Springer-Verlag Berlin Heidelberg.",
      "Keywords": "bad code smells | bug prediction | machine learning | software product metrics",
      "Publication venue": "Communications in Computer and Information Science",
      "Publication date": "2010-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Ferenc, Rudolf",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79952065945",
      "Primary study DOI": "10.1109/SBES.2010.14",
      "Title": "Defining and applying detection strategies for aspect-oriented code smells",
      "Abstract": "A code smell is any symptom in the source code that possibly indicates a bad design or programming problem. Many code smells in aspect-oriented programming (AOP) are very different from those in object-oriented programming. Therefore, new detection strategies should be conceived to identify whether a particular slice of aspect-oriented code is affected by a specific smell. Unfortunately, research on AOP usually focuses on providing abstract descriptions of code smells, without providing operational definitions of their detection strategies. Such strategies are becoming increasingly required due to the growing use of AOP in the development of long-living systems, including frameworks, libraries and software product lines. This paper presents a family of metric-based strategies that support the detection of recurring smells observed in existing aspect-oriented systems. We analyzed the accuracy of such smell detection strategies and also of those previously reported in the literature. Our study involved in total 17 releases of 3 evolving aspectoriented systems from different domains. The outcome of our evaluation suggests that strategies for previously-documented AOP smells do not present a satisfactory accuracy. Our analysis also revealed that: (1) newly-discovered strategies achieved better results than well-known ones, and (2) the detection strategies seem to have high accuracy with respect to the identification of both trivial and non-trivial code smells. © 2010 IEEE.",
      "Keywords": "AOP | Code smell | Detection strategy | Exploratory study",
      "Publication venue": "Proceedings - 24th Brazilian Symposium on Software Engineering, SBES 2010",
      "Publication date": "2010-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Macia, Isela;Garcia, Alessandro;Von Staa, Arndt",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-78650641168",
      "Primary study DOI": "10.1109/WCRE.2010.47",
      "Title": "Improving system testability and testing with microarchitectures",
      "Abstract": "Testing is essential to ensure software reliability and dependability. however, testing activities are very expensive and complex. Microarchitectures, such as design patterns and anti-patterns, widely exist in object oriented systems and are recognized as influencing many software quality attributes. Our goal is to identify their impact on system testability and analyse how they can be leveraged to reduce the testing effort while increasing the system reliability and dependability. The proposed research aims at contributing to reduce complexity and increase testing efficiency by using micro architectures. We will base our work on the rich existing tools of microarchitectures detection and code reverse-engineering. © 2010 IEEE.",
      "Keywords": "Anti-patterns | Design patterns | Test case generation | Testability | Testing",
      "Publication venue": "Proceedings - Working Conference on Reverse Engineering, WCRE",
      "Publication date": "2010-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Sabané, Aminata",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84873464175",
      "Primary study DOI": "",
      "Title": "Semantic wiki refactoring. A strategy to assist semantic wiki evolution",
      "Abstract": "The content and structure of a wiki evolve as a result of the collaborative effort of the wiki users. In semantic wikis, this also results in the evolution of the ontology that is implicitly expressed through the semantic annotations. Without proper guidance, the semantic wiki can evolve in a chaotic manner resulting in quality problems in the underlying ontology, e.g. inconsistencies. As the wiki grows in size, the detection and solution of quality problems become more diffcult. We propose an approach to detect quality problems in semantic wikis and assist users in the process of solving them. Our approach is inspired by the key principles of software refactoring, namely the cataloging and automated detection of quality problems (bad smells), and the application of quality improvement transformations (refactorings). In this paper we discuss the problem of evolving semantic wikis, present the core model of our approach, and introduce an extensible catalog of semantic wiki bad smells and an extensible toolkit of semantic wiki refactorings.",
      "Keywords": "",
      "Publication venue": "CEUR Workshop Proceedings",
      "Publication date": "2010-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Rosenfeld, Martin;Fernández, Alejandro;Díaz, Alicia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-78649282476",
      "Primary study DOI": "10.1109/EDT.2010.5496587",
      "Title": "Study on the detection and correction of software based on UML",
      "Abstract": "We have proposed an approach to the correction of anti-patterns; we believe that before attempting such corrections it is important to have confirmation from the developer. Assumptions are made when identifying or correcting certain anti-patterns; however, these assumptions that we recognize as the causes of the anti-pattern may be the behavior the developer actually intended with the design, or may not be the most optimal correction for the anti pattern. We propose these transformations as a guide for the improvement of the design; nevertheless the decision of applying the changes should be left to the user. ©2010 IEEE.",
      "Keywords": "Anti-patterns | Correction | Detection | UML",
      "Publication venue": "2010 International Conference on E-Health Networking, Digital Ecosystems and Technologies, EDT 2010",
      "Publication date": "2010-11-29",
      "Publication type": "Conference Paper",
      "Authors": "Xue, Qing Ji",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-78149252929",
      "Primary study DOI": "10.1145/1852786.1852797",
      "Title": "Building empirical support for automated code smell detection",
      "Abstract": "Identifying refactoring opportunities in software systems is an important activity in today's agile development environments. The concept of code smells has been proposed to characterize different types of design shortcomings in code. Additionally, metric-based detection algorithms claim to identify the \"smelly\" components automatically. This paper presents results for an empirical study performed in a commercial environment. The study investigates the way professional software developers detect god class code smells, then compares these results to automatic classification. The results show that, even though the subjects perceive detecting god classes as an easy task, the agreement for the classification is low. Misplaced methods are a strong driver for letting subjects identify god classes as such. Earlier proposed metric-based detection approaches performed well compared to the human classification. These results lead to the conclusion that an automated metric-based pre-selection decreases the effort spent on manual code inspections. Automatic detection accompanied by a manual review increases the overall confidence in the results of metric-based classifiers. © 2010 ACM.",
      "Keywords": "code inspection | code smells | empirical study | god class | maintainability",
      "Publication venue": "ESEM 2010 - Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement",
      "Publication date": "2010-11-12",
      "Publication type": "Conference Paper",
      "Authors": "Schumacher, Jan;Zazworka, Nico;Shull, Forrest;Seaman, Carolyn;Shaw, Michele",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-78149240562",
      "Primary study DOI": "",
      "Title": "ESEM 2010 - Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement",
      "Abstract": "The proceedings contain 68 papers. The topics discussed include: understanding the impact of code and process metrics on post-release defects: a case study on the eclipse project; an effective fault aware test case prioritization by incorporating a fault localization technique; a machine learning approach for text categorization of fixing-issue commits on CVS; object oriented design pattern decay: a taxonomy; building empirical support for automated code smell detection; strengthening the empirical analysis of the relationship between Linus' law and software security; test case selection and prioritization: risk-based or design-based?; an empirical investigation into a large-scale Java open source code repository; a survey of scientific software development; transition from a plan-driven process to scrum - a longitudinal case study on software quality; and are developers complying with the process: an XP study.",
      "Keywords": "",
      "Publication venue": "ESEM 2010 - Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement",
      "Publication date": "2010-11-12",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-77956404120",
      "Primary study DOI": "10.1016/j.infsof.2010.05.010",
      "Title": "Generating a catalog of unanticipated schemas in class hierarchies using Formal Concept Analysis",
      "Abstract": "Context: Inheritance is the cornerstone of object-oriented development, supporting conceptual modeling, subtype polymorphism and software reuse. But inheritance can be used in subtle ways that make complex systems hard to understand and extend, due to the presence of implicit dependencies in the inheritance hierarchy. Objective: Although these dependencies often specify well-known schemas (i.e., recurrent design or coding patterns, such as hook and template methods), new unanticipated dependency schemas arise in practice, and can consequently be hard to recognize and detect. Thus, a developer making changes or extensions to an object-oriented system needs to understand these implicit contracts defined by the dependencies between a class and its subclasses, or risk that seemingly innocuous changes break them. Method: To tackle this problem, we have developed an approach based on Formal Concept Analysis. Our Formal Concept Analysis based-Reverse Engineering methodology (FoCARE) identifies undocumented hierarchical dependencies in a hierarchy by taking into account the existing structure and behavior of classes and subclasses. Results: We validate our approach by applying it to a large and non-trivial case study, yielding a catalog of hierarchy schemas, each one composed of a set of dependencies over methods and attributes in a class hierarchy. We show how the discovered dependency schemas can be used not only to identify good design practices, but also to expose bad smells in design, thereby helping developers in initial reengineering phases to develop a first mental model of a system. Although some of the identified schemas are already documented in existing literature, with our approach based on Formal Concept Analysis (FCA), we are also able to identify previously unidentified schemas. Conclusions: FCA is an effective tool because it is an ideal classification mining tool to identify commonalities between software artifacts, and usually these commonalities reveal known and unknown characteristics of the software artifacts. We also show that once a catalog of useful schemas stabilizes after several runs of FoCARE, the added cost of FCA is no longer needed. © 2010 Elsevier B.V. All rights reserved.",
      "Keywords": "Class hierarchy schemas | Formal Concept Analysis | Object-oriented development | Source code analysis",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2010-11-01",
      "Publication type": "Conference Paper",
      "Authors": "Arévalo, Gabriela;Ducasse, Stéphane;Gordillo, Silvia;Nierstrasz, Oscar",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79551665781",
      "Primary study DOI": "10.1145/1932682.1869475",
      "Title": "Scalable and systematic detection of buggy inconsistencies in source code",
      "Abstract": "Software developers often duplicate source code to replicate functionality. This practice can hinder the maintenance of a software project: bugs may arise when two identical code segments are edited inconsistently. This paper presents DejaVu, a highly scalable system for detecting these general syntactic inconsistency bugs. DejaVu operates in two phases. Given a target code base, a parallel inconsistent clone analysis first enumerates all groups of source code fragments that are similar but not identical. Next, an extensible buggy change analysis framework refines these results, separating each group of inconsistent fragments into a fine-grained set of inconsistent changes and classifying each as benign or buggy. On a 75+ million line pre-production commercial code base, DejaVu executed in under five hours and produced a report of over 8,000 potential bugs. Our analysis of a sizable random sample suggests with high likelihood that at this report contains at least 2,000 true bugs and 1,000 code smells. These bugs draw from a diverse class of software defects and are often simple to correct: syntactic inconsistencies both indicate problems and suggest solutions. Copyright © 2010 ACM.",
      "Keywords": "",
      "Publication venue": "ACM SIGPLAN Notices",
      "Publication date": "2010-10-01",
      "Publication type": "Conference Paper",
      "Authors": "Gabel, Mark;Yang, Junfeng;Yu, Yuan;Goldszmidt, Moises;Su, Zhendong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-77955903017",
      "Primary study DOI": "10.1145/1831708.1831723",
      "Title": "Learning from 6,000 projects: Lightweight cross-project anomaly detection",
      "Abstract": "Real production code contains lots of knowledge - on the domain, on the architecture, and on the environment. How can we leverage this knowledge in new projects? Using a novel lightweight source code parser, we have mined more than 6,000 open source Linux projects (totaling 200,000,000 lines of code) to obtain 16,000,000 temporal properties reflecting normal interface usage. New projects can be checked against these rules to detect anomalies - that is, code that deviates from the wisdom of the crowds. In a sample of 20 projects, ∼25% of the top-ranked anomalies uncovered actual code smells or defects. © 2010 ACM.",
      "Keywords": "Formal concept analysis | Language independent parsing | Lightweight parsing | Mining specifications | Temporal properties",
      "Publication venue": "ISSTA'10 - Proceedings of the 2010 International Symposium on Software Testing and Analysis",
      "Publication date": "2010-08-27",
      "Publication type": "Conference Paper",
      "Authors": "Gruska, Natalie;Wasylkowski, Andrzej;Zeller, Andreas",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-77955148790",
      "Primary study DOI": "10.1109/ICCCYB.2010.5491288",
      "Title": "Detecting patterns and antipatterns in software using prolog rules",
      "Abstract": "Program comprehension is a key prerequisite for the maintainance and analysis of legacy software systems. Knowing about the presence of design patterns or antipatterns in a software system can significantly improve the program comprehension.Unfortunately, in many cases the usage of certain patterns is seldom explicitly described in the software documentation, while antipatterns are never described as such in the documentation. Since manual inspection of the code of large software systems is difficult, automatic or semi-automatic procedures for discovering patterns and antipatterns from source code can be very helpful. In this article we propose detection methods for a set of patterns and antipatterns, using a logic-based approach. We define with help of Prolog predicates both structural and behavioural aspects of patterns and antipatters. The detection results obtained for a number of test systems are also presented. © 2010 IEEE.",
      "Keywords": "Antipattern | Design pattern | Detection",
      "Publication venue": "ICCC-CONTI 2010 - IEEE International Joint Conferences on Computational Cybernetics and Technical Informatics, Proceedings",
      "Publication date": "2010-08-06",
      "Publication type": "Conference Paper",
      "Authors": "Stoianov, Alecsandar;Şora, Ioana",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-77954766482",
      "Primary study DOI": "",
      "Title": "2010 ICSE Workshop on Emerging Trends in Software Metrics, WETSoM 2010, in Conjunction with the 32nd ACM/IEEE International Conference on Software Engineering, ICSE 2010",
      "Abstract": "The proceedings contain 11 papers. The topics discussed include: problems adopting metrics from other disciplines; on the use of weighted sums in the definition of measures; does size matter? a preliminary investigation of the consequences of powerlaws in software; assessing traditional and new metrics for object-oriented systems; is a strategy for code smell assessment long overdue?; metrics-based detection of micro patterns; on the relationship between functional size and software code size; an empirical evaluation of coupling metrics on aspect-oriented programs; a metric model for aspects' coupling; a metric for composite service reusability analysis; and improving developer activity metrics with issue tracking annotations.",
      "Keywords": "",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2010-07-26",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-77954701746",
      "Primary study DOI": "10.1145/1810295.1810321",
      "Title": "Domain-specific tailoring of code smells: An empirical study",
      "Abstract": "Code smells refer to commonly occurring patterns in source code that indicate poor programming practices or code decay. Detecting code smells helps developers find design problems that can cause trouble in future maintenance. Detection rules for code smells, based on software metrics, have been proposed, but they do not take domain-specific characteristics into consideration. In this study we investigate whether such generic heuristics can be tailored to include domain-specific factors. Input into these domain-specific heuristics comes from an iterative empirical field study in a software maintenance project. The results yield valuable insight into code smell detection. © 2010 ACM.",
      "Keywords": "code smells | domain-specific | empirical study",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2010-07-23",
      "Publication type": "Conference Paper",
      "Authors": "Guo, Yuepu;Seaman, Carolyn;Zazworka, Nico;Shull, Forrest",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-77954569124",
      "Primary study DOI": "10.1145/1808877.1808880",
      "Title": "Digging into UML models to remove performance antipatterns",
      "Abstract": "Performance antipatterns have been informally defined and characterized as bad practices in software design that can originate performance problems. Such special type of patterns can involve static and dynamic aspects of software as well as deployment features. It has been shown that quite often the inability to meet performance requirements is due to the presence of antipatterns in the software design. However the problem of formally defining antipatterns and automatically detect them within a design model has not been investigated yet. In this paper we examine this problem within the UML context and show how performance antipatterns can be defined and detected in UML models by mean of OCL. A case study in UML annotated with the MARTE profile is presented where, after a performance analysis that shows unsatisfactory results, performance antipatterns are detected through an OCL engine. The identification of an antipattern suggests the architectural alternatives that can remove that specific problem. We show in our example that the removal of a certain antipattern actually allows to overcome a specific performance problem. © 2010 ACM.",
      "Keywords": "antipatterns | object constraint language | software performance engineering | unified modeling language",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2010-07-20",
      "Publication type": "Conference Paper",
      "Authors": "Cortellessa, Vittorio;Di Marco, Antinisca;Eramo, Romina;Pierantonio, Alfonso;Trubiani, Catia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-77952818058",
      "Primary study DOI": "10.1109/AERO.2010.5446811",
      "Title": "An ontological identification of relationships between anti-patterns and code smells",
      "Abstract": "Ensuring quality in software development is a challenging process. The concepts of anti-patterns and code smells utilize the knowledge of known problems to improve the quality of current and future software development. However, the knowledge and understanding of these indicators of low quality software is still insufficient to resolve many of the problems they represent. The identification and definition of anti-patterns is a heuristic process. Additionally, minimal research exists addressing the relationships between or among code smells and anti-patterns. Software quality issues such as understandability and maintainability can be improved by identifying and resolving anti-patterns associated with code smells as well as preventing code smells before coding begins. We present an ontological representation of the relationships between anti-patterns and code smells to enhance the understanding of these concepts with the goal of improving software quality. ©2010 IEEE.",
      "Keywords": "",
      "Publication venue": "IEEE Aerospace Conference Proceedings",
      "Publication date": "2010-06-02",
      "Publication type": "Conference Paper",
      "Authors": "Luo, Yixin;Hoss, Allyson;Carver, Doris L.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-77954089392",
      "Primary study DOI": "10.1007/s00165-009-0115-x",
      "Title": "From a domain analysis to the specification and detection of code and design smells",
      "Abstract": "Code and design smells are recurring design problems in software systems that must be identified to avoid their possible negative consequences on development and maintenance. Consequently, several smell detection approaches and tools have been proposed in the literature. However, so far, they allow the detection of predefined smells but the detection of new smells or smells adapted to the context of the analysed systems is possible only by implementing new detection algorithms manually. Moreover, previous approaches do not explain the transition from specifications of smells to their detection. Finally, the validation of the existing approaches and tools has been limited on few proprietary systems and on a reduced number of smells. In this paper, we introduce an approach to automate the generation of detection algorithms from specifications written using a domain-specific language. This language is defined from a thorough domain analysis. It allows the specification of smells using high-level domain-related abstractions. It allows the adaptation of the specifications of smells to the context of the analysed systems.We specify 10 smells, generate automatically their detection algorithms using templates, and validate the algorithms in terms of precision and recall on Xerces v2.7.0 and GanttProject v1.10.2, two open-source object-oriented systems.We also compare the detection results with those of a previous approach, iPlasma. BCS © 2010.",
      "Keywords": "Algorithm generation | Antipatterns | Code smells | Design smells | Detection | Domain-specific language | Java",
      "Publication venue": "Formal Aspects of Computing",
      "Publication date": "2010-05-01",
      "Publication type": "Conference Paper",
      "Authors": "Moha, Naouel;Guéhéneuc, Yann Gaël;Le Meur, Anne Françoise;Duchien, Laurence;Tiberghien, Alban",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-77951468934",
      "Primary study DOI": "10.1145/1730874.1730893",
      "Title": "A desiderata for refactoring-based software modularity improvement",
      "Abstract": "There exists number of large business critical software systems of recent vintage that are becoming increasingly difficult to maintain. These systems, written in newer languages such as C and Java are fast becoming legacy and showing the same symptoms of modularity deterioration reminiscent of older legacy systems written in Cobol and PL1. However, this problem has not received much attention from the software modularization community. In this paper, we argue that the modularization needs of these relatively newer systems, which generally have some modular structure, is significantly different from the needs of older, mainly monolithic, legacy systems. We emphasize the need for incrementally improving modularity and propose a software refactoring based approach to solve this problem, thereby, uniting hitherto two disparate threads of research - software modularization and software refactoring. As part of this refactoring based modularity improvement approach, we identify a set of modularity smells and specify how to detect these smells in poorly modularized software systems. A validation of these modularity smells is carried out using several open source systems. Copyright 2010 ACM.",
      "Keywords": "Code smells | Preventive maintenance | Software maintenance | Software modularity | Software refactoring",
      "Publication venue": "ISEC'10 - Proceedings of the 2010 India Software Engineering Conference",
      "Publication date": "2010-04-30",
      "Publication type": "Conference Paper",
      "Authors": "Rama, Girish Maskeri",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-77749264927",
      "Primary study DOI": "10.1007/978-3-642-11503-5_10",
      "Title": "Similar code detection and elimination for Erlang programs",
      "Abstract": "A well-known bad code smell in refactoring and software maintenance is duplicated code, that is the existence of code clones, which are code fragments that are identical or similar to one another. Unjustified code clones increase code size, make maintenance and comprehension more difficult, and also indicate design problems such as a lack of encapsulation or abstraction. This paper describes an approach to detecting 'similar' code based on the notion of anti-unification, or least-general common abstraction. This mechanism is used for detecting code clones in Erlang programs, and is supplemented by a collection of refactorings to support user-controlled automatic clone removal. The similar code detection algorithm and refactorings are integrated within Wrangler, a tool developed at the University of Kent for interactive refactoring of Erlang programs. We conclude with a report on case studies and comparisons with other tools. © 2010 Springer-Verlag.",
      "Keywords": "Anti-unification | Code clone detection | Erlang | Program analysis | Program transformation | Refactoring | Similar code | Wrangler",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2010-03-15",
      "Publication type": "Conference Paper",
      "Authors": "Li, Huiqing;Thompson, Simon",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-76749149064",
      "Primary study DOI": "10.1109/TSE.2009.50",
      "Title": "DECOR: A method for the specification and detection of code and design smells",
      "Abstract": "Code and design smells are poor solutions to recurring implementation and design problems. They may hinder the evolution of a system by making it hard for software engineers to carry out changes. We propose three contributions to the research field related to code and design smells: 1) Decor, a method that embodies and defines all the steps necessary for the specification and detection of code and design smells, 2) Detex, a detection technique that instantiates this method, and 3) an empirical validation in terms of precision and recall of Detex. The originality of Detex stems from the ability for software engineers to specify smells at a high level of abstraction using a consistent vocabulary and domain-specific language for automatically generating detection algorithms. Using Detex, we specify four well-known design smells: the antipatterns Blob, Functional Decomposition, Spaghetti Code, and Swiss Army Knife, and their 15 underlying code smells, and we automatically generate their detection algorithms. We apply and validate the detection algorithms in terms of precision and recall on Xerces v2.7.0, and discuss the precision of these algorithms on 11 open-source systems. © 2010 IEEE.",
      "Keywords": "Antipatterns | Code smells | Design smells | Detection | Java. | Metamodeling | Specification",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2010-02-22",
      "Publication type": "Article",
      "Authors": "Moha, Naouel;Guéhéneuc, Yann Gaël;Duchien, Laurence;Le Meur, Anne Françoise",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-72449192177",
      "Primary study DOI": "10.1109/ESEM.2009.5314231",
      "Title": "The evolution and impact of code smells: A case study of two open source systems",
      "Abstract": "Code smells are design flaws in object-oriented designs that may lead to maintainability issues in the further evolution of the software system. This study focuses on the evolution of code smells within a system and their impact on the change behavior (change frequency and size). The study investigates two code smells, God Class and Shotgun Surgery, by analyzing the historical data over several years of development of two large scale open source systems. The detection of code smells in the evolution of those systems was performed by the application of an automated approach using detection strategies. The results show that we can identify different phases in the evolution of code smells during the system development and that code smell infected components exhibit a different change behavior. This information is useful for the identification of risk areas within a software system that need refactoring to assure a future positive evolution. © 2009 IEEE.",
      "Keywords": "",
      "Publication venue": "2009 3rd International Symposium on Empirical Software Engineering and Measurement, ESEM 2009",
      "Publication date": "2009-12-28",
      "Publication type": "Conference Paper",
      "Authors": "Olbrich, Steffen;Cruzes, Daniela S.;Basili, Victor;Zazworka, Nico",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-72249120667",
      "Primary study DOI": "10.1145/1639950.1639959",
      "Title": "Refactoring UML models: Using OpenArchitectureWare to measure UML model quality and perform pattern matching on UML models with OCL queries",
      "Abstract": "In object oriented software development, the Unified Modeling Language (UML) [20] has become the de-facto modeling standard. UML plays an important role for software factories, in which a high quality abstract UML model is the primary source of input used to generate a working system. While there are many tools that enable assisted refactoring of source code, there are few tools that enable assisted refactoring of UML models. In order to determine UML model quality for UML models used in code generation projects, a selection of quality metrics has been made. While there are a large number of metrics available to determine code quality, there are only a limited number of metrics applicable to UML models. Most model quality metrics have been derived from code quality metrics [16]. Syntactic and semantic model check rules have been implemented, that allow detection of undesirablemodel properties. The syntactic model checkers have been derived directly from the UML specification. The semantic model checkers have been derived from a range of anti-pattern descriptions. We have delivered a prototype that detects undesirable model features in order to test the model improvement capabilities. The prototype contains selected model quality metrics, syntactic and semantic model check rules. Both metrics and rules have been formulated in the Object Constraint Language (OCL) [21], which operates on UML models. The system is built using Open Source tools, allowing easy extensions of the prototype. The effects of suggested repair actions on the model are measurable through the selected model quality metrics and by subjective comparison. The prototype was able to improve model quality for four industry models both by metrics and subjective comparison. Copyright © 2009 ACM.",
      "Keywords": "Metrics | Model quality | OCL | Semantic rules | Syntactic rules | UML",
      "Publication venue": "Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",
      "Publication date": "2009-12-24",
      "Publication type": "Conference Paper",
      "Authors": "Enckevort, Twan Van",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-70849122243",
      "Primary study DOI": "10.1109/ICSM.2009.5306278",
      "Title": "Maintenance and agile development: Challenges, opportunities and future directions",
      "Abstract": "Software entropy is a phenomenon where repeated changes gradually degrade the structure of the system, making it hard to understand and maintain. This phenomenon imposes challenges for organizations that have moved to agile methods from other processes, despite agile's focus on adaptability and responsiveness to change. We have investigated this issue through an industrial case study, and reviewed the literature on addressing software entropy, focussing on the detection of \"code smells\" and their treatment by refactoring. We found that in order to remain agile despite of software entropy, developers need better support for understanding, planning and testing the impact of changes. However, it is exactly work on refactoring decision support and task complexity analysis that is lacking in literature. Based on our findings, we discuss strategies for dealing with entropy in this context and present avenues for future research.",
      "Keywords": "",
      "Publication venue": "IEEE International Conference on Software Maintenance, ICSM",
      "Publication date": "2009-12-04",
      "Publication type": "Conference Paper",
      "Authors": "Hanssen, Geir K.;Yamashita, Aiko Fallas;Conradi, Reidar;Moonen, Leon",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-70749121233",
      "Primary study DOI": "10.1109/ICSEA.2009.15",
      "Title": "UML specification and correction of object-oriented anti-patterns",
      "Abstract": "Nowadays, the detection and correction of software defects has become a very hard task for software engineers. Most importantly, the lack of standard specifications of these software defects along with the lack of tools for their detection, correction and verification forces developers to perform manual modifications; resulting not only in mistakes, but also in costs of time and resources. The work presented here is a study of the specification and correction of a particular type of software defect: Object-Oriented Anti-patterns. More specifically, we define a UML-based specification of anti-patterns and establish design transformations for their correction. Through this work, we expect to open up the possibility to automate the detection and correction of these kinds of software defects. © 2009 IEEE.",
      "Keywords": "Object-oriented anti-patterns | Refactoring | UML",
      "Publication venue": "4th International Conference on Software Engineering Advances, ICSEA 2009, Includes SEDES 2009: Simposio para Estudantes de Doutoramento em Engenharia de Software",
      "Publication date": "2009-12-03",
      "Publication type": "Conference Paper",
      "Authors": "Llano, Maria Teresa;Pooley, Rob",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-70749113449",
      "Primary study DOI": "10.1109/ICSEA.2009.53",
      "Title": "Effective recognition of patterns in object-oriented designs",
      "Abstract": "An approach to detecting object oriented design patterns in existing UML models, based on a combination of existing techniques, is described. The current work considered is reviewed briefly. These approaches all look for similarities between the models or programs for the design and the model which define the pattern. From this review, two current methods are selected that have a real ability to detect design patterns at the design level. It is shown that these can work together to improve detection rates. Finally, there are ideas on how this work can be extended to detect more patterns and anti-patterns. © 2009 IEEE.",
      "Keywords": "Design patterns | Pattern detection",
      "Publication venue": "4th International Conference on Software Engineering Advances, ICSEA 2009, Includes SEDES 2009: Simposio para Estudantes de Doutoramento em Engenharia de Software",
      "Publication date": "2009-12-03",
      "Publication type": "Conference Paper",
      "Authors": "Nguyen, Trung;Pooley, Rob",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-73449136137",
      "Primary study DOI": "10.1109/WCRE.2009.28",
      "Title": "An exploratory study of the impact of code smells on software change-proneness",
      "Abstract": "Code smells are poor implementation choices, thought to make object-oriented systems hard to maintain. In this study, we investigate if classes with code smells are more change-prone than classes without smells. Specifically, we test the general hypothesis: classes with code smells are not more change prone than other classes. We detect 29 code smells in 9 releases of Azureus and in 13 releases of Eclipse, and study the relation between classes with these code smells and class change-proneness. We show that, in almost all releases of Azureus and Eclipse, classes with code smells are more change-prone than others, and that specific smells are more correlated than others to change-proneness. These results justify a posteriori previous work on the specification and detection of code smells and could help focusing quality assurance and testing activities. © 2009 IEEE.",
      "Keywords": "Code smells | Empirical software engineering | Mining software repositories",
      "Publication venue": "Proceedings - Working Conference on Reverse Engineering, WCRE",
      "Publication date": "2009-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Khomh, Foutse;Di Penta, Massimiliano;Guéhéneuc, Yann Gaël",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-74549148077",
      "Primary study DOI": "10.1109/SEAA.2009.85",
      "Title": "Approaching the model-driven generation of feedback to remove software performance flaws",
      "Abstract": "The problem of interpreting results of performance analysis and providing feedback on software models to overcome performance flaws is probably the most critical open issue in the field of software performance engineering. Automation in this step would help to introduce performance validation as an integrated activity in the software lifecycle, without dramatically affecting the daily practices of software developers. In this paper we approach the problem with model-driven techniques, on which we build a general solution. Basing on the concept of performance antipatterns, that are bad practices in software modeling leading to performance flaws, we introduce metamodels and transformations that can support the whole process of flaw detection and solution. The approach that we propose is notation-independent and can be embedded in any (existing or future) concrete modeling notation by using weaving models and automatically generated model transformations. Finally, we discuss the issues opened from this work and the future achievements that are at the hand in this domain thanks to model-driven techniques. © 2009 IEEE.",
      "Keywords": "Antipattern | Model transformation | Model-driven engineering | Software performance",
      "Publication venue": "Conference Proceedings of the EUROMICRO",
      "Publication date": "2009-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Cortellessa, Vittorio;Di Marco, Antinisca;Eramo, Romina;Pierantonio, Alfonso;Trubiani, Catia",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85051023807",
      "Primary study DOI": "10.1109/MLDS.2017.8",
      "Title": "A Support Vector Machine Based Approach for Code Smell Detection",
      "Abstract": "Code smells may be introduced in software due to market rivalry, work pressure deadline, improper functioning, skills or inexperience of software developers. Code smells indicate problems in design or code which makes software hard to change and maintain. Detecting code smells could reduce the effort of developers, resources and cost of the software. Many researchers have proposed different techniques like DETEX for detecting code smells which have limited precision and recall. To overcome these limitations, a new technique named as SVMCSD has been proposed for the detection of code smells, based on support vector machine learning technique. Four code smells are specified namely God Class, Feature Envy, Data Class and Long Method and the proposed technique is validated on two open source systems namely ArgoUML and Xerces. The accuracy of SVMCSD is found to be better than DETEX in terms of two metrics, precision and recall, when applied on a subset of a system. While considering the entire system, SVMCSD detect more occurrences of code smells than DETEX.",
      "Keywords": "Anti-patterns | Code smells | Software maintenance | Support Vector Machine",
      "Publication venue": "Proceedings - 2017 International Conference on Machine Learning and Data Science, MLDS 2017",
      "Publication date": "2017-07-02",
      "Publication type": "Conference Paper",
      "Authors": "Kaur, Amandeep;Jain, Sushma;Goel, Shivani",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-76949099174",
      "Primary study DOI": "",
      "Title": "Smell detection in UML designs which utilize pattern languages",
      "Abstract": "Smell detection is the idea of improving the quality of software by finding and fixing the problems (bad smells) in the source code. The same idea is applicable at the design level. Early detection of the problems in UML design models helps designers produce high quality software. In this paper, we present a process called Sign/Criteria/Repair (SCR) for detecting and fixing the smells in the application of a pattern language in a UML design. We investigate how the SCR process can be implemented in three different environments, ArgoUML, Epsilon, and OCLE, and how these tools can help the designer improve a UML model. © 2009 ACECR.",
      "Keywords": "MDD | Pattern language | Quality assessment | Smell detection",
      "Publication venue": "Iranian Journal of Electrical and Computer Engineering",
      "Publication date": "2009-12-01",
      "Publication type": "Article",
      "Authors": "Zamani, B.;Butler, G.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-77949373144",
      "Primary study DOI": "10.1145/1595696.1595767",
      "Title": "Graph-based mining of multiple object usage patterns",
      "Abstract": "The interplay of multiple objects in object-oriented programming often follows specific protocols, for example certain orders of method calls and/or control structure constraints among them that are parts of the intended object usages. Unfortunately, the information is not always documented. That creates long learning curve, and importantly, leads to subtle problems due to the misuse of objects. In this paper, we propose GrouMiner, a novel graph-based approach for mining the usage patterns of one or multiple objects. GrouMiner approach includes a graph-based representation for multiple object usages, a pattern mining algorithm, and an anomaly detection technique that are efficient, accurate, and resilient to software changes. Our experiments on several real-world programs show that our prototype is able to find useful usage patterns with multiple objects and control structures, and to translate them into user-friendly code skeletons to assist developers in programming. It could also detect the usage anomalies that caused yet undiscovered defects and code smells in those programs. Copyright 2009 ACM.",
      "Keywords": "Anomaly | API usage | Clone | Graph mining | Groum | Object usage | Pattern",
      "Publication venue": "ESEC-FSE'09 - Proceedings of the Joint 12th European Software Engineering Conference and 17th ACM SIGSOFT Symposium on the Foundations of Software Engineering",
      "Publication date": "2009-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Nguyen, Tung Thanh;Nguyen, Hoan Anh;Pham, Nam H.;Al-Kofahi, Jafar M.;Nguyen, Tien N.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-70450228805",
      "Primary study DOI": "10.1145/1595808.1595827",
      "Title": "Perspectives on automated correction of bad smells",
      "Abstract": "Keeping a software system conformant with a desired architecture and consistent with good design principles is a recurring task during the software evolution process. Deviations from good design principles can manifest in the form of bad smells: problems in the system's structure that can negatively affect software quality factors. Many authors have worked in identifying bad smells and in removing them with refactorings: tools have been built to suggest refactorings; successful approaches to detect bad smells have been developed, etc. We present a comprehensive and historical review on this subject, in order to model the current state of the art and to identify the open challenges, current trends and research opportunities. We also propose a technique based on automated planning, aimed at taking one step forward in the automatic improvement of a system's structure. This proposal will allow computing complex refactoring sequences which can be directed to the achievement of a certain objective, such as the correction of bad smells. Copyright 2009 ACM.",
      "Keywords": "Automated planning | Bad smells | Refactoring",
      "Publication venue": "International Workshop on Principles of Software Evolution (IWPSE)",
      "Publication date": "2009-11-30",
      "Publication type": "Conference Paper",
      "Authors": "Pérez, Javier;Crespo, Yania",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-71049173260",
      "Primary study DOI": "",
      "Title": "Architectures for Adaptive Software Systems - 5th International Conference on the Quality of Software Architectures, QoSA 2009, Proceedings",
      "Abstract": "The proceedings contain 13 papers. The topics discussed include: a model-based framework to design and debug safe component-based autonomic systems; automated architecture consistency checking for model driven software development; improved feedback for architectural performance prediction using software cartography visualizations; predicting performance properties for open systems with KAMI; compositional prediction of timed behavior for process control architecture; timed simulation of extended AADL-based architecture specifications with timed abstract state machines; achieving agility through architecture visibility; toward a catalogue of architectural bad smells; on the consolidation of data-centers with performance constraints; and adaptive application composition in quantum chemistry.",
      "Keywords": "",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2009-11-16",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-67349108267",
      "Primary study DOI": "10.1016/j.eswa.2009.04.024",
      "Title": "JEETuningExpert: A software assistant for improving Java Enterprise Edition application performance",
      "Abstract": "Designing a JEE (Java Enterprise Edition)-based enterprise application capable of achieving its performance objectives is rather hard. Predicting the performance of this type of systems at the design level is difficult and sometimes not viable, because this requires having precise knowledge of the expected load conditions and the underlying software infrastructure. Besides, the requirement for rapid time-to-market leads to postpone performance tuning until systems are developed, packaged and running. In this paper we present a novel approach for automatically detecting performance problems in JEE-based applications and, in turn, suggesting courses of actions to correct them. The idea is to allow developers to smoothly identify and eradicate performance anti-patterns by automatically analyzing execution traces. The approach has been implemented as a tool called JEETuningExpert, and validated using three well-known JEE reference applications. Specifically, we evaluated the effectiveness of JEETuningExpert for detecting performance problems, measured the overhead imposed by online monitoring each application and the improvements were achieved after following the suggested corrective actions. These results empirically showed that the refactored applications are 40.08%, 76.94% and 61.13% faster, on average. © 2009 Elsevier Ltd. All rights reserved.",
      "Keywords": "Expert systems | Intelligent systems | Java enterprise applications | Performance anti-patterns | Performance tuning",
      "Publication venue": "Expert Systems with Applications",
      "Publication date": "2009-11-01",
      "Publication type": "Article",
      "Authors": "Crasso, Marco;Zunino, Alejandro;Moreno, Leonardo;Campo, Marcelo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-74049104007",
      "Primary study DOI": "10.1017/S0269888909990075",
      "Title": "Detecting similarities in antipattern ontologies using semantic social networks: Implications for software project management",
      "Abstract": "Ontology has been recently proposed as an appropriate formalism to model software project management antipatterns, in order to encode antipatterns in a computer understandable form and introduce antipatterns to the Semantic Web. However, given two antipattern ontologies, the same entity can be described using different terminology. Therefore, the detection of similar antipattern ontologies is a difficult task. In this paper, we introduce a three-layered antipattern semantic social network, which involves the social network, the antipattern ontology network and the concept network. Social Network Analysis (SNA) techniques can be used to assist software project managers in finding similar antipattern ontologies. For this purpose, SNA measures are extracted from one layer of the semantic social network to another and this knowledge is used to infer new links between antipattern ontologies. The level of uncertainty associated with each new link is represented using Bayesian Networks (BNs). Furthermore, BNs address the issue of quantifying the uncertainty of the data collected regarding antipattern ontologies for the purposes of the conducted analysis. Finally, BNs are used to augment SNA by taking into account meta-information in their calculations. Hence, other knowledge not included in the social network can be used in order to search the social network for further inference. The benefits of using an antipattern semantic social network are illustrated using an example community of software project management antipattern ontologies. © 2009 Copyright Cambridge University Press.",
      "Keywords": "",
      "Publication venue": "Knowledge Engineering Review",
      "Publication date": "2009-09-01",
      "Publication type": "Article",
      "Authors": "Settas, Dimitrios L.;Sowe, Sulayman K.;Stamelos, Ioannis G.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-64549091962",
      "Primary study DOI": "10.1145/1480945.1480971",
      "Title": "Clone detection and removal for erlang/OTP within a refactoring environment",
      "Abstract": "A well-known bad code smell in refactoring and software maintenance is duplicated code, or code clones. A code clone is a code fragment that is identical or similar to another. Unjustified code clones increase code size, make maintenance and comprehension more difficult, and also indicate design problems such as lack of encapsulation or abstraction. This paper proposes a token and AST based hybrid approach to automatically detecting code clones in Erlang/OTP programs, underlying a collection of refactorings to support user-controlled automatic clone removal, and examines their application in substantial case studies. Both the clone detector and the refactorings are integrated within Wrangler, the refactoring tool developed at Kent for Erlang/OTP. ©2009 ACM.",
      "Keywords": "Duplicated code | Erlang | Program analysis | Program transformation | Refactoring | Wrangler",
      "Publication venue": "Proceedings of the 2009 ACM SIGPLAN Symposium on Partial Evaluation and Program Manipulation, PEPM'09",
      "Publication date": "2009-07-23",
      "Publication type": "Conference Paper",
      "Authors": "Li, Huiqing;Thompson, Simon",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-67650098689",
      "Primary study DOI": "10.1109/ICST.2009.34",
      "Title": "A flexible framework for quality assurance of software artefacts with applications to Java, UML, and TTCN-3 test specifications",
      "Abstract": "Manual reviews and inspections of software artefacts are time consuming and thus, automated analysis tools have been developed to support the quality assurance of software artefacts. Usually, software analysis tools are implemented for analysing only one specific language as target and for performing only one class of analyses. Furthermore, most software analysis tools support only common programming languages, but not those domain-specific languages that are used in a test process. As a solution, a framework for software analysis is presented that is based on a flexible, yet high-level facade layer that mediates between analysis rules and the underlying target software artefact; the analysis rules are specified using high-level XQuery expressions. Hence, further rules can be quickly added and new types of software artefacts can be analysed without needing to adapt the existing analysis rules. The applicability of this approach is demonstrated by examples from using this framework to calculate metrics and detect bad smells in Java source code, in UML models, and in test specifications written using the Testing and Test Control Notation (TTCN-3).",
      "Keywords": "",
      "Publication venue": "Proceedings - 2nd International Conference on Software Testing, Verification, and Validation, ICST 2009",
      "Publication date": "2009-07-15",
      "Publication type": "Conference Paper",
      "Authors": "Nödler, Jens;Neukirchen, Helmut;Grabowski, Jens",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-68949107797",
      "Primary study DOI": "10.1142/S0218194009004234",
      "Title": "Maintainability metrics for aspect-oriented software",
      "Abstract": "Maintainability is an important factor that developers should be concerned because two-thirds of software costs involve maintenance. Aspect-oriented programming (AOP) paradigm is aimed to increase the software maintainability. It solves code tangling and code scattering problems by introducing a new modular unit, called \"aspect\". Various research works are provided to support measuring the object-oriented software, but only few studies are set up to support measuring the aspect-oriented software. This paper proposes aspect-oriented software maintainability metrics and a set of aspect-oriented design guidelines to support the metrics. By combining the proposed guidelines, object-oriented design principles, and aspect-oriented design principles, the metrics are constructed according to the Factor-Strategy (FS) quality model and the Factor-Criteria-Metric (FCM) quality model. Principle violation check definitions in the form of Boolean expressions are also defined to conduct software measurement and to fulfill the metrics. Finally, the aspect-oriented software maintainability metrics are applied to detect design principle violations in fifty AspectJ systems. The results show that for all systems their hidden flaws are exposed. Moreover, the proposed metrics are used to compare the maintainability between two versions of systems written in Java and AspectJ. © 2009 World Scientific Publishing Company.",
      "Keywords": "Aspect-oriented programming | Bad smell | Design heuristics | Software maintainability | Software metrics",
      "Publication venue": "International Journal of Software Engineering and Knowledge Engineering",
      "Publication date": "2009-05-01",
      "Publication type": "Article",
      "Authors": "Thongmak, Mathupayas;Muenchaisri, Pornsiri",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-62849102661",
      "Primary study DOI": "10.1016/j.entcs.2009.02.066",
      "Title": "A Framework for Reverse Engineering Large C++ Code Bases",
      "Abstract": "When assessing the quality and maintainability of large C++ code bases, tools are needed for extracting several facts from the source code, such as: architecture, structure, code smells, and quality metrics. Moreover, these facts should be presented in such ways so that one can correlate them and find outliers and anomalies. We present SolidFX, an integrated reverse-engineering environment (IRE) for C and C++. SolidFX was specifically designed to support code parsing, fact extraction, metric computation, and interactive visual analysis of the results in much the same way IDEs and design tools offer for the forward engineering pipeline. In the design of SolidFX, we adapted and extended several existing code analysis and data visualization techniques to render them scalable for handling code bases of millions of lines. In this paper, we detail several design decisions taken to construct SolidFX. We also illustrate the application of our tool and our lessons learnt in using it in several types of analyses of real-world industrial code bases, including maintainability and modularity assessments, detection of coding patterns, and complexity analyses. © 2009 Elsevier B.V. All rights reserved.",
      "Keywords": "C++ | parsing | reverse engineering | software visualization",
      "Publication venue": "Electronic Notes in Theoretical Computer Science",
      "Publication date": "2009-03-27",
      "Publication type": "Article",
      "Authors": "Telea, Alexandru;Byelas, Heorhiy;Voinea, Lucian",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-70449633067",
      "Primary study DOI": "10.1145/1454247.1454262",
      "Title": "Project-specific deletion patterns - Short position paper",
      "Abstract": "We apply data mining to version control data in order to detect project-specific deletion patterns - subcomponents or features of the software that were deleted on purpose. We believe that locations that are similar to earlier deletions are likely to be code smells. Future recommendation tools can warn against such smells: \"People who used gets() in the past now use fgets(). Consider a change, too\". Copyright 2008 ACM.",
      "Keywords": "",
      "Publication venue": "Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering",
      "Publication date": "2008-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Mileva, Yana Momchilova;Zeller, Andreas",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-62749180884",
      "Primary study DOI": "",
      "Title": "A quantitative method to detect design defects and to ascertain the elimination of design defects after refactoring",
      "Abstract": "One of the ways to make object oriented software systems maintainable is refactoring. Effective refactoring requires a proper method to detect design defects. Recently, some quantitative design defects detection methods which are based on metrics have been developed. However, there is a scope for a design defects detection method which considers design change propagation probabilities between artifacts that are connected through intermediate artifacts. A quantitative method is proposed in this paper considering the above aspect. The main advantage of the proposed method is, it can be used not only for design defects detection, but also to ascertain quantitatively the elimination of design defects after refactoring. Making use of the proposed method, in example designs, two different design defects are detected and the elimination of these defects after refactoring from the design is known quantitatively. The frame work in which this method is used is given.",
      "Keywords": "Bad smells | Design change propagation probability matrix | Design defects | Divergent change | Refactoring | Shotgun surgery",
      "Publication venue": "Proceedings of the 2008 International Conference on Software Engineering Research and Practice, SERP 2008",
      "Publication date": "2008-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Reddy, K. Narendar;Rao, A. Ananda;Chand, M. Gopi;Kumar J., Kiran",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-63149126059",
      "Primary study DOI": "10.1145/1409720.1409735",
      "Title": "Cluster analysis of Java dependency graphs",
      "Abstract": "We present a novel approach to the analysis of dependency graphs of object-oriented programs. We propose to use the Girvan-Newman clustering algorithm to compute the modular structure of programs. This is useful in assisting software engineers to redraw component boundaries in software, in order to improve the level of reuse and maintainability. The results of this analysis can be used as a starting point for refactoring the software. We present BARRIO, an Eclipse plugin that can detect and visualise clusters in dependency graphs extracted from Java programs by means of source code and byte code analysis. These clusters are then compared with the modular structure of the analysed programs defined by package and container specifications. Two metrics are introduced to measure the degree of overlap between the defined and the computed modular structure. Some empirical results obtained from analysing non-trivial software packages are presented. © 2008 ACM.",
      "Keywords": "Anti-pattern detection | Cluster analysis | Dependency analysis | Refactoring",
      "Publication venue": "SOFTVIS 2008 - Proceedings of the 4th ACM Symposium on Software Visualization",
      "Publication date": "2008-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Dietrich, Jens;Yakovlev, Vyacheslav;McCartiny, Catherine;Jenson, Graham;Duchrow, Manfred",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79959444848",
      "Primary study DOI": "",
      "Title": "Finding synchronization defects in Java programs: Extended static analyses and code patterns",
      "Abstract": "Concurrent programming is getting more and more important. Managing concurrency requires the usage of synchronization mechanisms, which is error-prone. Well-known examples for synchronization defects are deadlocks and race conditions. Detecting such errors is known to be difficult. There are several approaches to identify potential errors, but they either produce a high number of false positives or suffer from high computational overhead, catching only a small number of defects. Our approach uses static analysis techniques combined with points-to and may-happen-in-parallel (MHP) information to reduce the number of false positives. Additionally, we present code patterns indicating possible synchronization problems. We have implemented our approach using the Java framework Soot. Our tool was tested with small code examples, an open source web server, and commercial software. First results show that the number of false positives is reduced significantly. Copyright 2008 ACM.",
      "Keywords": "Anti-pattern detection | Java | Lockset analysis | Synchronization defects",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2008-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Otto, Frank;Moschny, Thomas",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-52349088806",
      "Primary study DOI": "10.3724/SP.J.1001.2008.02167",
      "Title": "Anti-pattern based performance optimization for middleware applications",
      "Abstract": "This paper presents an approach to optimizing performance of middleware applications based on anti-pattern. This approach has three major features: First, a meta-model is offered to build more understandable and formalized representation of anti-patterns; second, the detection of anti-patterns is based on both the static and the dynamic information, which is retrieved at runtime; third, refactorings operate without interrupt the running systems, and is completed in an automated way with the help of the middleware. A prototype based on J2EE has been developed and an e-bookstore is used as a running example to illustrate the ideas introduced in this approach.",
      "Keywords": "Anti-pattern | Detection | Middleware | Performance optimization | Refacotring",
      "Publication venue": "Ruan Jian Xue Bao/Journal of Software",
      "Publication date": "2008-09-01",
      "Publication type": "Article",
      "Authors": "Lan, Ling;Huang, Gang;Wang, Wei Hu;Mei, Hong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-47249115803",
      "Primary study DOI": "10.1007/978-3-540-78743-3_20",
      "Title": "A domain analysis to specify design defects and generate detection algorithms",
      "Abstract": "Quality experts often need to identify in software systems design defects, which are recurring design problems, that hinder development and maintenance. Consequently, several defect detection approaches and tools have been proposed in the literature. However, we are not aware of any approach that defines and reifies the process of generating detection algorithms from the existing textual descriptions of defects. In this paper, we introduce an approach to automate the generation of detection algorithms from specifications written using a domain-specific language. The domain-specific is defined from a thorough domain analysis. We specify several design defects, generate automatically detection algorithms using templates, and validate the generated detection algorithms in terms of precision and recall on Xerces v2.7.0, an open-source object-oriented system. © 2008 Springer-Verlag Berlin Heidelberg.",
      "Keywords": "Algorithm generation | Antipatterns | Code smells | Design defects | Detection | Domain-specific language | Java",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2008-07-21",
      "Publication type": "Conference Paper",
      "Authors": "Moha, Naouel;Guéhéneuc, Yann Gaël;Le Meur, Anne Françoise;Duchien, Laurence",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-80052962421",
      "Primary study DOI": "10.14279/tuj.eceasst.8.116.115",
      "Title": "Supporting reengineering scenarios with FETCH: An experience report",
      "Abstract": "The exploration and analysis of large software systems is a labor-intensive activity in need of tool support. In recent years, a number of tools have been developed that provide key functionality for standard reverse engineering scenarios, such as (i) metric analysis; (ii) anti-pattern detection; (iii) dependency analysis; and (iv) visualization. However, either these tools support merely a subset of this list of scenarios, they are not made available to the research community for comparison or extension, or they impose strict restrictions on the source code. Accordingly, we observe a need for an extensible and robust open source alternative, which we present in this paper. Our main contributions are (i) a clarification of useful reverse engineering scenarios; (ii) a comparison among existing solutions; and (iii) an experience report on four recent cases illustrating the usefulness of tool support for these scenarios in an industrial setting.",
      "Keywords": "Industrial experience | Quality assurance | Static analysis",
      "Publication venue": "Electronic Communications of the EASST",
      "Publication date": "2008-01-01",
      "Publication type": "Article",
      "Authors": "Du Bois, Bart;Van Rompaey, Bart;Meijfroidt, Karel;Suijs, Eric",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-77954012613",
      "Primary study DOI": "10.1145/1321631.1321727",
      "Title": "DECOR: A tool for the detection of design defects",
      "Abstract": "Software engineers often need to identify design defects, recurring design problems that hinder the development process, to improve and assess the quality of their systems. However, this is difficult because of the lack of specifications and tools. We propose Decor, a method to specify design defects systematically and to generate automatically detection algorithms. With this method, software engineers analyse and specify design defects at a high-level of abstraction using a unified vocabulary and dedicated language for generating detection algorithms.",
      "Keywords": "algorithm generation | antipatterns | code smells | design defects | detection | domain-specific language | java",
      "Publication venue": "ASE'07 - 2007 ACM/IEEE International Conference on Automated Software Engineering",
      "Publication date": "2007-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Moha, Naouel;Guéhéneuc, Yann Gaël",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-56149091812",
      "Primary study DOI": "",
      "Title": "Source code enhancement using reduction of duplicated code",
      "Abstract": "In this paper a novel method is proposed which is capable of detecting code smell incidents of duplicated code in a source code. The proposed method is superior to other methods since it is capable of detecting subtle duplicates of code (or code clones) which is hard to detect using those methods. The proposed method first transforms source code into a middle language format. Being in this middle format each instruction represents an atomic simple operation. Then these instructions are compared and perceived matches are aggregated to form the largest possible code clone. Finally we have compared the results of our proposed method with some other methods and have shown that the results of those methods are subsets of the result set which is developed by our new method.",
      "Keywords": "Code smell detection | Duplicated code | Refactoring",
      "Publication venue": "Proceedings of the IASTED International Conference on Software Engineering, SE 2007",
      "Publication date": "2007-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Nasehi, Seyyed Mehdi;Sotudeh, Gholam Reza;Gomrokchi, Maziar",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-37849020292",
      "Primary study DOI": "10.1145/1287624.1287632",
      "Title": "Detecting object usage anomalies",
      "Abstract": "Interacting with objects often requires following a protocol - -for instance, a specific sequence of method calls. These protocols are not always documented, and violations can lead to subtle problems. Our approach takes code examples to automatically infer legal sequences of method calls. The resulting patterns can then be used to detect anomalies such as \"Before calling next, one normally calls hasNext\". To our knowledge, this is the first fully automatic defect detection approach that learns and checks methodcall sequences. Our JADET prototype has detected yet undiscovered defects and code smells in five popular open-source programs, including two new defects in AspectJ. Copyright 2007 ACM.",
      "Keywords": "Automated defect detection | Automated specification generation | Data mining for software engineering | Object usage anomalies | Pattern recognition | Programming rules | Static analysis",
      "Publication venue": "6th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2007",
      "Publication date": "2007-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Wasylkowski, Andrzej;Zeller, Andreas;Lindig, Christian",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-47349133501",
      "Primary study DOI": "10.1109/SE.2007.13",
      "Title": "Size and frequency of class change from a refactoring perspective",
      "Abstract": "A previous study by Bieman et al., investigated whether large, object-oriented classes were more susceptible to change than smaller classes. The measure of change used in the study was the frequency with which the features of a class had been changed over a specific period of time. From a refactoring perspective, the frequency of class change is of value. But even for a relatively simple refactoring such as 'Rename Method', multiple classes may undergo minor modification without any net increase in class (and system) size. In this paper, we suggest that the combination of 'versions of a class and number of added lines of code' in the bad code 'smell' detection process may give a better impression of which classes are most suitable candidates for refactoring; as such, effort in detecting bad code smells should apply to classes with a high growth rate as well as a high change frequency. To support our investigation, data relating to changes from 161 Java classes was collected. Results concluded that it is not necessarily the case that large classes are more change-prone than relatively smaller classes. Moreover, the bad code smell detection process is informed by using the combination of change frequency and class size as a heuristic. © 2007 IEEE.",
      "Keywords": "LOC | Refactoring | Version",
      "Publication venue": "3rd International IEEE Workshop on Software Evolvability 2007, SE",
      "Publication date": "2007-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Counsell, S.;Mendes, E.",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-67650803159",
      "Primary study DOI": "10.1145/1321631.1321661",
      "Title": "Object ownership profiling: A technique for finding and fixing memory leaks",
      "Abstract": "We introduce object ownership profiling, a technique forfinding and fixing memory leaks in object-oriented programs. Object ownership profiling is the first memory profiling technique that reports both a hierarchy of allocated objects along with size and time information aggregated up that hierarchy. In addition, it reveals the cross-hierarchy interactions that areessential to pinpointing the source of the leak. We identify five memory management anti-patterns, including two that involve rich heap structure and object interactions, and are novel contributions of this work. We apply object ownership profiling to find and fix memory leaks in the Alloy IDE v3 that users had complained about for years, and that had eluded detection by code reviews and type-based commercial memory profilers. Copyright 2007 ACM.",
      "Keywords": "java | memory leaks | object ownership",
      "Publication venue": "ASE'07 - 2007 ACM/IEEE International Conference on Automated Software Engineering",
      "Publication date": "2007-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Rayside, Derek;Mendel, Lucy",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-41149138249",
      "Primary study DOI": "",
      "Title": "IWPSE'07: Ninth International Workshop on Principles of Software Evolution - In conjunction with the 6th ESEC/FSE Joint Meeting",
      "Abstract": "The proceedings contain 21 papers. The topics discussed include: a small observatory for super-repositories; improving context awareness in subversion through fine-grained versioning of Java code; a tool to visualize behavior and design evolution; improving defect prediction using temporal features and nonlinear models; learning from bug-introducing changes to prevent fault prone code; requiem for software evolution research: a few steps toward the creative age; visual identification of software evolution patterns; assessing the impact of bad smells using historical information; divide and conquer- scalability and variability for adaptive middleware; aggregating changes to efficiently check consistency; and modification analysis support at the requirements level.",
      "Keywords": "",
      "Publication venue": "International Workshop on Principles of Software Evolution (IWPSE)",
      "Publication date": "2007-12-01",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-77950544658",
      "Primary study DOI": "",
      "Title": "9th International Workshop on Principles of Software Evolution, IWPSE 2007, Held in Conjunction with the 6th ESEC/FSE Joint Meeting",
      "Abstract": "The proceedings contain 21 papers. The topics discussed include: how documentation evolves over time; improving defect prediction using temporal features and non linear models; learning from bug-introducing changes to prevent fault prone code; visual identification of software evolution patterns; assessing the impact of bad smells using historical information; BugzillaMetrics - an adaptable tool for evaluating metric specifications on change requests; aggregating changes to efficiently check consistency; modification analysis support at the requirements level; talking tests: an empirical assessment of the role of fit acceptance tests in clarifying requirements; assessing influence on processes when evolving the software architecture; an evolutionary model of requirements correctness with early aspects; a deviation-tolerant approach to software process evolution; and structural analysis and visualization of C++ code evolution using syntax trees.",
      "Keywords": "",
      "Publication venue": "9th International Workshop on Principles of Software Evolution, IWPSE 2007, Held in Conjunction with the 6th ESEC/FSE Joint Meeting",
      "Publication date": "2007-12-01",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-41149155018",
      "Primary study DOI": "10.1145/1294948.1294970",
      "Title": "Using concept analysis to detect co-change patterns",
      "Abstract": "Software systems need to change over time to cope with new requirements, and due to design decisions, the changes happen to crosscut the system's structure. Understanding how changes appear in the system can reveal hidden dependencies between different entities of the system. We propose the usage of concept analysis to identify groups of entities that change in the same way and in the same time. We apply our approach at different levels of abstraction (i.e., method, class, package) and we detect fine grained changes (i.e., statements were added in a class, but no method was added there). Concept analysis is a technique that identifies entities that have the same properties, but it requires manual inspection due to the large number of candidates it detects. We propose a heuristic that dramatically eliminate the false positives. We apply our approach on two case studies and we show how we can identify hidden dependencies and detect bad smells. Copyright 2007 ACM.",
      "Keywords": "Co-change analysis | Concept analysis | Evolution analysis",
      "Publication venue": "International Workshop on Principles of Software Evolution (IWPSE)",
      "Publication date": "2007-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Grba, Tudor;Ducasse, Stéphanfs;Kuhn, Adrian;Marinescu, Radu;Daniel, Raiu",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-41149155018",
      "Primary study DOI": "",
      "Title": "Using concept analysis to detect co-change patterns",
      "Abstract": "Software systems need to change over time to cope with new requirements, and due to design decisions, the changes happen to crosscut the system's structure. Understanding how changes appear in the system can reveal hidden dependencies between different entities of the system. We propose the usage of concept analysis to identify groups of entities that change in the same way and in the same time. We apply our approach at different levels of abstraction (i.e., method, class, package) and we detect fine grained changes (i.e., statements were added in a class, but no method was added there). Concept analysis is a technique that identifies entities that have the same properties, but it requires manual inspection due to the large number of candidates it detects. We propose a heuristic that dramatically eliminate the false positives. We apply our approach on two case studies and we show how we can identify hidden dependencies and detect bad smells. Copyright 2007 ACM.",
      "Keywords": "",
      "Publication venue": "",
      "Publication date": "",
      "Publication type": "",
      "Authors": "Gîrba, T.;Ducasse, S.;Kuhn, A.;Marinescu, R.;Daniel, R.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-34548706553",
      "Primary study DOI": "10.1109/MSR.2007.21",
      "Title": "Mining software repositories with iSPARQL and a software evolution ontology",
      "Abstract": "One of the most important decisions researchers face when analyzing the evolution of software systems is the choice of a proper data analysis/exchange format. Most existing formats have to be processed with special programs written specifically for that purpose and are not easily extendible. Most scientists, therefore, use their own database(s) requiring each of them to repeat the work of writing the import/export programs to their format. We present EvoOnt, a software repository data exchange format based on the Web Ontology Language (OWL). EvoOnt includes software, release, and bug-related information. Since OWL describes the semantics of the data, EvoOnt is (1) easily extendible, (2) comes with many existing tools, and (3) allows to derive assertions through its inherent Description Logic reasoning capabilities. The paper also shows iSPARQL - our SPARQL-based Semantic Web query engine containing similarity joins. Together with EvoOnt, iSPARQL can accomplish a sizable number of tasks sought in software repository mining projects, such as an assessment of the amount of change between versions or the detection of bad code smells. To illustrate the usefulness of EvoOnt (and iSPARQL), we perform a series of experiments with a real-world Java project. These show that a number of software analyses can be reduced to simple iSPARQL queries on an EvoOnt dataset. © 2007 IEEE.",
      "Keywords": "",
      "Publication venue": "Proceedings - ICSE 2007 Workshops: Fourth International Workshop on Mining Software Repositories, MSR 2007",
      "Publication date": "2007-09-24",
      "Publication type": "Conference Paper",
      "Authors": "Kiefer, Christoph;Bernstein, Abraham;Tappolet, Jonas",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-34548099259",
      "Primary study DOI": "10.1007/978-3-540-73066-8_16",
      "Title": "Utilising code smells to detect quality problems in TTCN-3 test suites",
      "Abstract": "Today, test suites of several ten thousand lines of code are specified using the Testing and Test Control Notation (TTCN-3). Experience shows that the resulting test suites suffer from quality problems with respect to internal quality aspects like usability, maintainability, or reusability. Therefore, a quality assessment of TTCN-3 test suites is desirable. A powerful approach to detect quality problems in source code is the identification of code smells. Code smells are patterns of inappropriate language usage that is error-prone or may lead to quality problems. This paper presents a quality assessment approach for TTCN-3 test suites which is based on TTCN-3 code smells: To this aim, various TTCN-3 code smells have been identified and collected in a catalogue; the detection of instances of TTCN-3 code smells in test suites has been automated by a tool. The applicability of this approach is demonstrated by providing results from the quality assessment of several standardised TTCN-3 test suites. © IFIP International Federation for Information Processing 2007.",
      "Keywords": "",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2007-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Neukirchen, Helmut;Bisanz, Martin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-34547673007",
      "Primary study DOI": "10.1109/CSMR.2007.51",
      "Title": "Towards automated restructuring of object oriented systems",
      "Abstract": "Software aging is an important cost contributor to the maintenance of aging software systems. Recent years have brought significant progress in the area of automatic detection of \"code smells\" as well as tool support for refactoring and implementing design patterns in the code. Nonetheless, there is hardly any tool support to help the maintainer decide how to refactor in a given situation, such that the recommended refactorings are also meaningful in that particular situation. Most of the existing techniques are either merely supporting the process, such as visualizations, or cannot guarantee meaningful refactorings, such as optimization based techniques. This paper introduces and experimentally evaluates a novel, tool supported approach to determine meaningful refactorings to structural flaws in object oriented systems. The refactorings recommended by our approach are guaranteed to lead to a meaningful and more maintainable structure in each analyzed situation. The approach contributes to a dramatic reduction of costs, by reducing the need and scope of detailed, manual code analysis. © 2007 IEEE.",
      "Keywords": "",
      "Publication venue": "Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR",
      "Publication date": "2007-08-13",
      "Publication type": "Conference Paper",
      "Authors": "Trifu, Adrian;Reupke, Urs",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-34547420138",
      "Primary study DOI": "10.1142/S0218194007003318",
      "Title": "An ontology for understanding and applying object-oriented design knowledge",
      "Abstract": "After years of experience in object-oriented design, software engineers have accumulated a great deal of knowledge in the design and construction of object-oriented systems: important contributions to this field including principles, heuristics, lessons learned, bad smells, refactorings, and so on, with the resultant major improvements in software development. However, this large body of knowledge is still not well organized, its terminology is ambiguous, and it is very difficult to make practical use of the contributions made. In this regard, we believe it is important to define an ontology in order to structure and unify design knowledge, since a good understanding of the experience derived from practical work is critical for software engineers. This ontology could be used to improve communication between software engineers, inter-operability among designs, design re-usability, design knowledge searching and specification, software maintenance, knowledge acquisition, etc. In the ontology we incorporate knowledge specific to both domain and technology. Such an organized body of knowledge could also be used for registering and documenting design rationale issues. © World Scientific Publishing Company.",
      "Keywords": "Design knowledge | Heuristic | Ontology | Patterns | Principles | Refactoring",
      "Publication venue": "International Journal of Software Engineering and Knowledge Engineering",
      "Publication date": "2007-06-01",
      "Publication type": "Article",
      "Authors": "Garzás, Javier;Piattini, Mario",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-36348974040",
      "Primary study DOI": "10.1109/TSE.2007.70745",
      "Title": "On the detection of test smells: A metrics-based approach for general fixture and eager test",
      "Abstract": "As a testing method, white box testing has been demonstrated to be very efficient in early defect detection. However, white box testing introduces test co-evolution as an additional burden to software development. To mitigate the effects of co-evolution, tests should be written in a manner that makes them easy to change. Fortunately, we are able to concretely express what a good test is by exploiting the specific principles underlying white box testing. Analogous to the concept of code smells, violations of these principles are termed test smells. In this paper, we present a formal description of test smells, and propose metrics to support their detection. We validate the feasibility of detecting two test smells, General Fixture and Eager Test, by comparison with human evaluation. We demonstrate the effectiveness of the detection in the case the assessment is agreed upon by evaluators. For the General Fixture, a qualitative investigation showed that an ambiguous test smell definition prohibits the detection by metrics and suggests disentangling its definition. On the bright side, test evolvability can be more concretely expressed than general evolvability due to the exploitation of the specific principles underlying white box testing. In particular, adherence to a rigid setup-stimulate-verify-teardown cycle has been reported an essential characteristic of evolvable tests. In this article, we propose to incorporate structural characteristics of tests in the definition of test smells, thereby providing an objective means to detect test evolution obstacles. We validate the feasibility of detecting test evolution obstacles using such test smells, thereby contributing the first step to the mitigation of the cost of test co-evolution. © 2007 IEEE.",
      "Keywords": "Maintainability | Maintenance engineering | Performance evaluation | Quality assurance | Test design | Testing",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2007-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Van Rompaey, Bart;Du Bois, Bart;Demeyer, Serge;Rieger, Matthias",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-34547419521",
      "Primary study DOI": "10.1109/ASE.2006.22",
      "Title": "Automatic generation of detection algorithms for design defects",
      "Abstract": "Maintenance is recognised as the most difficult and expansive activity of the software development process. Numerous techniques and processes have been proposed to ease the maintenance of software. In particular, several authors published design defects formalising \"bad\" solutions to recurring design problems (e.g., anti-patterns, code smells). We propose a language and a framework to express design defects synthetically and to generate detection algorithms automatically. We show that this language is sufficient to describe some design defects and to generate detection algorithms, which have a good precision. We validate the generated algorithms on several programs. © 2006 IEEE.",
      "Keywords": "",
      "Publication venue": "Proceedings - 21st IEEE/ACM International Conference on Automated Software Engineering, ASE 2006",
      "Publication date": "2006-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Moha, Naouel;Guéhéneuc, Yann Gaël;Leduc, Pierre",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-34247331422",
      "Primary study DOI": "10.1145/1159733.1159778",
      "Title": "Drivers for software refactoring decisions",
      "Abstract": "This paper presents an empirical study of drivers for software refactoring decisions. We studied the refactoring decisions made by 37 students evaluating ten methods of a purposefully constructed Java program. The decision rationales reported by the evaluators were coded to identify the drivers behind the decisions. The identified drivers were categorized into Structure, Documentation, Visual Representation, and General drivers. The evaluators had conflicting opinions both regarding the internal quality of the methods and refactoring decisions. Complex code problems were detected only by experienced evaluators. Using regression analysis, we looked at the predictive value of drivers explaining the refactoring decisions. The most salient driver leading to a favourable refactoring decision was method size. This study provides information of the refactoring decisions and helps form a basis for creating code problem detectors. By comparing automatic detection and the identified drivers we gained understanding of code problems that are difficult or impossible to detect automatically, for example Poor Algorithm. Issues detected only by experienced developers, and code problems for which the human eye surpasses automatic detection indicate good areas for developer education. Copyright 2006 ACM.",
      "Keywords": "Code smells | Evolvability | Maintainability | Qualitative analysis | Refactoring",
      "Publication venue": "ISESE'06 - Proceedings of the 5th ACM-IEEE International Symposium on Empirical Software Engineering",
      "Publication date": "2006-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Mäntylä, Mika V.;Lassenius, Casper",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-34248402859",
      "Primary study DOI": "10.1145/1176617.1176725",
      "Title": "The composition refactoring triangle (CRT) practical toolkit: From spaghetti to lasagna",
      "Abstract": "Adding to and restructuring the design of a large existing code base is known to be a difficult and time consuming task. There are methods for assisting these challenges, such as composition refactoring, reengineering, and anti-patterns detection methods. Most of these methods concentrate on specific perspectives of software engineering.This practical paper presents the Composition Refactoring Triangle (CRT) unified approach for handling multiple changes across complex environments. The CRT is a combination of: the process (CRP), the management tool (CRMT), and the external and internal refactoring elements. The CRT was constructed during on-going need to implement major changes within a living product. This paper contains the \"Via Delarosa\" - The \"Path of Suffering\" which describes the road of how the CRT was created, and why other methods failed.Practical evaluation was conducted using the CRT, demonstrating its capabilities. The unified perspectives of the CRT enable improved risk analysis and technical control over multiple architectural evolution changes and their relative dependencies. Its implementation encourages quick testing procedure, code correctness, and short time-to-market response of the development team.",
      "Keywords": "Anti-patterns | Architecture centric evolution | Design | Methodology | Refactoring | Requirements | Traceability | Tracking",
      "Publication venue": "Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",
      "Publication date": "2006-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Hadar, Ethan;Hadar, Irit",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84899161730",
      "Primary study DOI": "10.4018/978-1-59140-896-3.ch001",
      "Title": "The object-oriented design knowledge",
      "Abstract": "In order to establish itself as a branch of engineering, a profession must understand its accumulated knowledge. In this regard, software engineering has advanced greatly in recent years, but it still suffers from the lack of a structured classification of its knowledge. In this sense, in the field of object-oriented micro-architectural design designers have accumulated a large body of knowledge and it is still have not organized or unified. Therefore, items such as design patterns are the most popular example of accumulated knowledge, but other elements of knowledge exist such as principles, heuristics, best practices, bad smells, refactorings, and so on, which are not clearly differentiated; indeed, many are synonymous and others are just vague concepts. © 2007, Idea Group Inc.",
      "Keywords": "",
      "Publication venue": "Object-Oriented Design Knowledge: Principles, Heuristics and Best Practices",
      "Publication date": "2006-12-01",
      "Publication type": "Book Chapter",
      "Authors": "Garzás, Javier;Piattini, Mario",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-34547364665",
      "Primary study DOI": "10.1109/ICSM.2006.69",
      "Title": "Towards portable metrics-based models for software maintenance problems",
      "Abstract": "The usage of software metrics for various purposes has become a hot research topic in academia and industry (e.g. detecting design patterns and bad smells, studying change-proneness, quality and maintainability, predicting faults). Most of these topics have one thing in common: they are all using some kind of metrics-based models to achieve their goal. Unfortunately, only few researchers have tested these models on unknown software systems so far. This paper tackles the question, which metrics are suitable for preparing portable models (which can be efficiently applied to unknown software systems). We have assessed several metrics on four large software systems and we found that the well-known RFC and WMC metrics differentiate the analyzed systems fairly well. Consequently, these metrics cannot be used to build portable models, while the CBO, LCOM and LOC metrics behave similarly on all systems, so they seem to be suitable for this purpose. © 2006 IEEE.",
      "Keywords": "C++ | Columbus | Metrics | Model | Mozilla | Open source | OpenOffice.org | Source code analysis",
      "Publication venue": "IEEE International Conference on Software Maintenance, ICSM",
      "Publication date": "2006-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Bakota, Tibor;Ferenc, Rudolf;Gyimóthy, Tibor;Riva, Claudio;Jianli, Xu",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84899343777",
      "Primary study DOI": "10.4018/978-1-59140-896-3.ch009",
      "Title": "A survey of object-oriented design quality improvement",
      "Abstract": "The use of object-oriented (OO) architecture knowledge such as patterns, heuristics, principles, refactorings and bad smells improve the quality of designs, as Garzás and Piattini (2005) state in their study; according to it, the application of those elements impact on the quality of an OO design and can serve as basis to establish some kind of software design improvement (SDI) method. But how can we measure the level of improvement? Is there a set of accepted internal attributes to measure the quality of a design? Furthermore, if such a set exists will it be possible to use a measurement model to guide the SDI in the same way software processimprovement models (Humphrey, 1989; Paulk, Curtis, Chrissis, & Weber, 1993) are guided by process metrics (Fenton & Pfleeger, 1998)? Since (Chidamber & Kemerer, 1991) several OO metrics suites have been proposed to measure OO properties, such as encapsulation, cohesion, coupling and abstraction, both in designs and in code, in this chapter we review the literature to find out to which high level quality properties are mapped and if an OO design evaluation model has been formally proposed or even is possible. © 2007, Idea Group Inc.",
      "Keywords": "",
      "Publication venue": "Object-Oriented Design Knowledge: Principles, Heuristics and Best Practices",
      "Publication date": "2006-12-01",
      "Publication type": "Book Chapter",
      "Authors": "Olmedilla, Juan José",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-33751032648",
      "Primary study DOI": "10.1145/1141277.1141574",
      "Title": "An approach to refactoring of executable UML models",
      "Abstract": "Design erosion is one of the unavoidable effects of software evolution. This destructive phenomenon occurs also in the context of executable UML models, which are primary artefacts in Agile MDA software development methodology. Model refactorings are model transformations that can be applied with the aim of counteracting design erosion of UML models. In this paper, we present a systematic approach to specification of both executable UML model refactorings as well as associated bad smells in models. The application of this method is illustrated on an exemplary refactoring and a related bad smell. Moreover, we show how this transformation and detection of the bad smell can be implemented in Telelogic TAU, a state-of-the art UML CASE tool. Copyright 2006 ACM.",
      "Keywords": "Executable UML | Model refactoring",
      "Publication venue": "Proceedings of the ACM Symposium on Applied Computing",
      "Publication date": "2006-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Dobrzański, Lukasz;Kuźniarz, Ludwik",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-33750805204",
      "Primary study DOI": "10.1109/aiccsa.2006.205164",
      "Title": "Refactoring tools and complementary techniques",
      "Abstract": "Poorly designed software systems are difficult to understand and maintain. Modifying code in one place could lead to unwanted repercussions elsewhere due to high coupling. Adding new features can cause further quality degradation to the code if proper design and architectural concerns were not implemented. Development in a large enterprise system with such attributes will, over time, lead to a myriad of concerns unless the system is periodically overhauled or refactored in some way. Refactoring can aid the developer to improve the design of the code and to make it cleaner, without changing its behaviour. This study provides answers for some of the questions on refactoring. A refactoring tool survey is given. The IDEs surveyed include some of the most popular commercial and open source offerings from IntelliJ's IDEA, IBM's Eclipse and Sun's Netbeans. We also explain a way to automatically find targets for refactorings via automatic detection of code smells from static code analysis. Concerns on viewing compiler refactorings as a fully automated refactorings are raised. We will perform a critical evaluation of refactoring by surveying these tools. © 2006 IEEE.",
      "Keywords": "",
      "Publication venue": "IEEE International Conference on Computer Systems and Applications, 2006",
      "Publication date": "2006-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Drozdz, Martin;Kourie, Derrick G.;Watson, Bruce W.;Boake, Andrew",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-33750271383",
      "Primary study DOI": "10.1145/1143997.1144315",
      "Title": "Search-based determination of refactorings for improving the class structure of object-oriented systems",
      "Abstract": "A software system's structure degrades over time, a phenomenon that is known as software decay or design drift. Since the quality of the structure has major impact on the maintainability of a system, the structure has to be reconditioned from time to time. Even if recent advances in the fields of automated detection of bad smells and refactorings have made life easier for software engineers, this is still a very complex and resource consuming task. Search-based approaches have turned out to be helpful in aiding a software engineer to improve the subsystem structure of a software system. In this paper we show that such techniques are also applicable when reconditioning the class structure of a system. We describe a novel search-based approach that assists a software engineer who has to perform this task by suggesting a list of refactorings. Our approach uses an evolutionary algorithm and simulated refactorings that do not change the system's externally visible behavior. The approach is evaluated using the open-source case study JHotDraw. Copyright 2006 ACM.",
      "Keywords": "Design Heuristics | Evolutionary Algorithms | Refactoring | Software Metrics",
      "Publication venue": "GECCO 2006 - Genetic and Evolutionary Computation Conference",
      "Publication date": "2006-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Seng, Olaf;Stammel, Johannes;Burkhart, David",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-33747891571",
      "Primary study DOI": "",
      "Title": "Detecting bad smells in AspectJ",
      "Abstract": "This paper defines algorithms to automatically detect five types of bad smells that occur in aspect-oriented systems, more specifically those written using the AspectJ language. We provide a prototype implementation to evaluate the detection algorithms in a case study, where bad smells are detected in three well-known AspectJ systems. © J.UCS.",
      "Keywords": "Aspect Oriented Software Development | AspectJ language | Refactoring",
      "Publication venue": "Journal of Universal Computer Science",
      "Publication date": "2006-09-01",
      "Publication type": "Conference Paper",
      "Authors": "Piveta, Eduardo Kessler;Hecht, Marcelo;Pimenta, Marcelo Soares;Price, Roberto Tom",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84941156547",
      "Primary study DOI": "",
      "Title": "Extreme Programming and Agile Processes in Software Engineering - 7th International Conference, XP 2006, Proceedings",
      "Abstract": "The proceedings contain 30 papers. The topics discussed include: a distributed cognition account of mature XP teams; foundations of agile decision making from agile mentors and developers; software development as a collaborative writing project; comparative analysis of job satisfaction in agile and non-agile software development teams; the collaborative nature of pair programming; leveraging code smell detection with inter-smell relations; studying the evolution of quality metrics in an agile/distributed project; the effect of test-driven development on program code; configuring hybrid agile-tradition software processes; agility in the avionics software world; security planning and refactoring in extreme programming; augmenting the agile planning toolbox; and storytelling in interaction: agility in practice.",
      "Keywords": "",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2006-01-01",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026999248",
      "Primary study DOI": "10.1109/MOBILESoft.2017.29",
      "Title": "Detecting Android Smells Using Multi-Objective Genetic Programming",
      "Abstract": "The evolution rate of mobile applications is much higher than regular software applications having shorter release deadlines and smaller code base. Mobile applications tend to be evolved quickly by developers to meet several new customer requirements and fix discovered bugs. However, evolving the existing features and design may introduce bad design practices, also called code smells, which can highly decrease the maintainability and performance of these mobile applications. However, unlike the area of object-oriented software systems, the detection of code smells in mobile applications received a very little of attention. Recent, few studies defined a set of quality metrics for Android applications and proposed a support to manually write a set of rules to detect code smells by combining these quality metrics. However, finding the best combination of metrics and their thresholds to identify code smells is left to the developer as a manual process. In this paper, we propose to automatically generate rules for the detection of code smells in Android applications using a multi-objective genetic programming algorithm (MOGP). The MOGP algorithm aims at finding the best set of rules that cover a set of code smell examples of Android applications based on two conflicting objective functions of precision and recall. We evaluate our approach on 184 Android projects with source code hosted in GitHub. The statistical test of our results show that the generated detection rules identified 10 Android smell types on these mobile applications with an average correctness higher than 82% and an average relevance of 77% based on the feedback of active developers of mobile apps.",
      "Keywords": "Android apps | quality | Search-based software engineering",
      "Publication venue": "Proceedings - 2017 IEEE/ACM 4th International Conference on Mobile Software Engineering and Systems, MOBILESoft 2017",
      "Publication date": "2017-07-07",
      "Publication type": "Conference Paper",
      "Authors": "Kessentini, Marouane;Ouni, Ali",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-33749058711",
      "Primary study DOI": "10.1109/ISESE.2005.1541837",
      "Title": "An experiment on subjective evolvability evaluation of object-oriented software: Explaining factors and interrater agreement",
      "Abstract": "Recent trends in software development have emphasized the importance of refactoring in preserving software evolvability. We performed two experiments on software evolvability evaluation, i.e. evaluating the existence of certain code problems called code smells and the refactoring decision. We studied the agreement of the evaluators. Interrater agreement was high for simple code smells and low for the refactoring decision. Furthermore, we analyzed evaluators' demographics and source code metrics as factors explaining the evaluations. The code metrics explained over 70% of the variation regarding the simple code smell evaluations, but only about 30% of the refactoring decision. Surprisingly, the demographics were not useful predictors neither for evaluating code smells nor the refactoring decision. The low agreement for the refactoring decisions may indicate difficulty in building tool support simulating real-life subjective refactoring decisions. However, code metrics tools should be effective in highlighting straightforward problems, e.g. simple code smells. © 2005 IEEE.",
      "Keywords": "",
      "Publication venue": "2005 International Symposium on Empirical Software Engineering, ISESE 2005",
      "Publication date": "2005-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Mäntylä, Mika V.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84872140239",
      "Primary study DOI": "",
      "Title": "SFMEA; Applying a six sigma method to Software Performance Engineering",
      "Abstract": "Software Performance Engineering Failure Modes and Effects Analysis (SFMEA) is a method to assess risk and determine the focus and scope of software performance predictive models and performance tests. SFMEA identifies the most important execution paths based on software performance anti-patterns, execution frequency, current control plan and customer requirements. The product of these dimensions is the risk priority number (RPN). The execution paths with the highest RPN values define the Software Performance Engineering focus and scope.",
      "Keywords": "",
      "Publication venue": "31st International Conference Computer Measurement Group",
      "Publication date": "2005-12-01",
      "Publication type": "Conference Paper",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-33244471927",
      "Primary study DOI": "10.1145/1052898.1052908",
      "Title": "Towards a catalog of Aspect-oriented refactorings",
      "Abstract": "In this paper, we present a collection of aspect-oriented refactorings covering both the extraction of aspects from object-oriented legacy code and the subsequent tidying up of the resulting aspects. In some cases, this tidying up entails the replacement of the original implementation with a different, centralized design, made possible by modularization. The collection of refactorings includes the extraction of common code in various aspects into abstract superaspects. We review the traditional object-oriented code smells in the light of aspect-orientation and propose some new smells for the detection of crosscutting concerns. In addition, we propose a new code smell that is specific to aspects. © 2005 ACM.",
      "Keywords": "Aspect-oriented programming | Code smells | Object-oriented programming | Programming style | Refactoring",
      "Publication venue": "AOSD 2005: 4th International Conference on Aspect-Oriented Software Development - Conference Proceedings",
      "Publication date": "2005-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Monteiro, Miguel P.;Fernandes, João M.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-33846248850",
      "Primary study DOI": "10.1109/WCRE.2005.15",
      "Title": "Diagnosing design problems in object oriented systems",
      "Abstract": "Software decay is a phenomenon that plagues aging software systems. While in recent years, there has been significant progress in the area of automatic detection of \"code smells\" on one hand, and code refactorings on the other hand, we claim that existing restructuring practices are seriously hampered by their symptomatic and informal (non-repeatable) nature. This paper makes a clear distinction between structural problems and structural symptoms (also known as code smells), and presents a novel, causal approach to restructuring object oriented systems. Our approach is based on two innovations: the encapsulation of correlations of symptoms and additional contextual information into higher-level design problems, and the univocal, explicit mapping of problems to unique refactoring solutions. Due to its explicit, repeatable nature, the approach shows high potential for increased levels of automation in the restructuring process, and consequently a decrease in maintenance costs. © 2005 IEEE.",
      "Keywords": "",
      "Publication venue": "Proceedings - Working Conference on Reverse Engineering, WCRE",
      "Publication date": "2005-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Trifu, Adrian;Marinescu, Radu",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85027418859",
      "Primary study DOI": "",
      "Title": "Proceedings of the 3rd International Workshop on Dynamic Analysis, WODA 2005",
      "Abstract": "The proceedings contain 10 papers. The topics discussed include: dynamic detection and visualization of software phases; analyzing clusters of web application user sessions; merging traces of hardware-assisted data breakpoints; event-based runtime verification of Java programs; an exploration of statistical models for automated test case generation; measuring precision for static and dynamic design pattern recognition as a function of coverage; selective capture and replay of program executions; refactoring GCC using structure field access traces and concept analysis; dynamic analysis of Java applications for multi-threaded anti-patterns; and efficient static analysis with path pruning using coverage data.",
      "Keywords": "",
      "Publication venue": "Proceedings of the 3rd International Workshop on Dynamic Analysis, WODA 2005",
      "Publication date": "2005-05-17",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84858012851",
      "Primary study DOI": "10.1145/1083246.1083247",
      "Title": "Dynamic analysis of Java applications for multithreaded antipatterns",
      "Abstract": "Formal verification is not always applicable to large industrial software systems due to scalability issues and difficulties in formal model and requirements specification. The scalability and model derivation problems could be alleviated by runtime trace analysis, which combines both testing and formal verification. We implement and compare an ad-hoc custom approach and a formal approach to detect common bug patterns in multithreaded Java software. We use the tracing platform of the Eclipse IDE and state-of-The-Art model checker Spin.",
      "Keywords": "Antipatterns | Bug patterns | Bytecode | Instrumentation | Java | Multithreading",
      "Publication venue": "Proceedings of the 3rd International Workshop on Dynamic Analysis, WODA 2005",
      "Publication date": "2005-05-17",
      "Publication type": "Conference Paper",
      "Authors": "Boroday, S.;Petrenko, A.;Singh, J.;Hallal, H.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-17644366789",
      "Primary study DOI": "10.1109/MS.2005.26",
      "Title": "An ontology for microarchitectural design knowledge",
      "Abstract": "A thorough grasp of object-oriented microarchitectural design knowledge is crucial to software engineering. The authors have developed an ontology that facilitates this task. The ontology considers not only hot topics such as refactorings and patterns but also important elements of knowledge such as heuristics, principles, best practices, and \"bad smells.\" Thus, it can help develop unified catalogs of knowledge, structure and unify accumulated knowledge, improve communication, teach concepts and their relationships, share knowledge, and resolve incompatibilities in terminology. © 2005 IEEE.",
      "Keywords": "",
      "Publication venue": "IEEE Software",
      "Publication date": "2005-03-01",
      "Publication type": "Article",
      "Authors": "Garzás, Javier;Piattini, Mario",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-14044255862",
      "Primary study DOI": "10.1109/QSIC.2004.1357968",
      "Title": "Antipattern-based detection of deficiencies in java multithreaded software",
      "Abstract": "In this paper, we investigate an antipattern-based approach to analyze Java multithreaded (MT) programs. We present a library of 38 antipatterns, which describe predefined recognized sources of multithreading related errors in the code. The antipatterns are archived in practical, easy to use templates, and are classified according to their potential effects on the program behavior. We also report on our experience in using these antipatterns in the analysis of real multithreaded applications.",
      "Keywords": "",
      "Publication venue": "Proceedings - Fourth International Conference on Quality Software, QSIC 2004",
      "Publication date": "2004-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Hallal, H. H.;Alikacem, E.;Tunney, W. P.;Boroday, S.;Petrenko, A.",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-79951752364",
      "Primary study DOI": "10.1145/1028664.1028781",
      "Title": "A framework for detecting, assessing and visualizing performance antipatterns in component based systems",
      "Abstract": "Component-based enterprise systems often suffer from performance issues as a result of poor system design. In this paper, we propose a framework to automatically detect, assess and visualize poor system design, from a performance perspective, by analyzing run-time data using data mining techniques.",
      "Keywords": "Antipatterns | Component-based systems (CBS) | Data mining | Dynamic analysis | Enterprise java beans (EJB)",
      "Publication venue": "Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA",
      "Publication date": "2004-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Parsons, Trevor",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84884713944",
      "Primary study DOI": "10.1109/CSMR.2003.1192416",
      "Title": "Identifying refactoring opportunities using logic meta programming",
      "Abstract": "We show how automated support can be provided for identifying refactoring opportunities, e.g., when an application's design should be refactored and which refactoring(s) in particular should be applied. Such support is achieved by using the technique of logic meta programming to detect so-called bad smells and by defining a framework that uses this information to propose adequate refactorings. We report on some initial but promising experiments that were applied using the proposed techniques. © 2003 IEEE.",
      "Keywords": "",
      "Publication venue": "Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR",
      "Publication date": "2003-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Tourwe, T.;Mens, T.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84943154187",
      "Primary study DOI": "10.1109/IWPSE.2003.1231207",
      "Title": "Beyond the refactoring browser: Advanced tool support for software refactoring",
      "Abstract": "Current refactoring tools only provide support for performing selected refactorings. We show how tool support can be provided for the preparatory phases of refactoring as well, by determining when a software application should be refactored and which refactoring(s) in particular should be applied. We implemented a tool to detect bad smells and to propose adequate refactorings based on these smells, and validated this tool by carrying out experiments in three concrete case studies: the Soul application, the Smalltalk collection hierarchy, and the HotDraw application framework. We also show how our tool complements the Smalltalk refactoring browser.",
      "Keywords": "Conferences | Software tools",
      "Publication venue": "International Workshop on Principles of Software Evolution (IWPSE)",
      "Publication date": "2003-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Mens, T.;Tourwé, T.;Muñoz, F.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-0141469630",
      "Primary study DOI": "10.1109/CSMR.2002.995795",
      "Title": "Architecture-centric software evolution by software metrics and design patterns",
      "Abstract": "It is shown how software metrics and architectural patterns can be used for the management of software evolution. In the presented architecture-centric software evolution method the quality of a software system is assured in the software design phase by computing various kinds of design metrics from the system architecture, by automatically exploring instances of design patterns and anti-patterns from the architecture, and by reporting potential quality problems to the designers. The same analysis is applied in the implementation phase to the software code, thus ensuring that it matches the quality and structure of the reference architecture. Finally, the quality of the ultimate system is predicted by studying the development history of previous projects with a similar composition of characteristic software metrics and patterns. The architecture-centric software evolution method is supported by two integrated software tools, the metrics and pattern-mining tool Maisa and the reverse-engineering tool Columbus. © 2002 IEEE.",
      "Keywords": "",
      "Publication venue": "Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR",
      "Publication date": "2002-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Gustafsson, J.;Paakki, J.;Nenonen, L.;Verkamo, A. I.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84952879270",
      "Primary study DOI": "10.1109/WCRE.2002.1173068",
      "Title": "Java quality assurance by detecting code smells",
      "Abstract": "Software inspection is a known technique for improving software quality. It involves carefully examining the code, the design, and the documentation of software and checking these for aspects that are known to be potentially problematic based on past experience. Code smells are a metaphor to describe patterns that are generally associated with bad design and bad programming practices. Originally, code smells are used to find the places in software that could benefit from refactoring. In this paper we investigate how the quality of code can be automatically assessed by checking for the presence of code smells and how this approach can contribute to automatic code inspection. We present an approach for the automatic detection and visualization of code smells and discuss how this approach can be used in the design of a software inspection tool. We illustrate the feasibility of our approach with the development of jCOSMO, a prototype code smell browser that detects and visualizes code smells in Java source code. Finally, we show how this tool was applied in a case study.",
      "Keywords": "code smells | Java | quality assurance | refactoring | Software inspection",
      "Publication venue": "Proceedings - Working Conference on Reverse Engineering, WCRE",
      "Publication date": "2002-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Van Emden, Eva;Moonen, Leon",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84945291251",
      "Primary study DOI": "10.1007/978-3-540-44995-9_20",
      "Title": "Object oriented design expertise reuse: An approach based on heuristics, design patterns and anti-patterns",
      "Abstract": "Object Oriented (OO) languages do not guarantee that a system is flexible enough to absorb future requirements, nor that its components can be reused in other contexts. This paper presents an approach to OO design expertise reuse, which is able to detect certain constructions that compromise future expansion or modification of OO systems, and suggest their replacement by more adequate ones. Both reengineering legacy systems, and systems that are still under development are considered by the approach. A tool (OOPDTool) was developed to support the approach, comprising a knowledge base of good design constructions, that correspond to heuristics and design patterns, as well as problematic constructions (i.e., anti-patterns).",
      "Keywords": "",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2000-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Correa, Alexandre L.;Werner, Cláudia M.L.;Zaverucha, Gerson",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-0034592733",
      "Primary study DOI": "10.1145/350391.350422",
      "Title": "Design patterns in performance prediction",
      "Abstract": "The use of design patterns in object-oriented software development is recommended to improve both the software structure and the development process. On the other hand, their effect on the performance of the final product may greatly depend on the particular application and the projected workload. In addition to intentional use, the design may also contain patterns (and anti-patterns) that have not been used on purpose. We propose an approach where all patterns and anti-patterns embedded in the design are discovered by a mining tool and their performance effects are evaluated using previous experience stored in a pattern library.",
      "Keywords": "",
      "Publication venue": "Proceedings Second International Workshop on Software and Performance WOSP 2000",
      "Publication date": "2000-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Verkamo, A. Inkeri;Gustafsson, Juha;Nenonen, Lilli;Paakki, Jukka",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84945288858",
      "Primary study DOI": "",
      "Title": "6th International Conference on Software Reuse, ICSR 2000",
      "Abstract": "The proceedings contain 26 papers. The special focus in this conference is on Generative Reuse, Formal Domain Languages, Object Oriented Methods, Product Line Architectures, Requirements Reuse and Business Modeling. The topics include: A new control structure for transformation-based generators; a reuse-oriented specification language for real-time systems; from application domains to executable domains; reuse of knowledge at an appropriate level of abstraction; building customizable frameworks for the telecommunications domain; object oriented analysis and modeling for families of systems with UML; from incremental development to incremental reasoning; achieving extensibility through product-lines and domain-specific languages; implementing product-line features with component reuse; representing requirements on generic software in an application family model; implementation issues in product line scoping; requirements classification and reuse; reuse measurement in the ERP requirements engineering process; business modeling and component mining based on rough set theory; reasoning about software-component behavior; use and identification of components in component-based software development methods; promoting reuse with active reuse repository systems; a method to recover design patterns using software product metrics; an approach based on heuristics, design patterns and anti-patterns; patterns leveraging analysis reuse of business processes; constructional design patterns as reusable components; a two-dimensional composition framework to support software adaptability and reuse; structuring mechanisms for an object-oriented formal specification language and software reuse in an object oriented framework.",
      "Keywords": "",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2000-01-01",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85085516355",
      "Primary study DOI": "10.1016/j.infsof.2020.106332",
      "Title": "Recommending refactorings via commit message analysis",
      "Abstract": "Context: The purpose of software restructuring, or refactoring, is to improve software quality and developer productivity. Objective: Prior studies have relied mainly on static and dynamic analysis of code to detect and recommend refactoring opportunities, such as code smells. Once identified, these smells are fixed by applying refactorings which then improve a set of quality metrics. While this approach has value and has shown promising results, many detected refactoring opportunities may not be related to a developer's current context and intention. Recent studies have shown that while developers document their refactoring intentions, they may miss relevant refactorings aligned with their rationale. Method: In this paper, we first identify refactoring opportunities by analyzing developer commit messages and check the quality improvements in the changed files, then we distill this knowledge into usable context-driven refactoring recommendations to complement static and dynamic analysis of code. Results: The evaluation of our approach, based on six open source projects, shows that we outperform prior studies that apply refactorings based on static and dynamic analysis of code alone. Conclusion: This study provides compelling evidence of the value of using the information contained in existing commit messages to recommend future refactorings.",
      "Keywords": "Commit message | Quality attributes | Refactoring recommendation",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2020-10-01",
      "Publication type": "Article",
      "Authors": "Rebai, Soumaya;Kessentini, Marouane;Alizadeh, Vahid;Sghaier, Oussama Ben;Kazman, Rick",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85084338028",
      "Primary study DOI": "10.1016/j.infsof.2020.106333",
      "Title": "A large scale study on how developers discuss code smells and anti-pattern in Stack Exchange sites",
      "Abstract": "Context: In this paper, we investigate how developers discuss code smells and anti-patterns across three technical Stack Exchange sites. Understanding developers perceptions of these issues is important to inform and align future research efforts and direct tools vendors to design tailored tools that best suit developers. Method: we mined three Stack Exchange sites and used quantitative and qualitative methods to analyse more than 4000 posts that discuss code smells and anti-patterns.Results: results showed that developers often asked their peers to smell their code, thus utilising those sites as an informal, crowd-based code smell/anti-pattern detector. The majority of questions (556) asked were focused on smells like Duplicated Code, Spaghetti Code, God and Data Classes. In terms of languages, most of discussions centred around popular languages such as C# (772 posts), JavaScript (720) and Java (699), however greater support is available for Java compared to other languages (especially modern languages such as Swift and Kotlin). We also found that developers often discuss the downsides of implementing specific design patterns and ‘flag’ them as potential anti-patterns to be avoided. Some well-defined smells and anti-patterns are discussed as potentially being acceptable practice in certain scenarios. In general, developers actively seek to consider trade-offs to decide whether to use a design pattern, an anti-pattern or not.Conclusion: our results suggest that there is a need for: 1) more context and domain sensitive evaluations of code smells and anti-patterns, 2) better guidelines for making trade-offs when applying design patterns or eliminating smells/anti-patterns in industry, and 3) a unified, constantly updated, catalog of smells and anti-patterns. We conjecture that the crowd-based detection approach considers contextual factors and thus tend to be more trusted by developers than automated detection tools.",
      "Keywords": "Anti-patterns | Code smells | Mining software repositories | Stack exchange",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2020-09-01",
      "Publication type": "Article",
      "Authors": "Tahir, Amjed;Dietrich, Jens;Counsell, Steve;Licorish, Sherlock;Yamashita, Aiko",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85084281534",
      "Primary study DOI": "10.1016/j.jss.2020.110610",
      "Title": "Code smells and refactoring: A tertiary systematic review of challenges and observations",
      "Abstract": "Refactoring and smells have been well researched by the software-engineering research community these past decades. Several secondary studies have been published on code smells, discussing their implications on software quality, their impact on maintenance and evolution, and existing tools for their detection. Other secondary studies addressed refactoring, discussing refactoring techniques, opportunities for refactoring, impact on quality, and tools support. In this paper, we present a tertiary systematic literature review of previous surveys, secondary systematic literature reviews, and systematic mappings. We identify the main observations (what we know) and challenges (what we do not know) on code smells and refactoring. We perform this tertiary review using eight scientific databases, based on a set of five research questions, identifying 40 secondary studies between 1992 and 2018. We organize the main observations and challenges about code smell and their refactoring into: smells definitions, most common code-smell detection approaches, code-smell detection tools, most common refactoring, and refactoring tools. We show that code smells and refactoring have a strong relationship with quality attributes, i.e., with understandability, maintainability, testability, complexity, functionality, and reusability. We argue that code smells and refactoring could be considered as the two faces of a same coin. Besides, we identify how refactoring affects quality attributes, more than code smells. We also discuss the implications of this work for practitioners, researchers, and instructors. We identify 13 open issues that could guide future research work. Thus, we want to highlight the gap between code smells and refactoring in the current state of software-engineering research. We wish that this work could help the software-engineering research community in collaborating on future work on code smells and refactoring.",
      "Keywords": "Code smells | Refactoring | Tertiary systematic review",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2020-09-01",
      "Publication type": "Article",
      "Authors": "Lacerda, Guilherme;Petrillo, Fabio;Pimenta, Marcelo;Guéhéneuc, Yann Gaël",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85077718451",
      "Primary study DOI": "10.1016/j.infsof.2019.106255",
      "Title": "Web service design defects detection: A bi-level multi-objective approach",
      "Abstract": "Context: Web services frequently evolve to integrate new features, update existing operations and fix errors to meet the new requirements of subscribers. While this evolution is critical, it may have a negative impact on the quality of services (QoS) such as reduced cohesion, increased coupling, poor response time and availability, etc. Thus, the design of services could become hard to maintain and extend in future releases. Recent studies addressed the problem of web service design antipatterns detection, also called design defects, by either manually defining detection rules, as combination of quality metrics, or generating them automatically from a set of defect examples. The manual definition of these rules is time-consuming and difficult due to the subjective nature of design issues, especially to find the right thresholds value. The efficiency of the generated rules, using automated approaches, will depend on the quality of the training set since examples of web services antipatterns are limited. Furthermore, the majority of existing studies for design defects detection for web services are limited to structural information (interface/code static metrics) and they ignore the use of quality of services (QoS) or performance metrics, such as response time and availability, for this detection process or understanding the impact of antipatterns on these QoS attributes. Objective: To address these challenges, we designed a bi-level multi-objective optimization approach to enable the generation of antipattern examples that can improve the efficiency of detection rules. Method: The upper-level generates a set of detection rules as a combination of quality metrics with their threshold values maximizing the coverage of defect examples extracted from several existing web services and artificial ones generated by a lower level. The lower level maximizes the number of generated artificial defects that cannot be detected by the rules of the upper level and minimizes the similarity to well-designed web service examples. The generated detection rules, by our approach, are based on a combination of dynamic QoS attributes and structural information of web service (static interface/code metrics). Results: The statistical analysis of our results, based on a data-set of 662 web services, confirms the efficiency of our approach in detecting web service antipatterns comparing to the current state of the art in terms of precision and recall. Conclusion: The multi-objective search formulation at both levels helped to diversify the generated artificial web service defects which produced better quality of detection rules. Furthermore, the combination of dynamic QoS attributes and structural information of web services improved the efficiency of the generated detection rules.",
      "Keywords": "Quality of services | Search based software engineering | Services design",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2020-05-01",
      "Publication type": "Article",
      "Authors": "Rebai, Soumaya;Kessentini, Marouane;Wang, Hanzhang;Maxim, Bruce",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85077525154",
      "Primary study DOI": "10.1007/s13369-019-04311-w",
      "Title": "Bad Smell Detection Using Machine Learning Techniques: A Systematic Literature Review",
      "Abstract": "Code smells are indicators of potential problems in software. They tend to have a negative impact on software quality. Several studies use machine learning techniques to detect bad smells. The objective of this study is to systematically review and analyze machine learning techniques used to detect code smells to provide interested research community with knowledge about the adopted techniques and practices for code smells detection. We use a systematic literature review approach to review studies that use machine learning techniques to detect code smells. Seventeen primary studies were identified. We found that 27 code smells were used in the identified studies; God Class and Long Method, Feature Envy, and Data Class are the most frequently detected code smells. In addition, we found that 16 machine learning algorithms were employed to detect code smells with acceptable prediction accuracy. Furthermore, we the results also indicate that support vector machine techniques were investigated the most. Moreover, we observed that J48 and Random Forest algorithms outperform the other algorithms. We also noticed that, in some cases, the use of boosting techniques on the models does not always enhance their performance. More studies are needed to consider the use of ensemble learning techniques, multiclassification, and feature selection technique for code smells detection. Thus, the application of machine learning algorithms to detect code smells in systems is still in its infancy and needs more research to facilitate the employment of machine learning algorithms in detecting code smells.",
      "Keywords": "Anti-pattern | Artificial intelligent | Bad smell | Code smell | Machine learning | Software quality",
      "Publication venue": "Arabian Journal for Science and Engineering",
      "Publication date": "2020-04-01",
      "Publication type": "Review",
      "Authors": "Al-Shaaby, Ahmed;Aljamaan, Hamoud;Alshayeb, Mohammad",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85078990945",
      "Primary study DOI": "10.1007/s13369-020-04365-1",
      "Title": "Recovering Android Bad Smells from Android Applications",
      "Abstract": "The demand for Android mobile software applications is continuously increasing with the evolution of technology and new enriching features to make the life of people easy and comfortable. The mobile-based software applications are frequently updated as compared to other web and desktop applications. Due to these frequent updating cycles, the developers sometimes make changes in a rush which leads to poor design choices known as antipatterns or code bad smells. Code bad smells degrade the performance of applications and make evolution difficult. The recovery of bad smells from mobile software applications is still at infancy but it is a very important research realm that requires the attention of researchers and practitioners. The results of recovery may be used for comprehension, maintenance, reengineering, evolution and refactoring of these applications. Most state-of-the-art approaches focused on the detection of code bad smells from object-oriented applications and they target only a few code smells. We present a novel approach supplemented with tool support to recover 25 Android code bad smells from Android-specific software applications. We evaluate our approach by performing experiments on 4 open source and 3 industrial Android-specific software applications and measure accuracy using standard metrics.",
      "Keywords": "Android smells | Code refactoring | Mobile software applications | Smart phone | Software quality",
      "Publication venue": "Arabian Journal for Science and Engineering",
      "Publication date": "2020-04-01",
      "Publication type": "Article",
      "Authors": "Rasool, Ghulam;Ali, Azhar",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85050982730",
      "Primary study DOI": "10.1109/TSE.2018.2861735",
      "Title": "Leveraging historical associations between requirements and source code to identify impacted classes",
      "Abstract": "As new requirements are introduced and implemented in a software system, developers must identify the set of source code classes which need to be changed. Therefore, past effort has focused on predicting the set of classes impacted by a requirement. In this paper, we introduce and evaluate a new type of information based on the intuition that the set of requirements which are associated with historical changes to a specific class are likely to exhibit semantic similarity to new requirements which impact that class. This new Requirements to Requirements Set (R2RS) family of metrics captures the semantic similarity between a new requirement and the set of existing requirements previously associated with a class. The aim of this paper is to present and evaluate the usefulness of R2RS metrics in predicting the set of classes impacted by a requirement. We consider 18 different R2RS metrics by combining six natural language processing techniques to measure the semantic similarity among texts (e.g., VSM) and three distribution scores to compute overall similarity (e.g., average among similarity scores). We evaluate if R2RS is useful for predicting impacted classes in combination and against four other families of metrics that are based upon temporal locality of changes, direct similarity to code, complexity metrics, and code smells. Our evaluation features five classifiers and 78 releases belonging to four large open-source projects, which result in over 700,000 candidate impacted classes. Experimental results show that leveraging R2RS information increases the accuracy of predicting impacted classes practically by an average of more than 60 percent across the various classifiers and projects.",
      "Keywords": "Impact analysis | mining software repositories | traceability",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2020-04-01",
      "Publication type": "Review",
      "Authors": "Falessi, Davide;Roll, Justin;Guo, Jin L.C.;Cleland-Huang, Jane",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84908635627",
      "Primary study DOI": "10.1109/TSE.2014.2331057",
      "Title": "A Cooperative Parallel Search-Based Software Engineering Approach for Code-Smells Detection",
      "Abstract": "We propose in this paper to consider code-smells detection as a distributed optimization problem. The idea is that different methods are combined in parallel during the optimization process to find a consensus regarding the detection of code-smells. To this end, we used Parallel Evolutionary algorithms (P-EA) where many evolutionary algorithms with different adaptations (fitness functions, solution representations, and change operators) are executed, in a parallel cooperative manner, to solve a common goal which is the detection of code-smells. An empirical evaluation to compare the implementation of our cooperative P-EA approach with random search, two single population-based approaches and two code-smells detection techniques that are not based on meta-heuristics search. The statistical analysis of the obtained results provides evidence to support the claim that cooperative P-EA is more efficient and effective than state of the art detection approaches based on a benchmark of nine large open source systems where more than 85 percent of precision and recall scores are obtained on a variety of eight different types of code-smells.",
      "Keywords": "code-smells | distributed evolutionary algorithms | Search-based software engineering | software quality",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2014-09-01",
      "Publication type": "Article",
      "Authors": "Kessentini, Wael;Kessentini, Marouane;Sahraoui, Houari;Bechikh, Slim;Ouni, Ali",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85049034786",
      "Primary study DOI": "10.1016/j.jksuci.2018.06.005",
      "Title": "Reducing redundancy of test cases generation using code smell detection and refactoring",
      "Abstract": "In software development life cycle (SDLC), the testing phase is important to test the functionalities of any software. In this phase, test cases are generated to test software functionalities. This paper presents an approach on how to detect and refactor code smells from the source codes of an Android application in order to reduce the redundancy in test case generation. Refactoring is one of the vital activities in software development and maintenance. Refactoring is a process of code alteration that aims to make structural modifications to the source code without altering any external functionality. These changes often improve software quality such as readability, execution time and maintainability. The proposed approach is then implemented and evaluated in order to determine its effectiveness in reducing the redundancy of test case generation.",
      "Keywords": "Code smell detection | Duplicate code smell | Lazy class | Refactoring | Small method",
      "Publication venue": "Journal of King Saud University - Computer and Information Sciences",
      "Publication date": "2020-03-01",
      "Publication type": "Article",
      "Authors": "Ibrahim, Rosziati;Ahmed, Maryam;Nayak, Richi;Jamel, Sapiee",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85076861613",
      "Primary study DOI": "10.1016/j.jss.2019.110493",
      "Title": "On the performance of method-level bug prediction: A negative result",
      "Abstract": "Bug prediction is aimed at identifying software artifacts that are more likely to be defective in the future. Most approaches defined so far target the prediction of bugs at class/file level. Nevertheless, past research has provided evidence that this granularity is too coarse-grained for its use in practice. As a consequence, researchers have started proposing defect prediction models targeting a finer granularity (particularly method-level granularity), providing promising evidence that it is possible to operate at this level. Particularly, models mixing product and process metrics provided the best results. We present a study in which we first replicate previous research on method-level bug-prediction, by using different systems and timespans. Afterwards, based on the limitations of existing research, we (1) re-evaluate method-level bug prediction models more realistically and (2) analyze whether alternative features based on textual aspects, code smells, and developer-related factors can be exploited to improve method-level bug prediction abilities. Key results of our study include that (1) the performance of the previously proposed models, tested using the same strategy but on different systems/timespans, is confirmed; but, (2) when evaluated with a more practical strategy, all the models show a dramatic drop in performance, with results close to that of a random classifier. Finally, we find that (3) the contribution of alternative features within such models is limited and unable to improve the prediction capabilities significantly. As a consequence, our replication and negative results indicate that method-level bug prediction is still an open challenge.",
      "Keywords": "Defect prediction | Empirical software engineering | Mining software repositories",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2020-03-01",
      "Publication type": "Article",
      "Authors": "Pascarella, Luca;Palomba, Fabio;Bacchelli, Alberto",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85082663762",
      "Primary study DOI": "10.1145/3385032.3385042",
      "Title": "Prediction ofWeb Service Anti-patterns Using Aggregate Software Metrics and Machine Learning Techniques",
      "Abstract": "Service-Oriented Architecture(SOA) can be characterized as an approximately coupled engineering intended to meet the business needs of an association/organization. Service-Based Systems (SBSs) are inclined to continually change to enjoy new client necessities and adjust the execution settings, similar to some other huge and complex frameworks. These changes may lead to the evolution of designs/products with poor Quality of Service (QoS), resulting in the bad practiced solutions, commonly known as Anti-patterns. Anti-patterns makes the evolution and maintenance of the software systems hard and complex. Early identification of modules, classes, or source code regions where anti-patterns are more likely to occur can help in amending and maneuvering testing efforts leading to the improvement of software quality. In this work, we investigate the application of three sampling techniques, three feature selection techniques, and sixteen different classification techniques to develop the models for web service anti-pattern detection. We report the results of an empirical study by evaluating the approach proposed, on a data set of 226 Web Service Description Language(i.e., WSDL)files, a variety of five types of web-service anti-patterns. Experimental results demonstrated that SMOTE is the best performing data sampling techniques. The experimental results also reveal that the model developed by considering Uncorrelated Significant Predictors(SUCP) as the input obtained better performance compared to the model developed by other metrics. Experimental results also show that the Least Square Support Vector Machine with Linear(LSLIN) function has outperformed all other classifier techniques.",
      "Keywords": "Aggregation measures | Anti-pattern | Class imbalance distribution | Classifiers. | Feature selection | Machine learning | Service-based systems(sbs) | Source code metrics | Web-services | Wsdl",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2020-02-27",
      "Publication type": "Conference Paper",
      "Authors": "Tummalapalli, Sahithi;Kumar, Lov;Murthy, N. L.Bhanu",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85123043159",
      "Primary study DOI": "",
      "Title": "13th Innovations in Software Engineering Conference, ISEC 2020",
      "Abstract": "The proceedings contain 25 papers. The topics discussed include: understanding and improving continuous integration and delivery practice using data from the wild; control-flow based anomaly detection in the bug-fixing process of open-source projects; clustering glossary terms extracted from large-sized software requirements using fasttext; StaBL: statecharts with local variables; prediction of web service anti-patterns using aggregate software metrics and machine learning techniques; towards a model-driven product line engineering process: an industrial case study; HACO: a framework for developing human-AI teaming; and a preliminary study on case-based learning teaching pedagogy: scope in SE education.",
      "Keywords": "",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2020-02-27",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85082651159",
      "Primary study DOI": "10.1145/3385032.3385048",
      "Title": "Functionality Based Code Smell Detection and Severity Classification",
      "Abstract": "The Long Method code smell is a symptom of design defects caused by implementing multiple tasks within a single method. It limits reusability, evolvability and maintainability of a method. In this paper, we present a functionality based approach for detecting long methods. Functionalities are identified through a novel block based dependency analysis technique called Segmentation. It clusters sets of statements into extract method opportunities (or tasks). The approach uses interdependencies among various extract method opportunities identified within the method as a means to measure severity of the long method smell. The approach is validated over a Java based open source code. A comparison with expert's assessment shows that the approach is promising in detecting severe methods irrespective of their sizes.",
      "Keywords": "Code smell | Extract method opportunity | Long method smell severity | Refactoring | Segmentation",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2020-02-27",
      "Publication type": "Conference Paper",
      "Authors": "Tiwari, Omkarendra;Joshi, Rushikesh K.",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85084334943",
      "Primary study DOI": "10.1109/IWSC50091.2020.9047641",
      "Title": "An Empirical Study on Accidental Cross-Project Code Clones",
      "Abstract": "Software clones are considered a code smell in software development. While most clones occur due to developers copy - paste behaviour, some of them arise accidentally as a symptom of coding idioms. If such accidental clones occur across projects, then they may indicate a lack of abstraction in the underlying programming language or libraries. In this research, we study accidental cross-project clones from the perspective of missing abstraction. We discuss the six cases of frequent cross-project clones, three of them symptoms of missing language features (which have been resolved with the release of Java 7 and Java 12), and two of them symptoms of missing library features (which have not yet been addressed).",
      "Keywords": "clone detection | cross-project clones | repository mining",
      "Publication venue": "IWSC 2020 - Proceedings of the 2020 IEEE 14th International Workshop on Software Clones",
      "Publication date": "2020-02-01",
      "Publication type": "Conference Paper",
      "Authors": "Pyl, Mitchel;Van Bladel, Brent;Demeyer, Serge",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85083565109",
      "Primary study DOI": "10.1109/SANER48275.2020.9054821",
      "Title": "Are SonarQube Rules Inducing Bugs?",
      "Abstract": "The popularity of tools for analyzing Technical Debt, and particularly the popularity of SonarQube, is increasing rapidly. SonarQube proposes a set of coding rules, which represent something wrong in the code that will soon be reflected in a fault or will increase maintenance effort. However, our local companies were not confident in the usefulness of the rules proposed by SonarQube and contracted us to investigate the fault-proneness of these rules. In this work we aim at understanding which SonarQube rules are actually fault-prone and to understand which machine learning models can be adopted to accurately identify fault-prone rules. We designed and conducted an empirical study on 21 well-known mature open-source projects. We applied the SZZ algorithm to label the fault-inducing commits. We analyzed the fault-proneness by comparing the classification power of seven machine learning models. Among the 202 rules defined for Java by SonarQube, only 25 can be considered to have relatively low fault-proneness. Moreover, violations considered as 'bugs' by SonarQube were generally not fault-prone and, consequently, the fault-prediction power of the model proposed by SonarQube is extremely low. The rules applied by SonarQube for calculating technical debt should be thoroughly investigated and their harmfulness needs to be further confirmed. Therefore, companies should carefully consider which rules they really need to apply, especially if their goal is to reduce fault-proneness.",
      "Keywords": "architectural smells | code smells | coding style | machine learning | SonarQube | static analysis | Technical Debt",
      "Publication venue": "SANER 2020 - Proceedings of the 2020 IEEE 27th International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2020-02-01",
      "Publication type": "Conference Paper",
      "Authors": "Lenarduzzi, Valentina;Lomio, Francesco;Huttunen, Heikki;Taibi, Davide",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85083563335",
      "Primary study DOI": "10.1109/SANER48275.2020.9054841",
      "Title": "Mining Version Control Systems and Issue Trackers with LibVCS4j",
      "Abstract": "Mining version control systems, such as Git, Mercurial, and Subversion, is a good way to analyze different aspects of software evolution. Several previous studies, for example, analyzed how cloning in software evolves by tracking findings through the change history of source code files. By linking in issues (reported and discussed in issue tracking systems), results can be further refined - for instance, identifying commits that removed long-lived clones and fixed known issues. However, due to the absence of suitable tools, most of these studies implemented their own data extraction approaches. In this paper we present LibVCS4j, a Java programming library which can be integrated into existing analysis tools - for example, tools that detect code smells in source code files - to i) iterate through the history of software repositories, ii) run the desired analysis on a selected set of revisions, and iii) fetch additional information (commit messages, file differences, and so on) while processing a repository. The library unites different version control systems and issue tracking systems under a common interface and, thus, is well suited to decouple data analysis from data extraction.",
      "Keywords": "",
      "Publication venue": "SANER 2020 - Proceedings of the 2020 IEEE 27th International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2020-02-01",
      "Publication type": "Conference Paper",
      "Authors": "Steinbeck, Marcel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85081621763",
      "Primary study DOI": "10.4230/OASIcs.Microservices.2017-2019.3",
      "Title": "Exploring maintainability assurance research for service- And microservice-based systems: Directions and differences",
      "Abstract": "To ensure sustainable software maintenance and evolution, a diverse set of activities and concepts like metrics, change impact analysis, or antipattern detection can be used. Special maintainability assurance techniques have been proposed for service- and microservice-based systems, but it is difficult to get a comprehensive overview of this publication landscape. We therefore conducted a systematic literature review (SLR) to collect and categorize maintainability assurance approaches for service-oriented architecture (SOA) and microservices. Our search strategy led to the selection of 223 primary studies from 2007 to 2018 which we categorized with a threefold taxonomy: a) architectural (SOA, microservices, both), b) methodical (method or contribution of the study), and c) thematic (maintainability assurance subfield). We discuss the distribution among these categories and present different research directions as well as exemplary studies per thematic category. The primary finding of our SLR is that, while very few approaches have been suggested for microservices so far (24 of 223, ∼11%), we identified several thematic categories where existing SOA techniques could be adapted for the maintainability assurance of microservices.",
      "Keywords": "And phrases Maintainability | Microservices | Quality Assurance | Service-Based Systems | SOA | Software Evolution | Systematic Literature Review",
      "Publication venue": "OpenAccess Series in Informatics",
      "Publication date": "2020-02-01",
      "Publication type": "Conference Paper",
      "Authors": "Bogner, Justus;Weller, Adrian;Wagner, Stefan;Zimmermann, Alfred",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85081170864",
      "Primary study DOI": "10.1117/12.2550500",
      "Title": "Software smell detection based on machine learning and its empirical study",
      "Abstract": "As an important maintenance measure, software reconfiguration is the key to detect the unreasonable part of the code module, namely code smell. Traditional detection methods rely on the experience of engineers, and the location efficiency of reconfiguration points is low. The existing automatic detection tools identify code smell with limited accuracy. Aiming at the problem that the number of reconstructed points in software system is huge and various, and the automation of reconstructed activities is low and difficult to optimize, the research framework of software smell prediction based on machine learning is studied and designed. Taking four common code smells as the research object, the classification algorithm and detection model of the best code smell are established, and the dimension reduction method of feature extraction is further improved. The highest accuracy rate is 89.8%, which can improve the automation level of software smell detection.",
      "Keywords": "Code smell | Machine learning | Software refactoring",
      "Publication venue": "Proceedings of SPIE - The International Society for Optical Engineering",
      "Publication date": "2020-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Yin, Yongfeng;Su, Qingran;Liu, Lijun",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-77950599865",
      "Primary study DOI": "10.1109/QSIC.2009.47",
      "Title": "A bayesian approach for the detection of code and design smells",
      "Abstract": "The presence of code and design smells can have a severe impact on the quality of a program. Consequently, their detection and correction have drawn the attention of both researchers and practitioners who have proposed various approaches to detect code and design smells in programs. However, none of these approaches handle the inherent uncertainty of the detection process. We propose a Bayesian approach to manage this uncertainty. First, we present a systematic process to convert existing state-of-the-art detection rules into a probabilistic model. We illustrate this process by generating a model to detect occurrences of the Blob antipattern. Second, we present results of the validation of the model: we built this model on two open-source programs, GanttProject v1.10.2 and Xerces v2.7.0, and measured its accuracy. Third, we compare our model with another approach to show that it returns the same candidate classes while ordering them to minimise the quality analysts' effort. Finally, we show that when past detection results are available, our model can be calibrated using machine learning techniques to offer an improved, context-specfic detection. © 2009 IEEE.",
      "Keywords": "",
      "Publication venue": "Proceedings - International Conference on Quality Software",
      "Publication date": "2009-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Khomh, Foutse;Vaucher, Stéephane;Guéehéeneuc, Yann Gaël;Sahraoui, Houari",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85083433363",
      "Primary study DOI": "10.1007/978-3-030-34706-2_5",
      "Title": "Software defect prediction using bad code smells: A systematic literature review",
      "Abstract": "The challenge of effective refactoring in the software development cycle brought forward the need to develop automated defect prediction models. Among many existing indicators of bad code, code smells have attracted particular interest of both the research community and practitioners in recent years. In this paper, we describe the current state-of-the-art in the field of bug prediction with the use of code smells and attempt to identify areas requiring further research. To achieve this goal, we conducted a systematic literature review of 27 research papers published between 2006 and 2019. For each paper, we (i) analysed the reported relationship between smelliness and bugginess, as well as (ii) evaluated the performance of code smell data used as a defect predictor in models developed using machine learning techniques. Our investigation confirms that code smells are both positively correlated with software defects and can positively influence the performance of fault detection models. However, not all types of smells and smell-related metrics are equally useful. God Class, God Method, Message Chains smells and Smell intensity metric stand out as particularly effective. Smells such as Inappropriate Intimacy, Variable Re-assign, Clones, Middle Man or Speculative Generality require further research to confirm their contribution. Metrics describing the introduction and evolution of anti-patterns in code present a promising opportunity for experimentation.",
      "Keywords": "",
      "Publication venue": "Lecture Notes on Data Engineering and Communications Technologies",
      "Publication date": "2020-01-01",
      "Publication type": "Book Chapter",
      "Authors": "Piotrowski, Paweł;Madeyski, Lech",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85083976203",
      "Primary study DOI": "10.1109/Confluence47617.2020.9058138",
      "Title": "Using software metrics to detect temporary field code smell",
      "Abstract": "Code smell is a characteristic of the source code which indicates some serious problem in the code which might affect the quality of the source code. There exists a list of 22 code smells as defined by Martin Fowler. But all these code smells have not been worked upon. Temporary field code smell is one of them, which has not been considered for its detection and refactoring. In this paper, we have reconstructed a motivating example of object oriented JAVA code that indicates the impact of code smell and need to remove temporary field based on metrics and rules.We have proposed a method to detect temporary field code smell based on software metrics derived from data flow and control flow graphs. We also proposed the process of refactoring the code to improve the maintainability. Analysis of results has shown that NFM, NMN, NCF metrics can help to detect Temporary field code smell. Extract class is more appropriate refactoring technique than parameter passing to remove Temporary Field code smell.",
      "Keywords": "Code Smell | Extract Class | Software Maintainability | Software Metrics | Temporary field",
      "Publication venue": "Proceedings of the Confluence 2020 - 10th International Conference on Cloud Computing, Data Science and Engineering",
      "Publication date": "2020-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Gupta, Ruchin;Singh, Sandeep Kumar",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85047195749",
      "Primary study DOI": "10.1109/TSE.2018.2838131",
      "Title": "Use and Misuse of Continuous Integration Features: An Empirical Study of Projects That (Mis)Use Travis CI",
      "Abstract": "Continuous Integration (CI) is a popular practice where software systems are automatically compiled and tested as changes appear in the version control system of a project. Like other software artifacts, CI specifications require maintenance effort. Although there are several service providers like Travis CI offering various CI features, it is unclear which features are being (mis)used. In this paper, we present a study of feature use and misuse in 9,312 open source systems that use Travis CI. Analysis of the features that are adopted by projects reveals that explicit deployment code is rare - 48.16 percent of the studied Travis CI specification code is instead associated with configuring job processing nodes. To analyze feature misuse, we propose Hansel - an anti-pattern detection tool for Travis CI specifications. We define four anti-patterns and Hansel detects anti-patterns in the Travis CI specifications of 894 projects in the corpus (9.60 percent), and achieves a recall of 82.76 percent in a sample of 100 projects. Furthermore, we propose Gretel - an anti-pattern removal tool for Travis CI specifications, which can remove 69.60 percent of the most frequently occurring anti-pattern automatically. Using Gretel, we have produced 36 accepted pull requests that remove Travis CI anti-patterns automatically.",
      "Keywords": "anti-patterns | Continuous integration | mining software repositories",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2020-01-01",
      "Publication type": "Article",
      "Authors": "Gallaba, Keheliya;McIntosh, Shane",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85083831268",
      "Primary study DOI": "10.1109/ACCESS.2020.2981742",
      "Title": "Improving the Security of UML Sequence Diagram Using Genetic Algorithm",
      "Abstract": "A sequence diagram is a modeling approach for visualizing the behavioral execution of a system. The objective of this research is to investigate the problem of security in a behavioral model (sequence diagram) through the application of model refactoring. We propose detection and correction techniques, empirical evaluation of the proposed techniques and assessment of security improvement in sequence diagrams. The detection of security bad smells is achieved through the adaptation of a genetic algorithm, while correction is accomplished by the model transformation approach. The results show significant detection recall and correction efficacy of the proposed detection and correction approaches, respectively. Our results show that the proposed approach is effective in detecting and correcting bad smells and can improve the security of UML Sequence Diagram.",
      "Keywords": "genetic algorithm | security bad smells | software metrics | software refactoring | Software security",
      "Publication venue": "IEEE Access",
      "Publication date": "2020-01-01",
      "Publication type": "Article",
      "Authors": "Alshayeb, Mohammad;Mumtaz, Haris;Mahmood, Sajjad;Niazi, Mahmood",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85082133117",
      "Primary study DOI": "10.1007/s11219-020-09502-5",
      "Title": "A preliminary analysis of self-adaptive systems according to different issues",
      "Abstract": "Self-adaptive systems dynamically change their structure and behavior in response to changes in their execution environment to ensure the quality of the services they provide. Self-adaptive systems are usually built of a managed part, which implements their functionality, and a managing part, which implements the self-adaptive mechanisms. Hence, the complexity of self-adaptive systems results also from the existence of the managing part and the interaction between the managed and the managing parts. The available evaluation approaches of self-adaptive systems focus on their performances, i.e., on the benefits (e.g., degree of autonomy, support for detecting anomalous behavior, adaptivity time, quality of response) achieved through the self-adaptive mechanisms of the managing part. In this paper, we evaluate the quality of the design of self-adaptive systems (including the managed and the managing parts) as it is done in traditional software engineering. We are interested in the internal software quality of self-adaptive systems, as the existence of the managing part and its interaction with the managed part leads to a tightly coupled system. We analyze the self-adaptive systems through the detection of different issues such as architectural and code smells and the detection of design patterns. The smells provide some hints on possible design and implementation problems, and help software engineers to improve the quality of the systems. While, design patterns are usually indicators of the application of good practices in the software development and allow to capture part of the design rationale. In this way, they can help software engineers to understand, reuse, and extend self-adaptive systems. In this paper, we have considered the detection of 3 architectural smells, 18 code smells, and 15 design patterns in 11 self-adaptive systems written in the Java programming language. The results indicate that the 3 architectural smells, 9 out of the 18 code smells, and the 15 design patterns have been detected in all the analyzed self-adaptive systems. We also discuss the possible reasons behind the presence of these quality issues, and provide our lessons learned.",
      "Keywords": "Architectural smells | Code smells | Design patterns | Self-adaptive systems | Software quality",
      "Publication venue": "Software Quality Journal",
      "Publication date": "2020-09-01",
      "Publication type": "Article",
      "Authors": "Raibulet, Claudia;Arcelli Fontana, Francesca;Carettoni, Simone",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85081370147",
      "Primary study DOI": "10.1007/978-981-15-0751-9_75",
      "Title": "A Support Vector Machine Based Approach for Effective Fault Localization",
      "Abstract": "Software maintenance is one of the most costly activities in software life cycle. It costs almost 70% of the total cost of the software. Testing aims to reveal the faults from the software under test (SUT). The fault localization is tiresome, dull, costly but crucial for program debugging. As size and complexity of software increase, manual locating faults becomes very tedious and hence necessitates automatic fault localization. If the fault proneness of the software components can be predicted, then such components may be given more focus. Such approach would not only save time but also enhance the quality of the software. Support vector machine (SVM) is a prominent machine learning algorithm. Regularization of parameters, convex optimization and kernel tricks are the prevailing features of SVM. This work reports a SVM-based framework for fault localization on the basis of code smells. Paper presents a performance analysis against four popular algorithms, namely ZeroR, OneR, Naive Bayes and Decision Stump. The proposed model is empirically evaluated in the reference of Json project. The results of the experimentation show that the proposed model can effectively classify the instances in the classes of their respective categories of code smells. Also, the kernel used in the proposed model gives better performance than counterpart kernels and the proposed model itself performs better than the other compared algorithms in terms of accuracy, precision, recall and F-measure.",
      "Keywords": "Fault localization | Software | Support vector machine",
      "Publication venue": "Advances in Intelligent Systems and Computing",
      "Publication date": "2020-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Tanwar, Neha;Singh, Ajmer;Singh, Rajvir",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85085338430",
      "Primary study DOI": "10.5373/JARDCS/V12I4/20201438",
      "Title": "A model for diagnosing the largest number of android application problems, based on reviews in download stores by use of the decision tree",
      "Abstract": "Mobile applications play a pivotal role in the daily life of the user, where millions of customers rely on smartphone applications for the purpose of social networking, banking, news and many other uses, with the ability to use it anytime and anywhere and in most conditions. So that software engineers‟ race to Create and provide the applications to customer service. However, despite good planning processes for software engineers in designing and building applications, the product may be accompanied by some errors that may lead to a malfunction in the application or one of its functions, which requires performing maintenance for it, and often that difficult and costly, in addition to the possibility of repeating the process whenever new problems in the application are discovered. It is therefore essential to look for ways to diagnose application performance issues and detect as many errors in Android apps as possible to avoid repetitive maintenance, and focus on tools that help build good apps with minimal \"Effort\", \"Time\" and \"Cost”. This paper contributes to the detection of errors and performance problems in Android applications and the diagnosis of problems by used the user‟s feedback within download platforms.",
      "Keywords": "Android | Code Smells | Feedback | Machine Learning | Mobile Applications | Software Engineering",
      "Publication venue": "Journal of Advanced Research in Dynamical and Control Systems",
      "Publication date": "2020-01-01",
      "Publication type": "Article",
      "Authors": "Mohsen, Raed Kazem;Abbas, Ahmed Saleem",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85064652692",
      "Primary study DOI": "10.1007/s10664-019-09703-y",
      "Title": "iPerfDetector: Characterizing and detecting performance anti-patterns in iOS applications",
      "Abstract": "Performance issues in mobile applications (i.e., apps) often have a direct impact on the user experience. However, due to limited testing resources and fast-paced software development cycles, many performance issues remain undiscovered when the apps are released. As found by a prior study, these performance issues are one of the most common complaints that app users have. Unfortunately, there is a limited support to help developers avoid or detect performance issues in mobile apps. In this paper, we conduct an empirical study on performance issues in iOS apps written in Swift language. To the best of our knowledge, this is the first study on performance issues of apps on the iOS platform. We manually studied 225 performance issues that are collected from four open source iOS apps. We found that most performance issues in iOS apps are related to inefficient UI design, memory issues, and inefficient thread handling. We also manually uncovered four performance anti-patterns that recurred in the studied issue reports. To help developers avoid these performance anti-patterns in the code, we implemented a static analysis tool called iPerfDetector. We evaluated iPerfDetector on eight open source and three commercial apps. iPerfDetector successfully detected 34 performance anti-pattern instances in the studied apps, where 31 of them are already confirmed and accepted by developers as potential performance issues. Our case study on the performance impact of the anti-patterns shows that fixing the anti-pattern may improve the performance (i.e., response time, GPU, or CPU) of the workload by up to 80%.",
      "Keywords": "Anti-pattern detection | Empirical study | iOS apps | Performance | Refactoring",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2019-12-01",
      "Publication type": "Article",
      "Authors": "Afjehei, Sara Seif;Chen, Tse Hsun (Peter);Tsantalis, Nikolaos",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85078118145",
      "Primary study DOI": "10.1109/IWESEP49350.2019.00017",
      "Title": "Studying Software Engineering Patterns for Designing Machine Learning Systems",
      "Abstract": "Machine-learning (ML) techniques are becoming more prevalent. ML techniques rely on mathematics and software engineering. Researchers and practitioners studying best practices strive to design ML systems and software that address software complexity and quality issues. Such design practices are often formalized as architecture and design patterns by encapsulating reusable solutions to common problems within given contexts. However, a systematic study to collect, classify, and discuss these software-engineering (SE) design patterns for ML techniques have yet to be reported. Our research collects good/bad SE design patterns for ML techniques to provide developers with a comprehensive classification of such patterns. Herein we report the preliminary results of a systematic-literature review (SLR) of good/bad design patterns for ML.",
      "Keywords": "Anti-patterns | Architecture Patterns | Design Patterns | Machine Learning | ML Patterns",
      "Publication venue": "Proceedings - 2019 10th International Workshop on Empirical Software Engineering in Practice, IWESEP 2019",
      "Publication date": "2019-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Washizaki, Hironori;Uchida, Hiromu;Khomh, Foutse;Guéhéneuc, Yann Gaël",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85083034948",
      "Primary study DOI": "10.1109/INDICON47234.2019.9029090",
      "Title": "An effective web service anti-pattern prediction model using SMOTE",
      "Abstract": "An anti-pattern is a common response to a recurring problem that is usually ineffective and risks being highly counterproductive. In this work, we empirically investigate the association between the occurrence of four different types of anti-patterns and source code metrics. SMOTE is being used for data sampling as the dataset considered is imbalanced. Principle component analysis and Rough set analysis are applied for feature extraction and selection. The features selected from this two techniques along with the significant features(SIGF) are considered as input for building the predictive models for the detection of antipatterns. The effectiveness of these techniques are evaluated using Logistic Regression(LOGR), Decision Tree(DT) and Least Square Support Vector Machine(LSSVM) with three different kernels:Linear(LSVVML), Polynomial(LSSVMP) and Radbas(LSSVMR). Experimental results reveal that the model developed using SMOTE is yielding better results when compared to the models developed with the original dataset. Furthermore, we also observe that the predictive model developed using LSSVM with linear and polynomial is more effective than the models developed using other classifier techniques.",
      "Keywords": "Anti-pattern | Service oriented architecture | Software Engineering | Source code metrics | Web services",
      "Publication venue": "2019 IEEE 16th India Council International Conference, INDICON 2019 - Symposium Proceedings",
      "Publication date": "2019-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Tummalapalli, Sahithi;Kumar, Lov;Murthy Neti, Lalita Bhanu",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85068122639",
      "Primary study DOI": "10.1007/s10664-019-09727-4",
      "Title": "An empirical study on quality of Android applications written in Kotlin language",
      "Abstract": "Context: During the last years, developers of mobile applications have the possibility to use new paradigms and tools for developing mobile applications. For instance, since 2017, Android developers have the official support to write Android applications using Kotlin language. Kotlin is programming language fully interoperable with Java that combines object-oriented and functional features. Objective: The goal of this paper is twofold. First, it aims to study the degree of adoption of Kotlin language on the development of open-source Android applications and to measure the amount of Kotlin code inside those applications. Secondly, it aims to measure the quality of Android applications that are written using Kotlin and to compare it with the quality of Android applications written using Java. Method: We first defined a method to detect Kotlin applications from a dataset of open-source Android applications. Then, we analyzed those applications to detect instances of code smells and computed an estimation of the quality of the applications. Finally, we studied how the introduction of Kotlin code impacts on the quality of an Android application. Results: Our experiment found that 11.26% of applications from a dataset with 2,167 open-source applications have been written (partially or fully) using Kotlin language. We found that the introduction of Kotlin code increases the quality, in terms of the presence of 10 different code smells studied, 4 object-oriented and 6 Android, of the majority of the Android applications initially written in Java.",
      "Keywords": "Android | Code evolution | Code smells | Java | Kotlin | Mobile development | Open-Source applications | Quality",
      "Publication venue": "Empirical Software Engineering",
      "Publication date": "2019-12-01",
      "Publication type": "Article",
      "Authors": "Góis Mateus, Bruno;Martinez, Matias",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85080146937",
      "Primary study DOI": "10.1109/FIT47737.2019.00040",
      "Title": "Software design patterns recommendation: A systematic literature review",
      "Abstract": "Design patterns are essential for development. It provides reusability, modularization, quality and consistency between design and implementation. Expert knowledge required for the selection of correct design patterns. Moreover, the recommendation of design patterns is an important task for software development. Many tools and techniques were proposed to recommend design patterns. The main objective of this research is to perform a systematic literature review based on Kitchenham guidelines. We have analyzed the studies with different perspectives: Study objective, validation, recommendation technique for design pattern and limitation of the study. We have considered the research publication from 2010 to 2019 and examined them on four electronic databases. A total of 22 studies are identified from 2010-2019 and classified them on predefined classification criteria. Overall research findings concluded that anti-pattern detection and selection and fuzzy technique are most widely used but still there is a research gap that exists from the recent 2-years. It was observed that the traditional and most practiced recommendation techniques are not used in software design patterns recommendation.",
      "Keywords": "Design patterns | Pattern detection | Pattern recommendation | Pattern selection | Systematic literature review",
      "Publication venue": "Proceedings - 2019 International Conference on Frontiers of Information Technology, FIT 2019",
      "Publication date": "2019-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Asghar, Muhammad Zeeshan;Alam, Khubaib Amjad;Javed, Shahzeb",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85070497621",
      "Primary study DOI": "10.1016/j.infsof.2019.08.005",
      "Title": "The usefulness of software metric thresholds for detection of bad smells and fault prediction",
      "Abstract": "Context: Software metrics may be an effective tool to assess the quality of software, but to guide their use it is important to define their thresholds. Bad smells and fault also impact the quality of software. Extracting metrics from software systems is relatively low cost since there are tools widely used for this purpose, which makes feasible applying software metrics to identify bad smells and to predict faults. Objective: To inspect whether thresholds of object-oriented metrics may be used to aid bad smells detection and fault predictions. Method: To direct this research, we have defined three research questions (RQ), two related to identification of bad smells, and one for identifying fault in software systems. To answer these RQs, we have proposed detection strategies for the bad smells: Large Class, Long Method, Data Class, Feature Envy, and Refused Bequest, based on metrics and their thresholds. To assess the quality of the derived thresholds, we have made two studies. The first one was conducted to evaluate their efficacy on detecting these bad smells on 12 systems. A second study was conducted to investigate for each of the class level software metrics: DIT, LCOM, NOF, NOM, NORM, NSC, NSF, NSM, SIX, and WMC, if the ranges of values determined by thresholds are useful to identify fault in software systems. Results: Both studies confirm that metric thresholds may support the prediction of faults in software and are significantly and effective in the detection of bad smells. Conclusion: The results of this work suggest practical applications of metric thresholds to identify bad smells and predict faults and hence, support software quality assurance activities.Their use may help developers to focus their efforts on classes that tend to fail, thereby minimizing the occurrence of future problems.",
      "Keywords": "Bad smell | Detection strategies | Fault prediction | Software metrics | Software quality | Thresholds",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2019-11-01",
      "Publication type": "Article",
      "Authors": "Bigonha, Mariza A.S.;Ferreira, K.;Souza, Priscila;Sousa, B.;Januário, Marcela;Lima, Daniele",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85032171412",
      "Primary study DOI": "10.11591/ijece.v7i6.pp3613-3621",
      "Title": "Finding bad code smells with neural network models",
      "Abstract": "Code smell refers to any symptom introduced in design or implementation phases in the source code of a program. Such a code smell can potentially cause deeper and serious problems during software maintenance. The existing approaches to detect bad smells use detection rules or standards using a combination of different object-oriented metrics. Although a variety of software detection tools have been developed, they still have limitations and constraints in their capabilities. In this paper, a code smell detection system is presented with the neural network model that delivers the relationship between bad smells and object-oriented metrics by taking a corpus of Java projects as experimental dataset. The most well-known object-oriented metrics are considered to identify the presence of bad smells. The code smell detection system uses the twenty Java projects which are shared by many users in the GitHub repositories. The dataset of these Java projects is partitioned into mutually exclusive training and test sets. The training dataset is used to learn the network model which will predict smelly classes in this study. The optimized network model will be chosen to be evaluated on the test dataset. The experimental results show when the modelis highly trained with more dataset, the prediction outcomes are improved more and more. In addition, the accuracy of the model increases when it performs with higher epochs and many hidden layers.",
      "Keywords": "Code smells | Neural networks | Object-oriented metrics | Software maintenance",
      "Publication venue": "International Journal of Electrical and Computer Engineering",
      "Publication date": "2017-12-01",
      "Publication type": "Article",
      "Authors": "Kim, Dong Kwan",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85044791951",
      "Primary study DOI": "10.1109/TSE.2018.2821670",
      "Title": "Bellwethers: A Baseline Method for Transfer Learning",
      "Abstract": "Software analytics builds quality prediction models for software projects. Experience shows that (a) the more projects studied, the more varied are the conclusions; and (b) project managers lose faith in the results of software analytics if those results keep changing. To reduce this conclusion instability, we propose the use of 'bellwethers': given N projects from a community the bellwether is the project whose data yields the best predictions on all others. The bellwethers offer a way to mitigate conclusion instability because conclusions about a community are stable as long as this bellwether continues as the best oracle. Bellwethers are also simple to discover (just wrap a for-loop around standard data miners). When compared to other transfer learning methods (TCA+, transfer Naive Bayes, value cognitive boosting), using just the bellwether data to construct a simple transfer learner yields comparable predictions. Further, bellwethers appear in many SE tasks such as defect prediction, effort estimation, and bad smell detection. We hence recommend using bellwethers as a baseline method for transfer learning against which future work should be compared.",
      "Keywords": "bad smells | defect prediction | effort estimation | issue close time | prediction | Transfer learning",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-11-01",
      "Publication type": "Article",
      "Authors": "Krishna, Rahul;Menzies, Tim",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85078951728",
      "Primary study DOI": "10.1109/ASE.2019.00095",
      "Title": "Active hotspot: An issue-oriented model to monitor software evolution and degradation",
      "Abstract": "Architecture degradation has a strong negative impact on software quality and can result in significant losses. Severe software degradation does not happen overnight. Software evolves continuously, through numerous issues, fixing bugs and adding new features, and architecture flaws emerge quietly and largely unnoticed until they grow in scope and significance when the system becomes difficult to maintain. Developers are largely unaware of these flaws or the accumulating debt as they are focused on their immediate tasks of address individual issues. As a consequence, the cumulative impacts of their activities, as they affect the architecture, go unnoticed. To detect these problems early and prevent them from accumulating into severe ones we propose to monitor software evolution by tracking the interactions among files revised to address issues. In particular, we propose and show how we can automatically detect active hotspots, to reveal architecture problems. We have studied hundreds of hotspots along the evolution timelines of 21 open source projects and showed that there exist just a few dominating active hotspots per project at any given time. Moreover, these dominating active hotspots persist over long time periods, and thus deserve special attention. Compared with state-of-the-art design and code smell detection tools we report that, using active hotspots, it is possible to detect signs of software degradation both earlier and more precisely.",
      "Keywords": "Architecture debt | Software evolution",
      "Publication venue": "Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019",
      "Publication date": "2019-11-01",
      "Publication type": "Conference Paper",
      "Authors": "Feng, Qiong;Cai, Yuanfang;Kazman, Rick;Cui, Di;Liu, Ting;Fang, Hongzhou",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85078017712",
      "Primary study DOI": "10.1109/ISMSIT.2019.8932855",
      "Title": "Comparison of Multi-Label Classification Algorithms for Code Smell Detection",
      "Abstract": "Code smells in a source code shows the weakness of design or implementation. To detect code smells, several detection tools have been developed. However, these tools generally produce different results, since code smells are subjectively interpreted, informally defined and configured by the developers, domain-dependent and based on opinions and experiences. To cope with these issues, in this paper, we have used machine learning techniques, especially multi-label classification methods, to classify whether the given source code is affected with more than one code smells or not. We have conducted experiments on four code smell datasets and transformed them into two multi-label datasets (one for method level and the other one for class level). Two multi-label classification methods (Classifier Chains and Label Combination) and their ensemble models performed on the converted datasets using five different base classifiers. The results show that, as a base classifier, Random Forest algorithm performs better than Decision Tree, Naive Bayes, Support Vector Machine and Neural Network algorithms.",
      "Keywords": "code smell detection | machine learning | multi-label classification | software engineering",
      "Publication venue": "3rd International Symposium on Multidisciplinary Studies and Innovative Technologies, ISMSIT 2019 - Proceedings",
      "Publication date": "2019-10-01",
      "Publication type": "Conference Paper",
      "Authors": "Kiyak, Elife Ozturk;Birant, Derya;Birant, Kokten Ulas",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85076762810",
      "Primary study DOI": "10.1145/3359061.3361076",
      "Title": "Improving performance and quality of database-backed software",
      "Abstract": "Modern web applications have stringent latency requirements while processing an ever-increasing amount of user data. To address these challenges and improve programmer productivity, Object Relational Mapping (ORM) frameworks have been developed to allow developers writing database processing code in an object-oriented manner. Despite such frameworks, prior work found that developers still struggle in developing ORM-based web applications. This paper presents a series of study and developed tools for optimizing web applications developed using the Ruby on Rails ORM. Using automated static analysis, we detect ORM related inefficiency problems and suggests fixes to developers. Our evaluation on 12 real-world applications shows that more than 1000 performance issues can be detected and fixed.",
      "Keywords": "Performance anti-patterns, database-backed applications",
      "Publication venue": "SPLASH Companion 2019 - Proceedings Companion of the 2019 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity",
      "Publication date": "2019-10-20",
      "Publication type": "Conference Paper",
      "Authors": "Yang, Junwen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-28444473551",
      "Primary study DOI": "10.1016/j.entcs.2005.02.059",
      "Title": "Adaptive detection of design flaws",
      "Abstract": "Criteria for software quality measurement depend on the application area. In large software systems criteria like maintainability, comprehensibility and extensibility play an important role. My aim is to identify design flaws in software systems automatically and thus to avoid \"bad\" - incomprehensible, hardly expandable and changeable - program structures. Depending on the perception and experience of the searching engineer, design flaws are interpreted in a different way. I propose to combine known methods for finding design flaws on the basis of metrics with machine learning mechanisms, such that design flaw detection is adaptable to different views. This paper presents the underlying method, describes an analysis tool for Java programs and shows results of an initial case study. © 2005 Elsevier B.V. All rights reserved.",
      "Keywords": "Code smell | Design flaw | Machine learning | Object-oriented design | Program analysis | Refactoring | Software quality",
      "Publication venue": "Electronic Notes in Theoretical Computer Science",
      "Publication date": "2005-12-12",
      "Publication type": "Conference Paper",
      "Authors": "Kreimer, Jochen",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85073920491",
      "Primary study DOI": "10.35940/ijitee.L2846.1081219",
      "Title": "Restructuring with Moora and measuring code smells",
      "Abstract": "The paper presents measuring various code smells by finding critical code smells and thereby concentration is increased in those parts through Structural Modeling for arranging those code smells. Arranging the code smells in the way that they will not produce a new smell on their detection and removal is very necessary. Structural modeling helps in clarifying Interrelationship among these code smells. The code smells that contains high driving effects are ordered as optimized code which resulted in the increase in the overall code maintenance of the software code which will be used afterwards for achieving the concept of re-usability. In addition to this we have added a technique for restructuring technology for the purpose to achieve high accuracy. It involves more objectives related to the performance and the code smells are implemented with the concept called as pairwise analysis based on the priority method. Pairwise analysis based on the weights attained by the bad smells provides a better optimized results since more problematic areas are neglected here. This work gives optimized results for the process of overall code maintainability by applying restructuring before the refactoring process with Fuzzy technique and it is followed by finding the code smells which results in high ripple effects and then removing them. Still more research ideologies are needed for removing the bad smells in the code.",
      "Keywords": "FODA | Fuzzy | MOORA",
      "Publication venue": "International Journal of Innovative Technology and Exploring Engineering",
      "Publication date": "2019-10-01",
      "Publication type": "Article",
      "Authors": "Pandiyavathi, T.;Manochandar, T.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85071552621",
      "Primary study DOI": "10.1109/TSE.2019.2936376",
      "Title": "Deep Learning Based Code Smell Detection",
      "Abstract": "Code smells are structures in the source code that suggest the possibility of refactorings. Consequently, developers may identify refactoring opportunities by detecting code smells. However, manual identification of code smells is challenging and tedious. To this end, a number of approaches have been proposed to identify code smells automatically or semi-automatically. Most of such approaches rely on manually designed heuristics to map manually selected source code metrics into predictions. However, it is challenging to manually select the best features. It is also difficult to manually construct the optimal heuristics. To this end, in this paper we propose a deep learning based novel approach to detecting code smells. The key insight is that deep neural networks and advanced deep learning techniques could automatically select features of source code for code smell detection, and could automatically build the complex mapping between such features and predictions. A big challenge for deep learning based smell detection is that deep learning often requires a large number of labeled training data (to tune a large number of parameters within the employed deep neural network) whereas existing datasets for code smell detection are rather small. To this end, we propose an automatic approach to generating labeled training data for the neural network based classifier, which does not require any human intervention. As an initial try, we apply the proposed approach to four common and well-known code smells, i.e., feature envy, long method, large class, and misplaced class. Evaluation results on open-source applications suggest that the proposed approach significantly improves the state-of-the-art.",
      "Keywords": "code smells | deep learning | identification | quality | Software refactoring",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2021-09-01",
      "Publication type": "Article",
      "Authors": "Liu, Hui;Jin, Jiahao;Xu, Zhifeng;Zou, Yanzhen;Bu, Yifan;Zhang, Lu",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85044354578",
      "Primary study DOI": "10.1109/TSE.2018.2819180",
      "Title": "UniDoSA: The Unified Specification and Detection of Service Antipatterns",
      "Abstract": "Service-based Systems (SBSs) are developed on top of diverse Service-Oriented Architecture (SOA) technologies or architectural styles. Like any other complex systems, SBSs face both functional and non-functional changes at the design or implementation-level. Such changes may degrade the design quality and quality of service (QoS) of the services in SBSs by introducing poor solutions-service antipatterns. The presence of service antipatterns in SBSs may hinder the future maintenance and evolution of SBSs. Assessing the quality of design and QoS of SBSs through the detection of service antipatterns may ease their maintenance and evolution. However, the current literature lacks a unified approach for modelling and evaluating the design of SBSs in term of design quality and QoS. To address this lack, this paper presents a meta-model unifying the three main service technologies: REST, SCA, and SOAP. Using the meta-model, it describes a unified approach, UniDoSA (Unified Specification and Detection of Service Antipatterns), supported by a framework, SOFA (Service Oriented Framework for Antipatterns), for modelling and evaluating the design quality and QoS of SBSs. We apply and validate UniDoSA on: (1) 18 RESTful APIs, (2) two SCA systems with more than 150 services, and (3) more than 120 SOAP Web services. With a high precision and recall, the detection results provide evidence of the presence of service antipatterns in SBSs, which calls for future studies of their impact on QoS.",
      "Keywords": "Antipatterns | design | detection | quality of service | REST | SCA | service-based systems | SOAP | software maintenance and evolution | specification | web services",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-10-01",
      "Publication type": "Article",
      "Authors": "Palma, Francis;Moha, Naouel;Gueheneuc, Yann Gael",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85074691154",
      "Primary study DOI": "",
      "Title": "Best keyword set recommendations for building service-based systems",
      "Abstract": "Software development has witnessed tremendous changes. One of the important paradigm shifts it has undergone is reusable services that get rid of the need for reinventing the wheel. Thus it could achieve productivity and service orientation rather having multiple disjointed applications. This was made possible due to technology innovations and emergence of distributed computing technologies like Web Services. With integrated interoperable services reusability is promoted in unprecedented fashion. In this context Service Oriented Architecture (SOA) has assumed significance beyond imagination. With third party services available and accessible in public domain, new service oriented application development became easier on top of the state-of-the-art services already available. It has resulted in building useful Service Based Systems (SBSs). Such systems are made up of multiple services integrated with Single Sign On (SSO) service on top of SOA. However, building robust SBS is very challenging. The literature found many contributions but still a framework that renders desired services in service discovery and composition to have robust SBSs is the need of the hour. Towards this end, in this paper a framework is designed to have multiple phases to have different activities like pre-processing, building SVM classifier to predict class labels like SBS with anti-patterns and SBS with no anti-patterns, generating keyword set recommendations for effective service discovery and composition of SBSs. Collaborative filtering based algorithm under multi-user and multi-item (SBSs) settings. A prototype application is built to demonstrate proof of the concept. The experimental results and evaluation revealed that the proposed method outperforms state of the art methods.",
      "Keywords": "Search | Service Based System (SBS) | Service composition | Service discovery | SVM classifier | Web service",
      "Publication venue": "International Journal of Scientific and Technology Research",
      "Publication date": "2019-10-01",
      "Publication type": "Article",
      "Authors": "Ramesh, Gajula;Somasekar, J.;Madhavi, Karanam;Ramu, Gandikota",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85068071058",
      "Primary study DOI": "10.1016/j.infsof.2019.06.009",
      "Title": "Pareto efficient multi-objective black-box test case selection for simulation-based testing",
      "Abstract": "Context: In many domains, engineers build simulation models (e.g., Simulink) before developing code to simulate the behavior of complex systems (e.g., Cyber-Physical Systems). Those models are commonly heavy to simulate which makes it difficult to execute the entire test suite. Furthermore, it is often difficult to measure white-box coverage of test cases when employing such models. In addition, the historical data related to failures might not be available. Objective: The objective of the approach presented in this paper is to cost-effectively select test cases without making use of white-box coverage information or historical data related to fault detection. Method: We propose a cost-effective approach for test case selection that relies on black-box data related to inputs and outputs of the system. The approach defines in total six effectiveness measures and one cost measure followed by deriving in total 21 objective combinations and integrating them within Non-Dominated Sorting Genetic Algorithm-II (NSGA-II). The proposed six effectiveness metrics are specific to simulation models and are based on anti-patterns and similarity measures. Results: We empirically evaluated our approach with these 21 combinations using six case studies by employing mutation testing to assess the fault revealing capability. We compared our approach with Random Search (RS), two many-objective algorithm, as well as three white-box metrics. The results demonstrated that our approach managed to improve Random Search by up to around 28% in terms of the Hypervolume quality indicator. Similarly, black-box metrics-based test case selection also significantly outperformed those of white-box metrics. Conclusion: We demonstrate that test case selection is a non-trivial problem in the context of simulation models. We also show that the proposed effectiveness metrics performed significantly better than traditional white-box metrics. Thus, we show that black-box test selection approaches are appropriate to solve the test case selection problem within simulation models.",
      "Keywords": "Cyber-physical systems | Search-based software engineering | Simulation-based testing | Test case selection",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2019-10-01",
      "Publication type": "Article",
      "Authors": "Arrieta, Aitor;Wang, Shuai;Markiegi, Urtzi;Arruabarrena, Ainhoa;Etxeberria, Leire;Sagardui, Goiuria",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85076775808",
      "Primary study DOI": "10.1145/3357141.3357143",
      "Title": "Incidence of code smells in the application of design patterns: A method-level analysis",
      "Abstract": "Design patterns are reusable solutions that can be applied to solve specific problems in software design. Such patterns can be misapplied, though, and give rise to code smells, i.e., fragments in the code that indicate possible design flaws. In this study, we aim to understand how often code smells co-occur with design patterns, as well as to determine the most common co-occurrences. To this end, we identified instances of code smells and design patterns in methods of 25 open source Java projects, by using automated detection tools. We also manually inspected fragments of the projects' source code to gather insight on the relationship between specific pairs of smells and patterns. Among other findings, we found that methods that are part of the Adapter pattern are more likely to contain code smells, especially the Feature Envy smell, although it can be argued that the detection of this smell in this context is a false positive.",
      "Keywords": "Code Smells | Design Patterns | Software Design",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2019-09-23",
      "Publication type": "Conference Paper",
      "Authors": "Assunção, Ederson;Souza, Rodrigo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85076769780",
      "Primary study DOI": "10.1145/3357141.3357600",
      "Title": "Detecting design violations in django-based web applications",
      "Abstract": "If on one hand frameworks allow programmers to reuse well-known architectural solutions, on the other hand they can make programmers unaware of important design decisions that should be followed during software construction, maintenance and evolution. And if programmers are unaware of these design decisions, there is a high risk of introducing design violations in the source code, and the accumulation of these violations might hinder software maintainability and evolvability. The use of static analysis tools might be employed to mitigate these problems by assisting the detection of recurring design violations in a given architectural pattern. In this work, we present MTV-Checker, a tool to assist the automatic detection of 5 design violations in Django-based web applications. We also conducted an empirical study in the context of the SUAP system, a large-scale Django-based information system with more than 175.000 lines of Python code currently deployed in more than 30 Brazilian institutions. Our results present the most recurrent violations, how they evolve along software evolution, and the opinions and experiences of software architects regarding these violations.",
      "Keywords": "Code Smells | Design Violations | Python Applications | Software Architecture | Software Design | Web-Applications",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2019-09-23",
      "Publication type": "Conference Paper",
      "Authors": "Correia, Renieri;Adachi, Eiji",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85123042636",
      "Primary study DOI": "",
      "Title": "Proceedings - 13th Brazilian Symposium on Software Components, Architectures, and Reuse, SBCARS 2019",
      "Abstract": "The proceedings contain 13 papers. The topics discussed include: a typology of architectural strategies for interoperability; an architecture for dynamic web services that integrates adaptive object models to existing frameworks; analysis of coupling evolution on open source systems; detecting design violations in Django-based web applications; implementing a classic ER algebra to automatically generate complex queries for document-oriented databases; an approach to detect false design patterns full strip; and incidence of code smells in the application of design patterns: a method-level analysis.",
      "Keywords": "",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2019-09-23",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85076742250",
      "Primary study DOI": "10.1145/3357141.3357142",
      "Title": "On the prioritization of design-relevant smelly elements: A mixed-method, multi-project study",
      "Abstract": "Software systems are likely to face what is called design problems. Given the typical lack of design documentation, developers have to rely on implementation-level symptoms, the so-called code smells, to identify and remove design problems. A code smell is a microstructure in the program that can indicate the presence of a design problem. Large programs have hundreds or thousands of program elements (e.g., classes) in which a significant proportion may be affected by smells. Consequently, due to time constraints and the large number of elements, developers have to prioritize the design-relevant program elements, i.e., locate a shortlist of elements that are suspects of having design-relevant smells. However, this task is hard and time-consuming. Unfortunately, the literature fails to provide developers with effective heuristics that automate such prioritization task. The objective of this paper is to propose heuristics that effectively locate a shortlist of design-relevant smelly program elements. For this purpose, we report two studies. In the first one, we investigated the criteria that developers used in practice to accurately prioritize design-relevant smelly elements. Based on these criteria, we derived a preliminary suite of prioritization heuristics. Since we do not know if the heuristics are suitable for an effective prioritization across multiple projects, we performed a second study to evaluate the proposed heuristics. We found that two out of nine heuristics reached an average precision higher than 75% for the four projects we analyzed. Thus, our findings suggest these heuristics are promising to support developers in prioritizing design-relevant smelly elements.",
      "Keywords": "Design problems | Heuristics | Prioritization",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2019-09-23",
      "Publication type": "Conference Paper",
      "Authors": "Oliveira, Anderson;Sousa, Leonardo;Oizumi, Willian;Garcia, Alessandro",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85076086059",
      "Primary study DOI": "10.1145/3345629.3345630",
      "Title": "The technical debt dataset",
      "Abstract": "Technical Debt analysis is increasing in popularity as nowadays researchers and industry are adopting various tools for static code analysis to evaluate the quality of their code. Despite this, empirical studies on software projects are expensive because of the time needed to analyze the projects. In addition, the results are dificult to compare as studies commonly consider different projects. In this work, we propose the Technical Debt Dataset, a curated set of project measurement data from 33 Java projects from the Apache Software Foundation. In the Technical Debt Dataset, we analyzed all commits from separately defined time frames with SonarQube to collect Technical Debt information and with Ptidej to detect code smells. Moreover, we extracted all available commit information from the git logs, the refactoring applied with Refactoring Miner, and fault information reported in the issue trackers (Jira). Using this information, we executed the SZZ algorithm to identify the fault-inducing and - fixing commits. We analyzed 78K commits from the selected 33 projects, detecting 1.8M SonarQube issues, 62K code smells, 28K faults and 57K refactorings. The project analysis took more than 200 days. In this paper, we describe the data retrieval pipeline together with the tools used for the analysis. The dataset is made available through CSV files and an SQLite database to facilitate queries on the data. The Technical Debt Dataset aims to open up diverse opportunities for Technical Debt research, enabling researchers to compare results on common projects.",
      "Keywords": "Dataset | Faults | Mining software repository | Software quality | SonarQube | SZZ | Technical debt",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2019-09-18",
      "Publication type": "Conference Paper",
      "Authors": "Lenarduzzi, Valentina;Saarimäki, Nyyti;Taibi, Davide",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85081967635",
      "Primary study DOI": "10.1145/3344948.3344951",
      "Title": "Automatic detection of architectural bad smells through semantic representation of code",
      "Abstract": "Bad design decisions in software development can progressively affect the internal quality of a software system, causing architecture erosion. Such bad decisions are called Architectural Smells (AS) and should be detected as soon as possible, because their presence heavily hinders the maintainability and evolvability of the software. Many detection approaches rely on software analysis techniques which inspect the structure of the system under analysis and check with rules the presence of AS. However, some recent approaches leverage natural language processing techniques to recover semantic information from the system. This kind of information is useful to detect AS which violate \"conceptual\" design principles, such as the separation of concerns one. In this research study, I propose two detection strategies for AS detection based on code2vec, a neural model which is able to predict semantic properties of given snippets of code.",
      "Keywords": "Architectural (bad) smells detection | Architecture erosion | Code embeddings | Software concerns",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2019-09-09",
      "Publication type": "Conference Paper",
      "Authors": "Pigazzini, Ilaria",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85056520897",
      "Primary study DOI": "10.1145/3238147.3238166",
      "Title": "Deep learning based feature envy detection",
      "Abstract": "Software refactoring is widely employed to improve software quality. A key step in software refactoring is to identify which part of the software should be refactored. To facilitate the identification, a number of approaches have been proposed to identify certain structures in the code (called code smells) that suggest the possibility of refactoring. Most of such approaches rely on manually designed heuristics to map manually selected source code metrics to predictions. However, it is challenging to manually select the best features, especially textual features. It is also difficult to manually construct the optimal heuristics. To this end, in this paper we propose a deep learning based novel approach to detecting feature envy, one of the most common code smells. The key insight is that deep neural networks and advanced deep learning techniques could automatically select features (especially textual features) of source code for feature envy detection, and could automatically build the complex mapping between such features and predictions. We also propose an automatic approach to generating labeled training data for the neural network based classifier, which does not require any human intervention. Evaluation results on open-source applications suggest that the proposed approach significantly improves the state-of-the-art in both detecting feature envy smells and recommending destinations for identified smelly methods.",
      "Keywords": "Code smells | Deep learning | Feature envy | Software refactoring",
      "Publication venue": "ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering",
      "Publication date": "2018-09-03",
      "Publication type": "Conference Paper",
      "Authors": "Liu, Hui;Xu, Zhifeng;Zou, Yanzhen",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85076500676",
      "Primary study DOI": "10.30534/ijatcse/2019/05852019",
      "Title": "SOAP and RESTful web service anti-patterns: A scoping review",
      "Abstract": "Web services provide a uniform framework to achieve a high level of interaction between distributed heterogeneous software systems and data resources shared over the Internet. Producing a well-designed web service is significant because it leads to a more understandable service and a higher level of interaction and leads to effective software maintainability. However, web service is suffering from a poor design problem named anti-patterns. Analysis of the literature returned a plethora of studies on anti-patterns that caused difficulties for developers to synthesize and summarized the possible types of anti-patterns and further comprehend each of them. Due to this limitation, this paper aims to provide organized literature on the types of anti-patterns found in web services. A scoping review was conducted by searching scholarly documents, analyzing, and classified them based on their anti-pattern types. The review provided in this paper could be used as a guide for developers to identify the anti-patterns that could be found in web services.",
      "Keywords": "Anti-pattern | Anti-pattern Detection | Interface Design | Web Service | Web Service Design",
      "Publication venue": "International Journal of Advanced Trends in Computer Science and Engineering",
      "Publication date": "2019-09-01",
      "Publication type": "Article",
      "Authors": "Alshraiedeh, Fuad;Katuk, Norliza",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85076894931",
      "Primary study DOI": "",
      "Title": "Proceedings - 2019 IEEE 27th International Requirements Engineering Conference, RE 2019",
      "Abstract": "The proceedings contain 69 papers. The topics discussed include: can a conversation paint a picture? mining requirements in software forums; RE4CPS: requirements engineering for cyber-physical systems; requirements classi?cation with interpretable machine learning and dependency parsing; predicting how to test requirements: an automated approach; using metamodeling for requirements engineering: a best-practice with ADOxx; learning requirements elicitation interviews with role-playing, self-assessment and peer-review; detecting bad smells in use case descriptions; consent verification under evolving privacy policies; and requirements engineering method for infrastructure automation and cloud projects.",
      "Keywords": "",
      "Publication venue": "Proceedings of the IEEE International Conference on Requirements Engineering",
      "Publication date": "2019-09-01",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85055955349",
      "Primary study DOI": "10.1007/s11219-018-9424-8",
      "Title": "Software Design Smell Detection: a systematic mapping study",
      "Abstract": "Design Smells are indicators of situations that negatively affect software quality attributes such as understandability, testability, extensibility, reusability, and maintainability in general. Improving maintainability is one of the cornerstones of making software evolution easier. Hence, Design Smell Detection is important in helping developers when making decisions that can improve software evolution processes. After a long period of research, it is important to organize the knowledge produced so far and to identify current challenges and future trends. In this paper, we analyze 18 years of research into Design Smell Detection. There is a wide variety of terms that have been used in the literature to describe concepts which are similar to what we have defined as “Design Smells,” such as design defect, design flaw, anomaly, pitfall, antipattern, and disharmony. The aim of this paper is to analyze all these terms and include them in the study. We have used the standard systematic literature review method based on a comprehensive set of 395 articles published in different proceedings, journals, and book chapters. We present the results in different dimensions of Design Smell Detection, such as the type or scope of smell, detection approaches, tools, applied techniques, validation evidence, type of artifact in which the smell is detected, resources used in evaluation, supported languages, and relation between detected smells and software quality attributes according to a quality model. The main contributions of this paper are, on the one hand, the application of domain modeling techniques to obtain a conceptual model that allows the organization of the knowledge on Design Smell Detection and a collaborative web application built on that knowledge and, on the other, finding how tendencies have moved across different kinds of smell detection, as well as different approaches and techniques. Key findings for future trends include the fact that all automatic detection tools described in the literature identify Design Smells as a binary decision (having the smell or not), which is an opportunity to evolve to fuzzy and prioritized decisions. We also find that there is a lack of human experts and benchmark validation processes, as well as demonstrating that Design Smell Detection positively influences quality attributes.",
      "Keywords": "Antipatterns | DesignSmell | Detection tools | Quality models | Systematic mapping study",
      "Publication venue": "Software Quality Journal",
      "Publication date": "2019-09-01",
      "Publication type": "Article",
      "Authors": "Alkharabsheh, Khalid;Crespo, Yania;Manso, Esperanza;Taboada, José A.",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85075932894",
      "Primary study DOI": "10.1109/MODELS-C.2019.00121",
      "Title": "Quality improvement for UML and OCL models through bad smell and metrics definition",
      "Abstract": "Detecting and fixing software quality issues early in the design phase is indispensable for a successful project applying model-based techniques. This paper presents an extension of the tool USE (UML-based Specification Environment) with features for (a) reflective model queries and model exploration, (b) metric measurement, (c) smell detection, and (d) quality assessment with metrics. The newly added functionalities can be fine-tuned by designers, are closely related and can be applied together interactively in order to help designers to achieve better models.",
      "Keywords": "Metamodel | Metrics | Model quality assessment | Smell detection | UML and OCL Model",
      "Publication venue": "Proceedings - 2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems Companion, MODELS-C 2019",
      "Publication date": "2019-09-01",
      "Publication type": "Conference Paper",
      "Authors": "Doan, Khanh Hoang;Gogolla, Martin",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85077195962",
      "Primary study DOI": "10.1109/ICSME.2019.00088",
      "Title": "Can Automated Impact Analysis Techniques Help Predict Decaying Modules?",
      "Abstract": "A decaying module refers to a module whose quality is getting worse and is likely to become smelly in the future. The concept has been proposed to mitigate the problem that developers cannot track the progression of code smells and prevent them from occurring. To support developers in proactive refactoring process to prevent code smells, a prediction approach has been proposed to detect modules that are likely to become decaying modules in the next milestone. Our prior study has shown that modules that developers will modify as an estimation of developers' context can be used to improve the performance of the prediction model significantly. Nevertheless, it requires the developer who has perfect knowledge of locations of changes to manually specify such information to the system. To this end, in this study, we explore the use of automated impact analysis techniques to estimate the developers' context. Such techniques will enable developers to improve the performance of the decaying module prediction model without the need of perfect knowledge or manual input to the system. Furthermore, we conduct a study on the relationship between the accuracy of an impact analysis technique and its effect on improving decaying module prediction, as well as the future direction that should be explored.",
      "Keywords": "Code smell | decaying module | impact analysis",
      "Publication venue": "Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019",
      "Publication date": "2019-09-01",
      "Publication type": "Conference Paper",
      "Authors": "Sae-Lim, Natthawute;Hayashi, Shinpei;Saeki, Motoshi",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85074301854",
      "Primary study DOI": "10.1109/ESEM.2019.8870177",
      "Title": "On the Impact of Refactoring on the Relationship between Quality Attributes and Design Metrics",
      "Abstract": "Background: Refactoring is a critical task in software maintenance and is generally performed to enforce the best design and implementation practices or to cope with design defects. Several studies attempted to detect refactoring activities through mining software repositories allowing to collect, analyze and get actionable data-driven insights about refactoring practices within software projects. Aim: We aim at identifying, among the various quality models presented in the literature, the ones that are more in-line with the developer's vision of quality optimization, when they explicitly mention that they are refactoring to improve them. Method: We extract a large corpus of design-related refactoring activities that are applied and documented by developers during their daily changes from 3,795 curated open source Java projects. In particular, we extract a large-scale corpus of structural metrics and anti-pattern enhancement changes, from which we identify 1,245 quality improvement commits with their corresponding refactoring operations, as perceived by software engineers. Thereafter, we empirically analyze the impact of these refactoring operations on a set of common state-of-the-art design quality metrics. Results: The statistical analysis of the obtained results shows that (i) a few state-of-the-art metrics are more popular than others; and (ii) some metrics are being more emphasized than others. Conclusions: We verify that there are a variety of structural metrics that can represent the internal quality attributes with different degrees of improvement and degradation of software quality. Most of the metrics that are mapped to the main quality attributes do capture developer intentions of quality improvement reported in the commit messages, but for some quality attributes, they don't.",
      "Keywords": "empirical study | refactoring | software quality",
      "Publication venue": "International Symposium on Empirical Software Engineering and Measurement",
      "Publication date": "2019-09-01",
      "Publication type": "Conference Paper",
      "Authors": "Alomar, Eman Abdullah;Mkaouer, Mohamed Wiem;Ouni, Ali;Kessentini, Marouane",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85074269349",
      "Primary study DOI": "10.1109/ESEM.2019.8870175",
      "Title": "Do Higher Incentives Lead to Better Performance? - An Exploratory Study on Software Crowdsourcing",
      "Abstract": "Background: Crowdsourcing, an emerging paradigm to accomplish tasks by calling unknown workers across the internet to compete, is gaining more and more popularity in various domains. Crowdsourcing task requesters usually offer different bonuses to assure desired worker performance. Most existing studies focus on the general crowdsourcing market, and lead to inconsistent observations on the impact of different incentive strategies on worker performance. There is a lack of studies investigating this issue in crowdsourcing more complex or intelligent tasks such as software crowdsourcing. Aims: To bridge the gap and develop better understanding of the relationship between task incentives and worker performance in the field of software crowdsourcing, this study aims at examining strategic pricing behaviors of task requesters on the most popular software crowdsourcing platform, i.e. TopCoder, and evaluating the impact of monetary incentives on worker performance. Method: We first present the characterization of two specific pricing strategies employed in software crowdsourcing marketplace, design a two-step methodology to detect and identify different pricing strategies, and propose an algorithm to examine the impact of pricing strategies on worker's behaviors in terms of task participation level, completion velocity and task quality. An exploratory case study is conducted to apply the proposed methodology and algorithm on a dataset extracted from the TopCoder platform. Results: The conceptualization of pricing strategies formulates common pricing behaviors in software crowdsourcing. Main analysis results include: 1) strategic pricing patterns are prevalent in software crowdsourcing practices; 2) higher task incentives can get potentially paid-off by higher performance such as more registrants, more submissions and quicker velocity; 3) however, higher incentives do not always improve submission score of software crowdsourcing tasks, similar to moral hazard problems in economics. This implies that it is necessary to increase task award modestly; 4) in addition, higher incentives can improve the internal code which is measured by code bugs and bad smells. Conclusions: We believe the preliminary findings on the pricing strategy are beneficial for both better pricing decision-making and improved crowdsourcing market efficiency and fairness, and hope to stimulate further discussions and research in strategic crowd coordination.",
      "Keywords": "monetary incentives | pricing strategy | software crowdsourcing | worker behaviors | worker performance",
      "Publication venue": "International Symposium on Empirical Software Engineering and Measurement",
      "Publication date": "2019-09-01",
      "Publication type": "Conference Paper",
      "Authors": "Wang, Lili;Yang, Ye;Wang, Yong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85076433758",
      "Primary study DOI": "10.1145/3340482.3342744",
      "Title": "On the role of data balancing for machine learning-based code smell detection",
      "Abstract": "Code smells can compromise software quality in the long term by inducing technical debt. For this reason, many approaches aimed at identifying these design flaws have been proposed in the last decade. Most of them are based on heuristics in which a set of metrics (e.g., code metrics, process metrics) is used to detect smelly code components. However, these techniques suffer of subjective interpretation, low agreement between detectors, and threshold dependability. To overcome these limitations, previouswork applied Machine Learning techniques that can learn from previous datasets without needing any threshold definition. However, more recent work has shown that Machine Learning is not always suitable for code smell detection due to the highly unbalanced nature of the problem. In this study we investigate several approaches able to mitigate data unbalancing issues to understand their impact on MLbased approaches for code smell detection. Our findings highlight a number of limitations and open issues with respect to the usage of data balancing in ML-based code smell detection.",
      "Keywords": "Code Smells | Data Balancing | Machine Learning",
      "Publication venue": "MaLTeSQuE 2019 - Proceedings of the 3rd ACM SIGSOFT International Workshop on Machine Learning Techniques for Software Quality Evaluation, co-located with ESEC/FSE 2019",
      "Publication date": "2019-08-27",
      "Publication type": "Conference Paper",
      "Authors": "Pecorelli, Fabiano;Di Nucci, Dario;De Roover, Coen;De Lucia, Andrea",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85071941138",
      "Primary study DOI": "10.1145/3338906.3342504",
      "Title": "Are existing code smells relevant in web games? An empirical study",
      "Abstract": "In software applications, code smells are considered as bad coding practices acquired at the time of development. The presence of such code smells in games may affect the process of game development adversely. Our preliminary study aims at investigating the existence of code smells in the games. To achieve this, we used JavaScript code smells detection tool JSNose against 361 JavaScript web games to find occurrences of JavaScript smells in games. Further, we conducted a manual study to find violations of known game programming patterns in 8 web games to verify the necessity of game-specific code smells detection tool. Our results shows that existing JavaScript code smells detection tool is not sufficient to find game-specific code smells in web games.",
      "Keywords": "Code Smells | Game-specific Code Smells | Web Games",
      "Publication venue": "ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "Publication date": "2019-08-12",
      "Publication type": "Conference Paper",
      "Authors": "Khanve, Vaishali",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85076040036",
      "Primary study DOI": "10.1109/SEAA.2019.00021",
      "Title": "Evaluating the Impact of Code Smell Refactoring on the Energy Consumption of Android Applications",
      "Abstract": "Energy consumption of mobile apps is receiving a lot of attention from researchers. Recent studies indicate that energy consumption of mobile devices could be lowered by improving the quality of mobile apps. Frequent refactoring is one way of achieving this goal. We explore the performance and energy impact of several common code refactorings in Android apps. Experimental results indicate that some code smell refactorings positively impact the energy consumption of Android apps. Refactoring of the code smells 'Duplicated code' and 'Type checking' reduce energy consumption by up to 10.8%. Significant reduction in energy consumption, however, does not seem to be directly related to the increase or decrease of execution time. In addition, the energy impact over permutations of code smell refactorings in the selected Android apps was small. When analyzing the order in which refactorings were made across code smell types, it turned out that some permutations resulted in a reduction and some in an increase of energy consumption for the analyzed apps.",
      "Keywords": "Code Power Consumption | Code Smell Detection | Code Smell Refactoring | Refactoring | Software Maintenance",
      "Publication venue": "Proceedings - 45th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2019",
      "Publication date": "2019-08-01",
      "Publication type": "Conference Paper",
      "Authors": "Anwar, H.;Pfahl, Dietmar;Srirama, Satish N.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85076712433",
      "Primary study DOI": "10.1145/3361149.3361178",
      "Title": "Software process anti-patterns catalogue",
      "Abstract": "For software project managers and other practitioners, an important activity is to detect, and consequently find solutions to, insufficiencies and mistakes in process and other project management (PM) activities. Particularly interesting among these are anti-patterns: commonly occurring solutions with known negative effects. Their detection in running, as well as finished, projects is often hard as it needs to be performed by specialists, demands expertise and skill, and is prone to biases. Obtaining the expertise is a long-time effort and the sources of relevant knowledge are scattered and vary in the depth of treatment. These issues could be alleviated by detecting PM and software process anti-patterns in data available in project Application Lifecycle Management tools. To facilitate the work towards such an approach, we have performed a review of academic, professional and grey literature sources to collect currently known and defined software PM anti-patterns. The collected set shows that they vary in terminology and description format which can lead to misunderstandings, different interpretations and other difficulties, especially when trying to devise a universally acceptable method of detection. In this paper we describe the findings of the review and the design of a catalogue of PM and process anti-patterns, based on the knowledge obtained. It uses a description template designed to support data-driven detection of anti-pattern occurrence. An initial version of the catalogue has been made publicly accessible, with the aim to reconcile the various sources and foster community discussion on understanding and descriptions of the individual anti-patterns.",
      "Keywords": "Anti-pattern catalogue | Anti-pattern templates | Literature review | Pattern detection | Project management anti-patterns | Software process anti-patterns",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2019-07-03",
      "Publication type": "Conference Paper",
      "Authors": "Brada, Premek;Picha, Petr",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85076688514",
      "Primary study DOI": "10.1145/3361149.3361169",
      "Title": "Software process anti-pattern detection in project data",
      "Abstract": "There is a significant amount of guidance on Project Management (PM) including software development methodologies, best practices and anti-patterns (APs). There is, however, a lack of automated way of applying this knowledge by analyzing readily available data from tools aiding in software PM, such as Application Lifecycle Management (ALM) tools. We propose a method of detecting process and PM anti-patterns in project data which can be used to warn software development teams about a potential threat to the project, or to conduct more general studies on the impact of AP occurrence on project success and product quality. We previously published a concept for the data mining and analysis toolset distinct from other research approaches and related work. Based on this toolset, we devised a formalized basis for our detection method in the form of standardized AP description template and a model for pattern operationalization over project data extracted from ALM tools. The main contribution of this paper is the general method for AP operationalization taking the description template as a starting point, discussed together with its potential limitations. We performed an initial validation of the method on data from student projects, using an AP we encountered in practice called \"Collective Procrastination\" which we also describe in this paper together with its detailed formal operationalization.",
      "Keywords": "ALM tools | Pattern detection | Project management anti-patterns | Software process anti-patterns",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2019-07-03",
      "Publication type": "Conference Paper",
      "Authors": "Picha, Petr;Brada, Premek",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84976584826",
      "Primary study DOI": "10.1109/TSE.2015.2503740",
      "Title": "Dynamic and Automatic Feedback-Based Threshold Adaptation for Code Smell Detection",
      "Abstract": "Most code smell detection tools expose thresholds to engineers for customization because code smell detection is essentially subjective and application specific. Another reason why engineers should customize these thresholds is that they have different working schedules and different requirements on software quality. They have their own unique need on precision and recall in smell detection. This unique need should be fulfilled by adjusting thresholds of smell detection tools. However, it is difficult for software engineers, especially inexperienced ones, to adjust often contradicting and related thresholds manually. One of the possible reasons is that engineers do not know the exact quantitative relation between threshold values and performance, e.g., precision. In this paper, we propose an approach to adapting thresholds automatically and dynamically. Engineers set a target precision manually according to their working schedules and quality requirements. With feedback from engineers, the proposed approach then automatically searches for a threshold setting to maximize recall while having precision close to the target precision. The proposed approach has been evaluated on open-source applications. Evaluation results suggest that the proposed approach is effective.",
      "Keywords": "Code Smells | Feedback Control | Smell Identification | Software Refactoring",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2016-06-01",
      "Publication type": "Article",
      "Authors": "Liu, Hui;Liu, Qiurong;Niu, Zhendong;Liu, Yang",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85072700113",
      "Primary study DOI": "10.1109/COMPSAC.2019.00025",
      "Title": "Metrics driven architectural analysis using dependency graphs for C language projects",
      "Abstract": "The highest share of cost for a software product is software maintenance. Identifying the quality merit of the software architecture is extremely vital as the quality directly relates to software maintenance. A good design always exhibits good quality characteristics because it is directly related to good architecture. Although C language is a major language in the software industry, few studies investigate the quality of the architecture in C language. This study aims to evaluate the quality of C language projects in a quantifiable form by focusing on dependency graphs, associated metrics, and software architecture. In particular, this study (i) formulates the architecture representation of C projects, (ii) determines the metrics capturing the quality of architecture, (iii) defines code smell and metrics relations and (iv) conducts an empirical analysis on 58 C projects. We show which metrics derived from dependency graphs can detect architectural issues and verify their relation to software architecture quality.",
      "Keywords": "C language | Code smells | Dependency graphs | Software architecture | Software metrics analysis",
      "Publication venue": "Proceedings - International Computer Software and Applications Conference",
      "Publication date": "2019-07-01",
      "Publication type": "Conference Paper",
      "Authors": "Tiwari, Devansh;Washizaki, Hironori;Fukazawa, Yoshiaki;Fukuoka, Tomoyuki;Tamaki, Junji;Hosotani, Nobuhiro;Kohama, Munetaka",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85041003076",
      "Primary study DOI": "10.1109/TSE.2018.2797899",
      "Title": "Design Rule Spaces: A New Model for Representing and Analyzing Software Architecture",
      "Abstract": "In this paper, we propose an architecture model called Design Rule Space (DRSpace). We model the architecture of a software system as multiple overlapping DRSpaces, reflecting the fact that any complex software system must contain multiple aspects, features, patterns, etc. We show that this model provides new ways to analyze software quality. In particular, we introduce an Architecture Root detection algorithm that captures DRSpaces containing large numbers of a project's bug-prone files, which are called Architecture Roots (ArchRoots). After investigating ArchRoots calculated from 15 open source projects, the following observations become clear: from 35 to 91 percent of a project's most bug-prone files can be captured by just 5 ArchRoots, meaning that bug-prone files are likely to be architecturally connected. Furthermore, these ArchRoots tend to live in the system for significant periods of time, serving as the major source of bug-proneness and high maintainability costs. Moreover, each ArchRoot reveals multiple architectural flaws that propagate bugs among files and this will incur high maintenance costs over time. The implication of our study is that the quality, in terms of bug-proneness, of a large, complex software project cannot be fundamentally improved without first fixing its architectural flaws.",
      "Keywords": "bug localization | code smells | defect prediction | reverse-engineering | Software architecture | technical debt",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-07-01",
      "Publication type": "Article",
      "Authors": "Cai, Yuanfang;Xiao, Lu;Kazman, Rick;Mo, Ran;Feng, Qiong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84872321538",
      "Primary study DOI": "10.1109/WCRE.2012.56",
      "Title": "SMURF: A SVM-based incremental anti-pattern detection approach",
      "Abstract": "In current, typical software development projects, hundreds of developers work asynchronously in space and time and may introduce anti-patterns in their software systems because of time pressure, lack of understanding, communication, and - or skills. Anti-patterns impede development and maintenance activities by making the source code more difficult to understand. Detecting anti-patterns incrementally and on subsets of a system could reduce costs, effort, and resources by allowing practitioners to identify and take into account occurrences of anti-patterns as they find them during their development and maintenance activities. Researchers have proposed approaches to detect occurrences of anti-patterns but these approaches have currently four limitations: (1) they require extensive knowledge of anti-patterns, (2) they have limited precision and recall, (3) they are not incremental, and (4) they cannot be applied on subsets of systems. To overcome these limitations, we introduce SMURF, a novel approach to detect anti-patterns, based on a machine learning technique - support vector machines - and taking into account practitioners' feedback. Indeed, through an empirical study involving three systems and four anti-patterns, we showed that the accuracy of SMURF is greater than that of DETEX and BDTEX when detecting anti-patterns occurrences. We also showed that SMURF can be applied in both intra-system and inter-system configurations. Finally, we reported that SMURF accuracy improves when using practitioners' feedback. © 2012 IEEE.",
      "Keywords": "Anti-pattern | empirical software engineering | program comprehension | program maintenance",
      "Publication venue": "Proceedings - Working Conference on Reverse Engineering, WCRE",
      "Publication date": "2012-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Maiga, Abdou;Ali, Nasir;Bhattacharya, Neelesh;Sabané, Aminata;Guéhéneuc, Yann Gaël;Aimeur, Esma",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85071152559",
      "Primary study DOI": "10.1109/ICPM.2019.00020",
      "Title": "Assessing software development teams' efficiency using process mining",
      "Abstract": "Context. Improving the efficiency and effectiveness of software development projects implies understanding their actual process. Given the same requirements specification, different software development teams may follow different strategies and that may lead to inappropriate use of tools or non-optimized allocation of effort on spurious activities, non-aligned with the desired goals. However, due to its intangibility, the actual process followed by each developer or team is often a black box. Objective. The overall goal of this study is to improve the knowledge on how to measure efficiency in development teams where a great deal of variability may exist due to the human-factor. The main focus is on the discovery of the underlying processes and compare them in terms of efficiency and effectiveness. By doing so, we expect to reveal potentially hidden costs and risks, so that corrective actions may take place on a timely manner during the software project life cycle. Method. Several independent teams of Java programmers, using the Eclipse IDE, were assigned the same software quality task, related to code smells detection for identifying refactoring opportunities and the quality of the outcomes were assessed by independent experts. The events corresponding to the activity of each team upon the IDE, while performing the given task, were captured. Then, we used process mining techniques to discover development process models, evaluate their quality and compare variants against a reference model used as 'best practice'. Results. Teams whose process model was less complex, had the best outcomes and vice-versa. Comparing less complex process variants with the \"best practice\" process, showed that they were also the ones with less differences in the control-flow perspective, based on activities frequencies. We have also determined which teams were most efficient through process analysis. Conclusions. We confirmed that, even for a well-defined software development task, there may be a great deal of process variability due to the human factor. We were able to identify when developers were more or less focused in the essential tasks they were required to perform. Less focused teams had the more complex process models, due to the spurious/non-essential actions that were carried out. In other words, they were less efficient. Experts' opinion confirmed that those teams also were less effective in their expected delivery. We therefore concluded that a self-awareness of the performed process rendered by our approach, may be used to identify corrective actions that will improve process efficiency (less wasted effort) and may yield to better deliverables, i.e. improved process effectiveness.",
      "Keywords": "Efficiency assessment | Ide usage | Process mining | Software development | Software process analytics",
      "Publication venue": "Proceedings - 2019 International Conference on Process Mining, ICPM 2019",
      "Publication date": "2019-06-01",
      "Publication type": "Conference Paper",
      "Authors": "Caldeira, Joao;Brito E Abreu, Fernando;Reis, Jose;Cardoso, Jorge",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85067107233",
      "Primary study DOI": "10.1109/JEEIT.2019.8717457",
      "Title": "Code Smells Analysis Mechanisms, Detection Issues, and Effect on Software Maintainability",
      "Abstract": "Software evolution is an inevitable need in most of the modern businesses, software that doesn't accommodate changes is hard to survive the market needs. Also, software changes can affect the overall design of the software and sometimes in a corrupting way, affecting the maintainability and evolvability of the software, which introduces technical debt that needs to be solved by continuous refactoring and restructuring of software. Code smells are useful indicators to identify the parts of the code to be refactored to improve the overall maintainability of the software. We present an overview of software code smells, detection and analysis mechanisms and difficulties. Also, we address the effect of refactoring on software maintainability and error-proneness of software.",
      "Keywords": "Code Smells | Design Patterns | Refactoring | Technical Debt",
      "Publication venue": "2019 IEEE Jordan International Joint Conference on Electrical Engineering and Information Technology, JEEIT 2019 - Proceedings",
      "Publication date": "2019-05-16",
      "Publication type": "Conference Paper",
      "Authors": "Lafi, Mohammed;Botros, Joseph Wassily;Kafaween, Hamzah;Al-Dasoqi, Ahmad Bassam;Al-Tamimi, Abdelfatah",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85066464189",
      "Primary study DOI": "10.1109/ICSA-C.2019.00054",
      "Title": "Refactoring Decision Support for Developers and Architects Based on Architectural Impact",
      "Abstract": "Refactorings are key activities for achieving sustainable software systems. However, refactorings demand high effort and features are often deemed more important. The time pressure to deliver is always high and to select important refactorings, one has to consider manifold criteria. Current approaches only relate to code smells and design flaws, and do not take architectural impact into consideration. The project aims at developing a decision framework integrating architecture smell detection, selection of appropriate refactorings and impact analysis to prioritize refactorings and support not only developers but also software architects. It shall be evaluated using control groups by measuring and comparing the required time and the resulting software quality.",
      "Keywords": "architecture refactoring | architecture smells | decision support | maintainability | refactoring sequence",
      "Publication venue": "Proceedings - 2019 IEEE International Conference on Software Architecture - Companion, ICSA-C 2019",
      "Publication date": "2019-05-09",
      "Publication type": "Conference Paper",
      "Authors": "Rachow, Paula",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84958238451",
      "Primary study DOI": "10.1007/s11219-016-9309-7",
      "Title": "Multi-objective code-smells detection using good and bad design examples",
      "Abstract": "Code-smells are identified, in general, by using a set of detection rules. These rules are manually defined to identify the key symptoms that characterize a code-smell using combinations of mainly quantitative (metrics), structural, and/or lexical information. We propose in this work to consider the problem of code-smell detection as a multi-objective problem where examples of code-smells and well-designed code are used to generate detection rules. To this end, we use multi-objective genetic programming (MOGP) to find the best combination of metrics that maximizes the detection of code-smell examples and minimizes the detection of well-designed code examples. We evaluated our proposal on seven large open-source systems and found that, on average, most of the different five code-smell types were detected with an average of 87 % of precision and 92 % of recall. Statistical analysis of our experiments over 51 runs shows that MOGP performed significantly better than state-of-the-art code-smell detectors.",
      "Keywords": "Search-based software engineering | Software maintenance | Software metrics",
      "Publication venue": "Software Quality Journal",
      "Publication date": "2017-06-01",
      "Publication type": "Article",
      "Authors": "Mansoor, Usman;Kessentini, Marouane;Maxim, Bruce R.;Deb, Kalyanmoy",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85073435706",
      "Primary study DOI": "10.1109/IWoR.2019.00015",
      "Title": "Toward proactive refactoring: An exploratory study on decaying modules",
      "Abstract": "Source code quality is often measured using code smell, which is an indicator of design flaw or problem in the source code. Code smells can be detected using tools such as static analyzer that detects code smells based on source code metrics. Further, developers perform refactoring activities based on the result of such detection tools to improve source code quality. However, such approach can be considered as reactive refactoring, i.e., developers react to code smells after they occur. This means that developers first suffer the effects of low quality source code (e.g., low readability and understandability) before they start solving code smells. In this study, we focus on proactive refactoring, i.e., refactoring source code before it becomes smelly. This approach would allow developers to maintain source code quality without having to suffer the impact of code smells. To support the proactive refactoring process, we propose a technique to detect decaying modules, which are non-smelly modules that are about to become smelly. We present empirical studies on open source projects with the aim of studying the characteristics of decaying modules. Additionally, to facilitate developers in the refactoring planning process, we perform a study on using a machine learning technique to predict decaying modules and report a factor that contributes most to the performance of the model under consideration.",
      "Keywords": "Code quality | Code smell | Refactoring",
      "Publication venue": "Proceedings - 2019 IEEE/ACM 3rd International Workshop on Refactoring, IWOR 2019",
      "Publication date": "2019-05-01",
      "Publication type": "Conference Paper",
      "Authors": "Sae-Lim, Natthawute;Hayashi, Shinpei;Saeki, Motoshi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85071149050",
      "Primary study DOI": "10.1109/TechDebt.2019.00015",
      "Title": "DV8: Automated architecture analysis tool suites",
      "Abstract": "This paper present our tool suite called DV8. The objective of DV8 is to measure software modularity, detect architecture anti-patterns as technical debts, quantify the maintenance cost of each instance of an anti-pattern, and enable return on investment analyses of architectural debts. Different from other tools, DV8 integrates data from both source code and revision history. We now elaborate on each of DV8's capabilities.",
      "Keywords": "Software Architecture | Software Maintenance | Software Quality",
      "Publication venue": "Proceedings - 2019 IEEE/ACM International Conference on Technical Debt, TechDebt 2019",
      "Publication date": "2019-05-01",
      "Publication type": "Conference Paper",
      "Authors": "Cai, Yuanfang;Kazman, Rick",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85072298762",
      "Primary study DOI": "10.1109/ICSE.2019.00028",
      "Title": "Automated Reporting of Anti-Patterns and Decay in Continuous Integration",
      "Abstract": "Continuous Integration (CI) is a widely-used software engineering practice. The software is continuously built so that changes can be easily integrated and issues such as unmet quality goals or style inconsistencies get detected early. Unfortunately, it is not only hard to introduce CI into an existing project, but it is also challenging to live up to the CI principles when facing tough deadlines or business decisions. Previous work has identified common anti-patterns that reduce the promised benefits of CI. Typically, these anti-patterns slowly creep into a project over time before they are identified. We argue that automated detection can help with early identification and prevent such a process decay. In this work, we further analyze this assumption and survey 124 developers about CI anti-patterns. From the results, we build CI-Odor, a reporting tool for CI processes that detects the existence of four relevant anti-patterns by analyzing regular build logs and repository information. In a study on the 18,474 build logs of 36 popular JAVA projects, we reveal the presence of 3,823 high-severity warnings spread across projects. We validate our reports in a survey among 13 original developers of these projects and through general feedback from 42 developers that confirm the relevance of our reports.",
      "Keywords": "Anti-Pattern | CI-Decay | CI-Smell | Continuous Integration | Detection",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2019-05-01",
      "Publication type": "Conference Paper",
      "Authors": "Vassallo, Carmine;Proksch, Sebastian;Gall, Harald C.;Di Penta, Massimiliano",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85077954877",
      "Primary study DOI": "10.1109/ICOIACT46704.2019.8938487",
      "Title": "Software quality prediction using data mining techniques",
      "Abstract": "The development of software systems is hard process. The software engineers doing their best during the software construction levels to produce high quality systems. Quality measured from multi perspectives. External and internal perspectives determined by McCall's quality model. In the scope of our work, we concerned with the internal quality. The code level represents the internal quality of the software. By using object-oriented metrics, we can measure code quality. These metrics reflects a clear idea about the code under investigation. Therefore, we proposed to use them during the code test level of the project. If there are any outliers in the values of the selected metrics, we can revise them by applying the appropriate corrections at the code. The code has variant design problems called design smells or code smells. These problems affects directly on the future maintenance of the project. One of the widely spread problem is the Feature Envy method. This type of code design problems causes complicated maintenance problems. In order to make the maintenance process completed in optimum time we have to get rid of this code problem. Therefore, earlier detection is the right way to minimize the time needed for expected maintenance and development. In this paper, we combined the OO metrics with some data mining algorithms to classify the code's methods into two types as Feature Envy or not. The paper applied the technique on an open source Java project. In addition, evaluate the results with other selected detection tools with different techniques. The results registered a high accuracy in the case of using cross validation against using training set option in K-Nearest neighbor algorithm.",
      "Keywords": "Code quality | Code testing | Data mining | Design smell | Software quality",
      "Publication venue": "2019 International Conference on Information and Communications Technology, ICOIACT 2019",
      "Publication date": "2019-07-01",
      "Publication type": "Conference Paper",
      "Authors": "Merzah, Bayadaa M.",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85071580812",
      "Primary study DOI": "10.13328/j.cnki.jos.005724",
      "Title": "God Class Detection Approach Based on Deep Learning [ä¸ç§åºäºæ·±åº¦å­¦ä¹ çä¸å¸ç±»æ£æµæ ¹æ³]",
      "Abstract": "God class refers to certain classes that have assumed more than one functionality, which obey the single responsibility principle and consequently impact on the maintainability and intelligibility of software system. Studies, detection and refactoring included, of god class have always attracted research attentions because of its commonness. As a result, a neural network based detection approach is proposed to detect god class code smell. This detection technology not only makes use of common metrics in software, but also exploits the textual information in source code, which is intended to reveal the main roles that the class plays through mining text semantics. In addition, in order to solve the massive labeled data required for supervised deep learning, an approach is proposed to construct labeled data based on open source code. Finally, the proposed approach is evaluated on an open source data set. The result of evaluation shows that the proposed approach outperforms the current method, especially the recall has been greatly improved by 35.58%.",
      "Keywords": "Code smell | Deep learning | Software refactoring",
      "Publication venue": "Ruan Jian Xue Bao/Journal of Software",
      "Publication date": "2019-05-01",
      "Publication type": "Article",
      "Authors": "Bu, Yi Fan;Liu, Hui;Li, Guang Jie",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85072515894",
      "Primary study DOI": "10.1109/MOBILESoft.2019.00022",
      "Title": "On the survival of android code smells in the wild",
      "Abstract": "The success of smartphones and app stores have contributed to the explosion of the number of mobile apps proposed to end-users. In this very competitive market, developers are rushed to regularly release new versions of their apps in order to retain users. Under such pressure, app developers may be tempted to adopt bad design or implementation choices, leading to the introduction of code smells. Mobile-specific code smells represent a real concern in mobile software engineering. Many studies have proposed tools to automatically detect their presence and quantify their impact on performance. However, there remainsso farno evidence about the lifespan of these code smells in the history of mobile apps. In this paper, we present the first large-scale empirical study that investigates the survival of Android code smells. This study covers 8 types of Android code smells, 324 Android apps, 255k commits, and the history of 180k code smell instances. Our study reports that while in terms of time Android code smells can remain in the codebase for years before being removed, it only takes 34 effective commits to remove 75% of them. Also, Android code smells disappear faster in bigger projects with higher releasing trends. Finally, we observed that code smells that are detected and prioritised by linters tend to disappear before other code smells.",
      "Keywords": "Android | code smells | Mobile apps",
      "Publication venue": "Proceedings - 2019 IEEE/ACM 6th International Conference on Mobile Software Engineering and Systems, MOBILESoft 2019",
      "Publication date": "2019-05-01",
      "Publication type": "Conference Paper",
      "Authors": "Habchi, Sarra;Rouvoy, Romain;Moha, Naouel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85071860603",
      "Primary study DOI": "10.1109/ICSE-Companion.2019.00062",
      "Title": "Characterizing and detecting duplicate logging code smells",
      "Abstract": "Software logs are widely used by developers to assist in various tasks. Despite the importance of logs, prior studies show that there is no industrial standard on how to write logging statements. Recent research on logs often only considers the appropriateness of a log as an individual item (e.g., one single logging statement); while logs are typically analyzed in tandem. In this paper, we focus on studying duplicate logging statements, which are logging statements that have the same static text message. Such duplications in the text message are potential indications of logging code smells, which may affect developers' understanding of the dynamic view of the system. We manually studied over 3K duplicate logging statements and their surrounding code in four large-scale open source systems and uncovered five patterns of duplicate logging code smells. For each instance of the problematic code smell, we contact developers in order to verify our manual study result. We integrated our manual study result and developers' feedback into our automated static analysis tool, DLFinder, which automatically detects problematic duplicate logging code smells. We evaluated DLFinder on the manually studied systems and two additional systems. In total, combining the results of DLFinder and our manual analysis, DLFinder is able to detect over 85% of the instances which were reported to developers and then fixed.",
      "Keywords": "Code smell | Duplicate log | Log | Static analysis",
      "Publication venue": "Proceedings - 2019 IEEE/ACM 41st International Conference on Software Engineering: Companion, ICSE-Companion 2019",
      "Publication date": "2019-05-01",
      "Publication type": "Conference Paper",
      "Authors": "Li, Zhenhao",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85070263786",
      "Primary study DOI": "10.23919/MIPRO.2019.08757009",
      "Title": "Adapting God Class thresholds for software defect prediction: A case study",
      "Abstract": "In software engineering there is an active research field of defect prediction using software metrics. While the research shows that the prediction of defects using software metrics performs well, prediction using metrics alone lacks clear refactoring capabilities. On the other hand, code smells have the ability to describe the code anomalies precisely, and suggest their refactoring. Therefore, code smells can be a much better starting position for software fault prediction. In this paper, we present the results of preliminary research on the ability to predict software defects with the code smell God Class. The aim of our research was to test the definition of God Class, as defined by Lanza and Marinescu in 2006, in the ability to predict defects in a case study of the open source projects JDT and PDE within the Eclipse framework. The definition of the God Class was adapted using the grid search technique, with the goal of maximizing the fault prediction ability while keeping the base of the original definition. The results show that adaption of the definition in the specific project resulted in improved fault prediction ability.",
      "Keywords": "",
      "Publication venue": "2019 42nd International Convention on Information and Communication Technology, Electronics and Microelectronics, MIPRO 2019 - Proceedings",
      "Publication date": "2019-05-01",
      "Publication type": "Conference Paper",
      "Authors": "Gradišnik, Mitja;Beranic, Tina;Karakatic, Sašo;Mauša, Goran",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85072319546",
      "Primary study DOI": "10.1109/ICPC.2019.00043",
      "Title": "Learning a classifier for prediction of maintainability based on static analysis tools",
      "Abstract": "Static Code Analysis Tools are a popular aid to monitor and control the quality of software systems. Still, these tools only provide a large number of measurements that have to be interpreted by the developers in order to obtain insights about the actual quality of the software. In cooperation with professional quality analysts, we manually inspected source code from three different projects and evaluated its maintainability. We then trained machine learning algorithms to predict the human maintainability evaluation of program classes based on code metrics. The code metrics include structural metrics such as nesting depth, cloning information and abstractions like the number of code smells. We evaluated this approach on a dataset of more than 115,000 Lines of Code. Our model is able to predict up to 81% of the threefold labels correctly and achieves a precision of 80%. Thus, we believe this is a promising contribution towards automated maintainability prediction. In addition, we analyzed the attributes in our created dataset and identified the features with the highest predictive power, i.e. code clones, method length, and the number of alerts raised by the tool Teamscale. This insight provides valuable help for users needing to prioritize tool measurements.",
      "Keywords": "Code Comprehension | Maintenance Tools | Software Maintenance | Software Quality | Static Code Analysis",
      "Publication venue": "IEEE International Conference on Program Comprehension",
      "Publication date": "2019-05-01",
      "Publication type": "Conference Paper",
      "Authors": "Schnappinger, Markus;Osman, Mohd Hafeez;Pretschner, Alexander;Fietzke, Arnaud",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85071869820",
      "Primary study DOI": "10.1109/ICSE-Companion.2019.00073",
      "Title": "Towards just-in-time rational refactoring",
      "Abstract": "Code smells have been defined as symptoms of poor design and implementation choices. Empirical studies showed that code smells can have a negative impact on the maintainability of code. For this reason, tools have been developed to auto-matically detect design flaws and, in some cases, to recommend developers how to remove them via refactoring. However, these tools are not able to prevent the introduction of design flaws. This means that the code has to experience a quality decay before state-of-the-art tools can be applied. In addition, existing tools recommend refactoring operations that mostly target the improvement of quality metrics (e.g., cohesion) rather than the generation of refactorings that are meaningful from the developers' perspective. Our goal is to develop techniques serving as the basis for a new generation of refactoring recommenders able to (i) predict code components likely to be affected by code smells in the near future, to refactor them before they experience a quality decay and (ii) recommend meaningful refactorings emulating the ones that developers would perform, rather than the ones targeting the improvement of metrics. We refer to such a perspective on refactoring as just-in-time rational refactoring.",
      "Keywords": "Code quality metrics | Maintenance | Refactoring",
      "Publication venue": "Proceedings - 2019 IEEE/ACM 41st International Conference on Software Engineering: Companion, ICSE-Companion 2019",
      "Publication date": "2019-05-01",
      "Publication type": "Conference Paper",
      "Authors": "Pantiuchina, Jevgenija",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85073475590",
      "Primary study DOI": "10.1109/IWoR.2019.00016",
      "Title": "GodExpo: An automated god structure detection tool for Golang",
      "Abstract": "God Class is a class that threatens maintainability and understandability of code by performing most of the work alone. Various tools exist that can detect God Class of Java or C++ programs, however, there is no existing tool for detecting God Class(Structure) in Golang. Although Golang is not an object-oriented language, it offers structures which are similar to classes in OOP as they can contain fields and methods. Unlike OOP, methods of a structure can be defined on any file in the package of Golang. This paper presents a tool entitled GodExpo to detect God Structures in Golang programs by calculating metrics namely Weighted Method Count, Tight Class Cohesion, and Access to Foreign Data. In addition, GodExpo can provide version wise result to observe the evolution of God structures. To evaluate GodExpo, an experiment has been conducted on several versions of two open source Golang projects and the tool successfully found God structures in all versions of those projects.",
      "Keywords": "Code smell | God class | Golang | OOP metrics",
      "Publication venue": "Proceedings - 2019 IEEE/ACM 3rd International Workshop on Refactoring, IWOR 2019",
      "Publication date": "2019-05-01",
      "Publication type": "Conference Paper",
      "Authors": "Yasir, Rafed Muhammad;Asad, Moumita;Galib, Asadullah Hill;Ganguly, Kishan Kumar;Siddik, Md Saeed",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85072298048",
      "Primary study DOI": "10.1109/ICSE.2019.00125",
      "Title": "Detecting Incorrect Build Rules",
      "Abstract": "Automated build systems are routinely used by software engineers to minimize the number of objects that need to be recompiled after incremental changes to the source files of a project. In order to achieve efficient and correct builds, developers must provide the build tools with dependency information between the files and modules of a project, usually expressed in a macro language specific to each build tool. In order to guarantee correctness, the authors of these tools are responsible for enumerating all the files whose contents an output depends on. Unfortunately, this is a tedious process and not all dependencies are captured in practice, which leads to incorrect builds. We automatically uncover such missing dependencies through a novel method that we call build fuzzing. The correctness of build definitions is verified by modifying files in a project, triggering incremental builds and comparing the set of changed files to the set of expected changes. These sets are determined using a dependency graph inferred by tracing the system calls executed during a clean build. We evaluate our method by exhaustively testing build rules of open-source projects, uncovering issues leading to race conditions and faulty builds in 31 of them. We provide a discussion of the bugs we detect, identifying anti-patterns in the use of the macro languages. We fix some of the issues in projects where the features of build systems allow a clean solution.",
      "Keywords": "build tools | exhaustive testing | verification",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2019-05-01",
      "Publication type": "Conference Paper",
      "Authors": "Licker, Nandor;Rice, Andrew",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85072345308",
      "Primary study DOI": "10.1109/ICPC.2019.00014",
      "Title": "Improving source code readability: Theory and practice",
      "Abstract": "There are several widely accepted metrics to measure code quality that are currently being used in both research and practice to detect code smells and to find opportunities for code improvement. Although these metrics have been proposed as a proxy of code quality, recent research suggests that more often than not, state-of-the-art code quality metrics do not successfully capture quality improvements in the source code as perceived by developers. More specifically, results show that there may be inconsistencies between, on the one hand, the results from metrics for cohesion, coupling, complexity, and readability, and, on the other hand, the interpretation of these metrics in practice. As code improvement tools rely on these metrics, there is a clear need to identify and resolve the aforementioned inconsistencies. This will allow for the creation of tools that are more aligned with developers' perception of quality, and can more effectively help source code improvement efforts. In this study, we investigate 548 instances of source code readability improvements, as explicitly stated by internal developers in practice, from 63 engineered software projects. We show that current readability models fail to capture readability improvements. We also show that tools to calculate additional metrics, to detect refactorings, and to detect style problems are able to capture characteristics that are specific to readability changes and thus should be considered by future readability models.",
      "Keywords": "Code Quality metrics | Developers perception | Readability",
      "Publication venue": "IEEE International Conference on Program Comprehension",
      "Publication date": "2019-05-01",
      "Publication type": "Conference Paper",
      "Authors": "Fakhoury, Sarah;Roy, Devjeet;Hassan, Adnan;Arnaoudova, Vernera",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85069608480",
      "Primary study DOI": "10.1109/MSR.2019.00072",
      "Title": "Assessing diffusion and perception of test smells in scala projects",
      "Abstract": "Test smells are, analogously to code smells, defined as the characteristics exhibited by poorly designed unit tests. Their negative impact on test effectiveness, understanding, and maintenance has been demonstrated by several empirical studies. However, the scope of these studies has been limited mostly to Java in combination with the JUnit testing framework. Results for other language and framework combinations are - despite their prevalence in practice - few and far between, which might skew our understanding of test smells. The combination of Scala and ScalaTest, for instance, offers more comprehensive means for defining and reusing test fixtures, thereby possibly reducing the diffusion and perception of fixture-related test smells. This paper therefore reports on two empirical studies conducted for this combination. In the first study, we analyse the tests of 164 open-source Scala projects hosted on GitHub for the diffusion of test smells. This required the transposition of their original definition to this new context, and the implementation of a tool (SOCRATES) for their automated detection. In the second study, we assess the perception and the ability of 14 Scala developers to identify test smells. For this context, our results show (i) that test smells have a low diffusion across test classes, (ii) that the most frequently occurring test smells are LazyTest, EagerTest, and AssertionRoulette, and (iii) that many developers were able to perceive but not to identify the smells.",
      "Keywords": "Scala Language | Test Quality | Test Smells",
      "Publication venue": "IEEE International Working Conference on Mining Software Repositories",
      "Publication date": "2019-05-01",
      "Publication type": "Conference Paper",
      "Authors": "De Bleser, Jonas;Di Nucci, Dario;De Roover, Coen",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85068602712",
      "Primary study DOI": "10.1145/3328833.3328834",
      "Title": "Comparative study for detecting mobile application's anti-patterns",
      "Abstract": "Software design has a main impact in the quality of the software systems. Anti-patterns are shortcomings exist in the software designs and impact negatively software quality. Mobile applications (apps) with anti-patterns have bad quality and short lifetime. Many empirical studies have assessed that the antipatterns have a negative impact on change-proneness, fault-proneness, memory consumption and energy efficiency. In addition to that, many studies showed that there was an improvement in the user interface and memory performance of mobile apps when correcting Android anti-patterns. The aim of our research is choosing the suitable UML modeling environment to detect Mobile applications' anti-patterns via reverse engineering. So, in this research, first we present a comparative study between nine UML tools for determining the tools that have the functionality for (reverse, forward) engineering and have the ability for validating the model against the anti-patterns. Second, we apply our proposed method to generate the class diagram model of the apps through decoding the Java source code and detects the design anti-patterns in the model. For validating the proposed method, we applied it in twenty-nine Mobile apps which were downloaded from APKmirror. The proposed method detects and treats ten anti-patterns which have appeared 749 times in the twenty-nine apps.",
      "Keywords": "Anti-patterns | Mobile Applications | Modelio | Reverse Engineering | UML",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2019-04-09",
      "Publication type": "Conference Paper",
      "Authors": "El-Dahshan, Kamal A.;Elsayed, Eman K.;Ghannam, Naglaa E.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85064168173",
      "Primary study DOI": "10.1109/SANER.2019.8667967",
      "Title": "Automating Performance Antipattern Detection and Software Refactoring in UML Models",
      "Abstract": "The satisfaction of ever more stringent performance requirements is one of the main reasons for software evolution. However, it is complex to determine the primary causes of performance degradation, because they may depend on the joint combination of multiple factors (e.g., workload, software deployment, hardware utilization). With the increasing complexity of software systems, classical bottleneck analysis shows limitations in capturing complex performance problems. Hence, in the last decade, the detection of performance antipatterns has gained momentum as an effective way to identify performance degradation causes. We introduce PADRE (Performance Antipattern Detection and REfactoring), that is a tool for: (i) detecting performance antipattern in UML models, and (ii) refactoring models with the aim of removing the detected antipatterns. PADRE has been implemented within Epsilon, an open-source platform for model-driven engineering. It is based on a methodology that allows performance antipattern detection and refactoring within the same implementation context.",
      "Keywords": "Model-Driven Development | Software Performance",
      "Publication venue": "SANER 2019 - Proceedings of the 2019 IEEE 26th International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2019-03-15",
      "Publication type": "Conference Paper",
      "Authors": "Arcelli, Davide;Cortellessa, Vittorio;Pompeo, Daniele Di",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85064148516",
      "Primary study DOI": "",
      "Title": "SANER 2019 - Proceedings of the 2019 IEEE 26th International Conference on Software Analysis, Evolution, and Reengineering",
      "Abstract": "The proceedings contain 70 papers. The topics discussed include: software engineering in a data science future; automating performance anti-pattern detection and software refactoring in UML models; program state coverage: a test coverage metric based on executed program states; DeepLink: a code knowledge graph based deep learning approach for issue-commit link recovery; AVATAR: fixing semantic bugs with fix patterns of static analysis violations; exploring regular expression evolution; testing the message flow of android auto apps; and please help! a preliminary study on the effect of social proof and legitimization of paltry contributions in donations to OSS.",
      "Keywords": "",
      "Publication venue": "SANER 2019 - Proceedings of the 2019 IEEE 26th International Conference on Software Analysis, Evolution, and Reengineering",
      "Publication date": "2019-03-15",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85063385452",
      "Primary study DOI": "10.1007/s11219-018-9405-y",
      "Title": "On the proposal and evaluation of a benchmark-based threshold derivation method",
      "Abstract": "Software-intensive systems have been growing in both size and complexity. Consequently, developers need better support for measuring and controlling the software quality. In this context, software metrics aim at quantifying different software quality aspects. However, the effectiveness of measurement depends on the definition of reliable metric thresholds, i.e., numbers that characterize a metric value as critical given a quality aspect. In fact, without proper metric thresholds, it might be difficult for developers to indicate problematic software components for correction, for instance. Based on a literature review, we have found several existing methods for deriving metric thresholds and observed their evolution. Such evolution motivated us to propose a new method that incorporates the best of the existing methods. In this paper, we propose a novel benchmark-based method for deriving metric thresholds. We assess our method, called Vale’s method, using a set of metric thresholds derived with the support of our method, aimed at composing detection strategies for two well-known code smells, namely god class and lazy class. For this purpose, we analyze three benchmarks composed of multiple software product lines. In addition, we demonstrate our method in practice by applying it to a benchmark composed of 103 Java open-source software systems. In the evaluation, we compare Vale’s method to two state-of-the-practice threshold derivation methods selected as a baseline, which are Lanza’s method and Alves’ method. Our results suggest that the proposed method provides more realistic and reliable thresholds, with better recall and precision in the code smell detection, when compared to both baseline methods.",
      "Keywords": "Benchmark | Code smell | Software metric | Software product lines | Threshold",
      "Publication venue": "Software Quality Journal",
      "Publication date": "2019-03-15",
      "Publication type": "Review",
      "Authors": "Vale, Gustavo;Fernandes, Eduardo;Figueiredo, Eduardo",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85078070235",
      "Primary study DOI": "",
      "Title": "Vetting anti-patterns in Java to Kotlin translation",
      "Abstract": "With Kotlin becoming a viable language replacement for Java, there is a need for translators and data flow analysis libraries to create maintainable and readable source code. Instagram, Uber, and Gradle are only a few of the large corporations that have either switched from Java to Kotlin completely or started to use it in internal tools in order to reduce code base size. Developers have claimed that Kotlin is fun to use in comparison to Java and much of the boilerplate code is reduced. With Java being the main language for the open source organization, PhenoApps, there is a need to support both Java and Kotlin to increase the maintainability of the code. Fortunately, JetBrains has an open-source IDE plugin for translating Java to Kotlin; however, the translation has some fundamental issues which shall be discussed further in this paper. Introducing, j2k, a CLI translation tool which includes various anti-pattern detection for syntactical formatting, performance, and other Android requirements. The new tool introduced within this paper, j2kCLI allows users to directly translate strings of Java code to Kotlin, or entire directories. This facilitates the maintainability of a large open source code base.",
      "Keywords": "Abstract syntax tree | Android | Command line interface | Kotlin | Null pointer exception",
      "Publication venue": "Proceedings of 34th International Conference on Computers and Their Applications, CATA 2019",
      "Publication date": "2019-03-13",
      "Publication type": "Conference Paper",
      "Authors": "Courtney, Chaney;Neilsen, Mitchell",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84989881553",
      "Primary study DOI": "10.1007/978-3-319-47106-8_24",
      "Title": "Interactive code smells detection: An initial investigation",
      "Abstract": "In this paper, we introduced a novel technique to generate more user-oriented detection rules by taking into account their feedback. Our techniques initially generate a set of detection rules that will be used to detect candidate code smells, these reported code smells will be exposed in an interactive fashion to the developer who will give his/her feedback by either approving or rejecting the identified code smell in the code fragment. This feedback will be fed to the GP as constraints and additional examples in order to converge towards more user-preferred detection rules. We initially investigated the detection of three types of code smells in four open source systems and reported that the interactive code smell detection achieves a precision of 89 % and recall on average when detecting infected classes. Results show that our approach can best imitate the user’s decision while omitting the complexity of manual tuning the detection rules.",
      "Keywords": "",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2016-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Mkaouer, Mohamed Wiem",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85063376990",
      "Primary study DOI": "10.1002/smr.2154",
      "Title": "A survey on UML model smells detection techniques for software refactoring",
      "Abstract": "Bad smells tend to have a negative impact on software by degrading its quality. It is beneficial to detect model smells to avoid their propagation to later stages of software development. The objective of this paper is to present the state-of-the-art research on techniques for detecting UML model bad smells. The detection techniques are compared and evaluated using a proposed evaluation framework. The framework consists of two parts. The first part of the framework compares the techniques in terms of the implemented approach, the investigated model, and the explored model smells, while the experimental design is explored in the second part of the framework. We found that the detection of bad smells in class and sequence diagrams is accomplished via design patterns, software metrics, and predefined rules, while model smells in use cases are detected using metrics and predefined rules. We also found that the class diagram is the most investigated UML model in the context of model smell detection, whereas there is a lack of work on other UML models. In addition, there is a scarcity of independent studies on sequence diagrams. Furthermore, the studies investigating class diagrams are mostly validated, whereas use case diagrams and sequence diagrams are rarely validated.",
      "Keywords": "bad smell detection | maintainability | model bad smells | software refactoring",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2019-03-01",
      "Publication type": "Review",
      "Authors": "Mumtaz, Haris;Alshayeb, Mohammad;Mahmood, Sajjad;Niazi, Mahmood",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85041438039",
      "Primary study DOI": "10.1109/ASE.2017.8115667",
      "Title": "Detecting unknown inconsistencies in web applications",
      "Abstract": "Although there has been increasing demand for more reliable web applications, JavaScript bugs abound in web applications. In response to this issue, researchers have proposed automated fault detection tools, which statically analyze the web application code to find bugs. While useful, these tools either only target a limited set of bugs based on predefined rules, or they do not detect bugs caused by cross-language interactions, which occur frequently in web application code. To address this problem, we present an anomaly-based inconsistency detection approach, implemented in a tool called HOLOCRON. The main novelty of our approach is that it does not look for hard-coded inconsistency classes. Instead, it applies subtree pattern matching to infer inconsistency classes and association rule mining to detect inconsistencies that occur both within a single language, and between two languages. We evaluated HOLOCRON, and it successfully detected 51 previously unreported inconsistencies - including 18 bugs and 33 code smells - in 12 web applications.",
      "Keywords": "cross-language interactions | fault detection | JavaScript",
      "Publication venue": "ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering",
      "Publication date": "2017-11-20",
      "Publication type": "Conference Paper",
      "Authors": "Ocariza, Frolin S.;Pattabiraman, Karthik;Mesbah, Ali",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85056226726",
      "Primary study DOI": "10.1016/j.infsof.2018.10.012",
      "Title": "On semantic detection of cloud API (anti)patterns",
      "Abstract": "Context: Open standards are urgently needed for enabling software interoperability in Cloud Computing. Open Cloud Computing Interface (OCCI) provides a set of best design principles to create interoperable REST management APIs. Although OCCI is the only standard addressing the management of any kind of cloud resources, it does not support a range of best principles related to REST design. This often worsens REST API quality by decreasing their understandability and reusability. Objective: We aim at assisting cloud developers to enhance their REST management APIs by providing a compliance evaluation of OCCI and REST best principles and a recommendation support to comply with these principles. Method: First, we leverage patterns and anti-patterns to drive respectively the good and poor practices of OCCI and REST best principles. Then, we propose a semantic-based approach for defining and detecting REST and OCCI (anti)patterns and providing a set of correction recommendations to comply with both REST and OCCI best principles. We validated this approach by applying it on cloud REST APIs and evaluating its accuracy, usefulness and extensibility. Results: We found that our approach accurately detects OCCI and REST(anti)patterns and provides useful recommendations. According to the compliance results, we reveal that there is no widespread adoption of OCCI principles in existing APIs. In contrast, these APIs have reached an acceptable level of maturity regarding REST principles. Conclusion: Our approach provides an effective and extensible technique for defining and detecting OCCI and REST (anti)patterns in Cloud REST APIs. Cloud software developers can benefit from our approach and defined principles to accurately evaluate their APIs from OCCI and REST perspectives. This contributes in designing interoperable, understandable, and reusable Cloud management APIs. Thank to the compliance analysis and the recommendation support, we also contribute to improving these APIs, which make them more straightforward.",
      "Keywords": "Analysis | Anti-pattern | Cloud computing | Detection | OCCI | Ontology | Pattern | REST | Specification",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2019-03-01",
      "Publication type": "Article",
      "Authors": "Brabra, Hayet;Mtibaa, Achraf;Petrillo, Fabio;Merle, Philippe;Sliman, Layth;Moha, Naouel;Gaaloul, Walid;Guéhéneuc, Yann Gaël;Benatallah, Boualem;Gargouri, Faïez",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85057992692",
      "Primary study DOI": "10.1016/j.infsof.2018.11.008",
      "Title": "A survey on software coupling relations and tools",
      "Abstract": "Context: Coupling relations reflect the dependencies between software entities and can be used to assess the quality of a program. For this reason, a vast amount of them has been developed, together with tools to compute their related metrics. However, this makes the coupling measures suitable for a given application challenging to find. Goals: The first objective of this work is to provide a classification of the different kinds of coupling relations, together with the metrics to measure them. The second consists in presenting an overview of the tools proposed until now by the software engineering academic community to extract these metrics. Method: This work constitutes a systematic literature review in software engineering. To retrieve the referenced publications, publicly available scientific research databases were used. These sources were queried using keywords inherent to software coupling. We included publications from the period 2002 to 2017 and highly cited earlier publications. A snowballing technique was used to retrieve further related material. Results: Four groups of coupling relations were found: structural, dynamic, semantic and logical. A fifth set of coupling relations includes approaches too recent to be considered an independent group and measures developed for specific environments. The investigation also retrieved tools that extract the metrics belonging to each coupling group. Conclusion: This study shows the directions followed by the research on software coupling: e.g., developing metrics for specific environments. Concerning the metric tools, three trends have emerged in recent years: use of visualization techniques, extensibility and scalability. Finally, some coupling metrics applications were presented (e.g., code smell detection), indicating possible future research directions. Public preprint [https://doi.org/10.5281/zenodo.2002001].",
      "Keywords": "Coupling relations | Software engineering | Software metrics",
      "Publication venue": "Information and Software Technology",
      "Publication date": "2019-03-01",
      "Publication type": "Review",
      "Authors": "Fregnan, Enrico;Baum, Tobias;Palomba, Fabio;Bacchelli, Alberto",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85034272704",
      "Primary study DOI": "10.1109/TSE.2017.2770122",
      "Title": "Toward a smell-aware bug prediction model",
      "Abstract": "Code smells are symptoms of poor design and implementation choices. Previous studies empirically assessed the impact of smells on code quality and clearly indicate their negative impact on maintainability, including a higher bug-proneness of components affected by code smells. In this paper, we capture previous findings on bug-proneness to build a specialized bug prediction model for smelly classes. Specifically, we evaluate the contribution of a measure of the severity of code smells (i.e., code smell intensity) by adding it to existing bug prediction models based on both product and process metrics, and comparing the results of the new model against the baseline models. Results indicate that the accuracy of a bug prediction model increases by adding the code smell intensity as predictor. We also compare the results achieved by the proposed model with the ones of an alternative technique which considers metrics about the history of code smells in files, finding that our model works generally better. However, we observed interesting complementarities between the set of buggy and smelly classes correctly classified by the two models. By evaluating the actual information gain provided by the intensity index with respect to the other metrics in the model, we found that the intensity index is a relevant feature for both product and process metrics-based models. At the same time, the metric counting the average number of code smells in previous versions of a class considered by the alternative model is also able to reduce the entropy of the model. On the basis of this result, we devise and evaluate a smell-aware combined bug prediction model that included product, process, and smell-related features. We demonstrate how such model classifies bug-prone code components with an F-Measure at least 13 percent higher than the existing state-of-the-art models.",
      "Keywords": "bug prediction | Code smells | empirical study | mining software repositories",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2019-02-01",
      "Publication type": "Article",
      "Authors": "Palomba, Fabio;Zanoni, Marco;Fontana, Francesca Arcelli;De Lucia, Andrea;Oliveto, Rocco",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85062071925",
      "Primary study DOI": "10.1109/ICTER.8615550",
      "Title": "Bug prediction model using code smells",
      "Abstract": "The term 'Code Smells' was first coined in the book Refactoring: Improving the design of existing code by M Fowler in 1999. Code smells are poor design choices which have the potential to cause an error or failure in a computer program. The objective of this study is to use code smells as a candidate metric to build a bug prediction model. In this study we have built a bug prediction model using both source code metrics and code smell based metrics proposed in the literature. We used Naive Bayes, Random Forest and Logistic Regression as our candidate algorithms to build the model. We have trained our model against multiple versions of 13 different Java based open source projects. The trained model was used to predict bugs in a particular version of a project, within a particular project and among different projects. We were able to demonstrate, that code smell based metrics can significantly improve the accuracy of a bug prediction model when integrated with source code metrics. Random Forest algorithm based model showed higher accuracy within a version, within a project and among projects when compared to other algorithms.",
      "Keywords": "Bug prediction | Code smells | Source code metrics",
      "Publication venue": "18th International Conference on Advances in ICT for Emerging Regions, ICTer 2018 - Proceedings",
      "Publication date": "2018-07-02",
      "Publication type": "Conference Paper",
      "Authors": "Ubayawardana, Gihan M.;Karunaratna, Damith D.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85071452558",
      "Primary study DOI": "10.5220/0006869201640175",
      "Title": "Towards a taxonomy of bad smells detection approaches",
      "Abstract": "Refactoring is a popular maintenance activity that improves the internal structure of a software system while maintaining its external behaviour. During the refactoring process, detecting bad smells plays a crucial role in establishing reliable and accurate results. So far, several approaches have been proposed in the literature to detect bad smells at different levels. In this paper, we focus on reviewing the state-of-the-art of object-oriented bad smells detection approaches. For the purpose of comparability, we propose a hierarchical taxonomy by following a development methodology. Our taxonomy encompasses three main dimensions describing the detection approach via the used method, analysis and assessment. The resulting taxonomy provides a deeper understanding of existing approaches. It highlights many key factors that concern the developers when making a choice of an existing detection approach or when proposing a new one.",
      "Keywords": "Anti-patterns | Code Smells | Design Smells | Detection Approaches | Taxonomy",
      "Publication venue": "ICSOFT 2018 - Proceedings of the 13th International Conference on Software Technologies",
      "Publication date": "2019-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Hadj-Kacem, Mouna;Bouassida, Nadia",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85061716371",
      "Primary study DOI": "10.1007/978-981-10-8201-6_16",
      "Title": "Detection of bad smell code for software refactoring",
      "Abstract": "Software refactoring is the process that requires modification in the source code to avoid bad smell. Professional has reviewed typical situations that may need refactoring call it bad smells, indicating that a few piece of the existing code stink dreadful. Bad smells are linked to consequent refactoring policy that can aid dismissing bad smells. Code smell is indication which representing some part is incorrect. It shows that code supposed to refractor or overall design should reconsider. Important is, where to refractor within in existing software is somewhat challenge to recognize region of bad design. Bad design is branded as “bad smells” in existing code. The detection of bad smell code can be done by parsing the particular code and store the related data in database, to detect the bad smell code, display result and provide solution.",
      "Keywords": "Detection of bad smell | Duplicate code | Lazy class | Long method | Parsing",
      "Publication venue": "Lecture Notes in Networks and Systems",
      "Publication date": "2019-01-01",
      "Publication type": "Book Chapter",
      "Authors": "Regulwar, Ganesh B.;Tugnayat, R. M.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85071681750",
      "Primary study DOI": "10.1007/978-3-030-29157-0_2",
      "Title": "A Rating Tool for the Automated Selection of Software Refactorings that Remove Antipatterns to Improve Performance and Stability",
      "Abstract": "Antipatterns are known to be bad solutions for recurring design problems. To detect and remove antipatterns has proven to be a useful mean to improve the quality of software. While there exist several approaches to detect antipatterns automatically, existing work on antipattern detection often does not solve the detected design problems automatically. Although there exist refactorings that have the potential to significantly increase the quality of a program, it is hard to decide which refactorings effectively yield improvements with respect to performance and stability. In this paper, we present a rating tool that makes use of static antipattern detection together with software profiling for the automated selection of refactorings that remove antipatterns and are promising candidates to improve performance and stability. Our key idea is to extend a previously proposed heuristics that utilizes software properties determined by both static code analyses and dynamic software analyses to compile a list of concrete refactorings sorted by their assessed potential to improve performance with an approach to identify refactorings that may improve stability. We do not impose an order on the refactorings that may improve stability. We demonstrate the practical applicability of our overall approach with experimental results.",
      "Keywords": "Performance | Software refactoring | Stability antipattern detection",
      "Publication venue": "Communications in Computer and Information Science",
      "Publication date": "2019-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Moesus, Nikolai;Scholze, Matthias;Schlesinger, Sebastian;Herber, Paula",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85068620741",
      "Primary study DOI": "10.1007/978-3-030-24305-0_31",
      "Title": "Code Smells Enabled by Artificial Intelligence: A Systematic Mapping",
      "Abstract": "Code smells are an indicator of poor design in software systems. Artificial intelligence techniques have been applied in several ways to improve soft-ware quality in code smells detection i.e. (detection rules or standards using a combination of object-oriented metrics and Bayesian inference graphs). Literature in the field has identified artificial intelligence techniques and compare different artificial intelligence algorithms, which are used in the detection of code smells. However, to the best of our knowledge, there is not a systematic literature review devoted to study in deep the interaction of these fields. In this paper, authors conduct a systematic mapping to get to know how artificial intelligence inter-acts with code smells. Results show the deep connection of Artificial Intelligence with code smells in a solid way, as well as, providing potential challenges and opportunities for future research.",
      "Keywords": "Artificial intelligence | Bad smells | Code smells | Systematic mapping",
      "Publication venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
      "Publication date": "2019-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Zaidi, Moayid Ali;Colomo-Palacios, Ricardo",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85065259117",
      "Primary study DOI": "10.1109/ACCESS.2019.2905133",
      "Title": "Assessment of code smell for predicting class change proneness using machine learning",
      "Abstract": "Assessment of code smell for predicting software change proneness is essential to ensure its significance in the area of software quality. While multiple studies have been conducted in this regard, the number of systems studied and the methods used in this paper are quite different, thus, causing confusion for understanding the best methodology. The objective of this paper is to approve the effect of code smell on the change inclination of a specific class in a product framework. This is the novelty and surplus of this work against the others. Furthermore, this paper aims to validate code smell for predicting class change proneness to find an error in the prediction of change proneness using code smell. Six typical machine learning algorithms (Naive Bayes Classifier, Multilayer Perceptron, LogitBoost, Bagging, Random Forest, and Decision Tree) have been used to predict change proneness using code smell from a set of 8200 Java classes spanning 14 software systems. The experimental results suggest that code smell is indeed a powerful predictor of class change proneness with multilayer perceptron being the most effective technique. The sensitivity and specificity values for all the models are well over 70% with a few exceptions.",
      "Keywords": "change proneness | Code smell | machine learning | multilayer perceptron | software maintenance",
      "Publication venue": "IEEE Access",
      "Publication date": "2019-01-01",
      "Publication type": "Article",
      "Authors": "Pritam, Nakul;Khari, Manju;Hoang Son, Le;Kumar, Raghvendra;Jha, Sudan;Priyadarshini, Ishaani;Abdel-Basset, Mohamed;Viet Long, Hoang",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85067098340",
      "Primary study DOI": "10.5220/0006938504130421",
      "Title": "On using UML diagrams to identify and assess software design smells",
      "Abstract": "Deficiencies in software design or architecture can severely impede and slow down the software development and maintenance progress. Bad smells and anti-patterns can be an indicator for poor software design and suggest for refactoring the affected source code fragment. In recent years, multiple techniques and tools have been proposed to assist software engineers in identifying smells and guiding them through corresponding refactoring steps. However, these detection tools only cover a modest amount of smells so far and also tend to produce false positives which represent conscious constructs with symptoms similar or identical to actual bad smells (e.g., design patterns). These and other issues in the detection process demand for a code or design review in order to identify (missed) design smells and/or re-assess detected smell candidates. UML diagrams are the quasi-standard for documenting software design and are often available in software projects. In this position paper, we investigate whether (and to which extend) UML diagrams can be used for identifying and assessing design smells. Based on a description of difficulties in the smell detection process, we discuss the importance of design reviews. We then investigate to which extend design documentation in terms of UML2 diagrams allows for representing and identifying software design smells. In particular, 14 kinds of design smells and their representability in UML class and sequence diagrams are analyzed. In addition, we discuss further challenges for UML-based identification and assessment of bad smells.",
      "Keywords": "Architectural Smells | Code and Design Review | Refactoring | Smell Detection and Assessment | Software Design Documentation | Software Design Smells | Technical Debt Management | Unified Modeling Language (UML2)",
      "Publication venue": "ICSOFT 2018 - Proceedings of the 13th International Conference on Software Technologies",
      "Publication date": "2019-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Haendler, Thorsten",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85013130017",
      "Primary study DOI": "10.1109/ICSME.2016.26",
      "Title": "Alternative sources of information for code smell detection: Postcards from far away",
      "Abstract": "Code smells have been defined as symptoms of poor design and implementation choices. Previous studies showed the negative impact of code smells on the comprehensibility and maintainability of code. For this reasons, several detection techniques have been proposed. Most of them rely on the analysis of the properties extractable from the source code. In the context of this work, we highlight several aspects that can possibly contribute to the improvement of the current state of the art and propose our solutions, based on the analysis on how code smells are actually introduced as well as the usefulness of historical and textual information to realize more reliable code smell detectors. Finally, we present an overview of the open issues and challenges related to code smell detection and management that the research community should focus on in the next future.",
      "Keywords": "",
      "Publication venue": "Proceedings - 2016 IEEE International Conference on Software Maintenance and Evolution, ICSME 2016",
      "Publication date": "2017-01-12",
      "Publication type": "Conference Paper",
      "Authors": "Palomba, Fabio",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85074154029",
      "Primary study DOI": "10.7717/peerj-cs.212",
      "Title": "Reverse engineering approach for improving the quality of mobile applications",
      "Abstract": "Background: Portable-devices applications (Android applications) are becoming complex software systems that must be developed quickly and continuously evolved to fit new user requirements and execution contexts. Applications must be produced rapidly and advance persistently in order to fit new client requirements and execution settings. However, catering to these imperativesmay bring about poor outline decisions on design choices, known as anti-patterns, which may possibly corrupt programming quality and execution. Thus, the automatic detection of anti-patterns is a vital process that facilitates bothmaintenance and evolution tasks. Additionally, it guides developers to refactor their applications and consequently enhance their quality. Methods: We proposed a general method to detect mobile applications' anti-patterns that can detect both semantic and structural design anti-patterns. The proposed method is via reverse-engineering and ontology by using a UML modeling environment, an OWL ontology-based platform and ontology-driven conceptual modeling. We present and test a new method that generates the OWL ontology of mobile applications and analyzes the relationships among object-oriented anti-patterns and offer methods to resolve the anti-patterns by detecting and treating 15 different design's semantic and structural anti-patterns that occurred in analyzing of 29 mobile applications. We choose 29 mobile applications randomly. Selecting a browser is not a criterion in this method because the proposed method is applied on a design level. We demonstrate a semantic integration method to reduce the incidence of anti-patterns using the ontology merging on mobile applications. Results: The proposed method detected 15 semantic and structural design anti-patterns which have appeared 1,262 times in a random sample of 29 mobile applications. The proposedmethod introduced a new classification of the anti-patterns divided into four groups. \"The anti-patterns in the class group\" is the most group that has themaximum occurrences of anti-patterns and \"The anti-patterns in the operation group\" is the smallest one that has the minimum occurrences of the anti-patterns which are detected by the proposed method. The results also showed the correlation between the selected tools which we used as Modelio, the Protégé platform, and the OLED editor of the OntoUML. The results showed that there was a high positive relation between Modelio and Protégé which implies that the combination between both increases the accuracy level of the detection of anti-patterns. In the evaluation and analyzing the suitable integration method, we applied the different methods on homogeneous mobile applications and found that using ontology increased the detection percentage approximately by 11.3% in addition to guaranteed consistency.",
      "Keywords": "Anti-patterns | Mobile applications | Ontology engineering | OntoUML | Reverse engineering | UML",
      "Publication venue": "PeerJ Computer Science",
      "Publication date": "2019-01-01",
      "Publication type": "Article",
      "Authors": "Elsayed, Eman K.;ElDahshan, Kamal A.;El-Sharawy, Enas E.;Ghannam, Naglaa E.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85068890299",
      "Primary study DOI": "10.1007/s11831-019-09348-6",
      "Title": "A Systematic Literature Review on Empirical Analysis of the Relationship Between Code Smells and Software Quality Attributes",
      "Abstract": "Code smells indicate problems in design or code which makes software hard to change and maintain. It has become a sign of software systems that cause complications in maintaining software quality. The detection of harmful code smells which deteriorate the software quality has resulted in a favourable shift in interest among researchers. Therefore, a significant research towards analysing the impact of code smells on software quality has been conducted over the last few years. This study aims at reporting a systematic literature review of such existing empirical studies investigate the impact of code smells on software quality attributes. The results indicate that the impact of code smells on software quality is not uniform as different code smells have the opposite effect on different software quality attributes. The findings of this review will provide the awareness to the researchers and a practitioner regarding the impact of code smells on software quality. It would be more advantageous to conduct further studies that consider less explored code smells, least or not investigated quality attributes, involve industry researchers and use large commercial software systems.",
      "Keywords": "",
      "Publication venue": "Archives of Computational Methods in Engineering",
      "Publication date": "2020-09-01",
      "Publication type": "Article",
      "Authors": "Kaur, Amandeep",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84951743253",
      "Primary study DOI": "10.1109/ICSE.2015.244",
      "Title": "Textual Analysis for Code Smell Detection",
      "Abstract": "The negative impact of smells on the quality of a software systems has been empirical investigated in several studies. This has recalled the need to have approaches for the identification and the removal of smells. While approaches to remove smells have investigated the use of both structural and conceptual information extracted from source code, approaches to identify smells are based on structural information only. In this paper, we bridge the gap analyzing to what extent conceptual information, extracted using textual analysis techniques, can be used to identify smells in source code. The proposed textual-based approach for detecting smells in source code, coined as TACO (Textual Analysis for Code smell detectiOn), has been instantiated for detecting the Long Method smell and has been evaluated on three Java open source projects. The results indicate that TACO is able to detect between 50% and 77% of the smell instances with a precision ranging between 63% and 67%. In addition, the results show that TACO identifies smells that are not identified by approaches based on solely structural information.",
      "Keywords": "",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2015-08-12",
      "Publication type": "Conference Paper",
      "Authors": "Palomba, Fabio",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85071507341",
      "Primary study DOI": "10.5220/0006918803890396",
      "Title": "Grasping primitive enthusiasm: Approaching primitive obsession in steps",
      "Abstract": "Primitive Obsession is a type of a code smell that has lacked the attention of the research community. Although, as a code smell it can be a useful indicator of underlying design problems in the source code, there was only one previously presented automated detection method. In this paper, the Primitive Obsession is discussed and multiple variants for Primitive Enthusiasm is defined. Primitive Enthusiasm is a metric designed to highlight possible Primitive Obsession infected code parts. Additionally other supplemental metrics are presented to grasp more aspects of Primitive Obsession as well. The current implementation of the described metrics is for Java and the evaluation was done on three open-source Java systems.",
      "Keywords": "Code Smells | Primitive Enthusiasm | Primitive Obsession | Refactoring | Static Analysis",
      "Publication venue": "ICSOFT 2018 - Proceedings of the 13th International Conference on Software Technologies",
      "Publication date": "2019-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Pengo, Edit;Gál, Péter",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85070670179",
      "Primary study DOI": "10.1002/smr.2208",
      "Title": "A delta-oriented approach to support the safe reuse of black-box code rewriters",
      "Abstract": "Large-scale corrective and perfective maintenance is often automated thanks to rewriting rules using tools such as Python2to3, Spoon, or Coccinelle. Such tools consider these rules as black-boxes and compose multiple rules by chaining them: giving the output of a given rewriting rule as input to the next one. It is up to the developer to identify the right order (if it exists) among all the different rules to yield the right program. In this paper, we define a formal model compatible with the black-box assumption that reifies the modifications (Δs) made by each rule. Leveraging these Δs, we propose a way to safely compose multiple rules when applied to the same program by (a) ensuring the isolated application of the different rules and (b) identifying unexpected behaviors that were silently ignored before. We assess this approach on two large-scale case studies: (a) identifying conflicts in the Linux source-code automated maintenance and (b) fixing energy antipatterns existing in Android applications available on GitHub.",
      "Keywords": "code rewriting | conflict detection | rule composition | software reuse",
      "Publication venue": "Journal of Software: Evolution and Process",
      "Publication date": "2019-08-01",
      "Publication type": "Conference Paper",
      "Authors": "Benni, Benjamin;Mosser, Sébastien;Moha, Naouel;Riveill, Michel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85080087473",
      "Primary study DOI": "10.29007/l86k",
      "Title": "On architectural decay prediction in real-time software systems",
      "Abstract": "As the number of software applications including the widespread of real-time and embedded systems are constantly increasing and tend to grow in complexity, the architecture tends to decay over the years, leading to the occurrence of a spectrum of defects and bad smells (i.e., instances of architectural decay) that are manifested and sustained over time in a software system’s life cycle. Thus, the implemented system is not compliant to the specified architecture and such architectural decay becomes an increasing challenge for the developers. We propose a set of constructive architecture views at different levels of granularity, which monitor and ensure that the modifications made by developers at the implementation level are in compliance with those of the different architectural timed-event elements of real-time systems. Thus, we investigated a set of orthogonal architectural decay paradigms timed-event component decay, timed-event interface decay, timed-event connector decay and timed-event port decay. All of this has led to predicting, forecasting, and detecting architectural decay with a greater degree of structure, abstraction techniques, architecture reconstruction; and hence offered a series of potential effectiveness and enhancement in gaining a deeper understanding of implementation-level bad smells in real-time systems. Furthermore, to support this research towards an effective architectural decay prediction and detection geared towards real-time and embedded systems, we investigated and evaluated the effect of our approach through a real-time Internet of Things (IoT) case study.",
      "Keywords": "",
      "Publication venue": "EPiC Series in Computing",
      "Publication date": "2019-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Fellah, Aziz;Bandi, Ajay",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85061278790",
      "Primary study DOI": "10.1145/3226593",
      "Title": "A hybrid approach for improving the design quality of web service interfaces",
      "Abstract": "A key success of a Web service is to appropriately design its interface to make it easy to consume and understand. In the context of service-oriented computing (SOC), the service's interface is the main source of interaction with the consumers to reuse the service functionality in real-world applications. The SOC paradigm provides a collection of principles and guidelines to properly design services to provide best practice of third-party reuse. However, recent studies showed that service designers tend to pay little care to the design of their service interfaces, which often lead to several side effects known as antipatterns. One of the most common Web service interface antipatterns is to expose a large number of semantically unrelated operations, implementing different abstractions, in one single interface. Such bad design practices may have a significant impact on the service reusability, understandability, as well as the development and run-time characteristics. To address this problem, in this article, we propose a hybrid approach to improve the design quality of Web service interfaces and fix antipatterns as a combination of both deterministic and heuristic-based approaches. The first step consists of a deterministic approach using a graph partitioning-based technique to split the operations of a large service interface into more cohesive interfaces, each one representing a distinct abstraction. Then, the produced interfaces will be checked using a heuristic-based approach based on the non-dominated sorting genetic algorithm (NSGA-II) to correct potential antipatterns while reducing the interface design deviation to avoid taking the service away from its original design. To evaluate our approach, we conduct an empirical study on a benchmark of 26 real-world Web services provided by Amazon and Yahoo. Our experiments consist of a quantitative evaluation based on design quality metrics, as well as a qualitative evaluation with developers to assess its usefulness in practice. The results show that our approach significantly outperforms existing approaches and provides more meaningful results from a developer's perspective.",
      "Keywords": "Antipattern | Search-based software engineering | Service-oriented computing | Web service design | Web services",
      "Publication venue": "ACM Transactions on Internet Technology",
      "Publication date": "2019-01-01",
      "Publication type": "Article",
      "Authors": "Ouni, Ali;Wang, Hanzhang;Kessentini, Marouane;Bouktif, Salah;Inoue, Katsuro",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85075245481",
      "Primary study DOI": "10.1007/978-981-15-0111-1_35",
      "Title": "An Efficient Architectural Framework for Non-obtrusive and Instantaneous Real-Time Identification of Clones During the Software Development Process in IDE",
      "Abstract": "Code Clones are well-known Source Code Smells that impacts the Software maintenance thus research community proposed various real-time clone detection approaches to proactively manage them during the software development process. The present-day real-time Code Clone identifiers have at least one of the five inadequacies: (a) entails Developer involvement to start the Clone Detection process, (b) despite of having focused search capability from few tools, Clone Detection necessitates to be triggered by the Developer, (c) in spite of few plug-in tools instigating concentrated search a large portion of available plug-ins procedure Clones in bunch mode and in this way expends much time to find clones, (d) despite being plugins to the IDEs, current tools require Software Programmer to trigger the visualization of Clone Detection results thus deficits instantaneous real-time Clone recognition functionality, (e) uses indexing techniques that can further be replaced by other available more efficient techniques to reduce the response time. This paper presents the Architectural Framework of underdevelopment real-time Code Clone Detection plug-in tool, which is proficiently adequate as a resolution to all the above-unveiled issues. The tool architecture description clearly indicates the proficiency of our approach in the application of automatic triggering of Clone Detection process as well as focused block level search on interception of block end leading to instantaneous real-time identification of clones and immediate recommendation mechanism.",
      "Keywords": "Architectural framework | Instantaneous real-time identification | Non-obtrusive Code Clone Detection | Software clones",
      "Publication venue": "Communications in Computer and Information Science",
      "Publication date": "2019-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Bharti, Sarveshwar;Singh, Hardeep",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85076490291",
      "Primary study DOI": "",
      "Title": "Survey on static analysis tools of python programs",
      "Abstract": "Static program analysis is a popular software technique performed by automated tools for verifying large scale software systems. It works without executing the program, just analyzing the source code and applying various heuristics to find possible errors, code smells and style discrepancies. Programming languages with strong static type system, like C, C++, Java are the usual targets of static analysis, as their type system provides additional information for the analyzer. However, there is a growing demand for applying static analysis for dynamically typed languages, like Python. In this paper we overview the current methods and tools available for static analysis on Python code base and describe some new research directions.",
      "Keywords": "",
      "Publication venue": "CEUR Workshop Proceedings",
      "Publication date": "2019-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Gulabovska, Hristina;Porkoláb, Zoltán",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85054360412",
      "Primary study DOI": "10.1002/spe.2639",
      "Title": "A systematic literature review on the detection of smells and their evolution in object-oriented and service-oriented systems",
      "Abstract": "This systematic literature review paper investigates the key techniques employed to identify smells in different paradigms of software engineering from object-oriented (OO) to service-oriented (SO). In this review, we want to identify commonalities and differences in the identification of smells in OO and SO systems. Our research method relies on an automatic search from the relevant digital libraries to find the studies published since January 2000 on smells until December 2017. We have conducted a pilot and author-based search that allows us to select the 78 most relevant studies after applying inclusion and exclusion criteria. We evaluated the studies based on the smell detection techniques and the evolution of different methodologies in OO and SO. Among the 78 relevant studies selected, we have identified six different studies in which linguistic source code analysis received less attention from the researchers as compared to the static source code analysis. Smells like the yo-yo problem, unnamed coupling, intensive coupling, and interface bloat received considerably less attention in the literature. We also identified a catalog of 30 smells infrequently reported for SO systems and that require further attention. Moreover, a suite of 20 smells reported for SO systems can also be detected using static source code metrics in OO. Finally, our review highlighted three major research trends that are further subdivided into 20 research patterns initiating the detection of smells toward their correction.",
      "Keywords": "antipatterns | design smells | object-oriented (OO) systems | service-oriented (SO) systems | smells",
      "Publication venue": "Software - Practice and Experience",
      "Publication date": "2019-01-01",
      "Publication type": "Review",
      "Authors": "Sabir, Fatima;Palma, Francis;Rasool, Ghulam;Guéhéneuc, Yann Gaël;Moha, Naouel",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85072836114",
      "Primary study DOI": "",
      "Title": "12th International Conference on the Quality of Information and Communications Technology, QUATIC 2019",
      "Abstract": "The proceedings contain 25 papers. The special focus in this conference is on Quality of Information and Communications Technology. The topics include: A Systematic Review on Software Testing Ontologies; hamcrest vs AssertJ: An Empirical Assessment of Tester Productivity; do We Rework? A Path to Manage One of the Primary Cause of Uncertainty in Software Industry; Studying Continual Service Improvement and Monitoring the Quality of ITSM; strategies for Developing Process Reference Models: An Analysis of Selected Cases from the Product Development and Management Domain; concern Metrics for Modularity-Oriented Modernizations; a Family of Domain-Specific Languages for Integrated Modular Avionics; proFit – Performing Dynamic Analysis of Software Systems; code Smells Survival Analysis in Web Apps; privacy Oriented Software Development; evaluation of Maritime Event Detection Against Missing Data; Toward the Measure of Credibility of Hospital Administrative Datasets in the Context of DRG Classification; RETORCH: Resource-Aware End-to-End Test Orchestration; android Testing Crawler; local Observability and Controllability Enforcement in Distributed Testing; mutation-Based Web Test Case Generation; Assessing Data Cybersecurity Using ISO/IEC 25012; data-Driven Elicitation of Quality Requirements in Agile Companies; On the Use of Non-technical Requirements for the Evaluation of FOSS Software Components; challenges in Requirement Engineering: Could Design Thinking Help?; understanding Process Models Using the Eye-Tracking: A Systematic Mapping; fakeChain: A Blockchain Architecture to Ensure Trust in Social Media Networks; an Experience in Modelling Business Process Architecture.",
      "Keywords": "",
      "Publication venue": "Communications in Computer and Information Science",
      "Publication date": "2019-01-01",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85067473212",
      "Primary study DOI": "10.5220/0007758804940501",
      "Title": "On the evolutionary relationship between change coupling and fix-inducing changes",
      "Abstract": "Change Coupling (CC) is the implicit relation formed between two or more changing software artifacts (e.g. source code). These artifacts are found to have design issues and code smells. Existing research has revealed the relationship between the change coupled relation of a class with the number of bugs in bug repositories. However, this ignored their true relation at the creation time of bugs or erroneous changes known as Fix-Inducing Changes (FIC). This paper tries to find the actual relationship between FIC and change coupled relations with respect to considering recent and all commits. This is done by traversing the entire history of a repository with a commit window of 100 commits and collecting data about FICs and metrics related to change coupling and object oriented system. It is found from the analysis that recent CC relations at the time of error are more correlated with new errors. Besides, it is found that explanatory power for predicting future erroneous change is more in recent CC relation than the one formed by considering all commits starting from the 1st commit.",
      "Keywords": "Change Coupling | Fix-Inducing Change | Software Bug | Software Defect",
      "Publication venue": "ENASE 2019 - Proceedings of the 14th International Conference on Evaluation of Novel Approaches to Software Engineering",
      "Publication date": "2019-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Sadiq, Ali Zafar;Jubair Ibna Mostafa, Md;Sakib, Kazi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84893525320",
      "Primary study DOI": "10.1109/ASE.2013.6693086",
      "Title": "Detecting bad smells in source code using change history information",
      "Abstract": "Code smells represent symptoms of poor implementation choices. Previous studies found that these smells make source code more difficult to maintain, possibly also increasing its fault-proneness. There are several approaches that identify smells based on code analysis techniques. However, we observe that many code smells are intrinsically characterized by how code elements change over time. Thus, relying solely on structural information may not be sufficient to detect all the smells accurately. We propose an approach to detect five different code smells, namely Divergent Change, Shotgun Surgery, Parallel Inheritance, Blob, and Feature Envy, by exploiting change history information mined from versioning systems. We applied approach, coined as HIST (Historical Information for Smell deTection), to eight software projects written in Java, and wherever possible compared with existing state-of-the-art smell detectors based on source code analysis. The results indicate that HIST's precision ranges between 61% and 80%, and its recall ranges between 61% and 100%. More importantly, the results confirm that HIST is able to identify code smells that cannot be identified through approaches solely based on code analysis. © 2013 IEEE.",
      "Keywords": "Change History Information | Code Smells",
      "Publication venue": "2013 28th IEEE/ACM International Conference on Automated Software Engineering, ASE 2013 - Proceedings",
      "Publication date": "2013-12-01",
      "Publication type": "Conference Paper",
      "Authors": "Palomba, Fabio;Bavota, Gabriele;Di Penta, Massimiliano;Oliveto, Rocco;De Lucia, Andrea;Poshyvanyk, Denys",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85058477221",
      "Primary study DOI": "10.1109/VISSOFT.2018.00010",
      "Title": "Detecting Bad Smells in Software Systems with Linked Multivariate Visualizations",
      "Abstract": "Parallel coordinates plots and RadViz are two visualization techniques that deal with multivariate data. They complement each other in identifying data patterns, clusters, and outliers. In this paper, we analyze multivariate software metrics linking the two approaches for detecting outliers, which could be the indicators for bad smells in software systems. Parallel coordinates plots provide an overview, whereas the RadViz representation allows for comparing a smaller subset of metrics in detail. We develop an interactive visual analytics system supporting automatic detection of bad smell patterns. In addition, we investigate the distinctive properties of outliers that are not considered harmful, but noteworthy for other reasons. We demonstrate our approach with open source Java systems and describe detected bad smells and other outlier patterns.",
      "Keywords": "Bad smells | Multivariate visualization | Software metrics",
      "Publication venue": "Proceedings - 6th IEEE Working Conference on Software Visualization, VISSOFT 2018",
      "Publication date": "2018-11-09",
      "Publication type": "Conference Paper",
      "Authors": "Mumtaz, Haris;Beck, Fabian;Weiskopf, Daniel",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85058273553",
      "Primary study DOI": "10.1109/ICSME.2018.00064",
      "Title": "Towards feature envy design flaw detection at block level",
      "Abstract": "Software is continuously evolving as bugs need to be fixed and new features need to be added. Design flaws hinder the simple evolution of software and thus, we have to detect and correct them. Feature Envy is an object-oriented design issue that can be detected at the level of methods using different state-of-The-Art approaches. Unfortunately, these are insufficient because only a portion of a method may actually be affected by this flaw. Thus, only that part needs to be treated using the corresponding correction strategy, not the entire method. To address this issue, we propose the detection of Feature Envy code smell at the level of blocks of code. Initial evaluation suggests that our approach is promising in spotting the envious areas within a method.",
      "Keywords": "Design flaw detection | feature envy | Refactoring",
      "Publication venue": "Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018",
      "Publication date": "2018-11-09",
      "Publication type": "Conference Paper",
      "Authors": "Kiss, Arpad;Mihancea, Petru Florin",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85058305515",
      "Primary study DOI": "",
      "Title": "Proceedings - 18th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2018",
      "Abstract": "The proceedings contain 27 papers. The topics discussed include: combining obfuscation and optimizations in the real world; obfuscating java programs by translating selected portions of bytecode to native libraries; enabling the continuous analysis of security vulnerabilities with VulData7; towards anticipation of architectural smells using link prediction techniques; periodic developer metrics in software defect prediction; which method-stereotype changes are indicators of code smells?; semantics-based code search using input/output examples; detecting evolutionary coupling using transitive association rules; the case for adaptive change recommendation; and on the use of machine learning techniques towards the design of cloud based automatic code clone validation tools.",
      "Keywords": "",
      "Publication venue": "Proceedings - 18th IEEE International Working Conference on Source Code Analysis and Manipulation, SCAM 2018",
      "Publication date": "2018-11-09",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85060630757",
      "Primary study DOI": "",
      "Title": "Code-smells identification by using PSO approach",
      "Abstract": "Code-smell defines the smells which arise in coding part of the software development life cycle. It is very crucial to detect smells in software projects. If smells are detected earlier then the possibility of occurrence of errors, faults will be reduced. Hence, quality of the software is improved. The existing work used Bayesian approaches, manual approaches and search-based approaches to detect smells. These approaches lack in getting optimization solutions in detecting process. So, paper makes use of one of the popular optimization technique called Particle Swarm Optimization (PSO) for detecting the smells in programming part. The technique shows how intelligently the smells are detected and mainly concentrated on five types of smells namely Long Methods, Long Parameters, Large Classes, Duplicated Codes, and Primitive Obsessions. Implementation of this technique is, considering source-code of any software applications or programs and injecting PSO technique into the system. Here, PSO has trained to detect five types of smells whenever their appear in the source-code. Detecting the smells in initial stages of the project gives best performance of the software, and in other hand quality of the software is achieved. Experimental results are shown by using PSO technique, where searching time will be less consumed and accuracy of the system is gained.",
      "Keywords": "Code-smells | Duplicated codes | PSO | Software quality",
      "Publication venue": "International Journal of Recent Technology and Engineering",
      "Publication date": "2018-11-01",
      "Publication type": "Article",
      "Authors": "Ramesh, G.;Mallikarjuna Rao, Ch",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85058321067",
      "Primary study DOI": "10.1145/3236024.3236027",
      "Title": "The impact of Regular Expression Denial of Service (ReDoS) in practice: An empirical study at the ecosystem scale",
      "Abstract": "Regular expressions (regexes) are a popular and powerful means of automatically manipulating text. Regexes are also an understudied denial of service vector (ReDoS). If a regex has super-linear worst-case complexity, an attacker may be able to trigger this complexity, exhausting the victim's CPU resources and causing denial of service. Existing research has shown how to detect these superlinear regexes, and practitioners have identified super-linear regex anti-pattern heuristics that may lead to such complexity. In this paper, we empirically study three major aspects of ReDoS that have hitherto been unexplored: The incidence of super-linear regexes in practice, how they can be prevented, and how they can be repaired. In the ecosystems of two of the most popular programming languages D JavaScript and Python S we detected thousands of super-linear regexes affecting over 10,000 modules across diverse application domains. We also found that the conventional wisdom for super-linear regex anti-patterns has few false negatives but many false positives; these anti-patterns appear to be necessary, but not sufficient, signals of super-linear behavior. Finally, we found that when faced with a super-linear regex, developers favor revising it over truncating input or developing a custom parser, regardless of whether they had been shown examples of all three fix strategies. These findings motivate further research into ReDoS, since many modules are vulnerable to it and existing mechanisms to avoid it are insufficient. We believe that ReDoS vulnerabilities are a larger threat in practice than might have been guessed.",
      "Keywords": "catastrophic backtracking | empirical software engineering | mining software repositories | ReDoS | Regular expressions",
      "Publication venue": "ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering",
      "Publication date": "2018-10-26",
      "Publication type": "Conference Paper",
      "Authors": "Davis, James C.;Coghlan, Christy A.;Servant, Francisco;Lee, Dongyoon",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85057175684",
      "Primary study DOI": "10.1109/SEAA.2018.00046",
      "Title": "Structural quality metrics as indicators of the long method bad smell: An empirical study",
      "Abstract": "Empirical evidence has pointed out that Extract Method refactorings are among the most commonly applied refactorings by software developers. The identification of Long Method code smells and the ranking of the associated refactoring opportunities is largely based on the use of metrics, primarily with measures of cohesion, size and coupling. Despite the relevance of these proper-ties to the presence of large, complex and non-cohesive pieces of code, the empirical validation of these metrics has exhibited relatively low accuracy (max precision: 66%) regarding their predictive power for long methods or extract method opportunities. In this work we perform an empirical validation of the ability of cohesion, coupling and size metrics to predict the existence and the intensity of long method occurrences. According to the statistical analysis, the existence and the intensity of the Long Method smell can be effectively predicted by two size (LoC and NoLV), two coupling (MPC and RFC), and four cohesion (LCOM1, LCOM2, Coh, and CC) metrics. Furthermore, the integration of these metrics into a multiple logistic regression model can predict whether a method should be refactored with a precision of 89% and a recall of 91%. The model yields suggestions whose ranking is strongly correlated to the ranking based on the effect of the corresponding refactorings on source code (correl. coef. 0.520). The results are discussed by providing interpretations and implications for research and practice.",
      "Keywords": "Case study | Cohesion | Coupling | Long method | Size",
      "Publication venue": "Proceedings - 44th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2018",
      "Publication date": "2018-10-18",
      "Publication type": "Conference Paper",
      "Authors": "Charalampidou, Sofia;Arvanitou, Elvira Maria;Ampatzoglou, Apostolos;Avgeriou, Paris;Chatzigeorgiou, Alexander;Stamelos, Ioannis",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85054769455",
      "Primary study DOI": "10.3390/app8101902",
      "Title": "Towards a reliable identification of deficient code with a combination of software metrics",
      "Abstract": "Different challenges arise while detecting deficient software source code. Usually a large number of potentially problematic entities are identified when an individual software metric or individual quality aspect is used for the identification of deficient program entities. Additionally, a lot of these entities quite often turn out to be false positives, i.e., the metrics indicate poor quality whereas experienced developers do not consider program entities as problematic. The number of entities identified as potentially deficient does not decrease significantly when the identification of deficient entities is carried out by applying code smell detection rules. Moreover, the intersection of entities identified as allegedly deficient among different code smell detection tools is small, which suggests that the implementation of code smell detection rules are not consistent and uniform. To address these challenges, we present a novel approach for identifying deficient entities that is based on applying the majority function on the combination of software metrics. Program entities are assessed according to selected quality aspects that are evaluated with a set of software metrics and corresponding threshold values derived from benchmark data, considering the statistical distributions of software metrics values. The proposed approach was implemented and validated on projects developed in Java, C++ and C#. The validation of the proposed approach was done with expert judgment, where software developers and architects with multiple years of experiences assessed the quality of the software classes. Using a combination of software metrics as the criteria for the identification of deficient source code, the number of potentially deficient object-oriented program entities proved to be reduced. The results show the correctness of quality ratings determined by the proposed identification approach, and most importantly, confirm the absence of false positive entities.",
      "Keywords": "Expert judgment | Majority function | Metric thresholds | Object-oriented | Reliable identification | Smelling entities | Smells in source code | Software metrics | Software quality",
      "Publication venue": "Applied Sciences (Switzerland)",
      "Publication date": "2018-10-12",
      "Publication type": "Article",
      "Authors": "Beranič, Tina;Podgorelec, Vili;Heričko, Marjan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85050081131",
      "Primary study DOI": "10.1016/j.jss.2018.07.035",
      "Title": "A systematic review on the code smell effect",
      "Abstract": "Context: Code smell is a term commonly used to describe potential problems in the design of software. The concept is well accepted by the software engineering community. However, some studies have presented divergent findings about the usefulness of the smell concept as a tool to support software development tasks. The reasons of these divergences have not been considered because the studies are presented independently. Objective: To synthesize current knowledge related to the usefulness of the smell concept. We focused on empirical studies investigating how smells impact the software development, the code smell effect. Method: A systematic review about the smell effect is carried out. We grouped the primary studies findings in a thematic map. Result: The smell concept does not support the evaluation of quality design in practice activities of software development. There is no strong evidence correlating smells and some important software development attributes, such as effort in maintenance. Moreover, the studies point out that human agreement on smell detection is low. Conclusion: In order to improve analysis on the subject, the area needs to better outline: (i) factors affecting human evaluation of smells; and (ii) a classification of types of smells, grouping them according to relevant characteristics.",
      "Keywords": "Code smell | Systematic review | Thematic synthesis",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2018-10-01",
      "Publication type": "Article",
      "Authors": "Santos, José Amancio M.;Rocha-Junior, João B.;Prates, Luciana Carla Lins;Nascimento, Rogeres Santos do;Freitas, Mydiã Falcão;Mendonça, Manoel Gomes de",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85063626553",
      "Primary study DOI": "10.1145/3242163.3242165",
      "Title": "Support for architectural smell refactoring",
      "Abstract": "To preserve high quality of a project, it is necessary to perform many refactoring steps during the development and maintenance phases. Code refactoring received a great attention in the literature, often directed to the refactoring of code smells. While at the architectural level, architectural smells received less attention. One of the most common architectural smell which may affect many parts of a software application is the Cyclic Dependency smell. This position paper presents a tool prototype that suggests which path a developer could follow to remove Cyclic Dependency smells in Java applications. The tool has been developed as an extension of the Arcan tool for architectural smells detection. In this paper we describe how the tool identifies the path to follow for the refactoring of Cyclic Dependency on real projects. The tool has been thought only to suggest a possible refactoring approach which a developer could considerate to resolve Cyclic Dependency.",
      "Keywords": "Architectural Refactoring | Architectural Smells | Cyclic Dependency",
      "Publication venue": "IWoR 2018 - Proceedings of the 2nd International Workshop on Refactoring, co-located with ASE 2018",
      "Publication date": "2018-09-04",
      "Publication type": "Conference Paper",
      "Authors": "Rizzi, Luca;Fontana, Francesca Arcelli;Roveda, Riccardo",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84865304314",
      "Primary study DOI": "10.1145/04000800.2336785",
      "Title": "Static detection of brittle parameter typing",
      "Abstract": "To avoid receiving incorrect arguments, a method specifies the expected type of each formal parameter. However, some parameter types are too general and have subtypes that the method does not expect as actual argument types. For example, this may happen if there is no common supertype that precisely describes all expected types. As a result of such brittle parameter typing, a caller may accidentally pass arguments unexpected by the callee without any warnings from the type system. This paper presents a fully automatic, static analysis to find brittle parameter typing and unexpected arguments given to brittle parameters. First, the analysis infers from callers of a method the types that arguments commonly have. Then, the analysis reports potentially unexpected arguments that stand out by having an unusual type. We apply the approach to 21 real-world Java programs that use the Swing API, an API providing various methods with brittle parameters. The analysis reveals 15 previously unknown bugs and code smells where programmers pass arguments that are compatible with the declared parameter type but nevertheless unexpected by the callee. The warnings reported by the analysis have 47% precision and 83% recall. © 2012 ACM.",
      "Keywords": "anomaly detection | Automatic bug finding | static analysis",
      "Publication venue": "2012 International Symposium on Software Testing and Analysis, ISSTA 2012 - Proceedings",
      "Publication date": "2012-08-28",
      "Publication type": "Conference Paper",
      "Authors": "Pradel, Michael;Heiniger, Severin;Gross, Thomas R.",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85072524983",
      "Primary study DOI": "10.1109/MOBILESoft.2019.00025",
      "Title": "Sniffing android code smells: An association rules mining-based approach",
      "Abstract": "Interest in mobile applications (mobile apps) has grown significantly in recent years and has become an important part of the software development market. Indeed, mobile apps become more and more complex and evolve constantly, while their development time decreases. This complexity and time pressure might lead developers to adopt bad design and implementation choices, which are known as code smells. Code smells in mobile apps could lead to performance issues such as overconsumption of hardware resources (CPU, RAM, battery) or even downtime and crashes. Some tools have been proposed for the detection of code smells in Android apps, such as PAPRIKA or ADOCTOR tools. These tools rely on metrics-based detection rules, which are defined manually according to code smell definitions. However, manually defined rules might be inaccurate and subjective because they are based on user interpretations. In this paper, we present a tool-based approach, called FAKIE, which allows the automatic inference of detection rules by analysing code smells data using an association rules algorithm: FPGROWTH. We validated FAKIE by applying it on a manually analysed validation dataset of 48 opensource mobile apps. We were able to generate detection rules for a dozen code smells, with an average F-measure of 0.95. After all of that, we performed an empirical study by applying FAKIE on 2,993 apps downloaded from ANDROZOO, a repository of mobile apps.",
      "Keywords": "Android | association rules | code smells | detection | mobile applications",
      "Publication venue": "Proceedings - 2019 IEEE/ACM 6th International Conference on Mobile Software Engineering and Systems, MOBILESoft 2019",
      "Publication date": "2019-05-01",
      "Publication type": "Conference Paper",
      "Authors": "Rubin, Jehan;Henniche, Adel Nassim;Moha, Naouel;Bouguessa, Mohamed;Bousbia, Nabila",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85059979430",
      "Primary study DOI": "10.1166/jctn.2018.7587",
      "Title": "Dynamic understanding of software system using structural and semantic based detection of design patterns and antipatterns",
      "Abstract": "Software understanding is very important to the design context for the developers. Hence it is necessary to identify the knowledge about the design solutions and possible mistakes among the software systems. Design patterns are considered as good software practices to the design anomalies perceived by the programmer. When the pattern solutions do not fit into the development context or not according to the programmer's preferences these may evolve into an antipattern. These antipatterns when creep deep into the code may cause negative effects on the quality of the software. This research work proposes a novel algorithm for structural and semantic matching of design patterns using algebraic relationships as static and dynamic conditions of design patterns. Also combine the metric information for the detection of antipatterns. The approach is applied on three open source systems such as java libraries and the results have shown that the algorithm scales well to existing methods. The approach is able to und erstae n rd the softwn a tirf e ic system by detecting more antipatterns and design patterns.",
      "Keywords": "Algebraic relationships | Antipatterns | Refactoring | Subgraphs",
      "Publication venue": "Journal of Computational and Theoretical Nanoscience",
      "Publication date": "2018-09-01",
      "Publication type": "Article",
      "Authors": "Sreeji, K. S.;Lakshmi, C.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85053687595",
      "Primary study DOI": "10.1109/CONFLUENCE.2018.8443066",
      "Title": "Detection Strategies of Bad Smells in Highly Configurable Software",
      "Abstract": "Software maintenance is a tough work and when code become very large its hard to track the changes and bad code makes it harder. One way to track the quality of software is to track for bad smells in the software. They can help track the code which can cause problems in near future. The objective is to build a bot that crawls through our code daily and gives a status of smells in the code. To achieve this, developed a set of instruction and strategies and implemented them in python to parse code of Java and track the software with time. All statistics are shown with the python library matplotlib and the bot can be automated to crawl in Linux, Mac, and Windows. Found that, the bot can detect smells which are not detectable by the developer. The life of software can be increased by a refracting the code time to time with the help of smells detected by the bot. Only Java is supported till now but this paper increased the support to other languages also.",
      "Keywords": "Bad smells | Code Smells | Software Maintainability",
      "Publication venue": "Proceedings of the 8th International Conference Confluence 2018 on Cloud Computing, Data Science and Engineering, Confluence 2018",
      "Publication date": "2018-08-20",
      "Publication type": "Conference Paper",
      "Authors": "Faujdar, Neetu;Srivastav, Kshitij;Gupta, Megha;Saraswat, Shipra",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85051547001",
      "Primary study DOI": "10.1145/3205651.3208294",
      "Title": "Assessing single-objective performance convergence and time complexity for refactoring detection",
      "Abstract": "The automatic detection of refactoring recommendations has been tackled in prior optimization studies involving bad code smells, semantic coherence and importance of classes; however, such studies informally addressed formalisms to standardize and replicate refactoring models. We propose to assess the refactoring detection by means of performance convergence and time complexity. Since the reported approaches are diicult to reproduce, we employ an Artiicial Refactoring Generation (ARGen) as a formal and naive computational solution for the Refactoring Detection Problem. ARGen is able to detect massive refactoring's sets in feasible areas of the search space. We used a refactoring formalization to adapt search techniques (Hill Climbing, Simulated Annealing and Hybrid Adaptive Evolutionary Algorithm) that assess the performance and complexity on three open software systems. Combinatorial techniques are limited in solving the Refactoring Detection Problem due to the relevance of developers' criteria (human factor) when designing reconstructions. Without performance convergence and time complexity analysis, a software empirical analysis that utilizes search techniques is incomplete.",
      "Keywords": "Combinatorial Optimization | Mathematical Software Performance | Refactoring | Software Maintenance",
      "Publication venue": "GECCO 2018 Companion - Proceedings of the 2018 Genetic and Evolutionary Computation Conference Companion",
      "Publication date": "2018-07-06",
      "Publication type": "Conference Paper",
      "Authors": "Nader-Palacio, David;Rodríguez-Cárdenas, Daniel;Gomez, Jonatan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85065903525",
      "Primary study DOI": "10.1109/ICODSE.2018.8705923",
      "Title": "CODECOD: Crowdsourcing Platform for Code Smell Detection",
      "Abstract": "Finding code smells in a program code must be done as soon as possible to improve the software maintainability. Nowadays, various automatic code smell detection tools have been developed. However, to increase the quality, the role of humans who do manual detection is still needed. Therefore, in this work, we develop a platform called CODECEOD, which involves crowd to detect code smells. This platform implements crowdsourcing method by decomposing requested tasks, in the form of uploaded source codes, into microtasks to enable the distribution of tasks to multiple workers. To guarantee the quality of the detection results, we introduced a quality assurance method called Find, Vote, Verify. Based on the evaluation involving software engineers, CODECOD is capable to detect more code smells with high accuracy compared to an automatic tool. Moreover, we also show that the proposed Find, Vote, Verify technique delivers an improved accuracy compared to the traditional output-agreement quality assurance technique.",
      "Keywords": "Code smell | Crowdsourcing | Microtask | Task",
      "Publication venue": "Proceedings of 2018 5th International Conference on Data and Software Engineering, ICoDSE 2018",
      "Publication date": "2018-07-02",
      "Publication type": "Conference Paper",
      "Authors": "Paramita, Andi Jamiati;Catur Candra, Muhamad Zuhri",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85065241098",
      "Primary study DOI": "10.1109/SIET.2018.8693156",
      "Title": "Refused Bequest Code Smells Detection on Software Design",
      "Abstract": "Code smells are a characteristic of a software that indicates problems in the code structure and system design that result in the software is being difficult to develop and maintain. The quite famous kind of code smells is Refused Bequest, which is a condition in the concept of inheritance that subclasses do not use the derived functionality of the superclass to happen inheritance rejection. Generally, code smells only can identified through the code structure. However, in this study developed the detection code smells in the stage of software development is design. Design is a very important phase in the software development phase because the success of a software depends on good analysis and design. At the design stage performed detection code smells with the kind of Refused Bequest is on the design of components, namely class diagrams design. The design of the class diagram in the.vpp format of the UML Creator Visual Paradigm application converted into the xml language. Once converted, the xml file detected on the detection software by parsing and finding the code smells Refused Bequest level referring to the smells thermometer as the measured Refused Bequest intensity meter. In this study, the results of detection refused bequest at design stage that two test cases were valid from the three existing test cases. The test case based on research conducted by Ligu et al. on an application. In addition, the accuracy value from the results of the comparison between this study and the Ligu et al. research is 66.7% because the adjustment in the design stage.",
      "Keywords": "class diagram design | code smells | detection system | refused bequest",
      "Publication venue": "3rd International Conference on Sustainable Information Engineering and Technology, SIET 2018 - Proceedings",
      "Publication date": "2018-07-02",
      "Publication type": "Conference Paper",
      "Authors": "Firdaus, Muhammad Faishal;Priyambadha, Bayu;Pradana, Fajar",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85067115135",
      "Primary study DOI": "10.1109/INFOCOMTECH.2018.8722418",
      "Title": "Analyzing Code Smell Removal Sequences for Enhanced Software Maintainability",
      "Abstract": "Code smells are the surface indications which affect the maintainability of the software. Code smells disturb the maintainability of the code by starting a chain reaction of breakages in dependent modules which makes it difficult to read and modify. Applying appropriate refactoring sequences by prioritizing the classes to obtain maintainable software is a tedious process due to strict deadlines of the projects for the developers. Recent researches have explored varied ways of ranking the classes to improve the maintainability. This work empirically investigates the impact of eliminating three prominent code smells by considering their six possible combinations. Our work prioritizes the object oriented software classes in the code that are in the need of refactoring. For prioritizing the refactoring prone classes, a proposed metric, maintainability complexity index is calculated using the values of maintainability index and relative logical complexity as the inputs. The study outcomes show the values of maintainability predicting metrics for the corresponding permutation of the code smell removal sequence. Also, the work aims to yield the sequence which gives software with maximum maintainability so that developers and researchers can save their effort and time to produce high quality software.",
      "Keywords": "code smell | maintainability | maintainability metrics | prioritization | refactoring | removal sequences",
      "Publication venue": "2018 Conference on Information and Communication Technology, CICT 2018",
      "Publication date": "2018-07-02",
      "Publication type": "Conference Paper",
      "Authors": "Mehta, Yukti;Singh, Paramvir;Sureka, Ashish",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85066804270",
      "Primary study DOI": "10.1109/APSEC.2018.00046",
      "Title": "Are Smell-Based Metrics Actually Useful in Effort-Aware Structural Change-Proneness Prediction? An Empirical Study",
      "Abstract": "Bad code smells (also named as code smells) are symptoms of poor design choices in implementation. Existing increases the likelihood of subsequent changes (i.e., change-proness). However, to the best of our knowledge, no prior studies have leveraged smell-based metrics to predict particular change type (i.e., structural changes). Moreover, when evaluating the effectiveness of smell-based metrics in structural change-proneness prediction, none of existing studies take into account of the effort inspecting those change-prone source code. In this paper, we consider five smell-based metrics for effort-aware structural change-proneness prediction and compare these metrics with a baseline of well-known CK metrics in predicting particular categories of change types. Specifically, we first employ univariate logistic regression to analyze the correlation between each smellbased metric and structural change-proneness. Then, we build multivariate prediction models to examine the effectiveness of smell-based metrics in effort-aware structural change-proneness prediction when used alone and used together with the baseline metrics, respectively. Our experiments are conducted on six Java open-source projects with up to 60 versions and results indicate that: (1) all smell-based metrics are significantly related to structural change-proneness, except metric ANOS in hive and SCM in camel after removing confounding effect of file size; (2) in most cases, smell-based metrics outperform the baseline metrics in predicting structural change-proneness; and (3) when used together with the baseline metrics, the smell-based metrics are more effective to predict change-prone files with being aware of inspection effort.",
      "Keywords": "change-proneness prediction | code smell | effort-aware | structual change",
      "Publication venue": "Proceedings - Asia-Pacific Software Engineering Conference, APSEC",
      "Publication date": "2018-07-02",
      "Publication type": "Conference Paper",
      "Authors": "Liu, Huihui;Yu, Yijun;Li, Bixin;Yang, Yibiao;Jia, Ru",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85049385820",
      "Primary study DOI": "10.1587/transinf.2017KBP0006",
      "Title": "An investigative study on how developers filter and prioritize code smells",
      "Abstract": "Code smells are indicators of design flaws or problems in the source code. Various tools and techniques have been proposed for detecting code smells. These tools generally detect a large number of code smells, so approaches have also been developed for prioritizing and filtering code smells. However, lack of empirical data detailing how developers filter and prioritize code smells hinders improvements to these approaches. In this study, we investigated ten professional developers to determine the factors they use for filtering and prioritizing code smells in an open source project under the condition that they complete a list of five tasks. In total, we obtained 69 responses for code smell filtration and 50 responses for code smell prioritization from the ten professional developers. We found that Task relevance and Smell severity were most commonly considered during code smell filtration, while Module importance and Task relevance were employed most often for code smell prioritization. These results may facilitate further research into code smell detection, prioritization, and filtration to better focus on the actual needs of developers.",
      "Keywords": "Code smell filtration | Code smell prioritization | Code smells | Practitioner's perspective",
      "Publication venue": "IEICE Transactions on Information and Systems",
      "Publication date": "2018-07-01",
      "Publication type": "Conference Paper",
      "Authors": "Sae-Lim, Natthawute;Hayashi, Shinpei;Saeki, Motoshi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85049385821",
      "Primary study DOI": "10.1587/transinf.2017KBP0023",
      "Title": "Detecting architectural violations using responsibility and dependency constraints of components",
      "Abstract": "Utilizing software architecture patterns is important for reducing maintenance costs. However, maintaining code according to the constraints defined by the architecture patterns is time-consuming work. As described herein, we propose a technique to detect code fragments that are incompliant to the architecture as fine-grained architectural violations. For this technique, the dependence graph among code fragments extracted from the source code and the inference rules according to the architecture are the inputs. A set of candidate components to which a code fragment can be affiliated is attached to each node of the graph and is updated step-by-step. The inference rules express the components' responsibilities and dependency constraints. They remove candidate components of each node that do not satisfy the constraints from the current estimated state of the surrounding code fragment. If the inferred role of a code fragment does not include the component that the code fragment currently belongs to, then it is detected as a violation. We have implemented our technique for the ModelView-Controller for Web Application architecture pattern. By applying the technique to web applications implemented using Play Framework, we obtained accurate detection results. We also investigated how much does each inference rule contribute to the detection of violations.",
      "Keywords": "Architecture pattern | Code smell | Program dependence graph",
      "Publication venue": "IEICE Transactions on Information and Systems",
      "Publication date": "2018-07-01",
      "Publication type": "Conference Paper",
      "Authors": "Hayashi, Shinpei;Minami, Fumiki;Saeki, Motoshi",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85053712946",
      "Primary study DOI": "10.1145/3210459.3210466",
      "Title": "Can you tell me if it smells? A study on how developers discuss code smells and anti-patterns in Stack Overflow",
      "Abstract": "This paper investigates how developers discuss code smells and anti-patterns over Stack Overflow to understand better their perceptions and understanding of these two concepts. Understanding developers' perceptions of these issues are important in order to inform and align future research efforts and direct tools vendors in the area of code smells and anti-patterns. In addition, such insights could lead the creation of solutions to code smells and anti-patterns that are better fit to the realities developers face in practice. We applied both quantitative and qualitative techniques to analyse discussions containing terms associated with code smells and anti-patterns. Our findings show that developers widely use Stack Overflow to ask for general assessments of code smells or anti-patterns, instead of asking for particular refactoring solutions. An interesting finding is that developers very often ask their peers 'to smell their code' (i.e., ask whether their own code 'smells' or not), and thus, utilize Stack Overflow as an informal, crowd-based code smell/anti-pattern detector. We conjecture that the crowd-based detection approach considers contextual factors, and thus, tends to be more trusted by developers over automated detection tools. We also found that developers often discuss the downsides of implementing specific design patterns, and 'flag' them as potential anti-patterns to be avoided. Conversely, we found discussions on why some anti-patterns previously considered harmful should not be flagged as anti-patterns. Our results suggest that there is a need for: 1) more context-based evaluations of code smells and anti-patterns, and 2) better guidelines for making trade-offs when applying design patterns or eliminating smells/anti-patterns in industry.",
      "Keywords": "Anti-patterns | Code smells | Empirical study | Mining software repositories | Stack Overflow",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2018-06-28",
      "Publication type": "Conference Paper",
      "Authors": "Tahir, Amjed;Yamashita, Aiko;Licorish, Sherlock;Dietrich, Jens;Counsell, Steve",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85052522941",
      "Primary study DOI": "10.1145/3234698.3234729",
      "Title": "A novel approach for improving the quality of software code using reverse engineering",
      "Abstract": "Copying and pasting program code fragments with minor changes is a common practice in software development. Software systems often have similar segments of code, called code clones. Due to many reasons, unintentional smells may also appear in the source code without awareness of program developers. Code smell may violate the principles of software design and negatively impact program design quality, thus making software development and maintenance very costly. This paper presents an enhanced approach to facilitate the process of identification and elimination of code smells. The proposed solution allows the detection and removal of code smells for refinement and improvement of the quality of software system. Code smells are analyzed, restructured and eliminated from the source code using reverse engineering techniques. The solution maintains the external behaviour of software system and judges the efficiency of systems code. An experiment has been conducted using a real software system, which is evaluated before and after using the approach. The results have been encouraging and help in detecting code smells.",
      "Keywords": "Code clone | Code refactoring | Code smells | Smells detection",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2018-06-19",
      "Publication type": "Conference Paper",
      "Authors": "Abdelaziz, Tawfig M.;Elghadhafi, Hamza A.;Maatuk, Abdelsalam M.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85055446660",
      "Primary study DOI": "10.1109/COMPSAC.2018.00015",
      "Title": "Using Code Evolution Information to Improve the Quality of Labels in Code Smell Datasets",
      "Abstract": "Several approaches are proposed to detect code smells. A set of important approaches are based on machine learning algorithms, which require the code smells have been labeled in source codes as training data firstly. The common labeling approaches are based on manual or tools, but it is difficult for current approaches to get reliable large-scale datasets. In this paper, an approach using the evolution information of source codes is proposed to get large-scale and more reliable training datasets for detecting code smells based on machine learning algorithms. Our approach analyzes the evolving of the source code smells firstly labeled by a tool from the baseline version into the contrastive version of a software system, and then constructs training datasets based on those 'changed smells'. Experiments conducted on three open source software projects for detecting four types of code smells(which are Data Class, God Class, Brain Class and Brain Method) show that the models obtained by changed smells datasets have better performance on code smell detection than those obtained by unchanged smells datasets (with an average improvement rate of 7.8% and a maximum increase of 30%). The experiments results indicate that using the evolution information of source codes can construct more reliable training datasets for detecting code smells based on machine learning algorithms.",
      "Keywords": "Code smells | Machine learning | Refactoring | Training dataset",
      "Publication venue": "Proceedings - International Computer Software and Applications Conference",
      "Publication date": "2018-06-08",
      "Publication type": "Conference Paper",
      "Authors": "Wang, Yijun;Hu, Songyuan;Yin, Linfeng;Zhou, Xiaocong",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85055422017",
      "Primary study DOI": "10.1109/COMPSAC.2018.00010",
      "Title": "An Empirical Analysis on Web Service Anti-pattern Detection Using a Machine Learning Framework",
      "Abstract": "Web Services are application components characterised by interoperability, extensibility, distributed application development and service oriented architecture. A complex distributed application can be developed by combing several third-party web-services. Anti-patterns are counter-productive and poor design and practices. Web-services suffer from a multitude of anti-patterns such as God object Web service and Fine grained Web service. Our work is motivated by the need to build techniques for automatically detecting common web-services anti-patterns by static analysis of the source code implementing a web-service. Our approach is based on the premise that summary values of object oriented source code metrics computed at a web-service level can be used as a predictor for anti-patterns. We present an empirical analysis of 4 data sampling techniques to encounter the class imbalance problem, 5 feature ranking techniques to identify the most informative and relevant features and 8 machine learning algorithms for predicting 5 different types of anti-patterns on 226 real-world web-services across several domains. We conclude that it is possible to predict anti-patterns using source code metrics and a machine learning framework. Our analysis reveals that the best performing classification algorithm is Random Forest, best performing data sampling technique is SMOTE and the best performing feature ranking method is OneR.",
      "Keywords": "Anti Patterns | Empirical Software Engineering | Feature Selection | Imbalanced Learning | Machine Learning | Predictive Modeling | Service Oriented Architecture | Software Analytics | Source Code Analysis | Source Code Metrics | Web Services",
      "Publication venue": "Proceedings - International Computer Software and Applications Conference",
      "Publication date": "2018-06-08",
      "Publication type": "Conference Paper",
      "Authors": "Kumar, Lov;Sureka, Ashish",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85051532094",
      "Primary study DOI": "10.1145/3196321.3196360",
      "Title": "Replicomment: Identifying clones in code comments",
      "Abstract": "Code comments are the primary means to document implementation and ease program comprehension. Thus, their quality should be a primary concern to improve program maintenance. While a lot of effort has been dedicated to detect bad smell in code, little work focuses on comments. In this paper we start working in this direction by detecting clones in comments. Our initial investigation shows that even well known projects have several comment clones, and just as clones are bad smell in code, they may be for comments. A manual analysis of the clones we identified revealed several issues in real Java projects.",
      "Keywords": "bad smell | clones | code comments | software quality",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2018-05-28",
      "Publication type": "Conference Paper",
      "Authors": "Blasi, Arianna;Gorla, Alessandra",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85051642515",
      "Primary study DOI": "10.1145/3196321.3196365",
      "Title": "Towards just-in-time refactoring recommenders",
      "Abstract": "Empirical studies have provided ample evidence that low code quality is generally associated with lower maintainability. For this reason, tools have been developed to automatically detect design flaws (e.g., code smells). However, these tools are not able to prevent the introduction of design flaws. This means that the code has to experience a quality decay (with a consequent increase of maintenance/evolution costs) before state-of-the-art tools can be applied to identify and refactor the design flaws. Our goal is to develop a new generation of refactoring recommenders aimed at preventing, via refactoring operations, the introduction of design flaws rather than fixing them once they already affect the system. We refer to such a novel perspective on software refactoring as just-in-time refactoring. In this paper, we make a first step towards this direction, presenting an approach able to predict which classes will be affected in the future by code smells.",
      "Keywords": "code smells | refactoring",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2018-05-28",
      "Publication type": "Conference Paper",
      "Authors": "Pantiuchina, Jevgenija;Bavota, Gabriele;Tufano, Michele;Poshyvanyk, Denys",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85049690506",
      "Primary study DOI": "10.1145/3183440.3183460",
      "Title": "Detecting and managing code smells: Research and practice",
      "Abstract": "Code smells indicate the presence of quality problems that make the software hard to maintain and evolve. A software development team can keep their software maintainable by identifying smells and refactor them. In the first part of the session, we present a comprehensive overview of the literature concerning smells covering various dimensions of the metaphor including defining characteristics, classification, types, as well as causes and impacts of smells. In the second part, we delve into the details of smell detection methods prevailed currently both in research prototypes and industrial tools. The final part present actionable and pragmatic strategies for practitioners to avoid, detect, and eradicate smells from their codebase.",
      "Keywords": "Antipatterns | Code Quality | Code Smells | Smell Detection Tools | Software Maintenance | Software Quality | Technical Debt",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2018-05-27",
      "Publication type": "Conference Paper",
      "Authors": "Sharma, Tushar",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85049672648",
      "Primary study DOI": "10.1145/3183440.3194974",
      "Title": "Poster: Machine learning based code smell detection through WekaNose",
      "Abstract": "Code smells can be subjectively interpreted, the results provided by detectors are usually different, the agreement in the results is scarce, and a benchmark for the comparison of these results is not yet available. The main approaches used to detect code smells are based on the computation of a set of metrics. However code smell detectors often use different metrics and/or different thresholds, according to their detection rules. As result of this inconsistency the number of detected smells can increase or decrease accordingly, and this makes hard to understand when, for a specific software, a certain characteristic identifies a code smell or not. In this work, we introduce WekaNose, a tool that allows to perform an experiment to study code smell detection through machine learning techniques. The experiment's purpose is to select rules, and/or obtain trained algorithms, that can classify an instance (method or class) as affected or not by a code smell. These rules have the main advantage of being extracted through an example-based approach, rather then a heuristic-based one.",
      "Keywords": "",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2018-05-27",
      "Publication type": "Conference Paper",
      "Authors": "Azadi, Umberto;Fontana, Francesca Arcelli;Zanoni, Marco",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85049791243",
      "Primary study DOI": "10.1145/3183399.3183420",
      "Title": "Towards saving money in using smart contracts",
      "Abstract": "Being a new kind of software leveraging blockchain to execute real contracts, smart contracts are in great demand due to many advantages. Ethereum is the largest blockchain platform that supports smart contracts by running them in its virtual machine. To ensure that a smart contract will terminate eventually and prevent abuse of resources, Ethereum charges the developers for deploying smart contracts and the users for executing smart contracts. Although our previous work shows that under-optimized smart contracts may cost more money than necessary, it just lists 7 anti-patterns and the detection method for 3 of them. In this paper, we conduct the first in-depth investigation on such under-optimized smart contracts. We first identify 24 anti-patterns from the execution traces of real smart contracts. Then, we design and develop GasReducer, the first tool to automatically detect all these anti-patterns from the bytecode of smart contracts and replace them with efficient code through bytecode-to-bytecode optimization. Using GasReducer to analyze all smart contracts and their execution traces, we detect 9,490,768 and 557,565,754 anti-pattern instances in deploying and invoking smart contracts, respectively.",
      "Keywords": "Anti-patterns | Detection | Optimization | Smart contract",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2018-05-27",
      "Publication type": "Conference Paper",
      "Authors": "Chen, Ting;Li, Zihao;Zhou, Hao;Chen, Jiachi;Luo, Xiapu;Li, Xiaoqi;Zhang, Xiaosong",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85049674778",
      "Primary study DOI": "10.1145/3183440.3194950",
      "Title": "How do community smells influence code smells?",
      "Abstract": "Code smells reflect sub-optimal patterns of code that often lead to critical software flaws or failure. In the same way, community smells reflect sub-optimal organisational and socio-Technical patterns in the organisational structure of the software community. To understand the relation between the community smells and code smells we start by surveying 162 developers of nine open-source systems. Then we look deeper into this connection by conducting an empirical study of 117 releases from these systems. Our results indicate that community-related factors are intuitively perceived by most developers as causes of the persistence of code smells. Inspired by this observation we design a community-Aware prediction model for code smells and show that it outperforms a model that does not consider community factors.",
      "Keywords": "Code smells | Community smells | Organisational structure",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2018-05-27",
      "Publication type": "Conference Paper",
      "Authors": "Palomba, Fabio;Tamburri, Damian A.;Serebrenik, Alexander;Zaidman, Andy;Fontana, Francesca Arcelli;Oliveto, Rocco",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85049772205",
      "Primary study DOI": "10.1145/3183399.3183402",
      "Title": "Combining spreadsheet smells for improved fault prediction",
      "Abstract": "Spreadsheets are commonly used in organizations as a programming tool for business-related calculations and decision making. Since faults in spreadsheets can have severe business impacts, a number of approaches from general software engineering have been applied to spreadsheets in recent years, among them the concept of code smells. Smells can in particular be used for the task of fault prediction. An analysis of existing spreadsheet smells, however, revealed that the predictive power of individual smells can be limited. In this work we therefore propose a machine learning based approach which combines the predictions of individual smells by using an AdaBoost ensemble classifier. Experiments on two public datasets containing real-world spreadsheet faults show significant improvements in terms of fault prediction accuracy.",
      "Keywords": "Fault Prediction | Spreadsheet QA | Spreadsheet Smells",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2018-05-27",
      "Publication type": "Conference Paper",
      "Authors": "Koch, Patrick;Schekotihin, Konstantin;Jannach, Dietmar;Hofer, Birgit;Wotawa, Franz;Schmitz, Thomas",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85051486330",
      "Primary study DOI": "10.1145/3194164.3194173",
      "Title": "Evaluating domain-specific metric thresholds: An empirical study",
      "Abstract": "Software metrics and thresholds provide means to quantify several quality attributes of software systems. Indeed, they have been used in a wide variety of methods and tools for detecting different sorts of technical debts, such as code smells. Unfortunately, these methods and tools do not take into account characteristics of software domains, as the intrinsic complexity of geo-localization and scientific software systems or the simple protocols employed by messaging applications. Instead, they rely on generic thresholds that are derived from heterogeneous systems. Although derivation of reliable thresholds has long been a concern, we still lack empirical evidence about threshold variation across distinct software domains. To tackle this limitation, this paper investigates whether and how thresholds vary across domains by presenting a large-scale study on 3,107 software systems from 15 domains. We analyzed the derivation and distribution of thresholds based on 8 well-known source code metrics. As a result, we observed that software domain and size are relevant factors to be considered when building benchmarks for threshold derivation. Moreover, we also observed that domain-specific metric thresholds are more appropriated than generic ones for code smell detection.",
      "Keywords": "software domains | software metrics | thresholds",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2018-05-27",
      "Publication type": "Conference Paper",
      "Authors": "Mori, Allan;Vale, Gustavo;Viggiato, Markos;Oliveira, Johnatan;Figueiredo, Eduardo;Cirilo, Elder;Jamshidi, Pooyan;Kastner, Christian",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85049683967",
      "Primary study DOI": "10.1145/3183440.3183452",
      "Title": "Characteristics of defective infrastructure as code scripts in DevOps",
      "Abstract": "Defects in infrastructure as code (IaC) scripts can have serious consequences for organizations who adopt DevOps. By identifying which characteristics of IaC scripts correlate with defects, we can identify anti-patterns, and help software practitioners make informed decisions on better development and maintenance of IaC scripts, and increase quality of IaC scripts. The goal of this paper is to help practitioners increase the quality of IaC scripts by identifying characteristics of IaC scripts and IaC development process that correlate with defects, and violate security and privacy objectives. We focus on characteristics of IaC scripts and IaC development that (i) correlate with IaC defects, and (ii) violate security and privacy-related objectives namely, confidentiality, availability, and integrity. For our initial studies, we mined open source version control systems from three organizations: Mozilla, Openstack, and Wikimedia, to identify the defect-related characteristics and conduct our case studies. From our empirical analysis, we identify (i) 14 IaC code and four churn characteristics that correlate with defects; and (ii) 12 process characteristics such as, frequency of changes, and ownership of IaC scripts that correlate with defects. We propose the following studies: (i) identify structural characteristics that correlate with defects; (ii) with respect to prediction performance, compare which characteristics of IaC scripts are more correlated with defects; and (iii) identify characteristics that violate security and privacy objectives.",
      "Keywords": "Defects | Devops | Infrastructure as code | Metrics",
      "Publication venue": "Proceedings - International Conference on Software Engineering",
      "Publication date": "2018-05-27",
      "Publication type": "Conference Paper",
      "Authors": "Rahman, Akond",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85051576215",
      "Primary study DOI": "10.3390/e20050372",
      "Title": "Software code smell prediction model using Shannon, RÃ©nyi and Tsallis entropies",
      "Abstract": "The current era demands high quality software in a limited time period to achieve new goals and heights. To meet user requirements, the source codes undergo frequent modifications which can generate the bad smells in software that deteriorate the quality and reliability of software. Source code of the open source software is easily accessible by any developer, thus frequently modifiable. In this paper, we have proposed a mathematical model to predict the bad smells using the concept of entropy as defined by the Information Theory. Open-source software Apache Abdera is taken into consideration for calculating the bad smells. Bad smells are collected using a detection tool from sub components of the Apache Abdera project, and different measures of entropy (Shannon, Rényi and Tsallis entropy). By applying non-linear regression techniques, the bad smells that can arise in the future versions of software are predicted based on the observed bad smells and entropy measures. The proposed model has been validated using goodness of fit parameters (prediction error, bias, variation, and Root Mean Squared Prediction Error (RMSPE)). The values of model performance statistics (R2, adjusted R2, Mean Square Error (MSE) and standard error) also justify the proposed model. We have compared the results of the prediction model with the observed results on real data. The results of the model might be helpful for software development industries and future researchers.",
      "Keywords": "Code smell | Entropy | Regression | Software design defects | Software quality | Statistical model",
      "Publication venue": "Entropy",
      "Publication date": "2018-05-03",
      "Publication type": "Article",
      "Authors": "Gupta, Aakanshi;Suri, Bharti;Kumar, Vijay;Misra, Sanjay;Blažauskas, Tomas;Damaševičius, Robertas",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85055265395",
      "Primary study DOI": "10.1145/3220228.3220245",
      "Title": "DT: An upgraded detection tool to automatically detect two kinds of code smell: Duplicated code and feature envy",
      "Abstract": "Code smell is unreasonable programming, and is produced when software developers don't have good habits of development and experience of development and other reasons. Code becomes more and more chaotic, the code structure become bloated. Code smell can make degradation of code quality. It also can make some difficulties for software developers to understand and maintain the source code of projects, and then cause unnecessary maintenance costs. This study presents an evolutionary version of detection tool DT. DT can support detection of two kinds of code smell - Duplicated code and Feature envy . At the same time, two types of code smell are mainly detected by two detection thoughts: dynamic programming algorithm (DP) and abstract grammar tree (AST). DP can be applied in code smell - Duplicated code. DP uses the similarity between comparative lines to determine whether there is duplicated code; AST is used to detect code smell- feature envy, AST use tree structure to represent the source code, the grammatical structure of the source code transforms to each tree node. Through statistical analysis of existence of these nodes, we can determine whether is a kind of code smell or not. In experiment, detection tool DT compares with four famous detection tools Checkstyle, PMD, iPlasma and JDeodorant. Detection accuracy is higher than above detection tools. In addition, these four well-known tools can't support detection of large industrial projects, while DT can support detection of industrial projects. In future work, we want to detect more kinds of code smell, meanwhile we want update detection precision of detection tool.",
      "Keywords": "Code smell | Detection tool | Duplicated code | Feature envy",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2018-04-20",
      "Publication type": "Conference Paper",
      "Authors": "Liu, Xinghua;Zhang, Cheng",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85047010931",
      "Primary study DOI": "10.1109/ICSESS.2017.8342882",
      "Title": "An investigation of code cycles and Poltergeist anti-pattern",
      "Abstract": "The aim of this paper is to propose a method for detecting the Poltergeist anti-pattern. Anti-patterns are poor designs that lower software quality, especially by increasing complexity and decreasing maintainability. As maintenance of software projects costs as much as their development, it is necessary to detect poorly designed code and refactor it.",
      "Keywords": "anti-patterns | cycle detection | poltergeist",
      "Publication venue": "Proceedings of the IEEE International Conference on Software Engineering and Service Sciences, ICSESS",
      "Publication date": "2017-07-02",
      "Publication type": "Conference Paper",
      "Authors": "Al-Rubaye, Samer Raad Azzawi;Selcuk, Yunus Emre",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84908128129",
      "Primary study DOI": "10.1145/2675067",
      "Title": "Code-smell detection as a bilevel problem",
      "Abstract": "Code smells represent design situations that can affect the maintenance and evolution of software. They make the system difficult to evolve. Code smells are detected, in general, using quality metrics that represent some symptoms. However, the selection of suitable quality metrics is challenging due to the absence of consensus in identifying some code smells based on a set of symptoms and also the high calibration effort in determining manually the threshold value for each metric. In this article, we propose treating the generation of code smell detection rules as a bilevel optimization problem. Bilevel optimization problems represent a class of challenging optimization problems, which contain two levels of optimization tasks. In these problems, only the optimal solutions to the lower-level problem become possible feasible candidates to the upper-level problem. In this sense, the code-smell detection problem can be treated as a bilevel optimization problem, but due to lack of suitable solution techniques, it has been attempted to be solved as a single-level optimization problem in the past. In our adaptation here, the upper-level problem generates a set of detection rules, a combination of quality metrics, which maximizes the coverage of the base of code-smell examples and artificial code smells generated by the lower level. The lower level maximizes the number of generated artificial code smells that cannot be detected by the rules produced by the upper level. The main advantage of our bilevel formulation is that the generation of detection rules is not limited to some code-smell examples identified manually by developers that are difficult to collect, but it allows the prediction of new code-smell behavior that is different from those of the base of examples. The statistical analysis of our experiments over 31 runs on nine open source systems and one industrial project shows that seven types of code smells were detected with an average of more than 86% in terms of precision and recall. The results confirm the out performance of our bilevel proposal compared to state-of-art code-smell detection techniques. The evaluation performed by software engineers also confirms the relevance of detected code smells to improve the quality of software systems.",
      "Keywords": "",
      "Publication venue": "ACM Transactions on Software Engineering and Methodology",
      "Publication date": "2014-10-07",
      "Publication type": "Article",
      "Authors": "Sahin, Dilan;Kessentini, Marouane;Bechikh, Slim;Deb, Kalyanmoy",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85051046431",
      "Primary study DOI": "10.1109/SANER.2018.8330196",
      "Title": "Micro-clones in evolving software",
      "Abstract": "Detection, tracking, and refactoring of code clones (i.e., identical or nearly similar code fragments in the code-base of a software system) have been extensively investigated by a great many studies. Code clones have often been considered bad smells. While clone refactoring is important for removing code clones from the code-base, clone tracking is important for consistently updating code clones that are not suitable for refactoring. In this research we investigate the importance of micro-clones (i.e., code clones of less than five lines of code) in consistent updating of the code-base. While the existing clone detectors and trackers have ignored micro clones, our investigation on thousands of commits from six subject systems imply that around 80% of all consistent updates during system evolution occur in micro clones. The percentage of consistent updates occurring in micro clones is significantly higher than that in regular clones according to our statistical significance tests. Also, the consistent updates occurring in micro-clones can be up to 23% of all updates during the whole period of evolution. According to our manual analysis, around 83% of the consistent updates in micro-clones are non-trivial. As micro-clones also require consistent updates like the regular clones, tracking or refactoring micro-clones can help us considerably minimize effort for consistently updating such clones. Thus, micro-clones should also be taken into proper consideration when making clone management decisions.",
      "Keywords": "Code Clones | Consistent Updates | Micro-Clones | Software Evolution",
      "Publication venue": "25th IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2018 - Proceedings",
      "Publication date": "2018-04-02",
      "Publication type": "Conference Paper",
      "Authors": "Mondai, Manishankar;Roy, Chanchal K.;Schneider, Kevin A.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85044651549",
      "Primary study DOI": "10.1016/j.jss.2017.12.034",
      "Title": "A survey on software smells",
      "Abstract": "Context: Smells in software systems impair software quality and make them hard to maintain and evolve. The software engineering community has explored various dimensions concerning smells and produced extensive research related to smells. The plethora of information poses challenges to the community to comprehend the state-of-the-art tools and techniques. Objective: We aim to present the current knowledge related to software smells and identify challenges as well as opportunities in the current practices. Method: We explore the definitions of smells, their causes as well as effects, and their detection mechanisms presented in the current literature. We studied 445 primary studies in detail, synthesized the information, and documented our observations. Results: The study reveals five possible defining characteristics of smells — indicator, poor solution, violates best-practices, impacts quality, and recurrence. We curate ten common factors that cause smells to occur including lack of skill or awareness and priority to features over quality. We classify existing smell detection methods into five groups — metrics, rules/heuristics, history, machine learning, and optimization-based detection. Challenges in the smells detection include the tools’ proneness to false-positives and poor coverage of smells detectable by existing tools.",
      "Keywords": "Antipatterns | Code smells | Maintainability | Smell detection tools | Software quality | Software smells | Technical debt",
      "Publication venue": "Journal of Systems and Software",
      "Publication date": "2018-04-01",
      "Publication type": "Article",
      "Authors": "Sharma, Tushar;Spinellis, Diomidis",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85052015359",
      "Primary study DOI": "10.1145/3190645.3190697",
      "Title": "Causes, impacts, and detection approaches of code smell: A survey",
      "Abstract": "Code smells are anomalies often generated in design, implementation or maintenance phase of software development life cycle. Researchers established several catalogues characterizing the smells. Fowler and Beck developed the most popular catalogue of 22 smells covering varieties of development issues. This literature presents an overview of the existing research conducted on these 22 smells. Our motivation is to represent these smells with an easier interpretation for the software developers, determine the causes that generate these issues in applications and their impact from different aspects of software maintenance. This paper also highlights previous and recent research on smell detection with an effort to categorize the approaches based on the underlying concept.",
      "Keywords": "Code Smell | Software Engineering | Survey",
      "Publication venue": "Proceedings of the ACMSE 2018 Conference",
      "Publication date": "2018-03-29",
      "Publication type": "Conference Paper",
      "Authors": "Haque, Md Shariful;Carver, Jeff;Atkison, Travis",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85049674853",
      "Primary study DOI": "10.1109/IWSC.2018.8327311",
      "Title": "Large scale clone detection, analysis, and benchmarking: An evolutionary perspective (Keynote)",
      "Abstract": "Copying a code fragment and then reusing it by pasting and adapting (e.g., adding/modifying/deleting statements) is a common practice in software development, which results in a significant amount of duplicated code in software systems. Developers consider cloning as one of the principled re-engineering approaches and often intentionally practice cloning for a variety of reasons such as faster development, avoiding risk by reusing stable old code, or for time pressure. On the other hand, duplicated code poses a number of threats to the maintenance of software systems such as clones are the #1 'bad smell' in Flower's refactoring list and several recent studies including studies with industrial systems show that although for many cases clones are not really harmful, and even could be useful for some cases, they could be also detrimental to software maintenance. For example, reusing a fragment containing unknown bugs may result in bugs propagation, or any changes in requirements involving a cloned fragment may lead to changes to all the similar fragments to it, multiplying the work to be done. Furthermore, inconsistent changes to the cloned fragments during any updating processes may lead to severe unexpected behaviour. Software clones are thus considered to be one of the major contributors to the high software maintenance cost, which could be up to 80% of total software development cost. The era of Big Data has introduced new applications for clone detection. For example, clone detection has been used to find similar mobile applications, to intelligently tag code snippets, to identify code examples, and so on from large inter-project repositories. The dual role of clones in software development and maintenance, along with these many emerging new applications of clone detection, has led to a great many clone detection tools and analysis frameworks. In this keynote talk, I will review the cloning literature to date, in particular, I will talk about our recent work on large scale clone detection, and the challenges in evaluating such clone detectors and how we have overcome them at least in part with our BigCloneBench and Mutation framework. I will then talk about the recent advances in clone analysis and management along with a vision for a comprehensive clone management system.",
      "Keywords": "",
      "Publication venue": "2018 IEEE 12th International Workshop on Software Clones, IWSC 2018 - Proceedings",
      "Publication date": "2018-03-27",
      "Publication type": "Conference Paper",
      "Authors": "Roy, Chanchal K.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85048081626",
      "Primary study DOI": "10.1109/CAIPT.2017.8320730",
      "Title": "Systematic literature review: Model refactoring",
      "Abstract": "Refactoring is the method to detecting and fixing bad smells in software. Refactoring techniques that have developed is a refactoring technique that is done on the source code. Along with the development of model driven software engineering (MDSE), it also developed the method of refactoring on the model. Refactoring method in the model is considered more effective and efficient because the detection and repair of bad smell is done at the design phase. The method of refactoring on the model evolves into a variety of techniques. Due to this, the systematic literature review is done to get the development of refactoring method on the developing model.",
      "Keywords": "Model Driven Refactoring | Pattern Based Model Refactoring | Refactoring Model | Software Evolution | Systematic Literature Review",
      "Publication venue": "Proceedings of the 2017 4th International Conference on Computer Applications and Information Processing Technology, CAIPT 2017",
      "Publication date": "2017-07-02",
      "Publication type": "Conference Paper",
      "Authors": "Dharmawan, Tio;Rochimah, Siti",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85050813643",
      "Primary study DOI": "10.1109/CICN.2017.8319355",
      "Title": "Metric based detection of refused bequest code smell",
      "Abstract": "The concept of code smell was introduced as a signs of internal design flaws within the software. Code smells detection has become a mandatory technique to detect code issues that may affect negatively on the software quality by causing problems for further development and maintenance. Accordingly, the consensus is that all types of code smells need to be refactored to deny or diminish such issues. The refactoring techniques can get rid of particular design flaws or principle violations, and restore the code fragment that present a smell, to an acceptable quality level. In the context of object-oriented systems, the concept of inheritance has been known as a key feature proposed to increase the amount of software reusability. However, using inheritance is not always the best solution, particularly if it is utilized in improper cases where other types of relationships would be more appropriate. One of the particular issues that violate inheritance principles is the Refused Bequest code smell. It is related to an inheritance hierarchy where a subclass does not obligate the interface inherited from its parent class. Some studies, mentioned in Section 2, had been made to detect the Refused Bequest smell. In this paper we present a new detection strategy by computing the similarity between common methods of the base class with the overridden methods of the sub-class, and then by calculating the average of these values for the given sub-class. That average value is defined as a new metric in our detection mechanism.",
      "Keywords": "code smells | object oriented metrics | refused bequest | similarity between methods | software quality",
      "Publication venue": "Proceedings - 9th International Conference on Computational Intelligence and Communication Networks, CICN 2017",
      "Publication date": "2017-07-02",
      "Publication type": "Conference Paper",
      "Authors": "Merzah, Baydaa M.;Selcuk, Yunus E.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85047197753",
      "Primary study DOI": "10.1109/IBCAST.2018.8312264",
      "Title": "Probing into code analysis tools: A comparison of C# supporting static code analyzers",
      "Abstract": "Essence of a software system lies in the quality of source code and the degree to which it follows the underlying coding standard. The widely-used code analysis techniques focus upon examining the programs without actually executing them. The major purpose is the detection of complex code constructs and the potential defects that result in the decrement in quality of the code. The code complexity is often assumed to inflate the maintenance cost and lead to the unexpected system behaviour. A number of code analyzing tools are available and currently many researches are being conducted to improve software quality, bring down software complexity without affecting its external behaviour. A brief review of the existing code analyzers is presented in this paper; however the major focus is upon the tools that analyze the source code written in C#. Researchers round the globe have identified that the potential problems in the source code are: code smells, code clones, anti-detect pattern etc that often lead to the increase in system complexity and hence amplification in the system response time. The code analyzing tools have their own significance depending upon the domain where they are being applied. Code analyzers are of extreme value especially in the mission critical systems, where the system efficiency is one of the factors for mission success. The methodology adopted for the purpose of review of the existing tools is that first of all, the purpose of tool is described, secondly, the personal findings, along with reviews from the clients and technology analysts regarding the tool are taken into account, thirdly, a sample C# application is chosen and the outputs from the tools for this sample are taken for analysis and comparison. Upon the basis of analysis, reviews and comparison, the relatively best tool among the 8 analyzed tools has been figured out.",
      "Keywords": "JPL standards | Programming languages | Software Quality Assurance | Software Testing | Static analysis",
      "Publication venue": "Proceedings of 2018 15th International Bhurban Conference on Applied Sciences and Technology, IBCAST 2018",
      "Publication date": "2018-03-09",
      "Publication type": "Conference Paper",
      "Authors": "Shaukat, Rida;Shahoor, Arooba;Urooj, Aniqa",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85026811632",
      "Primary study DOI": "10.1016/j.aej.2017.07.006",
      "Title": "Model level code smell detection using EGAPSO based on similarity measures",
      "Abstract": "Software maintenance is an essential part of any software that finds its use in the day-to-day activities of any organization. During the maintenance phase bugs detected must be corrected and the software must evolve with respect to changing requirements without ripple effects. Software maintenance is a cumbersome process if code smells exist in the software. The impact of poor design is code smells. In code smells detection, majority of the existing approaches are rule based, where a rule represents the combination of metrics and threshold. In rule based approach, defining the rules that detect the code smells are time consuming because identifying the correct threshold value is a tedious task, which can be fixed only through trial and error method. To address this issue, in this work Euclidean distance based Genetic Algorithm and Particle Swarm Optimization (EGAPSO) is used. Instead of relying on threshold value, this approach detects all code smells based on similarity between the system under study and the set of defect examples, where the former is the initial model and the latter is the base example. The approach is tested on the open source projects, namely Gantt Project and Log4j for identifying the five code smells namely Blob, Functional Decomposition, Spaghetti Code, Data Class and Feature Envy. Finally, the approach is compared with code smell detection using Genetic Algorithm (GA), DEtection and CORrection (DECOR), Parallel Evolutionary Algorithm (PEA) and Multi-Objective Genetic Programming (MOGP). The result of EGAPSO proves to be effective when compared to other code smell detection approaches.",
      "Keywords": "Code smell | Euclidean distance | Open source software | Search based software engineering | Software maintenance | Software metrics",
      "Publication venue": "Alexandria Engineering Journal",
      "Publication date": "2018-09-01",
      "Publication type": "Review",
      "Authors": "Saranya, G.;Khanna Nehemiah, H.;Kannan, A.;Nithya, V.",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85010815578",
      "Primary study DOI": "10.3923/ajit.2016.3205.3216",
      "Title": "Detecting shotgun surgery bad smell using similarity measure distribution model",
      "Abstract": "Bad smells are the symptoms of code decay which leads to the severe maintenance problem. Shotgun surgery is a smell where a change in a class may cause many small changes to other different classes. There are several approaches that identify bad smells based upon the definition of rules and change history information. These rules are the combination of software metrics and threshold values which sometimes not able to detect the code decay such as shotgun surgery. Since, it is difficult to find the best threshold value for rule based detection and also finding the best combination of metrics from the historical information seems to be difficult. To detect the shotgun surgery, the co-change should be feasible. In that case, the need for having sufficient history of observable co-changes, without which the approach of change history is not possible. Therefore these techniques cannot report the accurate instance of smells to detect the shotgun surgery bad smell. So as an alternate, in this study, a framework similarity measure distribution model for detecting shotgun surgery bad smell for object oriented program without the need for change history information is proposed. The framework is experimented on HSQLDB, TYRANT, XERCES-J and JFREE CHART open source software's and compared to the bad smell detection tools namely, in Fusion and iPlasma in terms of precision and recall. From the results it is inferred that the shotgun surgery can be detected more accurately using this proposed approach. The proposed framework improves the maintainability by detecting the bad smell shotgun surgery.",
      "Keywords": "Bad smell detection | Distribution model | Frequency histogram | Similarity measure | Software maintenance | Sturge's rule",
      "Publication venue": "Asian Journal of Information Technology",
      "Publication date": "2016-01-01",
      "Publication type": "Article",
      "Authors": "Saranya, G.;Khanna Nehemiah, H.;Kannan, A.;Vimala, S.",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85072534589",
      "Primary study DOI": "10.22059/JITM.2019.274968.2335",
      "Title": "Investigating the role of code smells in preventive maintenance",
      "Abstract": "The quest for improving the software quality has given rise to various studies which focus on the enhancement of the quality of software through various processes. Code smells, which are indicators of the software quality have not been put to an extensive study for as to determine their role in the prediction of defects in the software. This study aims to investigate the role of code smells in prediction of non-faulty classes. We examine the Eclipse software with four versions (3.2, 3.3, 3.6, and 3.7) for metrics and smells. Further, different code smells, derived subjectively through iPlasma, are taken into conjugation and three efficient, but subjective models are developed to detect code smells on each of Random Forest, J48 and SVM machine learning algorithms. This model is then used to detect the absence of defects in the four Eclipse versions. The effect of balanced and unbalanced datasets is also examined for these four versions. The results suggest that the code smells can be a valuable feature in discriminating absence of defects in a software.",
      "Keywords": "Code smells | Machine learning | Preventive maintenance | Random forest",
      "Publication venue": "Journal of Information Technology Management",
      "Publication date": "2018-01-01",
      "Publication type": "Article",
      "Authors": "Reshi, Junaid Ali;Singh, Satwinder",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85049189336",
      "Primary study DOI": "10.17013/risti.26.54-67",
      "Title": "Characterization and automatic detection of bad smells MVC [CaracterizaciÃ³n y detecciÃ³n automÃ¡tica de bad smells MVC]",
      "Abstract": "Bad smells are a frequent cause of technical debt, which denotes the cost of adopting a quick and dirty design or development approach. There are works on characterizing bad smells as well as on detecting and fixing them automatically. However, few of these works characterize, detect and fix architectural bad smells. The work presented in this article represents an initial effort to fill this by contributing to: (i) the characterization of bad smells tha are relevant to the MVC architecture style, and (ii) the automatic detection of these using static analysis of software techniques. The obtained results show that most of the defined bad smells exist in practice and that the proposed detection method reduces by a wide margin the detection time required by a code review.",
      "Keywords": "Bad Bad smells | MVC | Software Architecture | Static Analysis | Yii",
      "Publication venue": "RISTI - Revista Iberica de Sistemas e Tecnologias de Informacao",
      "Publication date": "2018-01-01",
      "Publication type": "Article",
      "Authors": "Velasco-Elizondo, Perla;Castañeda-Calvillo, Lucero;García-Fernández, Alejandro;Vazquez-Reyes, Sodel",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85053705671",
      "Primary study DOI": "",
      "Title": "Eclipse plugin for automatic detection of code smells in source codes [Kaynak kodlardaki kÃ¶tÃ¼ kokularÄ±n otomatik tespiti iÃ§in eclipse eklenti Ã¶nerisi]",
      "Abstract": "Code smells in source codes are code fragments; that do not prevent the functionality of the developed application, but which reduce code quality, make code maintenance and understandability difficult and require refactoring. Those types of smells could be found in a class as a whole or in a specific method of a class. Detecting those code smells by manual reviewing is a process that could increase the probability of unintentional omission in terms of the requirement of time, budget, and manpower as the project grows. Code smells can be caused by errors in the design phase as well as by the developer's preferences in the design to code conversion phase. In this article, we will introduce an Eclipse plugin that enables automatic detection of code smells in Java source code and presents the detected code smells to developers and maintainers. In this way, the software developers and maintainers can continuously evaluate the quality of the software with realistic values, recognize and refactor the modules that could cause a bug. This provides better quality, eas¬ ier maintainability and effective testability in software. The developed plugin is tested on the data sets used in fault estimation, and statistical correlation between software fault and code smells is presented. Accord¬ ing to results, existence of code smells is unrelated with the software faults. However, existing faults are statistically related with code smells.",
      "Keywords": "",
      "Publication venue": "",
      "Publication date": "",
      "Publication type": "",
      "Authors": "AltÄ±ntaÅ, M. and Sezer, E.A.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85053410385",
      "Primary study DOI": "10.1504/IJBIC.2018.094624",
      "Title": "Hybrid particle swarm optimisation with mutation for code smell detection",
      "Abstract": "Code smells are characterised as the structural defects in the software which indicate a poor software design and in turn makes the software hard to maintain. However, detecting and fixing the code smell in the software is a time consuming process, and it is difficult to fix manually. In this paper, an algorithm named as hybrid particle swarm optimisation with mutation (HPSOM) is used for identification of code smell by automatic generation of rules which represent the combination of metrics and threshold. Moreover, an empirical evaluation to compare HPSOM with other evolutionary approaches such as the parallel evolutionary algorithm (PEA), genetic algorithm (GA), genetic programming (GP) and particle swarm optimisation (PSO) to detect the code smell is done. The analysis shows that the HPSOM algorithm performs better than other approaches when applied on nine open source projects, namely, JfreeChart, GanttProject, ApacheAnt 5.2, ApacheAnt 7.0, Nutch, Log4J, Lucene, Xerces-J and Rhino. HPSOM approach has achieved precision of 94% and recall of 92% on five different types of code smells namely, blob, data class, spaghetti code, functional decomposition and feature envy.",
      "Keywords": "Code smell | Cohesion | Coupling | Evolutionary algorithms | Hybrid particle swarm optimisation | Open source software | Particle swarm optimisation | PSO | Search-based software engineering | Software maintenance | Software metrics",
      "Publication venue": "International Journal of Bio-Inspired Computation",
      "Publication date": "2018-01-01",
      "Publication type": "Article",
      "Authors": "Saranya, G.;Nehemiah, H. Khanna;Kannan, A.",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85082344132",
      "Primary study DOI": "",
      "Title": "Design of testing framework for code smell detection (OOPS) using BFO algorithm",
      "Abstract": "Detection of bad smells refers to any indication in the program code of a execution that perhaps designate a issue, maintain the software and software evolution. Code Smell detection is a main challenging for software developers and their informal classification direct to the designing of various smell detection methods and software tools. It appraises 4 code smell detection tool in software like as a in Fusion, JDeodorant, PMD and Jspirit. In this research proposes a method for detection the bad code smells in software is called as code smell. Bad smell detection in software, OOSMs are used to identify the Source Code whereby Plug-in were implemented for code detection in which position of program initial code the bad smell appeared so that software refactoring can then acquire position. Classified the code smell, as a type of codes: long method, PIH, LPL, LC, SS and GOD class etc. Detection of the code smell and as a result applying the correct detection phases when require is significant to enhance the Quality of the code or program. The various tool has been proposed for detection of the code smell each one featured by particular properties. The main objective of this research work described our proposed method on using various tools for code smell detection. We find the major differences between them and dissimilar consequences we attained. The major drawback of current research work is that it focuses on one particular language which makes them restricted to one kind of programs only. These tools fail to detect the smelly code if any kind of change in environment is encountered. The base paper compares the most popular code smell detection tools on basis of various factors like accuracy, False Positive Rate etc. which gives a clear picture of functionality these tools possess. In this paper, a unique technique is designed to identify CSs. For this purpose, various object-oriented programming (OOPs)-based-metrics with their maintainability index are used. Further, code refactoring and optimization technique are applied to obtain low maintainability Index. Finally, the proposed scheme is evaluated to achieve satisfactory results. The results of the BFOA test defined that the lazy class caused framework defects in DLS, DR, and SE. However, the LPL caused no frame-work defects what so ever. The consequences of the connection rules test searched that the LCCS (Lazy Class Code Smell) caused structured defects in DE and DLS, which corresponded to the consequences of the BFOA test. In this research work, a proposed method is designed to verify the code smell. For this purpose, different OOPs based Software Metrics with their MI (Maintainability Index) are utilized. Further Code refactoring and optimization method id applied to attained the less maintainability index and evaluated to achieved satisfactory results.",
      "Keywords": "BFOA method | Code smell detection | God class and lazy class | Software metrics",
      "Publication venue": "International Journal of Engineering and Technology(UAE)",
      "Publication date": "2018-01-01",
      "Publication type": "Article",
      "Authors": "Sharma, Pratiksha;Kaur, Er Arshpreet",
      "Ground truth": "Include",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85057852491",
      "Primary study DOI": "10.1109/TSE.2018.2883603",
      "Title": "Beyond Technical Aspects: How Do Community Smells Influence the Intensity of Code Smells?",
      "Abstract": "Code smells are poor implementation choices applied by developers during software evolution that often lead to critical flaws or failure. Much in the same way, community smells reflect the presence of organizational and socio-Technical issues within a software community that may lead to additional project costs. Recent empirical studies provide evidence that community smells are often-if not always-connected to circumstances such as code smells. In this paper we look deeper into this connection by conducting a mixed-methods empirical study of 117 releases from 9 open-source systems. The qualitative and quantitative sides of our mixed-methods study were run in parallel and assume a mutually-confirmative connotation. On the one hand, we survey 162 developers of the 9 considered systems to investigate whether developers perceive relationship between community smells and the code smells found in those projects. On the other hand, we perform a fine-grained analysis into the 117 releases of our dataset to measure the extent to which community smells impact code smell intensity (i.e., criticality). We then propose a code smell intensity prediction model that relies on both technical and community-related aspects. The results of both sides of our mixed-methods study lead to one conclusion: community-related factors contribute to the intensity of code smells. This conclusion supports the joint use of community and code smells detection as a mechanism for the joint management of technical and social problems around software development communities.",
      "Keywords": "Code smells | community smells | mixed-methods study | organizational structure",
      "Publication venue": "IEEE Transactions on Software Engineering",
      "Publication date": "2021-01-01",
      "Publication type": "Article",
      "Authors": "Palomba, Fabio;Andrew Tamburri, Damian;Arcelli Fontana, Francesca;Oliveto, Rocco;Zaidman, Andy;Serebrenik, Alexander",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85056883561",
      "Primary study DOI": "10.18293/SEKE2018-081",
      "Title": "A preliminary investigation of self-Admitted refactorings in open source software",
      "Abstract": "In software development, developers commit code changes to the version control system. In a commit message, the committer may explicitly claim that the commit is a refactoring with the intention of code quality improvement. We defined such a commit as a self-Admitted refactoring (SAR). Currently, there is little knowledge about the SAR phenomenon, and the impact of SARs on software projects is not clear. In this work, we performed a preliminary investigation on SARs with an emphasis on their impact on code quality using the assessment of code smells. We used two non-Trivial open source software projects as cases and employed the PMD tool to detect code smells. The study results shows that: (1) SARs tend to improve code quality, though a small proportion of SARs introduced new code smells; and (2) projects that contain SARs have different results on frequently affected code smells.",
      "Keywords": "Case study | Code quality | Code smell | Self-Admitted refactoring",
      "Publication venue": "Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE",
      "Publication date": "2018-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Zhang, Di;Li, Bing;Li, Zengyang;Liang, Peng",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85048976185",
      "Primary study DOI": "10.1007/978-3-319-93375-7_15",
      "Title": "An approach for semantically-enriched recommendation of refactorings based on the incidence of code smells",
      "Abstract": "Code smells are symptoms of bad decisions on the design and development of software. The occurrence of code smells in software can lead to costly consequences. Refactorings are considered adequate resources when it comes to reducing or removing the undesirable effects of smells in software. Ontologies and semantics can play a substantial role in reducing the interpretation burden of software engineers as they have to decide about adequate refactorings to mitigate the impact of smells. However, related work has given little attention to associating the recommendation of refactorings with the use of ontologies and semantics. Developers can benefit from the combination of code smells detection with a semantically-oriented approach for recommendation of refactorings. To make this possible, we expand the application of our previous ontology, ONTOlogy for Code smEll ANalysis (ONTOCEAN), to combine it with a new one, Ontology for SOftware REfactoring (OSORE). We also introduce a new tool, our REfactoring REcommender SYStem (RESYS) which is capable of binding our two ontologies. As a result, refactorings are automatically chosen and semantically linked to their respective code smells. We also conducted a preliminary evaluation of our approach in a real usage scenario with four open-source software projects.",
      "Keywords": "Code smells | Ontology | Recommendation | Refactoring | Semantic",
      "Publication venue": "Lecture Notes in Business Information Processing",
      "Publication date": "2018-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Carvalho, Luis Paulo da Silva;Novais, Renato Lima;Salvador, Laís do Nascimento;Neto, Manoel Gomes de Mendonça",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85054358477",
      "Primary study DOI": "",
      "Title": "Impact of code smells on the rate of defects in software: A literature review",
      "Abstract": "A consequence of the rush in software development is that there are often suboptimal design decisions that, in some aspects, deviate from best practice in software development. In the software development community, there is a general belief that such suboptimal design decisions, which can be identified in source code as code smells, negatively impact software quality, e.g. the rate of identified bugs in post-release software. Consequently, the presence of code smells in a software project could be a predictor of upcoming issues related to low quality software. The methods and approaches applied to control and eventually remedy source code affected by code smells, constitute an important research field in software engineering. The literature review presented in this paper aims to answer the question of whether the correlation between the presence of code smells in source code and a higher incidence of defects in software has been sufficiently analyzed in previous studies. The goal of the literature review is also to determine, which defined code smells were analyzed in studies and where the impact of studied code smells on low quality was confirmed. Furthermore, with the literature review we want to find out how effective the refactoring of smelly classes from the perspective of quality of software can be.",
      "Keywords": "",
      "Publication venue": "CEUR Workshop Proceedings",
      "Publication date": "2018-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Gradišnik, Mitja;HERIčKO, Marjan",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85049085330",
      "Primary study DOI": "10.1016/j.procs.2018.05.080",
      "Title": "Code Clones: Detection and Management",
      "Abstract": "In a software system, similar or identical fragments of code are known as code clones. Instead of implementing a new code from scratch, most of the developers prefer copy-paste programming in which they use existing code fragments. So, the primary reason behind code cloning isboth developers and programming languages used by them. Reusing existing software for increasing software productivity is a key element of object oriented programming which makes clone detection and management a primary concern for current industry. As a software system grows, it becomes more complex which leads to difficulty in maintaining it. The main reason behind difficulty in software maintenance is code clones which do not lead to conclusion that code clones are only harmful for software development. Code clones can be both advantageous and disastrous for software development. Therefore, clones should be analysed before refactoring or removing them. For analysing the clones properly, there is a need to study all the clone detection techniques, various types of clones and techniques to manage them.The main purpose of this paper is to gain insight in to the research available in the area of clone detection and management and identify the research gaps to work upon. It will help the researchers to get started easily with clones as they can study the basic concepts, techniques, general steps for code clone detection and management and research gaps together at one place. Also, it will help in the selection of appropriate techniques for detecting and managing clones as comparative analysis of different techniques on the basis of various parameters is also given in the paper.",
      "Keywords": "Bad Smell | Bug Detection | Clone Detection | Clone Lifecycle | Clone Management | Clone Refactoring | Software Maintenance",
      "Publication venue": "Procedia Computer Science",
      "Publication date": "2018-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Saini, Neha;Singh, Sukhdip;Suman, ",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85053393119",
      "Primary study DOI": "10.1145/3191314",
      "Title": "Assessing the refactoring of brain methods",
      "Abstract": "Code smells are a popular mechanism for identifying structural design problems in software systems. Several tools have emerged to support the detection of code smells and propose some refactorings. However, existing tools do not guarantee that a smell will be automatically fixed by means of refactorings. This article presents Bandago, an automated approach to fix a specific type of code smell called Brain Method. A Brain Method centralizes the intelligence of a class and manifests itself as a long and complex method that is difficult to understand and maintain by developers. For each Brain Method, Bandago recommends several refactoring solutions to remove the smell using a search strategy based on simulated annealing. Our approach has been evaluated with several open-source Java applications, and the results show that Bandago can automatically fix more than 60% of Brain Methods. Furthermore, we conducted a survey with 35 industrial developers that showed evidence about the usefulness of the refactorings proposed by Bandago. Also, we compared the performance of the Bandago against that of a third-party refactoring tool.",
      "Keywords": "",
      "Publication venue": "ACM Transactions on Software Engineering and Methodology",
      "Publication date": "2018-01-01",
      "Publication type": "Article",
      "Authors": "Vidal, Santiago;Berra, Iñaki;Zulliani, Santiago;Marcos, Claudia;Andrés Díaz Pace, J.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85054970762",
      "Primary study DOI": "10.3966/160792642018091905031",
      "Title": "Enhancing software robustness by detecting and removing exception handling smells: An empirical study",
      "Abstract": "We propose a systematic way to uncover and fix bugs through detecting smells associated with exception handling. First, code of software under improvement is scanned for exception handling smells by a static analysis tool. The smells are reviewed for confirming if they are bugs by writing failing tests. Finally, code that contains the smells is refactored until the failing test passes and the smells are removed. We have also conducted an empirical study to demonstrate the efficacy of the proposed approach. In the empirical study, an open source static analysis tool is applied to detect exception handling smells in an open source web application. The result shows that out of the 357 smells reported by the tool, 124 are confirmed to be bugs that could affect the robustness of the web application.",
      "Keywords": "Code smells | Exception handling | Refactoring | Robustness | Software testing",
      "Publication venue": "Journal of Internet Technology",
      "Publication date": "2018-01-01",
      "Publication type": "Article",
      "Authors": "Hsieh, Chin Yun;Chen, You Lun;Liao, Zhen Jie",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85057887721",
      "Primary study DOI": "10.1109/ACCESS.2018.2883769",
      "Title": "Dynamic Ranking of Refactoring Menu Items for Integrated Development Environment",
      "Abstract": "Software refactoring is popular and thus most mainstream IDEs, e.g., Eclipse, provide a top level menu, especially for refactoring activities. The refactoring menu is designed to facilitate refactorings, and it has become one of the most commonly used menus. However, to support a large number of refactoring types, the refactoring menu contains a long list of menu items. As a result, it is tedious to select the intended menu item from the lengthy menu. To facilitate the menu selection, in this paper, we propose an approach to dynamic ranking of refactoring menu items for IDE. We put the most likely refactoring menu item on the top of the refactoring menu according to developers' source code selection and code smells associated with the selected source code. The ranking is dynamic because it changes frequently according to the context. First, we collect the refactoring history of the open source applications and detect the code smells. Based on the refactoring history, we design questionnaires and analyze the responses from developers to discover the source code selection patterns for different refactoring types. Subsequently, we analyze the relationship between code smells associated with the refactoring software entities and the corresponding refactoring types. Finally, based on the preceding analysis, we calculate the likelihood of different refactoring types to be applied when a specific part of source code is selected, and rank the menu items according to the resulting likelihood. We conduct a case study to evaluate the proposed approach. Evaluation results suggest that the proposed approach is accurate, and in most cases (95.69%), it can put the intended refactoring menu item on the top of the menu.",
      "Keywords": "IDE | menu ranking | Software development | software refactoring",
      "Publication venue": "IEEE Access",
      "Publication date": "2018-01-01",
      "Publication type": "Article",
      "Authors": "Oo, Thida;Liu, Hui;Nyirongo, Bridget",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85047741452",
      "Primary study DOI": "10.5220/0006701704200431",
      "Title": "The smell of processing",
      "Abstract": "Most novice programmers write code that contains design smells which indicates that they are not understanding and applying important design concepts. This is especially true for students in degrees where programming, and by extension software design, is only a small part of the curriculum. This paper studies design smells in PROCESSING a language for new media and visual arts derived from Java. Language features - as well as common practices in the PROCESSING community - lead to language specific design smells. This paper defines design smells for PROCESSING, informed by a manual analysis of student code and community code. The paper describes how to detect these smells with static analysis. This serves two purposes, first to standardize design requirements, and second to assist educators with giving quality feedback. To validate its effectiveness we apply the tool to student code, community code, and code examples used by textbooks and instructors. This analysis also gives a good sense of common design problems in PROCESSING, their prevalence in novice code, and the quality of resources that students use for reference.",
      "Keywords": "Code smells | Design smells | Programming education | Software design | Software smells",
      "Publication venue": "CSEDU 2018 - Proceedings of the 10th International Conference on Computer Supported Education",
      "Publication date": "2018-01-01",
      "Publication type": "Conference Paper",
      "Authors": "De Man, Remco;Fehnker, Ansgar",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85052534465",
      "Primary study DOI": "",
      "Title": "ENASE 2018 - Proceedings of the 13th International Conference on Evaluation of Novel Approaches to Software Engineering",
      "Abstract": "The proceedings contain 36 papers. The topics discussed include: an action research study towards the use of cloud computing scenarios in undergraduate computer science courses; problem-based elicitation of security requirements - the ProCOR method; incremental bidirectional transformations: applying QVT relations to the families to persons benchmark; a framework to support behavioral design pattern detection from software execution data; mapping of periodic tasks in reconfigurable heterogeneous multi-core platforms; refactoring object-oriented applications for a deployment in the cloud - workflow generation based on static analysis of source code; using COSMIC FSM method to analyze the impact of functional changes in business process models; a hybrid approach to detect code smells using deep learning; cultural influences on requirements engineering process in the context of Saudi Arabia; verification of feature coordination using the fluent calculus; iterative process for generating ER diagram from unrestricted requirements; an approach to prioritize classes in a multi-objective software maintenance framework; model-aware software engineering - a knowledge-based approach to model-driven software engineering; comprehensive view on architectural requirements for maintenance information systems; design and implementation of a Geis for the genomic diagnosis using the SILE methodology. case study: congenital cataract; and a novel formal approach to automatically suggest metrics in software measurement plans.",
      "Keywords": "",
      "Publication venue": "ENASE 2018 - Proceedings of the 13th International Conference on Evaluation of Novel Approaches to Software Engineering",
      "Publication date": "2018-01-01",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85048985450",
      "Primary study DOI": "",
      "Title": "19th International Conference on Enterprise Information Systems, ICEIS 2017",
      "Abstract": "The proceedings contain 28 papers. The special focus in this conference is on Enterprise Information Systems. The topics include: Task-oriented requirements engineering for personal decision support systems; feature model as a design-pattern-based service contract for the service provider in the service oriented architecture; A method and programming model for developing interacting cloud applications based on the TOSCA standard; security requirements and tests for smart toys; an approach for semantically-enriched recommendation of refactorings based on the incidence of code smells; a scoring method based on criteria matching for cloud computing provider ranking and selection; describing scenarios and architectures for time-aware recommender systems for learning; towards generating spam queries for retrieving spam accounts in large-scale twitter data; statistical methods for use in analysis of trust-skyline sets; using a time-based weighting criterion to enhance link prediction in social networks; enabling semantics in enterprises; behavioral economics through the lens of persuasion context analysis: A review of contributions in leading information systems journals; youMake: A low-cost, easy for prototyping, didactic and generic platform for acquisition and conditioning of biomedical signals; a human-centered approach for interactive data processing and analytics; understanding governance mechanisms and health in software ecosystems: A systematic literature review; exploring the ambidextrous analysis of business processes: A design science research; toward an understanding of the tradeoffs of adopting the mean web server stack; recognition of business process elements in natural language texts; an interface prototype proposal to a semiautomatic process model verification method based on process modeling guidelines.",
      "Keywords": "",
      "Publication venue": "Lecture Notes in Business Information Processing",
      "Publication date": "2018-01-01",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85046485781",
      "Primary study DOI": "10.1109/CLEI.2017.8226440",
      "Title": "A visualization tool to detect refactoring opportunities in SOA applications",
      "Abstract": "Service-oriented computing (SOC) has been widely used by software industry for building distributed software applications that can be run in heterogeneous environments. It has also been required that these applications should be both high-quality and adaptable to market changes. However, a major problem in this type of applications is its growth; as the size and complexity of applications increase, the probability of duplicity of code increases. This problem could have a negative impact on quality attributes, such as performance, maintenance and evolution, among others. This paper presents a web tool called VizSOC to assist software developers in detecting refactoring opportunities in service-oriented applications. The tool receives WSDL (Web Service Description Language) documents, detects anti-patterns and suggests how to resolve them, and delivers a list of refactoring suggestions to start working on the refactoring process. To visualize the results in an orderly and comprehensible way, we use the Hierarchical Edge Bundles (HEB) visualization technique. The experimentation of the tool has been supported using two real-life case-studies, where we measured the amount of anti-patterns detected and the performance of clustering algorithms by using internal validity criteria. The results indicate that VizSOC is an effective aid to detect refactoring opportunities, and also allows developers to reduce effort along the detection process.",
      "Keywords": "Service understability | Service-oriented applications | Software visualization | Unsupervised machine learning | Web service description language | Web services",
      "Publication venue": "2017 43rd Latin American Computer Conference, CLEI 2017",
      "Publication date": "2017-12-18",
      "Publication type": "Conference Paper",
      "Authors": "Rodriguez, Guillermo;Teyseyre, Alfredo;Soria, Álvaro;Berdun, Luis",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85046538027",
      "Primary study DOI": "10.1109/I2CT.2017.8226297",
      "Title": "Evolution of code smells over multiple versions of softwares: An empirical investigation",
      "Abstract": "The symptoms which reflect the poor design quality of code are known as code smells. Refactoring is one of the possible ways to remove code smells, but refactoring does not come for free to developer. So there is a need to have efficient refactoring strategies. For this purpose an empirical study on distribution of different code smells over different versions of projects is provided in this paper, so that refactoring strategies can be built keeping in a view that which smell is more effective and at which time during evolution of software. For experiment, different versions of Junit, GCviewer and Gitblit have been taken. To detect the smells and violations in code JDeodorant and PMD tools are used. Study shows that a) Latest version of software has more design issues than that of oldest ones; b) God smell and urgent violations have more contribution than other smells whereas instances having Type Checking smell are very less in all the versions. Study shows that initial version of project can be used as reference architecture for purpose of reverse Engineering. We also found that Gitblit has better code design than that of other two softwares.",
      "Keywords": "Code smell | Empirical study | Object oriented maintainence | Refactoring",
      "Publication venue": "2017 2nd International Conference for Convergence in Technology, I2CT 2017",
      "Publication date": "2017-12-18",
      "Publication type": "Conference Paper",
      "Authors": "Rani, Anshul;Chhabra, Jitender Kumar",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85051667588",
      "Primary study DOI": "10.1145/3121264.3121266",
      "Title": "Android apps and user feedback: A dataset for software evolution and quality improvement",
      "Abstract": "Nowadays, Android represents the most popular mobile platform with a market share of around 80%. Previous research showed that data contained in user reviews and code change history of mobile apps represent a rich source of information for reducing software maintenance and development effort, increasing customers' satisfaction. Stemming from this observation, we present in this paper a large dataset of Android applications belonging to 23 different apps categories, which provides an overview of the types of feedback users report on the apps and documents the evolution of the related code metrics. The dataset contains about 395 applications of the F-Droid repository, including around 600 versions, 280,000 user reviews and more than 450,000 user feedback (extracted with specific text mining approaches). Furthermore, for each app version in our dataset, we employed the Paprika tool and developed several Python scripts to detect 8 different code smells and compute 22 code quality indicators. The paper discusses the potential usefulness of the dataset for future research in the field.",
      "Keywords": "App reviews | Mobile applications | Software maintenance and evolution | Software quality",
      "Publication venue": "WAMA 2017 - Proceedings of the 2nd ACM SIGSOFT International Workshop on App Market Analytics, Co-located with FSE 2017",
      "Publication date": "2017-09-05",
      "Publication type": "Conference Paper",
      "Authors": "Grano, Giovanni;Di Sorbo, Andrea;Mercaldo, Francesco;Visaggio, Corrado A.;Canfora, Gerardo;Panichella, Sebastiano",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85044452678",
      "Primary study DOI": "10.1109/ETFA.2017.8247574",
      "Title": "Exploring code clones in programmable logic controller software",
      "Abstract": "The reuse of code fragments by copying and pasting is widely practiced in software development and results in code clones. Cloning is considered an anti-pattern as it negatively affects program correctness and increases maintenance efforts. Programmable Logic Controller (PLC) software is no exception in the code clone discussion as reuse in development and maintenance is frequently achieved through copy, paste, and modification. Even though the presence of code clones may not necessary be a problem per se, it is important to detect, track and manage clones as the software system evolves. Unfortunately, tool support for clone detection and management is not commonly available for PLC software systems or limited to generic tools with a reduced set of features. In this paper, we investigate code clones in a real-world PLC software system based on IEC 61131-3 Structured Text and C/C++. We extended a widely used tool for clone detection with normalization support. Furthermore, we evaluated the different types and natures of code clones in the studied system and their relevance for refactoring. Results shed light on the applicability and usefulness of clone detection in the context of industrial automation systems and it demonstrates the benefit of adapting detection and management tools for IEC 611313-3 languages.",
      "Keywords": "",
      "Publication venue": "IEEE International Conference on Emerging Technologies and Factory Automation, ETFA",
      "Publication date": "2017-06-28",
      "Publication type": "Conference Paper",
      "Authors": "Thaller, Hannes;Ramler, Rudolf;Pichler, Josef;Egyed, Alexander",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85030248172",
      "Primary study DOI": "10.1145/3021460.3021474",
      "Title": "Impact of gamification on code review process - An experimental study",
      "Abstract": "Researchers have supported the idea of gamification to enhance students’ interest in activities like code reviews, change management, knowledge management, issue tracking, etc. which might otherwise be repetitive and monotonous. We performed an experimental study consisting of nearly 180+ participants to measure the impact of gamification on code review process using 5 different code review tools, including one gamified code review instance from our extensible architectural framework. We assess the impact of gamification based on the code smells and bugs identified in a gamified and non-gamified environment as per code inspection report. Further, measurement and comparison of the quantity and usefulness of code review comments was done using machine learning techniques.",
      "Keywords": "Architectural framework | Classification | Code reviews | Evaluation | Gamification | Text analysis",
      "Publication venue": "ACM International Conference Proceeding Series",
      "Publication date": "2017-02-05",
      "Publication type": "Conference Paper",
      "Authors": "Khandelwal, Shivam;Sripada, Sai Krishna;Raghu Reddy, Y.",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85045441326",
      "Primary study DOI": "",
      "Title": "Test refactoring: A research agenda",
      "Abstract": "Research on software testing generally fo-cusses on the eectiveness of test suites to detect bugs. The quality of the test code in terms of maintainability remains mostly ignored. However, just like production code, test code can suer from code smells that imply refactoring opportunities. In this paper, we will summerize the state-of-the-art in the field of test refactoring. We will show that there is a gap in the tool support, and propose future work which will aim to fill this gap.",
      "Keywords": "",
      "Publication venue": "CEUR Workshop Proceedings",
      "Publication date": "2017-01-01",
      "Publication type": "Conference Paper",
      "Authors": "Van Bladel, Brent;Demeyer, Serge",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85041550518",
      "Primary study DOI": "10.1145/2975945.2975951",
      "Title": "Automated translation among EPSILON languages for performance-driven UML software model refactoring",
      "Abstract": "Although performance represents a crucial non-functional attribute of software, few model-based approaches have been introduced up today for reducing the gap between performance analysis results (e.g., mean response time) and the feedback expected by software engineers when performance problems are detected (i.e., refactoring actions). However, existing approaches aimed at performance-driven refactoring of software models suffer from fragmentation across different paradigms, languages, and meta-models. This paper aims at reducing such fragmentation by exploiting the EPSILON environment, which provides a suite of languages for checking properties and applying refactoring on models. In particular, we introduce automation aimed at translating performance antipattern detection rules and refactoring actions among three EPSILON languages. Such automation helps to reduce code writing effort, in the context of performance-driven refactoring of UML models, while exploiting the specific support provided by the different execution semantics of considered languages.",
      "Keywords": "EPSILON platform | Model refactoring | Performance antipatterns | Software performance engineering | UML",
      "Publication venue": "IWoR 2016 - Proceedings of the 1st International Workshop on Software Refactoring, co-located with ASE 2016",
      "Publication date": "2016-09-04",
      "Publication type": "Conference Paper",
      "Authors": "Arcelli, Davide;Cortellessa, Vittorio;Di Pompeo, Daniele",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85022041237",
      "Primary study DOI": "10.1109/AICCSA.2016.7945776",
      "Title": "AntiPattren-based cloud ontology evaluation",
      "Abstract": "Nowadays, cloud computing is an emerging technology thanks to its ability to provide on-demand computing services (hardware and software) with less description standardization effort. Multiple issues and challenges in discovering cloud services appear due to the lack of the cloud service description standardization. In fact, the existing cloud providers describe, their similar offered services in different ways. Thus, various existing works aim at standardizing the representation of cloud computing services while proposing ontologies. However, since the existing proposals were not evaluated, they might be less adopted and considered. Indeed, the ontology evaluation has a direct impact on its understandability and reusability. In this paper, we propose an evaluation approach to validate our proposed Cloud Service Ontology (CSO), to guarantee an adequate cloud service discovery. This paper contribution is threefold. First, it specifies a set of patterns and anti-patterns in order to evaluate CSO. Second, it defines an anti-pattern detection method based on SPARQL queries which provides a set of correction recommendations to help ontologists revise the ontology. Finally, some experiment tests were conducted in relation to: (i) the method efficiency and (ii) anti-pattern detection of design anomalies as well as taxonomic and domain errors within CSO.",
      "Keywords": "",
      "Publication venue": "Proceedings of IEEE/ACS International Conference on Computer Systems and Applications, AICCSA",
      "Publication date": "2016-07-02",
      "Publication type": "Conference Paper",
      "Authors": "Loukil, Faiza;Rekik, Molka;Boukadi, Khouloud",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-84877304489",
      "Primary study DOI": "",
      "Title": "Proceedings of the 17th European Conference on Software Maintenance and Reengineering, CSMR 2013",
      "Abstract": "The proceedings contain 63 papers. The topics discussed include: understanding widespread changes: a taxonomic study; on the relationship between program evolution and fault-proneness: an empirical study; an exploratory study of cloning in industrial software product lines; predicting project outcome leveraging socio-technical network patterns; leveraging crowd knowledge for software comprehension and development; finding duplicates of your yet unwritten bug report; analyzing networks of issue reports; empirical evaluation of bug linking; change-based test selection in the presence of developer tests; an empirical analysis of bug reports and bug fixing in open source android apps; feature detection in Ajax-enabled web applications; study on the relation between antipatterns and the cost of class unit testing; and enhancing the detection of code anomalies with architecture-sensitive strategies.",
      "Keywords": "",
      "Publication venue": "Proceedings of the European Conference on Software Maintenance and Reengineering, CSMR",
      "Publication date": "2013-05-13",
      "Publication type": "Conference Review",
      "Authors": "",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85087225798",
      "Primary study DOI": "10.1145/1083142.1083155",
      "Title": "Improving evolvability through refactoring",
      "Abstract": "Refactoring is one means of improving the structure of existing software. Locations for the application of refactoring are often based on subjective perceptions such as \"bad smells\", which are vague suspicions of design shortcomings. We exploit historical data extracted from repositories such as CVS and focus on change couplings: if some software parts change at the same time very often over several releases, this data can be used to point to candidates for refactoring. We adopt the concept of bad smells and provide additional change smells. Such a smell is hardly visible in the code, but easy to spot when viewing the change history. Our approach enables the detection of such smells allowing an engineer to apply refactoring on these parts of the source code to improve the evolvability of the software. For that, we analyzed the history of a large industrial system for a period of 15 months, proposed spots for refactorings based on change couplings, and performed them with the developers. After observing the system for another 15 months we finally analyzed the effectiveness of our approach. Our results support our hypothesis that the combination of change dependency analysis and refactoring is applicable and effective.",
      "Keywords": "Change smells | Refactoring | Software evolution",
      "Publication venue": "Proceedings of the 2005 International Workshop on Mining Software Repositories, MSR 2005",
      "Publication date": "2005-05-17",
      "Publication type": "Conference Paper",
      "Authors": "Ratzinger, Jacek;Fischer, Michael;Gall, Harald",
      "Ground truth": "Exclude",
      "Reason": ""
    },
    {
      "Secondary study DOI": "10.1016/j.infsof.2021.106783",
      "Primary study Scopus EID": "2-s2.0-85074923411",
      "Primary study DOI": "10.1109/IEMECONX.2019.8877008",
      "Title": "An empirical framework for web service anti-pattern prediction using machine learning techniques",
      "Abstract": "In todays software industries, the concepts of Web Services are applied to design and develop distributed software system. These distributed software system can be designed and developed by integrating different Web Services provided by different parties. Similar to other software systems, Web Services based system also suffers from bad or poor design i.e., bad design selection, anti-pattern, poor planning etc.. Early prediction of anti-patterns can help developer and tester in fixing design issue and also effectively utilize the resources. The work in this paper empirically investigates and evaluates six classification techniques, 8 feature selection techniques (7 feature ranking techniques and 1 feature subset evaluation technique), and 1 data sampling technique to handle imbalance data in predicting 5 different types of anti-patterns. These all techniques are validated on 226 real-world web-services across several domains. The performance of the developed models using these techniques are evaluated using AUC value. Our analysis reveals that the model developed using these techniques able to predict different anti-patterns using source code metrics. Our analysis also reveals that the best feature selection technique is OneR, data sample is better that without sampling and Random Forest is best classification algorithm for anti-pattern predictions.",
      "Keywords": "Aggregation Measures | Anti-Patterns | Feature selection | Machine Learning | Software Engineering | Source Code Metrics | Web Services",
      "Publication venue": "IEMECON 2019 - 9th Annual Information Technology, Electromechanical Engineering and Microelectronics Conference",
      "Publication date": "2019-03-01",
      "Publication type": "Conference Paper",
      "Authors": "Tummalapalli, Sahithi;Kumar, Lov;Neti, Lalita Bhanu Murthy",
      "Ground truth": "Include",
      "Reason": ""
    }
  ]
}